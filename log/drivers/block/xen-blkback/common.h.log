commit cb9369bdbb84440e9a8fded7eb2d0f94014f9a8f
Author: SeongJae Park <sjpark@amazon.de>
Date:   Mon Jan 27 09:18:10 2020 +0100

    xen/blkback: Squeeze page pools if a memory pressure is detected
    
    Each `blkif` has a free pages pool for the grant mapping.  The size of
    the pool starts from zero and is increased on demand while processing
    the I/O requests.  If current I/O requests handling is finished or 100
    milliseconds has passed since last I/O requests handling, it checks and
    shrinks the pool to not exceed the size limit, `max_buffer_pages`.
    
    Therefore, host administrators can cause memory pressure in blkback by
    attaching a large number of block devices and inducing I/O.  Such
    problematic situations can be avoided by limiting the maximum number of
    devices that can be attached, but finding the optimal limit is not so
    easy.  Improper set of the limit can results in memory pressure or a
    resource underutilization.  This commit avoids such problematic
    situations by squeezing the pools (returns every free page in the pool
    to the system) for a while (users can set this duration via a module
    parameter) if memory pressure is detected.
    
    Discussions
    ===========
    
    The `blkback`'s original shrinking mechanism returns only pages in the
    pool which are not currently be used by `blkback` to the system.  In
    other words, the pages that are not mapped with granted pages.  Because
    this commit is changing only the shrink limit but still uses the same
    freeing mechanism it does not touch pages which are currently mapping
    grants.
    
    Once memory pressure is detected, this commit keeps the squeezing limit
    for a user-specified time duration.  The duration should be neither too
    long nor too short.  If it is too long, the squeezing incurring overhead
    can reduce the I/O performance.  If it is too short, `blkback` will not
    free enough pages to reduce the memory pressure.  This commit sets the
    value as `10 milliseconds` by default because it is a short time in
    terms of I/O while it is a long time in terms of memory operations.
    Also, as the original shrinking mechanism works for at least every 100
    milliseconds, this could be a somewhat reasonable choice.  I also tested
    other durations (refer to the below section for more details) and
    confirmed that 10 milliseconds is the one that works best with the test.
    That said, the proper duration depends on actual configurations and
    workloads.  That's why this commit allows users to set the duration as a
    module parameter.
    
    Memory Pressure Test
    ====================
    
    To show how this commit fixes the memory pressure situation well, I
    configured a test environment on a xen-running virtualization system.
    On the `blkfront` running guest instances, I attach a large number of
    network-backed volume devices and induce I/O to those.  Meanwhile, I
    measure the number of pages that swapped in (pswpin) and out (pswpout)
    on the `blkback` running guest.  The test ran twice, once for the
    `blkback` before this commit and once for that after this commit.  As
    shown below, this commit has dramatically reduced the memory pressure:
    
                    pswpin  pswpout
        before      76,672  185,799
        after          867    3,967
    
    Optimal Aggressive Shrinking Duration
    -------------------------------------
    
    To find a best squeezing duration, I repeated the test with three
    different durations (1ms, 10ms, and 100ms).  The results are as below:
    
        duration    pswpin  pswpout
        1           707     5,095
        10          867     3,967
        100         362     3,348
    
    As expected, the memory pressure decreases as the duration increases,
    but the reduction become slow from the `10ms`.  Based on this results, I
    chose the default duration as 10ms.
    
    Performance Overhead Test
    =========================
    
    This commit could incur I/O performance degradation under severe memory
    pressure because the squeezing will require more page allocations per
    I/O.  To show the overhead, I artificially made a worst-case squeezing
    situation and measured the I/O performance of a `blkfront` running
    guest.
    
    For the artificial squeezing, I set the `blkback.max_buffer_pages` using
    the `/sys/module/xen_blkback/parameters/max_buffer_pages` file.  In this
    test, I set the value to `1024` and `0`.  The `1024` is the default
    value.  Setting the value as `0` is same to a situation doing the
    squeezing always (worst-case).
    
    If the underlying block device is slow enough, the squeezing overhead
    could be hidden.  For the reason, I use a fast block device, namely the
    rbd[1]:
    
        # xl block-attach guest phy:/dev/ram0 xvdb w
    
    For the I/O performance measurement, I run a simple `dd` command 5 times
    directly to the device as below and collect the 'MB/s' results.
    
        $ for i in {1..5}; do dd if=/dev/zero of=/dev/xvdb \
                                 bs=4k count=$((256*512)); sync; done
    
    The results are as below.  'max_pgs' represents the value of the
    `blkback.max_buffer_pages` parameter.
    
        max_pgs   Min       Max       Median     Avg    Stddev
        0         417       423       420        419.4  2.5099801
        1024      414       425       416        417.8  4.4384682
        No difference proven at 95.0% confidence
    
    In short, even worst case squeezing on ramdisk based fast block device
    makes no visible performance degradation.  Please note that this is just
    a very simple and minimal test.  On systems using super-fast block
    devices and a special I/O workload, the results might be different.  If
    you have any doubt, test on your machine with your workload to find the
    optimal squeezing duration for you.
    
    [1] https://www.kernel.org/doc/html/latest/admin-guide/blockdev/ramdisk.html
    
    Reviewed-by: Roger Pau Monné <roger.pau@citrix.com>
    Signed-off-by: SeongJae Park <sjpark@amazon.de>
    Signed-off-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>

diff --git a/drivers/block/xen-blkback/common.h b/drivers/block/xen-blkback/common.h
index 49132b0adbbe..a3eeccf3ac5f 100644
--- a/drivers/block/xen-blkback/common.h
+++ b/drivers/block/xen-blkback/common.h
@@ -319,6 +319,7 @@ struct xen_blkif {
 	/* All rings for this device. */
 	struct xen_blkif_ring	*rings;
 	unsigned int		nr_rings;
+	unsigned long		buffer_squeeze_end;
 };
 
 struct seg_buf {

commit 14855954f63608c5622d5eaa964d3872ce5c5514
Author: Paul Durrant <pdurrant@amazon.com>
Date:   Mon Dec 2 11:41:17 2019 +0000

    xen-blkback: allow module to be cleanly unloaded
    
    Add a module_exit() to perform the necessary clean-up.
    
    Signed-off-by: Paul Durrant <pdurrant@amazon.com>
    Reviewed-by: "Roger Pau Monné" <roger.pau@citrix.com>
    Reviewed-by: Juergen Gross <jgross@suse.com>
    Signed-off-by: Juergen Gross <jgross@suse.com>

diff --git a/drivers/block/xen-blkback/common.h b/drivers/block/xen-blkback/common.h
index 1d3002d773f7..49132b0adbbe 100644
--- a/drivers/block/xen-blkback/common.h
+++ b/drivers/block/xen-blkback/common.h
@@ -375,9 +375,12 @@ struct phys_req {
 	struct block_device	*bdev;
 	blkif_sector_t		sector_number;
 };
+
 int xen_blkif_interface_init(void);
+void xen_blkif_interface_fini(void);
 
 int xen_blkif_xenbus_init(void);
+void xen_blkif_xenbus_fini(void);
 
 irqreturn_t xen_blkif_be_int(int irq, void *dev_id);
 int xen_blkif_schedule(void *arg);

commit 6f2f39ad1a54978394851c05e327419ebeb7227e
Author: Juergen Gross <jgross@suse.com>
Date:   Mon Aug 13 16:01:14 2018 +0200

    xen/blkback: remove unused pers_gnts_lock from struct xen_blkif_ring
    
    pers_gnts_lock isn't being used anywhere. Remove it.
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Reviewed-by: Roger Pau Monné <roger.pau@citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/drivers/block/xen-blkback/common.h b/drivers/block/xen-blkback/common.h
index 2339b8d39c5e..1d3002d773f7 100644
--- a/drivers/block/xen-blkback/common.h
+++ b/drivers/block/xen-blkback/common.h
@@ -269,7 +269,6 @@ struct xen_blkif_ring {
 	wait_queue_head_t	pending_free_wq;
 
 	/* Tree to store persistent grants. */
-	spinlock_t		pers_gnts_lock;
 	struct rb_root		persistent_gnts;
 	unsigned int		persistent_gnt_c;
 	atomic_t		persistent_gnt_in_use;

commit d77ff24e7fa2258877fa0b87efa06b9a58a37aab
Author: Juergen Gross <jgross@suse.com>
Date:   Mon Aug 13 16:01:13 2018 +0200

    xen/blkback: move persistent grants flags to bool
    
    The struct persistent_gnt flags member is meant to be a bitfield of
    different flags. There is only PERSISTENT_GNT_ACTIVE flag left, so
    convert it to a bool named "active".
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Reviewed-by: Roger Pau Monné <roger.pau@citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/drivers/block/xen-blkback/common.h b/drivers/block/xen-blkback/common.h
index 7bff72db3b7e..2339b8d39c5e 100644
--- a/drivers/block/xen-blkback/common.h
+++ b/drivers/block/xen-blkback/common.h
@@ -233,11 +233,6 @@ struct xen_vbd {
 
 struct backend_info;
 
-/* Number of available flags */
-#define PERSISTENT_GNT_FLAGS_SIZE	1
-/* This persistent grant is currently in use */
-#define PERSISTENT_GNT_ACTIVE		0
-
 /* Number of requests that we can fit in a ring */
 #define XEN_BLKIF_REQS_PER_PAGE		32
 
@@ -246,7 +241,7 @@ struct persistent_gnt {
 	grant_ref_t gnt;
 	grant_handle_t handle;
 	unsigned long last_used;
-	DECLARE_BITMAP(flags, PERSISTENT_GNT_FLAGS_SIZE);
+	bool active;
 	struct rb_node node;
 	struct list_head remove_node;
 };

commit 973e5405f2f67ddbb2bf07b3ffc71908a37fea8e
Author: Juergen Gross <jgross@suse.com>
Date:   Mon Aug 13 16:01:10 2018 +0200

    xen/blkback: don't keep persistent grants too long
    
    Persistent grants are allocated until a threshold per ring is being
    reached. Those grants won't be freed until the ring is being destroyed
    meaning there will be resources kept busy which might no longer be
    used.
    
    Instead of freeing only persistent grants until the threshold is
    reached add a timestamp and remove all persistent grants not having
    been in use for a minute.
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Reviewed-by: Roger Pau Monné <roger.pau@citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/drivers/block/xen-blkback/common.h b/drivers/block/xen-blkback/common.h
index ecb35fe8ca8d..7bff72db3b7e 100644
--- a/drivers/block/xen-blkback/common.h
+++ b/drivers/block/xen-blkback/common.h
@@ -234,14 +234,9 @@ struct xen_vbd {
 struct backend_info;
 
 /* Number of available flags */
-#define PERSISTENT_GNT_FLAGS_SIZE	2
+#define PERSISTENT_GNT_FLAGS_SIZE	1
 /* This persistent grant is currently in use */
 #define PERSISTENT_GNT_ACTIVE		0
-/*
- * This persistent grant has been used, this flag is set when we remove the
- * PERSISTENT_GNT_ACTIVE, to know that this grant has been used recently.
- */
-#define PERSISTENT_GNT_WAS_ACTIVE	1
 
 /* Number of requests that we can fit in a ring */
 #define XEN_BLKIF_REQS_PER_PAGE		32
@@ -250,6 +245,7 @@ struct persistent_gnt {
 	struct page *page;
 	grant_ref_t gnt;
 	grant_handle_t handle;
+	unsigned long last_used;
 	DECLARE_BITMAP(flags, PERSISTENT_GNT_FLAGS_SIZE);
 	struct rb_node node;
 	struct list_head remove_node;

commit 089bc0143f489bd3a4578bdff5f4ca68fb26f341
Author: Jan Beulich <jbeulich@suse.com>
Date:   Tue Jun 13 16:28:27 2017 -0400

    xen-blkback: don't leak stack data via response ring
    
    Rather than constructing a local structure instance on the stack, fill
    the fields directly on the shared ring, just like other backends do.
    Build on the fact that all response structure flavors are actually
    identical (the old code did make this assumption too).
    
    This is XSA-216.
    
    Cc: stable@vger.kernel.org
    
    Signed-off-by: Jan Beulich <jbeulich@suse.com>
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/drivers/block/xen-blkback/common.h b/drivers/block/xen-blkback/common.h
index 638597b17a38..ecb35fe8ca8d 100644
--- a/drivers/block/xen-blkback/common.h
+++ b/drivers/block/xen-blkback/common.h
@@ -75,9 +75,8 @@ extern unsigned int xenblk_max_queues;
 struct blkif_common_request {
 	char dummy;
 };
-struct blkif_common_response {
-	char dummy;
-};
+
+/* i386 protocol version */
 
 struct blkif_x86_32_request_rw {
 	uint8_t        nr_segments;  /* number of segments                   */
@@ -129,14 +128,6 @@ struct blkif_x86_32_request {
 	} u;
 } __attribute__((__packed__));
 
-/* i386 protocol version */
-#pragma pack(push, 4)
-struct blkif_x86_32_response {
-	uint64_t        id;              /* copied from request */
-	uint8_t         operation;       /* copied from request */
-	int16_t         status;          /* BLKIF_RSP_???       */
-};
-#pragma pack(pop)
 /* x86_64 protocol version */
 
 struct blkif_x86_64_request_rw {
@@ -193,18 +184,12 @@ struct blkif_x86_64_request {
 	} u;
 } __attribute__((__packed__));
 
-struct blkif_x86_64_response {
-	uint64_t       __attribute__((__aligned__(8))) id;
-	uint8_t         operation;       /* copied from request */
-	int16_t         status;          /* BLKIF_RSP_???       */
-};
-
 DEFINE_RING_TYPES(blkif_common, struct blkif_common_request,
-		  struct blkif_common_response);
+		  struct blkif_response);
 DEFINE_RING_TYPES(blkif_x86_32, struct blkif_x86_32_request,
-		  struct blkif_x86_32_response);
+		  struct blkif_response __packed);
 DEFINE_RING_TYPES(blkif_x86_64, struct blkif_x86_64_request,
-		  struct blkif_x86_64_response);
+		  struct blkif_response);
 
 union blkif_back_rings {
 	struct blkif_back_ring        native;

commit 46464411307746e6297a034a9983a22c9dfc5a0c
Author: Juergen Gross <jgross@suse.com>
Date:   Thu May 18 17:28:47 2017 +0200

    xen/blkback: fix disconnect while I/Os in flight
    
    Today disconnecting xen-blkback is broken in case there are still
    I/Os in flight: xen_blkif_disconnect() will bail out early without
    releasing all resources in the hope it will be called again when
    the last request has terminated. This, however, won't happen as
    xen_blkif_free() won't be called on termination of the last running
    request: xen_blkif_put() won't decrement the blkif refcnt to 0 as
    xen_blkif_disconnect() didn't finish before thus some xen_blkif_put()
    calls in xen_blkif_disconnect() didn't happen.
    
    To solve this deadlock xen_blkif_disconnect() and
    xen_blkif_alloc_rings() shouldn't use xen_blkif_put() and
    xen_blkif_get() but use some other way to do their accounting of
    resources.
    
    This at once fixes another error in xen_blkif_disconnect(): when it
    returned early with -EBUSY for another ring than 0 it would call
    xen_blkif_put() again for already handled rings on a subsequent call.
    This will lead to inconsistencies in the refcnt handling.
    
    Cc: stable@vger.kernel.org
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Tested-by: Steven Haigh <netwiz@crc.id.au>
    Acked-by: Roger Pau Monné <roger.pau@citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/drivers/block/xen-blkback/common.h b/drivers/block/xen-blkback/common.h
index dea61f6ab8cb..638597b17a38 100644
--- a/drivers/block/xen-blkback/common.h
+++ b/drivers/block/xen-blkback/common.h
@@ -281,6 +281,7 @@ struct xen_blkif_ring {
 
 	wait_queue_head_t	wq;
 	atomic_t		inflight;
+	bool			active;
 	/* One thread per blkif ring. */
 	struct task_struct	*xenblkd;
 	unsigned int		waiting_reqs;

commit 641203549a21ba6a701aecd05c3dfc969ec670cc
Merge: 404a47410c26 e93d12ae3be9
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Jan 21 18:19:38 2016 -0800

    Merge branch 'for-4.5/drivers' of git://git.kernel.dk/linux-block
    
    Pull block driver updates from Jens Axboe:
     "This is the block driver pull request for 4.5, with the exception of
      NVMe, which is in a separate branch and will be posted after this one.
    
      This pull request contains:
    
       - A set of bcache stability fixes, which have been acked by Kent.
         These have been used and tested for more than a year by the
         community, so it's about time that they got in.
    
       - A set of drbd updates from the drbd team (Andreas, Lars, Philipp)
         and Markus Elfring, Oleg Drokin.
    
       - A set of fixes for xen blkback/front from the usual suspects, (Bob,
         Konrad) as well as community based fixes from Kiri, Julien, and
         Peng.
    
       - A 2038 time fix for sx8 from Shraddha, with a fix from me.
    
       - A small mtip32xx cleanup from Zhu Yanjun.
    
       - A null_blk division fix from Arnd"
    
    * 'for-4.5/drivers' of git://git.kernel.dk/linux-block: (71 commits)
      null_blk: use sector_div instead of do_div
      mtip32xx: restrict variables visible in current code module
      xen/blkfront: Fix crash if backend doesn't follow the right states.
      xen/blkback: Fix two memory leaks.
      xen/blkback: make st_ statistics per ring
      xen/blkfront: Handle non-indirect grant with 64KB pages
      xen-blkfront: Introduce blkif_ring_get_request
      xen-blkback: clear PF_NOFREEZE for xen_blkif_schedule()
      xen/blkback: Free resources if connect_ring failed.
      xen/blocks: Return -EXX instead of -1
      xen/blkback: make pool of persistent grants and free pages per-queue
      xen/blkback: get the number of hardware queues/rings from blkfront
      xen/blkback: pseudo support for multi hardware queues/rings
      xen/blkback: separate ring information out of struct xen_blkif
      xen/blkfront: correct setting for xen_blkif_max_ring_order
      xen/blkfront: make persistent grants pool per-queue
      xen/blkfront: Remove duplicate setting of ->xbdev.
      xen/blkfront: Cleanup of comments, fix unaligned variables, and syntax errors.
      xen/blkfront: negotiate number of queues/rings to be used with backend
      xen/blkfront: split per device io_lock
      ...

commit db6fbc106786f26d95889c50c18b1f28aa543a17
Author: Bob Liu <bob.liu@oracle.com>
Date:   Wed Dec 9 07:44:02 2015 +0800

    xen/blkback: make st_ statistics per ring
    
    Make st_* statistics per ring and the VBD sysfs would iterate over all the
    rings.
    
    Note: xenvbd_sysfs_delif() is called in xen_blkbk_remove() before all rings
    are torn down, so it's safe.
    
    Signed-off-by: Bob Liu <bob.liu@oracle.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    ---
    v2: Aligned the variables on the same column.

diff --git a/drivers/block/xen-blkback/common.h b/drivers/block/xen-blkback/common.h
index 3c244ecf22a4..b27c5ba15600 100644
--- a/drivers/block/xen-blkback/common.h
+++ b/drivers/block/xen-blkback/common.h
@@ -298,6 +298,16 @@ struct xen_blkif_ring {
 	atomic_t		persistent_gnt_in_use;
 	unsigned long           next_lru;
 
+	/* Statistics. */
+	unsigned long		st_print;
+	unsigned long long	st_rd_req;
+	unsigned long long	st_wr_req;
+	unsigned long long	st_oo_req;
+	unsigned long long	st_f_req;
+	unsigned long long	st_ds_req;
+	unsigned long long	st_rd_sect;
+	unsigned long long	st_wr_sect;
+
 	/* Used by the kworker that offload work from the persistent purge. */
 	struct list_head	persistent_purge_list;
 	struct work_struct	persistent_purge_work;
@@ -328,16 +338,6 @@ struct xen_blkif {
 	struct completion	drain_complete;
 	atomic_t		drain;
 
-	/* statistics */
-	unsigned long		st_print;
-	unsigned long long			st_rd_req;
-	unsigned long long			st_wr_req;
-	unsigned long long			st_oo_req;
-	unsigned long long			st_f_req;
-	unsigned long long			st_ds_req;
-	unsigned long long			st_rd_sect;
-	unsigned long long			st_wr_sect;
-
 	struct work_struct	free_work;
 	unsigned int 		nr_ring_pages;
 	/* All rings for this device. */

commit d4bf0065b7251afb723a29b2fd58f7c38f8ce297
Author: Bob Liu <bob.liu@oracle.com>
Date:   Sat Nov 14 11:12:19 2015 +0800

    xen/blkback: make pool of persistent grants and free pages per-queue
    
    Make pool of persistent grants and free pages per-queue/ring instead of
    per-device to get better scalability.
    
    Test was done based on null_blk driver:
    dom0: v4.2-rc8 16vcpus 10GB "modprobe null_blk"
    domu: v4.2-rc8 16vcpus 10GB
    
    [test]
    rw=read
    direct=1
    ioengine=libaio
    bs=4k
    time_based
    runtime=30
    filename=/dev/xvdb
    numjobs=16
    iodepth=64
    iodepth_batch=64
    iodepth_batch_complete=64
    group_reporting
    
    Results:
    iops1: After patch "xen/blkfront: make persistent grants per-queue".
    iops2: After this patch.
    
    Queues:                   1        4              8              16
    Iops orig(k):           810     1064            780             700
    Iops1(k):               810     1230(~20%)      1024(~20%)      850(~20%)
    Iops2(k):               810     1410(~35%)      1354(~75%)      1440(~100%)
    
    With 4 queues after this commit we can get ~75% increase in IOPS, and
    performance won't drop if increasing queue numbers.
    
    Please find the respective chart in this link:
    https://www.dropbox.com/s/agrcy2pbzbsvmwv/iops.png?dl=0
    
    Signed-off-by: Bob Liu <bob.liu@oracle.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/drivers/block/xen-blkback/common.h b/drivers/block/xen-blkback/common.h
index 847444dc1df4..3c244ecf22a4 100644
--- a/drivers/block/xen-blkback/common.h
+++ b/drivers/block/xen-blkback/common.h
@@ -291,6 +291,22 @@ struct xen_blkif_ring {
 	spinlock_t		pending_free_lock;
 	wait_queue_head_t	pending_free_wq;
 
+	/* Tree to store persistent grants. */
+	spinlock_t		pers_gnts_lock;
+	struct rb_root		persistent_gnts;
+	unsigned int		persistent_gnt_c;
+	atomic_t		persistent_gnt_in_use;
+	unsigned long           next_lru;
+
+	/* Used by the kworker that offload work from the persistent purge. */
+	struct list_head	persistent_purge_list;
+	struct work_struct	persistent_purge_work;
+
+	/* Buffer of free pages to map grant refs. */
+	spinlock_t		free_pages_lock;
+	int			free_pages_num;
+	struct list_head	free_pages;
+
 	struct work_struct	free_work;
 	/* Thread shutdown wait queue. */
 	wait_queue_head_t	shutdown_wq;
@@ -312,22 +328,6 @@ struct xen_blkif {
 	struct completion	drain_complete;
 	atomic_t		drain;
 
-	/* tree to store persistent grants */
-	spinlock_t		pers_gnts_lock;
-	struct rb_root		persistent_gnts;
-	unsigned int		persistent_gnt_c;
-	atomic_t		persistent_gnt_in_use;
-	unsigned long           next_lru;
-
-	/* used by the kworker that offload work from the persistent purge */
-	struct list_head	persistent_purge_list;
-	struct work_struct	persistent_purge_work;
-
-	/* buffer of free pages to map grant refs */
-	spinlock_t		free_pages_lock;
-	int			free_pages_num;
-	struct list_head	free_pages;
-
 	/* statistics */
 	unsigned long		st_print;
 	unsigned long long			st_rd_req;

commit d62d86000316d7ef38e1c2e9602c3ce6d1cb57bd
Author: Bob Liu <bob.liu@oracle.com>
Date:   Sat Nov 14 11:12:17 2015 +0800

    xen/blkback: get the number of hardware queues/rings from blkfront
    
    Backend advertises "multi-queue-max-queues" to front, also get the negotiated
    number from "multi-queue-num-queues" written by blkfront.
    
    Signed-off-by: Bob Liu <bob.liu@oracle.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/drivers/block/xen-blkback/common.h b/drivers/block/xen-blkback/common.h
index 310eff3cf43f..847444dc1df4 100644
--- a/drivers/block/xen-blkback/common.h
+++ b/drivers/block/xen-blkback/common.h
@@ -46,6 +46,7 @@
 #include <xen/interface/io/protocols.h>
 
 extern unsigned int xen_blkif_max_ring_order;
+extern unsigned int xenblk_max_queues;
 /*
  * This is the maximum number of segments that would be allowed in indirect
  * requests. This value will also be passed to the frontend.

commit 2fb1ef4f1226ea6d6d3481036cabe01a4415b68c
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Fri Dec 11 12:08:48 2015 -0500

    xen/blkback: pseudo support for multi hardware queues/rings
    
    Preparatory patch for multiple hardware queues (rings). The number of
    rings is unconditionally set to 1, larger number will be enabled in
    "xen/blkback: get the number of hardware queues/rings from blkfront".
    
    Signed-off-by: Arianna Avanzini <avanzini.arianna@gmail.com>
    Signed-off-by: Bob Liu <bob.liu@oracle.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    ---
    v2: Align variables in the structures.

diff --git a/drivers/block/xen-blkback/common.h b/drivers/block/xen-blkback/common.h
index dbdf4164c83f..310eff3cf43f 100644
--- a/drivers/block/xen-blkback/common.h
+++ b/drivers/block/xen-blkback/common.h
@@ -340,7 +340,8 @@ struct xen_blkif {
 	struct work_struct	free_work;
 	unsigned int 		nr_ring_pages;
 	/* All rings for this device. */
-	struct xen_blkif_ring 	ring;
+	struct xen_blkif_ring	*rings;
+	unsigned int		nr_rings;
 };
 
 struct seg_buf {

commit 597957000ab5b1b38085c20868f3f7b9c305bae5
Author: Bob Liu <bob.liu@oracle.com>
Date:   Sat Nov 14 11:12:15 2015 +0800

    xen/blkback: separate ring information out of struct xen_blkif
    
    Split per ring information to an new structure "xen_blkif_ring", so that one vbd
    device can be associated with one or more rings/hardware queues.
    
    Introduce 'pers_gnts_lock' to protect the pool of persistent grants since we
    may have multi backend threads.
    
    This patch is a preparation for supporting multi hardware queues/rings.
    
    Signed-off-by: Arianna Avanzini <avanzini.arianna@gmail.com>
    Signed-off-by: Bob Liu <bob.liu@oracle.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    ---
    v2: Align the variables in the structure.

diff --git a/drivers/block/xen-blkback/common.h b/drivers/block/xen-blkback/common.h
index 68e87a037b99..dbdf4164c83f 100644
--- a/drivers/block/xen-blkback/common.h
+++ b/drivers/block/xen-blkback/common.h
@@ -269,34 +269,50 @@ struct persistent_gnt {
 	struct list_head remove_node;
 };
 
+/* Per-ring information. */
+struct xen_blkif_ring {
+	/* Physical parameters of the comms window. */
+	unsigned int		irq;
+	union blkif_back_rings	blk_rings;
+	void			*blk_ring;
+	/* Private fields. */
+	spinlock_t		blk_ring_lock;
+
+	wait_queue_head_t	wq;
+	atomic_t		inflight;
+	/* One thread per blkif ring. */
+	struct task_struct	*xenblkd;
+	unsigned int		waiting_reqs;
+
+	/* List of all 'pending_req' available */
+	struct list_head	pending_free;
+	/* And its spinlock. */
+	spinlock_t		pending_free_lock;
+	wait_queue_head_t	pending_free_wq;
+
+	struct work_struct	free_work;
+	/* Thread shutdown wait queue. */
+	wait_queue_head_t	shutdown_wq;
+	struct xen_blkif 	*blkif;
+};
+
 struct xen_blkif {
 	/* Unique identifier for this interface. */
 	domid_t			domid;
 	unsigned int		handle;
-	/* Physical parameters of the comms window. */
-	unsigned int		irq;
 	/* Comms information. */
 	enum blkif_protocol	blk_protocol;
-	union blkif_back_rings	blk_rings;
-	void			*blk_ring;
 	/* The VBD attached to this interface. */
 	struct xen_vbd		vbd;
 	/* Back pointer to the backend_info. */
 	struct backend_info	*be;
-	/* Private fields. */
-	spinlock_t		blk_ring_lock;
 	atomic_t		refcnt;
-
-	wait_queue_head_t	wq;
 	/* for barrier (drain) requests */
 	struct completion	drain_complete;
 	atomic_t		drain;
-	atomic_t		inflight;
-	/* One thread per one blkif. */
-	struct task_struct	*xenblkd;
-	unsigned int		waiting_reqs;
 
 	/* tree to store persistent grants */
+	spinlock_t		pers_gnts_lock;
 	struct rb_root		persistent_gnts;
 	unsigned int		persistent_gnt_c;
 	atomic_t		persistent_gnt_in_use;
@@ -311,12 +327,6 @@ struct xen_blkif {
 	int			free_pages_num;
 	struct list_head	free_pages;
 
-	/* List of all 'pending_req' available */
-	struct list_head	pending_free;
-	/* And its spinlock. */
-	spinlock_t		pending_free_lock;
-	wait_queue_head_t	pending_free_wq;
-
 	/* statistics */
 	unsigned long		st_print;
 	unsigned long long			st_rd_req;
@@ -328,9 +338,9 @@ struct xen_blkif {
 	unsigned long long			st_wr_sect;
 
 	struct work_struct	free_work;
-	/* Thread shutdown wait queue. */
-	wait_queue_head_t	shutdown_wq;
-	unsigned int nr_ring_pages;
+	unsigned int 		nr_ring_pages;
+	/* All rings for this device. */
+	struct xen_blkif_ring 	ring;
 };
 
 struct seg_buf {
@@ -352,7 +362,7 @@ struct grant_page {
  * response queued for it, with the saved 'id' passed back.
  */
 struct pending_req {
-	struct xen_blkif	*blkif;
+	struct xen_blkif_ring   *ring;
 	u64			id;
 	int			nr_segs;
 	atomic_t		pendcnt;
@@ -394,7 +404,7 @@ int xen_blkif_xenbus_init(void);
 irqreturn_t xen_blkif_be_int(int irq, void *dev_id);
 int xen_blkif_schedule(void *arg);
 int xen_blkif_purge_persistent(void *arg);
-void xen_blkbk_free_caches(struct xen_blkif *blkif);
+void xen_blkbk_free_caches(struct xen_blkif_ring *ring);
 
 int xen_blkbk_flush_diskcache(struct xenbus_transaction xbt,
 			      struct backend_info *be, int state);

commit 1f13d75ccb806260079e0679d55d9253e370ec8a
Author: Roger Pau Monné <roger.pau@citrix.com>
Date:   Tue Nov 3 16:34:09 2015 +0000

    xen-blkback: only read request operation from shared ring once
    
    A compiler may load a switch statement value multiple times, which could
    be bad when the value is in memory shared with the frontend.
    
    When converting a non-native request to a native one, ensure that
    src->operation is only loaded once by using READ_ONCE().
    
    This is part of XSA155.
    
    CC: stable@vger.kernel.org
    Signed-off-by: Roger Pau Monné <roger.pau@citrix.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/drivers/block/xen-blkback/common.h b/drivers/block/xen-blkback/common.h
index 68e87a037b99..c929ae22764c 100644
--- a/drivers/block/xen-blkback/common.h
+++ b/drivers/block/xen-blkback/common.h
@@ -408,8 +408,8 @@ static inline void blkif_get_x86_32_req(struct blkif_request *dst,
 					struct blkif_x86_32_request *src)
 {
 	int i, n = BLKIF_MAX_SEGMENTS_PER_REQUEST, j;
-	dst->operation = src->operation;
-	switch (src->operation) {
+	dst->operation = READ_ONCE(src->operation);
+	switch (dst->operation) {
 	case BLKIF_OP_READ:
 	case BLKIF_OP_WRITE:
 	case BLKIF_OP_WRITE_BARRIER:
@@ -456,8 +456,8 @@ static inline void blkif_get_x86_64_req(struct blkif_request *dst,
 					struct blkif_x86_64_request *src)
 {
 	int i, n = BLKIF_MAX_SEGMENTS_PER_REQUEST, j;
-	dst->operation = src->operation;
-	switch (src->operation) {
+	dst->operation = READ_ONCE(src->operation);
+	switch (dst->operation) {
 	case BLKIF_OP_READ:
 	case BLKIF_OP_WRITE:
 	case BLKIF_OP_WRITE_BARRIER:

commit 67de5dfbc176ea86ab0278658b5d55f64207ff2d
Author: Julien Grall <julien.grall@citrix.com>
Date:   Tue May 5 16:25:56 2015 +0100

    block/xen-blkback: Make it running on 64KB page granularity
    
    The PV block protocol is using 4KB page granularity. The goal of this
    patch is to allow a Linux using 64KB page granularity behaving as a
    block backend on a non-modified Xen.
    
    It's only necessary to adapt the ring size and the number of request per
    indirect frames. The rest of the code is relying on the grant table
    code.
    
    Note that the grant table code is allocating a Linux page per grant
    which will result to waste 6OKB for every grant when Linux is using 64KB
    page granularity. This could be improved by sharing the page between
    multiple grants.
    
    Signed-off-by: Julien Grall <julien.grall@citrix.com>
    Acked-by: "Roger Pau Monné" <roger.pau@citrix.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/drivers/block/xen-blkback/common.h b/drivers/block/xen-blkback/common.h
index 45a044a53d1e..68e87a037b99 100644
--- a/drivers/block/xen-blkback/common.h
+++ b/drivers/block/xen-blkback/common.h
@@ -39,6 +39,7 @@
 #include <asm/pgalloc.h>
 #include <asm/hypervisor.h>
 #include <xen/grant_table.h>
+#include <xen/page.h>
 #include <xen/xenbus.h>
 #include <xen/interface/io/ring.h>
 #include <xen/interface/io/blkif.h>
@@ -51,12 +52,20 @@ extern unsigned int xen_blkif_max_ring_order;
  */
 #define MAX_INDIRECT_SEGMENTS 256
 
-#define SEGS_PER_INDIRECT_FRAME \
-	(PAGE_SIZE/sizeof(struct blkif_request_segment))
+/*
+ * Xen use 4K pages. The guest may use different page size (4K or 64K)
+ * Number of Xen pages per segment
+ */
+#define XEN_PAGES_PER_SEGMENT   (PAGE_SIZE / XEN_PAGE_SIZE)
+
+#define XEN_PAGES_PER_INDIRECT_FRAME \
+	(XEN_PAGE_SIZE/sizeof(struct blkif_request_segment))
+#define SEGS_PER_INDIRECT_FRAME	\
+	(XEN_PAGES_PER_INDIRECT_FRAME / XEN_PAGES_PER_SEGMENT)
+
 #define MAX_INDIRECT_PAGES \
 	((MAX_INDIRECT_SEGMENTS + SEGS_PER_INDIRECT_FRAME - 1)/SEGS_PER_INDIRECT_FRAME)
-#define INDIRECT_PAGES(_segs) \
-	((_segs + SEGS_PER_INDIRECT_FRAME - 1)/SEGS_PER_INDIRECT_FRAME)
+#define INDIRECT_PAGES(_segs) DIV_ROUND_UP(_segs, XEN_PAGES_PER_INDIRECT_FRAME)
 
 /* Not a real protocol.  Used to generate ring structs which contain
  * the elements common to all protocols only.  This way we get a

commit 7adf12b87f45a77d364464018fb8e9e1ac875152
Merge: 02201e3f1b46 6684fa1cdb1e
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Jul 1 11:53:46 2015 -0700

    Merge tag 'for-linus-4.2-rc0-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip
    
    Pull xen updates from David Vrabel:
     "Xen features and cleanups for 4.2-rc0:
    
       - add "make xenconfig" to assist in generating configs for Xen guests
    
       - preparatory cleanups necessary for supporting 64 KiB pages in ARM
         guests
    
       - automatically use hvc0 as the default console in ARM guests"
    
    * tag 'for-linus-4.2-rc0-tag' of git://git.kernel.org/pub/scm/linux/kernel/git/xen/tip:
      block/xen-blkback: s/nr_pages/nr_segs/
      block/xen-blkfront: Remove invalid comment
      block/xen-blkfront: Remove unused macro MAXIMUM_OUTSTANDING_BLOCK_REQS
      arm/xen: Drop duplicate define mfn_to_virt
      xen/grant-table: Remove unused macro SPP
      xen/xenbus: client: Fix call of virt_to_mfn in xenbus_grant_ring
      xen: Include xen/page.h rather than asm/xen/page.h
      kconfig: add xenconfig defconfig helper
      kconfig: clarify kvmconfig is for kvm
      xen/pcifront: Remove usage of struct timeval
      xen/tmem: use BUILD_BUG_ON() in favor of BUG_ON()
      hvc_xen: avoid uninitialized variable warning
      xenbus: avoid uninitialized variable warning
      xen/arm: allow console=hvc0 to be omitted for guests
      arm,arm64/xen: move Xen initialization earlier
      arm/xen: Correctly check if the event channel interrupt is present

commit 6684fa1cdb1ebe804e9707f389255d461b2e95b0
Author: Julien Grall <julien.grall@linaro.org>
Date:   Wed Jun 17 15:28:08 2015 +0100

    block/xen-blkback: s/nr_pages/nr_segs/
    
    Make the code less confusing to read now that Linux may not have the
    same page size as Xen.
    
    Signed-off-by: Julien Grall <julien.grall@citrix.com>
    Acked-by: Roger Pau Monné <roger.pau@citrix.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/drivers/block/xen-blkback/common.h b/drivers/block/xen-blkback/common.h
index f620b5d3f77c..7a03e07f52f3 100644
--- a/drivers/block/xen-blkback/common.h
+++ b/drivers/block/xen-blkback/common.h
@@ -343,7 +343,7 @@ struct grant_page {
 struct pending_req {
 	struct xen_blkif	*blkif;
 	u64			id;
-	int			nr_pages;
+	int			nr_segs;
 	atomic_t		pendcnt;
 	unsigned short		operation;
 	int			status;

commit 86839c56dee28c315a4c19b7bfee450ccd84cd25
Author: Bob Liu <bob.liu@oracle.com>
Date:   Wed Jun 3 13:40:03 2015 +0800

    xen/block: add multi-page ring support
    
    Extend xen/block to support multi-page ring, so that more requests can be
    issued by using more than one pages as the request ring between blkfront
    and backend.
    As a result, the performance can get improved significantly.
    
    We got some impressive improvements on our highend iscsi storage cluster
    backend. If using 64 pages as the ring, the IOPS increased about 15 times
    for the throughput testing and above doubled for the latency testing.
    
    The reason was the limit on outstanding requests is 32 if use only one-page
    ring, but in our case the iscsi lun was spread across about 100 physical
    drives, 32 was really not enough to keep them busy.
    
    Changes in v2:
     - Rebased to 4.0-rc6.
     - Document on how multi-page ring feature working to linux io/blkif.h.
    
    Changes in v3:
     - Remove changes to linux io/blkif.h and follow the protocol defined
       in io/blkif.h of XEN tree.
     - Rebased to 4.1-rc3
    
    Changes in v4:
     - Turn to use 'ring-page-order' and 'max-ring-page-order'.
     - A few comments from Roger.
    
    Changes in v5:
     - Clarify with 4k granularity to comment
     - Address more comments from Roger
    
    Signed-off-by: Bob Liu <bob.liu@oracle.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/drivers/block/xen-blkback/common.h b/drivers/block/xen-blkback/common.h
index 043f13b7b7b0..8ccc49d01c8e 100644
--- a/drivers/block/xen-blkback/common.h
+++ b/drivers/block/xen-blkback/common.h
@@ -44,6 +44,7 @@
 #include <xen/interface/io/blkif.h>
 #include <xen/interface/io/protocols.h>
 
+extern unsigned int xen_blkif_max_ring_order;
 /*
  * This is the maximum number of segments that would be allowed in indirect
  * requests. This value will also be passed to the frontend.
@@ -320,6 +321,7 @@ struct xen_blkif {
 	struct work_struct	free_work;
 	/* Thread shutdown wait queue. */
 	wait_queue_head_t	shutdown_wq;
+	unsigned int nr_ring_pages;
 };
 
 struct seg_buf {

commit 69b91ede5cab843dcf345c28bd1f4b5a99dacd9b
Author: Bob Liu <bob.liu@oracle.com>
Date:   Wed Jun 3 13:40:01 2015 +0800

    drivers: xen-blkback: delay pending_req allocation to connect_ring
    
    This is a pre-patch for multi-page ring feature.
    In connect_ring, we can know exactly how many pages are used for the shared
    ring, delay pending_req allocation here so that we won't waste too much memory.
    
    Signed-off-by: Bob Liu <bob.liu@oracle.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/drivers/block/xen-blkback/common.h b/drivers/block/xen-blkback/common.h
index f620b5d3f77c..043f13b7b7b0 100644
--- a/drivers/block/xen-blkback/common.h
+++ b/drivers/block/xen-blkback/common.h
@@ -248,7 +248,7 @@ struct backend_info;
 #define PERSISTENT_GNT_WAS_ACTIVE	1
 
 /* Number of requests that we can fit in a ring */
-#define XEN_BLKIF_REQS			32
+#define XEN_BLKIF_REQS_PER_PAGE		32
 
 struct persistent_gnt {
 	struct page *page;

commit 77387b82d1b2bada25a7b566ab7716408fedc5e9
Author: Tao Chen <boby.chen@huawei.com>
Date:   Wed Apr 1 15:04:22 2015 +0000

    xen-blkback: define pr_fmt macro to avoid the duplication of DRV_PFX
    
    Define pr_fmt macro with {xen-blkback: } prefix, then remove all use
    of DRV_PFX in the pr sentences. Replace all DPRINTK with pr sentences,
    and get rid of DPRINTK macro. It will simplify the code.
    
    And if the pr sentences miss a \n, add it in the end. If the DPRINTK
    sentences have redundant \n, remove it. It will format the code.
    
    These all make the readability of the code become better.
    
    Signed-off-by: Tao Chen <boby.chen@huawei.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Acked-by: Roger Pau Monné <roger.pau@citrix.com>

diff --git a/drivers/block/xen-blkback/common.h b/drivers/block/xen-blkback/common.h
index 375d28851860..f620b5d3f77c 100644
--- a/drivers/block/xen-blkback/common.h
+++ b/drivers/block/xen-blkback/common.h
@@ -44,12 +44,6 @@
 #include <xen/interface/io/blkif.h>
 #include <xen/interface/io/protocols.h>
 
-#define DRV_PFX "xen-blkback:"
-#define DPRINTK(fmt, args...)				\
-	pr_debug(DRV_PFX "(%s:%d) " fmt ".\n",		\
-		 __func__, __LINE__, ##args)
-
-
 /*
  * This is the maximum number of segments that would be allowed in indirect
  * requests. This value will also be passed to the frontend.

commit 8494bcf5b7c4b2416687e233dd34d4c6b6fe5653
Merge: 3e12cefbe143 b042a3ca9490
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Thu Feb 12 14:30:53 2015 -0800

    Merge branch 'for-3.20/drivers' of git://git.kernel.dk/linux-block
    
    Pull block driver changes from Jens Axboe:
     "This contains:
    
       - The 4k/partition fixes for brd from Boaz/Matthew.
    
       - A few xen front/back block fixes from David Vrabel and Roger Pau
         Monne.
    
       - Floppy changes from Takashi, cleaning the device file creation.
    
       - Switching libata to use the new blk-mq tagging policy, removing
         code (and a suboptimal implementation) from libata.  This will
         throw you a merge conflict, since a bug in the original libata
         tagging code was fixed since this code was branched.  Trivial.
         From Shaohua.
    
       - Conversion of loop to blk-mq, from Ming Lei.
    
       - Cleanup of the io_schedule() handling in bsg from Peter Zijlstra.
         He claims it improves on unreadable code, which will cost him a
         beer.
    
       - Maintainer update or NDB, now handled by Markus Pargmann.
    
       - NVMe:
            - Optimization from me that avoids a kmalloc/kfree per IO for
              smaller (<= 8KB) IO. This cuts about 1% of high IOPS CPU
              overhead.
            - Removal of (now) dead RCU code, a relic from before NVMe was
              converted to blk-mq"
    
    * 'for-3.20/drivers' of git://git.kernel.dk/linux-block:
      xen-blkback: default to X86_32 ABI on x86
      xen-blkfront: fix accounting of reqs when migrating
      xen-blkback,xen-blkfront: add myself as maintainer
      block: Simplify bsg complete all
      floppy: Avoid manual call of device_create_file()
      NVMe: avoid kmalloc/kfree for smaller IO
      MAINTAINERS: Update NBD maintainer
      libata: make sata_sil24 use fifo tag allocator
      libata: move sas ata tag allocation to libata-scsi.c
      libata: use blk taging
      NVMe: within nvme_free_queues(), delete RCU sychro/deferred free
      null_blk: suppress invalid partition info
      brd: Request from fdisk 4k alignment
      brd: Fix all partitions BUGs
      axonram: Fix bug in direct_access
      loop: add blk-mq.h include
      block: loop: don't handle REQ_FUA explicitly
      block: loop: introduce lo_discard() and lo_req_flush()
      block: loop: say goodby to bio
      block: loop: improve performance via blk-mq

commit b042a3ca949053231950a1b15f31cccca9e305f3
Author: David Vrabel <david.vrabel@citrix.com>
Date:   Thu Feb 5 17:09:56 2015 +0000

    xen-blkback: default to X86_32 ABI on x86
    
    Prior to the existance of 64-bit backends using the X86_64 ABI,
    frontends used the X86_32 ABI.  These old frontends do not specify the
    ABI and when used with a 64-bit backend do not work.
    
    On x86, default to the X86_32 ABI if one is not specified.  Backends
    on ARM continue to default to their NATIVE ABI.
    
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>
    Acked-by: Roger Pau Monné <roger.pau@citrix.com>

diff --git a/drivers/block/xen-blkback/common.h b/drivers/block/xen-blkback/common.h
index f65b807e3236..78b0411e5ed5 100644
--- a/drivers/block/xen-blkback/common.h
+++ b/drivers/block/xen-blkback/common.h
@@ -214,6 +214,15 @@ enum blkif_protocol {
 	BLKIF_PROTOCOL_X86_64 = 3,
 };
 
+/*
+ * Default protocol if the frontend doesn't specify one.
+ */
+#ifdef CONFIG_X86
+#  define BLKIF_PROTOCOL_DEFAULT BLKIF_PROTOCOL_X86_32
+#else
+#  define BLKIF_PROTOCOL_DEFAULT BLKIF_PROTOCOL_NATIVE
+#endif
+
 struct xen_vbd {
 	/* What the domain refers to this vbd as. */
 	blkif_vdev_t		handle;

commit c43cf3ea838541ea9f066f4f1aa7b197cba6276e
Author: Jennifer Herbert <jennifer.herbert@citrix.com>
Date:   Mon Jan 5 16:49:22 2015 +0000

    xen-blkback: safely unmap grants in case they are still in use
    
    Use gnttab_unmap_refs_async() to wait until the mapped pages are no
    longer in use before unmapping them.
    
    This allows blkback to use network storage which may retain refs to
    pages in queued skbs after the block I/O has completed.
    
    Signed-off-by: Jennifer Herbert <jennifer.herbert@citrix.com>
    Acked-by: Roger Pau Monné <roger.pau@citrix.com>
    Acked-by: Jens Axboe <axboe@kernel.de>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/drivers/block/xen-blkback/common.h b/drivers/block/xen-blkback/common.h
index f65b807e3236..cc90a840e616 100644
--- a/drivers/block/xen-blkback/common.h
+++ b/drivers/block/xen-blkback/common.h
@@ -350,6 +350,9 @@ struct pending_req {
 	struct grant_page	*indirect_pages[MAX_INDIRECT_PAGES];
 	struct seg_buf		seg[MAX_INDIRECT_SEGMENTS];
 	struct bio		*biolist[MAX_INDIRECT_SEGMENTS];
+	struct gnttab_unmap_grant_ref unmap[MAX_INDIRECT_SEGMENTS];
+	struct page                   *unmap_pages[MAX_INDIRECT_SEGMENTS];
+	struct gntab_unmap_queue_data gnttab_unmap_data;
 };
 
 

commit 814d04e7dfc4a9cf7e36656afe2da5c0c08dde2b
Author: Valentin Priescu <priescuv@amazon.com>
Date:   Tue May 20 22:28:50 2014 +0200

    xen-blkback: defer freeing blkif to avoid blocking xenwatch
    
    Currently xenwatch blocks in VBD disconnect, waiting for all pending I/O
    requests to finish. If the VBD is attached to a hot-swappable disk, then
    xenwatch can hang for a long period of time, stalling other watches.
    
     INFO: task xenwatch:39 blocked for more than 120 seconds.
     "echo 0 > /proc/sys/kernel/hung_task_timeout_secs" disables this message.
     ffff880057f01bd0 0000000000000246 ffff880057f01ac0 ffffffff810b0782
     ffff880057f01ad0 00000000000131c0 0000000000000004 ffff880057edb040
     ffff8800344c6080 0000000000000000 ffff880058c00ba0 ffff880057edb040
     Call Trace:
     [<ffffffff810b0782>] ? irq_to_desc+0x12/0x20
     [<ffffffff8128f761>] ? list_del+0x11/0x40
     [<ffffffff8147a080>] ? wait_for_common+0x60/0x160
     [<ffffffff8147bcef>] ? _raw_spin_lock_irqsave+0x2f/0x50
     [<ffffffff8147bd49>] ? _raw_spin_unlock_irqrestore+0x19/0x20
     [<ffffffff8147a26a>] schedule+0x3a/0x60
     [<ffffffffa018fe6a>] xen_blkif_disconnect+0x8a/0x100 [xen_blkback]
     [<ffffffff81079f70>] ? wake_up_bit+0x40/0x40
     [<ffffffffa018ffce>] xen_blkbk_remove+0xae/0x1e0 [xen_blkback]
     [<ffffffff8130b254>] xenbus_dev_remove+0x44/0x90
     [<ffffffff81345cb7>] __device_release_driver+0x77/0xd0
     [<ffffffff81346488>] device_release_driver+0x28/0x40
     [<ffffffff813456e8>] bus_remove_device+0x78/0xe0
     [<ffffffff81342c9f>] device_del+0x12f/0x1a0
     [<ffffffff81342d2d>] device_unregister+0x1d/0x60
     [<ffffffffa0190826>] frontend_changed+0xa6/0x4d0 [xen_blkback]
     [<ffffffffa019c252>] ? frontend_changed+0x192/0x650 [xen_netback]
     [<ffffffff8130ae50>] ? cmp_dev+0x60/0x60
     [<ffffffff81344fe4>] ? bus_for_each_dev+0x94/0xa0
     [<ffffffff8130b06e>] xenbus_otherend_changed+0xbe/0x120
     [<ffffffff8130b4cb>] frontend_changed+0xb/0x10
     [<ffffffff81309c82>] xenwatch_thread+0xf2/0x130
     [<ffffffff81079f70>] ? wake_up_bit+0x40/0x40
     [<ffffffff81309b90>] ? xenbus_directory+0x80/0x80
     [<ffffffff810799d6>] kthread+0x96/0xa0
     [<ffffffff81485934>] kernel_thread_helper+0x4/0x10
     [<ffffffff814839f3>] ? int_ret_from_sys_call+0x7/0x1b
     [<ffffffff8147c17c>] ? retint_restore_args+0x5/0x6
     [<ffffffff81485930>] ? gs_change+0x13/0x13
    
    With this patch, when there is still pending I/O, the actual disconnect
    is done by the last reference holder (last pending I/O request). In this
    case, xenwatch doesn't block indefinitely.
    
    Signed-off-by: Valentin Priescu <priescuv@amazon.com>
    Reviewed-by: Steven Kady <stevkady@amazon.com>
    Reviewed-by: Steven Noonan <snoonan@amazon.com>
    Reviewed-by: David Vrabel <david.vrabel@citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/drivers/block/xen-blkback/common.h b/drivers/block/xen-blkback/common.h
index be052773ad03..f65b807e3236 100644
--- a/drivers/block/xen-blkback/common.h
+++ b/drivers/block/xen-blkback/common.h
@@ -314,7 +314,7 @@ struct xen_blkif {
 	unsigned long long			st_rd_sect;
 	unsigned long long			st_wr_sect;
 
-	wait_queue_head_t	waiting_to_free;
+	struct work_struct	free_work;
 	/* Thread shutdown wait queue. */
 	wait_queue_head_t	shutdown_wq;
 };
@@ -361,7 +361,7 @@ struct pending_req {
 #define xen_blkif_put(_b)				\
 	do {						\
 		if (atomic_dec_and_test(&(_b)->refcnt))	\
-			wake_up(&(_b)->waiting_to_free);\
+			schedule_work(&(_b)->free_work);\
 	} while (0)
 
 struct phys_req {

commit abb97b8c502a270d59c0c2e1ecea78ad752372ee
Author: Roger Pau Monne <roger.pau@citrix.com>
Date:   Tue Feb 11 20:34:03 2014 -0700

    xen-blkback: init persistent_purge_work work_struct
    
    Initialize persistent_purge_work work_struct on xen_blkif_alloc (and
    remove the previous initialization done in purge_persistent_gnt). This
    prevents flush_work from complaining even if purge_persistent_gnt has
    not been used.
    
    Signed-off-by: Roger Pau Monné <roger.pau@citrix.com>
    Reviewed-by: David Vrabel <david.vrabel@citrix.com>
    Tested-by: Sander Eikelenboom <linux@eikelenboom.it>
    Signed-off-by: Jens Axboe <axboe@fb.com>

diff --git a/drivers/block/xen-blkback/common.h b/drivers/block/xen-blkback/common.h
index 9eb34e24b4fe..be052773ad03 100644
--- a/drivers/block/xen-blkback/common.h
+++ b/drivers/block/xen-blkback/common.h
@@ -385,6 +385,7 @@ int xen_blkbk_flush_diskcache(struct xenbus_transaction xbt,
 int xen_blkbk_barrier(struct xenbus_transaction xbt,
 		      struct backend_info *be, int state);
 struct xenbus_device *xen_blkbk_xenbus(struct backend_info *be);
+void xen_blkbk_unmap_purged_grants(struct work_struct *work);
 
 static inline void blkif_get_x86_32_req(struct blkif_request *dst,
 					struct blkif_x86_32_request *src)

commit 80bfa2f6e2e81049fc6cd3bfaeedcb64db3a9ba6
Author: Roger Pau Monne <roger.pau@citrix.com>
Date:   Tue Feb 4 11:26:15 2014 +0100

    xen-blkif: drop struct blkif_request_segment_aligned
    
    This was wrongly introduced in commit 402b27f9, the only difference
    between blkif_request_segment_aligned and blkif_request_segment is
    that the former has a named padding, while both share the same
    memory layout.
    
    Also correct a few minor glitches in the description, including for it
    to no longer assume PAGE_SIZE == 4096.
    
    Signed-off-by: Roger Pau Monné <roger.pau@citrix.com>
    [Description fix by Jan Beulich]
    Signed-off-by: Jan Beulich <jbeulich@suse.com>
    Reported-by: Jan Beulich <jbeulich@suse.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: David Vrabel <david.vrabel@citrix.com>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Tested-by: Matt Rushton <mrushton@amazon.com>
    Cc: Matt Wilson <msw@amazon.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/drivers/block/xen-blkback/common.h b/drivers/block/xen-blkback/common.h
index e40326a7f707..9eb34e24b4fe 100644
--- a/drivers/block/xen-blkback/common.h
+++ b/drivers/block/xen-blkback/common.h
@@ -57,7 +57,7 @@
 #define MAX_INDIRECT_SEGMENTS 256
 
 #define SEGS_PER_INDIRECT_FRAME \
-	(PAGE_SIZE/sizeof(struct blkif_request_segment_aligned))
+	(PAGE_SIZE/sizeof(struct blkif_request_segment))
 #define MAX_INDIRECT_PAGES \
 	((MAX_INDIRECT_SEGMENTS + SEGS_PER_INDIRECT_FRAME - 1)/SEGS_PER_INDIRECT_FRAME)
 #define INDIRECT_PAGES(_segs) \

commit c05f3e3c85df1d89673e00cee7ece5ae4eb4c6ec
Author: Roger Pau Monne <roger.pau@citrix.com>
Date:   Tue Feb 4 11:26:14 2014 +0100

    xen-blkback: fix shutdown race
    
    Introduce a new variable to keep track of the number of in-flight
    requests. We need to make sure that when xen_blkif_put is called the
    request has already been freed and we can safely free xen_blkif, which
    was not the case before.
    
    Signed-off-by: Roger Pau Monné <roger.pau@citrix.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: David Vrabel <david.vrabel@citrix.com>
    Reviewed-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Tested-by: Matt Rushton <mrushton@amazon.com>
    Reviewed-by: Matt Rushton <mrushton@amazon.com>
    Cc: Matt Wilson <msw@amazon.com>
    Cc: Ian Campbell <Ian.Campbell@citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/drivers/block/xen-blkback/common.h b/drivers/block/xen-blkback/common.h
index f733d7627120..e40326a7f707 100644
--- a/drivers/block/xen-blkback/common.h
+++ b/drivers/block/xen-blkback/common.h
@@ -278,6 +278,7 @@ struct xen_blkif {
 	/* for barrier (drain) requests */
 	struct completion	drain_complete;
 	atomic_t		drain;
+	atomic_t		inflight;
 	/* One thread per one blkif. */
 	struct task_struct	*xenblkd;
 	unsigned int		waiting_reqs;

commit ef753411339eae46b9a3151906901f8bfd12b0f1
Author: Roger Pau Monne <roger.pau@citrix.com>
Date:   Tue Feb 4 11:26:13 2014 +0100

    xen-blkback: fix memory leaks
    
    I've at least identified two possible memory leaks in blkback, both
    related to the shutdown path of a VBD:
    
    - blkback doesn't wait for any pending purge work to finish before
      cleaning the list of free_pages. The purge work will call
      put_free_pages and thus we might end up with pages being added to
      the free_pages list after we have emptied it. Fix this by making
      sure there's no pending purge work before exiting
      xen_blkif_schedule, and moving the free_page cleanup code to
      xen_blkif_free.
    - blkback doesn't wait for pending requests to end before cleaning
      persistent grants and the list of free_pages. Again this can add
      pages to the free_pages list or persistent grants to the
      persistent_gnts red-black tree. Fixed by moving the persistent
      grants and free_pages cleanup code to xen_blkif_free.
    
    Also, add some checks in xen_blkif_free to make sure we are cleaning
    everything.
    
    Signed-off-by: Roger Pau Monné <roger.pau@citrix.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Reviewed-by: David Vrabel <david.vrabel@citrix.com>
    Cc: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Tested-by: Matt Rushton <mrushton@amazon.com>
    Reviewed-by: Matt Rushton <mrushton@amazon.com>
    Cc: Matt Wilson <msw@amazon.com>
    Cc: Ian Campbell <Ian.Campbell@citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/drivers/block/xen-blkback/common.h b/drivers/block/xen-blkback/common.h
index 8d8807563d99..f733d7627120 100644
--- a/drivers/block/xen-blkback/common.h
+++ b/drivers/block/xen-blkback/common.h
@@ -376,6 +376,7 @@ int xen_blkif_xenbus_init(void);
 irqreturn_t xen_blkif_be_int(int irq, void *dev_id);
 int xen_blkif_schedule(void *arg);
 int xen_blkif_purge_persistent(void *arg);
+void xen_blkbk_free_caches(struct xen_blkif *blkif);
 
 int xen_blkbk_flush_diskcache(struct xenbus_transaction xbt,
 			      struct backend_info *be, int state);

commit 8e3f8755545cc4a7f4da8e9ef76d6d32e0dca576
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Wed Jan 23 16:54:32 2013 -0500

    xen/blkback: Check for insane amounts of request on the ring (v6).
    
    Check that the ring does not have an insane amount of requests
    (more than there could fit on the ring).
    
    If we detect this case we will stop processing the requests
    and wait until the XenBus disconnects the ring.
    
    The existing check RING_REQUEST_CONS_OVERFLOW which checks for how
    many responses we have created in the past (rsp_prod_pvt) vs
    requests consumed (req_cons) and whether said difference is greater or
    equal to the size of the ring, does not catch this case.
    
    Wha the condition does check if there is a need to process more
    as we still have a backlog of responses to finish. Note that both
    of those values (rsp_prod_pvt and req_cons) are not exposed on the
    shared ring.
    
    To understand this problem a mini crash course in ring protocol
    response/request updates is in place.
    
    There are four entries: req_prod and rsp_prod; req_event and rsp_event
    to track the ring entries. We are only concerned about the first two -
    which set the tone of this bug.
    
    The req_prod is a value incremented by frontend for each request put
    on the ring. Conversely the rsp_prod is a value incremented by the backend
    for each response put on the ring (rsp_prod gets set by rsp_prod_pvt when
    pushing the responses on the ring).  Both values can
    wrap and are modulo the size of the ring (in block case that is 32).
    Please see RING_GET_REQUEST and RING_GET_RESPONSE for the more details.
    
    The culprit here is that if the difference between the
    req_prod and req_cons is greater than the ring size we have a problem.
    Fortunately for us, the '__do_block_io_op' loop:
    
            rc = blk_rings->common.req_cons;
            rp = blk_rings->common.sring->req_prod;
    
            while (rc != rp) {
    
                    ..
                    blk_rings->common.req_cons = ++rc; /* before make_response() */
    
            }
    
    will loop up to the point when rc == rp. The macros inside of the
    loop (RING_GET_REQUEST) is smart and is indexing based on the modulo
    of the ring size. If the frontend has provided a bogus req_prod value
    we will loop until the 'rc == rp' - which means we could be processing
    already processed requests (or responses) often.
    
    The reason the RING_REQUEST_CONS_OVERFLOW is not helping here is
    b/c it only tracks how many responses we have internally produced
    and whether we would should process more. The astute reader will
    notice that the macro RING_REQUEST_CONS_OVERFLOW provides two
    arguments - more on this later.
    
    For example, if we were to enter this function with these values:
    
            blk_rings->common.sring->req_prod =  X+31415 (X is the value from
                    the last time __do_block_io_op was called).
            blk_rings->common.req_cons = X
            blk_rings->common.rsp_prod_pvt = X
    
    The RING_REQUEST_CONS_OVERFLOW(&blk_rings->common, blk_rings->common.req_cons)
    is doing:
    
            req_cons - rsp_prod_pvt >= 32
    
    Which is,
            X - X >= 32 or 0 >= 32
    
    And that is false, so we continue on looping (this bug).
    
    If we re-use said macro RING_REQUEST_CONS_OVERFLOW and pass in the rp
    instead (sring->req_prod) of rc, the this macro can do the check:
    
         req_prod - rsp_prov_pvt >= 32
    
    Which is,
           X + 31415 - X >= 32 , or 31415 >= 32
    
    which is true, so we can error out and break out of the function.
    
    Unfortunatly the difference between rsp_prov_pvt and req_prod can be
    at 32 (which would error out in the macro). This condition exists when
    the backend is lagging behind with the responses and still has not finished
    responding to all of them (so make_response has not been called), and
    the rsp_prov_pvt + 32 == req_cons. This ends up with us not being able
    to use said macro.
    
    Hence introducing a new macro called RING_REQUEST_PROD_OVERFLOW which does
    a simple check of:
    
        req_prod - rsp_prod_pvt > RING_SIZE
    
    And with the X values from above:
    
       X + 31415 - X > 32
    
    Returns true. Also not that if the ring is full (which is where
    the RING_REQUEST_CONS_OVERFLOW triggered), we would not hit the
    same condition:
    
       X + 32 - X > 32
    
    Which is false.
    
    Lets use that macro.
    Note that in v5 of this patchset the macro was different - we used an
    earlier version.
    
    Cc: stable@vger.kernel.org
    [v1: Move the check outside the loop]
    [v2: Add a pr_warn as suggested by David]
    [v3: Use RING_REQUEST_CONS_OVERFLOW as suggested by Jan]
    [v4: Move wake_up after kthread_stop as suggested by Jan]
    [v5: Use RING_REQUEST_PROD_OVERFLOW instead]
    [v6: Use RING_REQUEST_PROD_OVERFLOW - Jan's version]
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Reviewed-by: Jan Beulich <jbeulich@suse.com>
    
    gadsa

diff --git a/drivers/block/xen-blkback/common.h b/drivers/block/xen-blkback/common.h
index c6b4cb9af6c2..8d8807563d99 100644
--- a/drivers/block/xen-blkback/common.h
+++ b/drivers/block/xen-blkback/common.h
@@ -314,6 +314,8 @@ struct xen_blkif {
 	unsigned long long			st_wr_sect;
 
 	wait_queue_head_t	waiting_to_free;
+	/* Thread shutdown wait queue. */
+	wait_queue_head_t	shutdown_wq;
 };
 
 struct seg_buf {

commit bb642e8315fd573795e8b6fa9b9629064d73add1
Author: Roger Pau Monne <roger.pau@citrix.com>
Date:   Thu May 2 10:21:17 2013 +0200

    xen-blkback: allocate list of pending reqs in small chunks
    
    Allocate pending requests in smaller chunks instead of allocating them
    all at the same time.
    
    This change also removes the global array of pending_reqs, it is no
    longer necessay.
    
    Variables related to the grant mapping have been grouped into a struct
    called "grant_page", this allows to allocate them in smaller chunks,
    and also improves memory locality.
    
    Signed-off-by: Roger Pau Monné <roger.pau@citrix.com>
    Reported-by: Sander Eikelenboom <linux@eikelenboom.it>
    Tested-by: Sander Eikelenboom <linux@eikelenboom.it>
    Reviewed-by: David Vrabel <david.vrabel@citrix.com>
    Cc: David Vrabel <david.vrabel@citrix.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/drivers/block/xen-blkback/common.h b/drivers/block/xen-blkback/common.h
index 1ac53da8410f..c6b4cb9af6c2 100644
--- a/drivers/block/xen-blkback/common.h
+++ b/drivers/block/xen-blkback/common.h
@@ -297,8 +297,6 @@ struct xen_blkif {
 	int			free_pages_num;
 	struct list_head	free_pages;
 
-	/* Allocation of pending_reqs */
-	struct pending_req	*pending_reqs;
 	/* List of all 'pending_req' available */
 	struct list_head	pending_free;
 	/* And its spinlock. */
@@ -323,6 +321,13 @@ struct seg_buf {
 	unsigned int nsec;
 };
 
+struct grant_page {
+	struct page 		*page;
+	struct persistent_gnt	*persistent_gnt;
+	grant_handle_t		handle;
+	grant_ref_t		gref;
+};
+
 /*
  * Each outstanding request that we've passed to the lower device layers has a
  * 'pending_req' allocated to it. Each buffer_head that completes decrements
@@ -337,14 +342,9 @@ struct pending_req {
 	unsigned short		operation;
 	int			status;
 	struct list_head	free_list;
-	struct page		*pages[MAX_INDIRECT_SEGMENTS];
-	struct persistent_gnt	*persistent_gnts[MAX_INDIRECT_SEGMENTS];
-	grant_handle_t		grant_handles[MAX_INDIRECT_SEGMENTS];
-	grant_ref_t		grefs[MAX_INDIRECT_SEGMENTS];
+	struct grant_page	*segments[MAX_INDIRECT_SEGMENTS];
 	/* Indirect descriptors */
-	struct persistent_gnt	*indirect_persistent_gnts[MAX_INDIRECT_PAGES];
-	struct page		*indirect_pages[MAX_INDIRECT_PAGES];
-	grant_handle_t		indirect_handles[MAX_INDIRECT_PAGES];
+	struct grant_page	*indirect_pages[MAX_INDIRECT_PAGES];
 	struct seg_buf		seg[MAX_INDIRECT_SEGMENTS];
 	struct bio		*biolist[MAX_INDIRECT_SEGMENTS];
 };

commit 402b27f9f2c22309d5bb285628765bc27b82fcf5
Author: Roger Pau Monne <roger.pau@citrix.com>
Date:   Thu Apr 18 16:06:54 2013 +0200

    xen-block: implement indirect descriptors
    
    Indirect descriptors introduce a new block operation
    (BLKIF_OP_INDIRECT) that passes grant references instead of segments
    in the request. This grant references are filled with arrays of
    blkif_request_segment_aligned, this way we can send more segments in a
    request.
    
    The proposed implementation sets the maximum number of indirect grefs
    (frames filled with blkif_request_segment_aligned) to 256 in the
    backend and 32 in the frontend. The value in the frontend has been
    chosen experimentally, and the backend value has been set to a sane
    value that allows expanding the maximum number of indirect descriptors
    in the frontend if needed.
    
    The migration code has changed from the previous implementation, in
    which we simply remapped the segments on the shared ring. Now the
    maximum number of segments allowed in a request can change depending
    on the backend, so we have to requeue all the requests in the ring and
    in the queue and split the bios in them if they are bigger than the
    new maximum number of segments.
    
    [v2: Fixed minor comments by Konrad.
    [v1: Added padding to make the indirect request 64bit aligned.
     Added some BUGs, comments; fixed number of indirect pages in
     blkif_get_x86_{32/64}_req. Added description about the indirect operation
     in blkif.h]
    Signed-off-by: Roger Pau Monné <roger.pau@citrix.com>
    [v3: Fixed spaces and tabs mix ups]
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/drivers/block/xen-blkback/common.h b/drivers/block/xen-blkback/common.h
index e33fafa0facd..1ac53da8410f 100644
--- a/drivers/block/xen-blkback/common.h
+++ b/drivers/block/xen-blkback/common.h
@@ -50,6 +50,19 @@
 		 __func__, __LINE__, ##args)
 
 
+/*
+ * This is the maximum number of segments that would be allowed in indirect
+ * requests. This value will also be passed to the frontend.
+ */
+#define MAX_INDIRECT_SEGMENTS 256
+
+#define SEGS_PER_INDIRECT_FRAME \
+	(PAGE_SIZE/sizeof(struct blkif_request_segment_aligned))
+#define MAX_INDIRECT_PAGES \
+	((MAX_INDIRECT_SEGMENTS + SEGS_PER_INDIRECT_FRAME - 1)/SEGS_PER_INDIRECT_FRAME)
+#define INDIRECT_PAGES(_segs) \
+	((_segs + SEGS_PER_INDIRECT_FRAME - 1)/SEGS_PER_INDIRECT_FRAME)
+
 /* Not a real protocol.  Used to generate ring structs which contain
  * the elements common to all protocols only.  This way we get a
  * compiler-checkable way to use common struct elements, so we can
@@ -83,12 +96,31 @@ struct blkif_x86_32_request_other {
 	uint64_t       id;           /* private guest value, echoed in resp  */
 } __attribute__((__packed__));
 
+struct blkif_x86_32_request_indirect {
+	uint8_t        indirect_op;
+	uint16_t       nr_segments;
+	uint64_t       id;
+	blkif_sector_t sector_number;
+	blkif_vdev_t   handle;
+	uint16_t       _pad1;
+	grant_ref_t    indirect_grefs[BLKIF_MAX_INDIRECT_PAGES_PER_REQUEST];
+	/*
+	 * The maximum number of indirect segments (and pages) that will
+	 * be used is determined by MAX_INDIRECT_SEGMENTS, this value
+	 * is also exported to the guest (via xenstore
+	 * feature-max-indirect-segments entry), so the frontend knows how
+	 * many indirect segments the backend supports.
+	 */
+	uint64_t       _pad2;        /* make it 64 byte aligned */
+} __attribute__((__packed__));
+
 struct blkif_x86_32_request {
 	uint8_t        operation;    /* BLKIF_OP_???                         */
 	union {
 		struct blkif_x86_32_request_rw rw;
 		struct blkif_x86_32_request_discard discard;
 		struct blkif_x86_32_request_other other;
+		struct blkif_x86_32_request_indirect indirect;
 	} u;
 } __attribute__((__packed__));
 
@@ -127,12 +159,32 @@ struct blkif_x86_64_request_other {
 	uint64_t       id;           /* private guest value, echoed in resp  */
 } __attribute__((__packed__));
 
+struct blkif_x86_64_request_indirect {
+	uint8_t        indirect_op;
+	uint16_t       nr_segments;
+	uint32_t       _pad1;        /* offsetof(blkif_..,u.indirect.id)==8   */
+	uint64_t       id;
+	blkif_sector_t sector_number;
+	blkif_vdev_t   handle;
+	uint16_t       _pad2;
+	grant_ref_t    indirect_grefs[BLKIF_MAX_INDIRECT_PAGES_PER_REQUEST];
+	/*
+	 * The maximum number of indirect segments (and pages) that will
+	 * be used is determined by MAX_INDIRECT_SEGMENTS, this value
+	 * is also exported to the guest (via xenstore
+	 * feature-max-indirect-segments entry), so the frontend knows how
+	 * many indirect segments the backend supports.
+	 */
+	uint32_t       _pad3;        /* make it 64 byte aligned */
+} __attribute__((__packed__));
+
 struct blkif_x86_64_request {
 	uint8_t        operation;    /* BLKIF_OP_???                         */
 	union {
 		struct blkif_x86_64_request_rw rw;
 		struct blkif_x86_64_request_discard discard;
 		struct blkif_x86_64_request_other other;
+		struct blkif_x86_64_request_indirect indirect;
 	} u;
 } __attribute__((__packed__));
 
@@ -266,6 +318,11 @@ struct xen_blkif {
 	wait_queue_head_t	waiting_to_free;
 };
 
+struct seg_buf {
+	unsigned long offset;
+	unsigned int nsec;
+};
+
 /*
  * Each outstanding request that we've passed to the lower device layers has a
  * 'pending_req' allocated to it. Each buffer_head that completes decrements
@@ -280,9 +337,16 @@ struct pending_req {
 	unsigned short		operation;
 	int			status;
 	struct list_head	free_list;
-	struct page		*pages[BLKIF_MAX_SEGMENTS_PER_REQUEST];
-	struct persistent_gnt	*persistent_gnts[BLKIF_MAX_SEGMENTS_PER_REQUEST];
-	grant_handle_t		grant_handles[BLKIF_MAX_SEGMENTS_PER_REQUEST];
+	struct page		*pages[MAX_INDIRECT_SEGMENTS];
+	struct persistent_gnt	*persistent_gnts[MAX_INDIRECT_SEGMENTS];
+	grant_handle_t		grant_handles[MAX_INDIRECT_SEGMENTS];
+	grant_ref_t		grefs[MAX_INDIRECT_SEGMENTS];
+	/* Indirect descriptors */
+	struct persistent_gnt	*indirect_persistent_gnts[MAX_INDIRECT_PAGES];
+	struct page		*indirect_pages[MAX_INDIRECT_PAGES];
+	grant_handle_t		indirect_handles[MAX_INDIRECT_PAGES];
+	struct seg_buf		seg[MAX_INDIRECT_SEGMENTS];
+	struct bio		*biolist[MAX_INDIRECT_SEGMENTS];
 };
 
 
@@ -321,7 +385,7 @@ struct xenbus_device *xen_blkbk_xenbus(struct backend_info *be);
 static inline void blkif_get_x86_32_req(struct blkif_request *dst,
 					struct blkif_x86_32_request *src)
 {
-	int i, n = BLKIF_MAX_SEGMENTS_PER_REQUEST;
+	int i, n = BLKIF_MAX_SEGMENTS_PER_REQUEST, j;
 	dst->operation = src->operation;
 	switch (src->operation) {
 	case BLKIF_OP_READ:
@@ -344,6 +408,18 @@ static inline void blkif_get_x86_32_req(struct blkif_request *dst,
 		dst->u.discard.sector_number = src->u.discard.sector_number;
 		dst->u.discard.nr_sectors = src->u.discard.nr_sectors;
 		break;
+	case BLKIF_OP_INDIRECT:
+		dst->u.indirect.indirect_op = src->u.indirect.indirect_op;
+		dst->u.indirect.nr_segments = src->u.indirect.nr_segments;
+		dst->u.indirect.handle = src->u.indirect.handle;
+		dst->u.indirect.id = src->u.indirect.id;
+		dst->u.indirect.sector_number = src->u.indirect.sector_number;
+		barrier();
+		j = min(MAX_INDIRECT_PAGES, INDIRECT_PAGES(dst->u.indirect.nr_segments));
+		for (i = 0; i < j; i++)
+			dst->u.indirect.indirect_grefs[i] =
+				src->u.indirect.indirect_grefs[i];
+		break;
 	default:
 		/*
 		 * Don't know how to translate this op. Only get the
@@ -357,7 +433,7 @@ static inline void blkif_get_x86_32_req(struct blkif_request *dst,
 static inline void blkif_get_x86_64_req(struct blkif_request *dst,
 					struct blkif_x86_64_request *src)
 {
-	int i, n = BLKIF_MAX_SEGMENTS_PER_REQUEST;
+	int i, n = BLKIF_MAX_SEGMENTS_PER_REQUEST, j;
 	dst->operation = src->operation;
 	switch (src->operation) {
 	case BLKIF_OP_READ:
@@ -380,6 +456,18 @@ static inline void blkif_get_x86_64_req(struct blkif_request *dst,
 		dst->u.discard.sector_number = src->u.discard.sector_number;
 		dst->u.discard.nr_sectors = src->u.discard.nr_sectors;
 		break;
+	case BLKIF_OP_INDIRECT:
+		dst->u.indirect.indirect_op = src->u.indirect.indirect_op;
+		dst->u.indirect.nr_segments = src->u.indirect.nr_segments;
+		dst->u.indirect.handle = src->u.indirect.handle;
+		dst->u.indirect.id = src->u.indirect.id;
+		dst->u.indirect.sector_number = src->u.indirect.sector_number;
+		barrier();
+		j = min(MAX_INDIRECT_PAGES, INDIRECT_PAGES(dst->u.indirect.nr_segments));
+		for (i = 0; i < j; i++)
+			dst->u.indirect.indirect_grefs[i] =
+				src->u.indirect.indirect_grefs[i];
+		break;
 	default:
 		/*
 		 * Don't know how to translate this op. Only get the

commit bf0720c48c7cefd127ed2329e6d0e40b39fa4d0e
Author: Roger Pau Monne <roger.pau@citrix.com>
Date:   Wed Apr 17 20:18:59 2013 +0200

    xen-blkback: make the queue of free requests per backend
    
    Remove the last dependency from blkbk by moving the list of free
    requests to blkif. This change reduces the contention on the list of
    available requests.
    
    Signed-off-by: Roger Pau Monné <roger.pau@citrix.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: xen-devel@lists.xen.org
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/drivers/block/xen-blkback/common.h b/drivers/block/xen-blkback/common.h
index af9bed48f773..e33fafa0facd 100644
--- a/drivers/block/xen-blkback/common.h
+++ b/drivers/block/xen-blkback/common.h
@@ -192,6 +192,9 @@ struct backend_info;
  */
 #define PERSISTENT_GNT_WAS_ACTIVE	1
 
+/* Number of requests that we can fit in a ring */
+#define XEN_BLKIF_REQS			32
+
 struct persistent_gnt {
 	struct page *page;
 	grant_ref_t gnt;
@@ -242,6 +245,14 @@ struct xen_blkif {
 	int			free_pages_num;
 	struct list_head	free_pages;
 
+	/* Allocation of pending_reqs */
+	struct pending_req	*pending_reqs;
+	/* List of all 'pending_req' available */
+	struct list_head	pending_free;
+	/* And its spinlock. */
+	spinlock_t		pending_free_lock;
+	wait_queue_head_t	pending_free_wq;
+
 	/* statistics */
 	unsigned long		st_print;
 	unsigned long long			st_rd_req;
@@ -255,6 +266,25 @@ struct xen_blkif {
 	wait_queue_head_t	waiting_to_free;
 };
 
+/*
+ * Each outstanding request that we've passed to the lower device layers has a
+ * 'pending_req' allocated to it. Each buffer_head that completes decrements
+ * the pendcnt towards zero. When it hits zero, the specified domain has a
+ * response queued for it, with the saved 'id' passed back.
+ */
+struct pending_req {
+	struct xen_blkif	*blkif;
+	u64			id;
+	int			nr_pages;
+	atomic_t		pendcnt;
+	unsigned short		operation;
+	int			status;
+	struct list_head	free_list;
+	struct page		*pages[BLKIF_MAX_SEGMENTS_PER_REQUEST];
+	struct persistent_gnt	*persistent_gnts[BLKIF_MAX_SEGMENTS_PER_REQUEST];
+	grant_handle_t		grant_handles[BLKIF_MAX_SEGMENTS_PER_REQUEST];
+};
+
 
 #define vbd_sz(_v)	((_v)->bdev->bd_part ? \
 			 (_v)->bdev->bd_part->nr_sects : \

commit 3f3aad5e6686ed49242bbf86de378b39f119ec9d
Author: Roger Pau Monne <roger.pau@citrix.com>
Date:   Wed Apr 17 20:18:57 2013 +0200

    xen-blkback: implement LRU mechanism for persistent grants
    
    This mechanism allows blkback to change the number of grants
    persistently mapped at run time.
    
    The algorithm uses a simple LRU mechanism that removes (if needed) the
    persistent grants that have not been used since the last LRU run, or
    if all grants have been used it removes the first grants in the list
    (that are not in use).
    
    The algorithm allows the user to change the maximum number of
    persistent grants, by changing max_persistent_grants in sysfs.
    
    Since we are storing the persistent grants used inside the request
    struct (to be able to mark them as "unused" when unmapping), we no
    longer need the bitmap (unmap_seg).
    
    Signed-off-by: Roger Pau Monné <roger.pau@citrix.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: xen-devel@lists.xen.org
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/drivers/block/xen-blkback/common.h b/drivers/block/xen-blkback/common.h
index 6c73c3855e65..af9bed48f773 100644
--- a/drivers/block/xen-blkback/common.h
+++ b/drivers/block/xen-blkback/common.h
@@ -182,12 +182,23 @@ struct xen_vbd {
 
 struct backend_info;
 
+/* Number of available flags */
+#define PERSISTENT_GNT_FLAGS_SIZE	2
+/* This persistent grant is currently in use */
+#define PERSISTENT_GNT_ACTIVE		0
+/*
+ * This persistent grant has been used, this flag is set when we remove the
+ * PERSISTENT_GNT_ACTIVE, to know that this grant has been used recently.
+ */
+#define PERSISTENT_GNT_WAS_ACTIVE	1
 
 struct persistent_gnt {
 	struct page *page;
 	grant_ref_t gnt;
 	grant_handle_t handle;
+	DECLARE_BITMAP(flags, PERSISTENT_GNT_FLAGS_SIZE);
 	struct rb_node node;
+	struct list_head remove_node;
 };
 
 struct xen_blkif {
@@ -219,6 +230,12 @@ struct xen_blkif {
 	/* tree to store persistent grants */
 	struct rb_root		persistent_gnts;
 	unsigned int		persistent_gnt_c;
+	atomic_t		persistent_gnt_in_use;
+	unsigned long           next_lru;
+
+	/* used by the kworker that offload work from the persistent purge */
+	struct list_head	persistent_purge_list;
+	struct work_struct	persistent_purge_work;
 
 	/* buffer of free pages to map grant refs */
 	spinlock_t		free_pages_lock;
@@ -262,6 +279,7 @@ int xen_blkif_xenbus_init(void);
 
 irqreturn_t xen_blkif_be_int(int irq, void *dev_id);
 int xen_blkif_schedule(void *arg);
+int xen_blkif_purge_persistent(void *arg);
 
 int xen_blkbk_flush_diskcache(struct xenbus_transaction xbt,
 			      struct backend_info *be, int state);

commit c6cc142dac52e62e1e8a2aff5de1300202b96c66
Author: Roger Pau Monne <roger.pau@citrix.com>
Date:   Wed Apr 17 20:18:56 2013 +0200

    xen-blkback: use balloon pages for all mappings
    
    Using balloon pages for all granted pages allows us to simplify the
    logic in blkback, especially in the xen_blkbk_map function, since now
    we can decide if we want to map a grant persistently or not after we
    have actually mapped it. This could not be done before because
    persistent grants used ballooned pages, whereas non-persistent grants
    used pages from the kernel.
    
    This patch also introduces several changes, the first one is that the
    list of free pages is no longer global, now each blkback instance has
    it's own list of free pages that can be used to map grants. Also, a
    run time parameter (max_buffer_pages) has been added in order to tune
    the maximum number of free pages each blkback instance will keep in
    it's buffer.
    
    Signed-off-by: Roger Pau Monné <roger.pau@citrix.com>
    Cc: xen-devel@lists.xen.org
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/drivers/block/xen-blkback/common.h b/drivers/block/xen-blkback/common.h
index 60103e2517ba..6c73c3855e65 100644
--- a/drivers/block/xen-blkback/common.h
+++ b/drivers/block/xen-blkback/common.h
@@ -220,6 +220,11 @@ struct xen_blkif {
 	struct rb_root		persistent_gnts;
 	unsigned int		persistent_gnt_c;
 
+	/* buffer of free pages to map grant refs */
+	spinlock_t		free_pages_lock;
+	int			free_pages_num;
+	struct list_head	free_pages;
+
 	/* statistics */
 	unsigned long		st_print;
 	unsigned long long			st_rd_req;

commit ffb1dabd1eb10c76a1e7af62f75a1aaa8d590b5a
Author: Roger Pau Monne <roger.pau@citrix.com>
Date:   Mon Mar 18 17:49:32 2013 +0100

    xen-blkback: don't store dev_bus_addr
    
    dev_bus_addr returned in the grant ref map operation is the mfn of the
    passed page, there's no need to store it in the persistent grant
    entry, since we can always get it provided that we have the page.
    
    This reduces the memory overhead of persistent grants in blkback.
    
    While at it, rename the 'seg[i].buf' to be 'seg[i].offset' as
    it makes much more sense - as we use that value in bio_add_page
    which as the fourth argument expects the offset.
    
    We hadn't used the physical address as part of this at all.
    
    Signed-off-by: Roger Pau Monné <roger.pau@citrix.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Cc: xen-devel@lists.xen.org
    [v1: s/buf/offset/]
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/drivers/block/xen-blkback/common.h b/drivers/block/xen-blkback/common.h
index da78346487ae..60103e2517ba 100644
--- a/drivers/block/xen-blkback/common.h
+++ b/drivers/block/xen-blkback/common.h
@@ -187,7 +187,6 @@ struct persistent_gnt {
 	struct page *page;
 	grant_ref_t gnt;
 	grant_handle_t handle;
-	uint64_t dev_bus_addr;
 	struct rb_node node;
 };
 

commit 986cacbd26abe5d498be922cd6632f1ec376c271
Author: Zoltan Kiss <zoltan.kiss@citrix.com>
Date:   Mon Mar 11 16:15:50 2013 +0000

    xen/blkback: Change statistics counter types to unsigned
    
    These values shouldn't be negative, but after an overflow their value
    can turn into negative, if they are signed. xentop can show bogus
    values in this case.
    
    Signed-off-by: Zoltan Kiss <zoltan.kiss@citrix.com>
    Reported-by: Ichiro Ogino <ichiro.ogino@citrix.co.jp>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/drivers/block/xen-blkback/common.h b/drivers/block/xen-blkback/common.h
index 195278ae993d..da78346487ae 100644
--- a/drivers/block/xen-blkback/common.h
+++ b/drivers/block/xen-blkback/common.h
@@ -223,13 +223,13 @@ struct xen_blkif {
 
 	/* statistics */
 	unsigned long		st_print;
-	int			st_rd_req;
-	int			st_wr_req;
-	int			st_oo_req;
-	int			st_f_req;
-	int			st_ds_req;
-	int			st_rd_sect;
-	int			st_wr_sect;
+	unsigned long long			st_rd_req;
+	unsigned long long			st_wr_req;
+	unsigned long long			st_oo_req;
+	unsigned long long			st_f_req;
+	unsigned long long			st_ds_req;
+	unsigned long long			st_rd_sect;
+	unsigned long long			st_wr_sect;
 
 	wait_queue_head_t	waiting_to_free;
 };

commit 0e367ae46503cfe7791460c8ba8434a5d60b2bd5
Author: David Vrabel <david.vrabel@citrix.com>
Date:   Thu Mar 7 17:32:01 2013 +0000

    xen/blkback: correctly respond to unknown, non-native requests
    
    If the frontend is using a non-native protocol (e.g., a 64-bit
    frontend with a 32-bit backend) and it sent an unrecognized request,
    the request was not translated and the response would have the
    incorrect ID.  This may cause the frontend driver to behave
    incorrectly or crash.
    
    Since the ID field in the request is always in the same place,
    regardless of the request type we can get the correct ID and make a
    valid response (which will report BLKIF_RSP_EOPNOTSUPP).
    
    This bug affected 64-bit SLES 11 guests when using a 32-bit backend.
    This guest does a BLKIF_OP_RESERVED_1 (BLKIF_OP_PACKET in the SLES
    source) and would crash in blkif_int() as the ID in the response would
    be invalid.
    
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>
    Cc: stable@vger.kernel.org
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/drivers/block/xen-blkback/common.h b/drivers/block/xen-blkback/common.h
index 6072390c7f57..195278ae993d 100644
--- a/drivers/block/xen-blkback/common.h
+++ b/drivers/block/xen-blkback/common.h
@@ -77,11 +77,18 @@ struct blkif_x86_32_request_discard {
 	uint64_t       nr_sectors;
 } __attribute__((__packed__));
 
+struct blkif_x86_32_request_other {
+	uint8_t        _pad1;
+	blkif_vdev_t   _pad2;
+	uint64_t       id;           /* private guest value, echoed in resp  */
+} __attribute__((__packed__));
+
 struct blkif_x86_32_request {
 	uint8_t        operation;    /* BLKIF_OP_???                         */
 	union {
 		struct blkif_x86_32_request_rw rw;
 		struct blkif_x86_32_request_discard discard;
+		struct blkif_x86_32_request_other other;
 	} u;
 } __attribute__((__packed__));
 
@@ -113,11 +120,19 @@ struct blkif_x86_64_request_discard {
 	uint64_t       nr_sectors;
 } __attribute__((__packed__));
 
+struct blkif_x86_64_request_other {
+	uint8_t        _pad1;
+	blkif_vdev_t   _pad2;
+	uint32_t       _pad3;        /* offsetof(blkif_..,u.discard.id)==8   */
+	uint64_t       id;           /* private guest value, echoed in resp  */
+} __attribute__((__packed__));
+
 struct blkif_x86_64_request {
 	uint8_t        operation;    /* BLKIF_OP_???                         */
 	union {
 		struct blkif_x86_64_request_rw rw;
 		struct blkif_x86_64_request_discard discard;
+		struct blkif_x86_64_request_other other;
 	} u;
 } __attribute__((__packed__));
 
@@ -278,6 +293,11 @@ static inline void blkif_get_x86_32_req(struct blkif_request *dst,
 		dst->u.discard.nr_sectors = src->u.discard.nr_sectors;
 		break;
 	default:
+		/*
+		 * Don't know how to translate this op. Only get the
+		 * ID so failure can be reported to the frontend.
+		 */
+		dst->u.other.id = src->u.other.id;
 		break;
 	}
 }
@@ -309,6 +329,11 @@ static inline void blkif_get_x86_64_req(struct blkif_request *dst,
 		dst->u.discard.nr_sectors = src->u.discard.nr_sectors;
 		break;
 	default:
+		/*
+		 * Don't know how to translate this op. Only get the
+		 * ID so failure can be reported to the frontend.
+		 */
+		dst->u.other.id = src->u.other.id;
 		break;
 	}
 }

commit 9228ff90387e276ad67b10c0eb525c9d6a57d5e9
Merge: 9360b53661a2 d2ec180c23a5
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Mon Dec 17 13:39:11 2012 -0800

    Merge branch 'for-3.8/drivers' of git://git.kernel.dk/linux-block
    
    Pull block driver update from Jens Axboe:
     "Now that the core bits are in, here are the driver bits for 3.8.  The
      branch contains:
    
       - A huge pile of drbd bits that were dumped from the 3.7 merge
         window.  Following that, it was both made perfectly clear that
         there is going to be no more over-the-wall pulls and how the
         situation on individual pulls can be improved.
    
       - A few cleanups from Akinobu Mita for drbd and cciss.
    
       - Queue improvement for loop from Lukas.  This grew into adding a
         generic interface for waiting/checking an even with a specific
         lock, allowing this to be pulled out of md and now loop and drbd is
         also using it.
    
       - A few fixes for xen back/front block driver from Roger Pau Monne.
    
       - Partition improvements from Stephen Warren, allowing partiion UUID
         to be used as an identifier."
    
    * 'for-3.8/drivers' of git://git.kernel.dk/linux-block: (609 commits)
      drbd: update Kconfig to match current dependencies
      drbd: Fix drbdsetup wait-connect, wait-sync etc... commands
      drbd: close race between drbd_set_role and drbd_connect
      drbd: respect no-md-barriers setting also when changed online via disk-options
      drbd: Remove obsolete check
      drbd: fixup after wait_even_lock_irq() addition to generic code
      loop: Limit the number of requests in the bio list
      wait: add wait_event_lock_irq() interface
      xen-blkfront: free allocated page
      xen-blkback: move free persistent grants code
      block: partition: msdos: provide UUIDs for partitions
      init: reduce PARTUUID min length to 1 from 36
      block: store partition_meta_info.uuid as a string
      cciss: use check_signature()
      cciss: cleanup bitops usage
      drbd: use copy_highpage
      drbd: if the replication link breaks during handshake, keep retrying
      drbd: check return of kmalloc in receive_uuids
      drbd: Broadcast sync progress no more often than once per second
      drbd: don't try to clear bits once the disk has failed
      ...

commit 0a8704a51f386cab7394e38ff1d66eef924d8ab8
Author: Roger Pau Monne <roger.pau@citrix.com>
Date:   Wed Oct 24 18:58:45 2012 +0200

    xen/blkback: Persistent grant maps for xen blk drivers
    
    This patch implements persistent grants for the xen-blk{front,back}
    mechanism. The effect of this change is to reduce the number of unmap
    operations performed, since they cause a (costly) TLB shootdown. This
    allows the I/O performance to scale better when a large number of VMs
    are performing I/O.
    
    Previously, the blkfront driver was supplied a bvec[] from the request
    queue. This was granted to dom0; dom0 performed the I/O and wrote
    directly into the grant-mapped memory and unmapped it; blkfront then
    removed foreign access for that grant. The cost of unmapping scales
    badly with the number of CPUs in Dom0. An experiment showed that when
    Dom0 has 24 VCPUs, and guests are performing parallel I/O to a
    ramdisk, the IPIs from performing unmap's is a bottleneck at 5 guests
    (at which point 650,000 IOPS are being performed in total). If more
    than 5 guests are used, the performance declines. By 10 guests, only
    400,000 IOPS are being performed.
    
    This patch improves performance by only unmapping when the connection
    between blkfront and back is broken.
    
    On startup blkfront notifies blkback that it is using persistent
    grants, and blkback will do the same. If blkback is not capable of
    persistent mapping, blkfront will still use the same grants, since it
    is compatible with the previous protocol, and simplifies the code
    complexity in blkfront.
    
    To perform a read, in persistent mode, blkfront uses a separate pool
    of pages that it maps to dom0. When a request comes in, blkfront
    transmutes the request so that blkback will write into one of these
    free pages. Blkback keeps note of which grefs it has already
    mapped. When a new ring request comes to blkback, it looks to see if
    it has already mapped that page. If so, it will not map it again. If
    the page hasn't been previously mapped, it is mapped now, and a record
    is kept of this mapping. Blkback proceeds as usual. When blkfront is
    notified that blkback has completed a request, it memcpy's from the
    shared memory, into the bvec supplied. A record that the {gref, page}
    tuple is mapped, and not inflight is kept.
    
    Writes are similar, except that the memcpy is peformed from the
    supplied bvecs, into the shared pages, before the request is put onto
    the ring.
    
    Blkback stores a mapping of grefs=>{page mapped to by gref} in
    a red-black tree. As the grefs are not known apriori, and provide no
    guarantees on their ordering, we have to perform a search
    through this tree to find the page, for every gref we receive. This
    operation takes O(log n) time in the worst case. In blkfront grants
    are stored using a single linked list.
    
    The maximum number of grants that blkback will persistenly map is
    currently set to RING_SIZE * BLKIF_MAX_SEGMENTS_PER_REQUEST, to
    prevent a malicios guest from attempting a DoS, by supplying fresh
    grefs, causing the Dom0 kernel to map excessively. If a guest
    is using persistent grants and exceeds the maximum number of grants to
    map persistenly the newly passed grefs will be mapped and unmaped.
    Using this approach, we can have requests that mix persistent and
    non-persistent grants, and we need to handle them correctly.
    This allows us to set the maximum number of persistent grants to a
    lower value than RING_SIZE * BLKIF_MAX_SEGMENTS_PER_REQUEST, although
    setting it will lead to unpredictable performance.
    
    In writing this patch, the question arrises as to if the additional
    cost of performing memcpys in the guest (to/from the pool of granted
    pages) outweigh the gains of not performing TLB shootdowns. The answer
    to that question is `no'. There appears to be very little, if any
    additional cost to the guest of using persistent grants. There is
    perhaps a small saving, from the reduced number of hypercalls
    performed in granting, and ending foreign access.
    
    Signed-off-by: Oliver Chick <oliver.chick@citrix.com>
    Signed-off-by: Roger Pau Monne <roger.pau@citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    [v1: Fixed up the misuse of bool as int]

diff --git a/drivers/block/xen-blkback/common.h b/drivers/block/xen-blkback/common.h
index 9ad3b5ec1dc1..ae7951f0e268 100644
--- a/drivers/block/xen-blkback/common.h
+++ b/drivers/block/xen-blkback/common.h
@@ -34,6 +34,7 @@
 #include <linux/vmalloc.h>
 #include <linux/wait.h>
 #include <linux/io.h>
+#include <linux/rbtree.h>
 #include <asm/setup.h>
 #include <asm/pgalloc.h>
 #include <asm/hypervisor.h>
@@ -160,10 +161,22 @@ struct xen_vbd {
 	sector_t		size;
 	bool			flush_support;
 	bool			discard_secure;
+
+	unsigned int		feature_gnt_persistent:1;
+	unsigned int		overflow_max_grants:1;
 };
 
 struct backend_info;
 
+
+struct persistent_gnt {
+	struct page *page;
+	grant_ref_t gnt;
+	grant_handle_t handle;
+	uint64_t dev_bus_addr;
+	struct rb_node node;
+};
+
 struct xen_blkif {
 	/* Unique identifier for this interface. */
 	domid_t			domid;
@@ -190,6 +203,10 @@ struct xen_blkif {
 	struct task_struct	*xenblkd;
 	unsigned int		waiting_reqs;
 
+	/* tree to store persistent grants */
+	struct rb_root		persistent_gnts;
+	unsigned int		persistent_gnt_c;
+
 	/* statistics */
 	unsigned long		st_print;
 	int			st_rd_req;

commit 1f999572f244f266c5b1b855025723541b0b475d
Author: Oliver Chick <oliver.chick@citrix.com>
Date:   Fri Sep 21 10:04:18 2012 +0100

    xen/blkback: Change xen_vbd's flush_support and discard_secure to have type unsigned int, rather than bool
    
    Changing the type of bdev parameters to be unsigned int :1, rather than bool.
    This is more consistent with the types of other features in the block drivers.
    
    Signed-off-by: Oliver Chick <oliver.chick@citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Signed-off-by: Jens Axboe <axboe@kernel.dk>

diff --git a/drivers/block/xen-blkback/common.h b/drivers/block/xen-blkback/common.h
index 9ad3b5ec1dc1..9a54623e52d7 100644
--- a/drivers/block/xen-blkback/common.h
+++ b/drivers/block/xen-blkback/common.h
@@ -158,8 +158,8 @@ struct xen_vbd {
 	struct block_device	*bdev;
 	/* Cached size parameter. */
 	sector_t		size;
-	bool			flush_support;
-	bool			discard_secure;
+	unsigned int		flush_support:1;
+	unsigned int		discard_secure:1;
 };
 
 struct backend_info;

commit 8c9ce606a60e4a0cb447bdc082ce383b96b227b4
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Fri May 25 16:11:09 2012 -0400

    xen/blkback: Copy id field when doing BLKIF_DISCARD.
    
    We weren't copying the id field so when we sent the response
    back to the frontend (especially with a 64-bit host and 32-bit
    guest), we ended up using a random value. This lead to the
    frontend crashing as it would try to pass to __blk_end_request_all
    a NULL 'struct request' (b/c it would use the 'id' to find the
    proper 'struct request' in its shadow array) and end up crashing:
    
    BUG: unable to handle kernel NULL pointer dereference at 000000e4
    IP: [<c0646d4c>] __blk_end_request_all+0xc/0x40
    .. snip..
    EIP is at __blk_end_request_all+0xc/0x40
    .. snip..
     [<ed95db72>] blkif_interrupt+0x172/0x330 [xen_blkfront]
    
    This fixes the bug by passing in the proper id for the response.
    
    Bugzilla: https://bugzilla.redhat.com/show_bug.cgi?id=824641
    
    CC: stable@kernel.org
    Tested-by: William Dauchy <wdauchy@gmail.com>
    Acked-by: Stefano Stabellini <stefano.stabellini@eu.citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/drivers/block/xen-blkback/common.h b/drivers/block/xen-blkback/common.h
index 773cf27dc23f..9ad3b5ec1dc1 100644
--- a/drivers/block/xen-blkback/common.h
+++ b/drivers/block/xen-blkback/common.h
@@ -257,6 +257,7 @@ static inline void blkif_get_x86_32_req(struct blkif_request *dst,
 		break;
 	case BLKIF_OP_DISCARD:
 		dst->u.discard.flag = src->u.discard.flag;
+		dst->u.discard.id = src->u.discard.id;
 		dst->u.discard.sector_number = src->u.discard.sector_number;
 		dst->u.discard.nr_sectors = src->u.discard.nr_sectors;
 		break;
@@ -287,6 +288,7 @@ static inline void blkif_get_x86_64_req(struct blkif_request *dst,
 		break;
 	case BLKIF_OP_DISCARD:
 		dst->u.discard.flag = src->u.discard.flag;
+		dst->u.discard.id = src->u.discard.id;
 		dst->u.discard.sector_number = src->u.discard.sector_number;
 		dst->u.discard.nr_sectors = src->u.discard.nr_sectors;
 		break;

commit 4dae76705fc8f9854bb732f9944e7ff9ba7a8e9f
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Tue Mar 13 18:43:23 2012 -0400

    xen/blkback: Squash the discard support for 'file' and 'phy' type.
    
    The only reason for the distinction was for the special case of
    'file' (which is assumed to be loopback device), was to reach inside
    the loopback device, find the underlaying file, and call fallocate on it.
    Fortunately "xen-blkback: convert hole punching to discard request on
    loop devices" removes that use-case and we now based the discard
    support based on blk_queue_discard(q) and extract all appropriate
    parameters from the 'struct request_queue'.
    
    CC: Li Dongyang <lidongyang@novell.com>
    Acked-by: Jan Beulich <JBeulich@suse.com>
    [v1: Dropping pointless initializer and keeping blank line]
    [v2: Remove the kfree as it is not used anymore]
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/drivers/block/xen-blkback/common.h b/drivers/block/xen-blkback/common.h
index d0ee7edc9be8..773cf27dc23f 100644
--- a/drivers/block/xen-blkback/common.h
+++ b/drivers/block/xen-blkback/common.h
@@ -146,11 +146,6 @@ enum blkif_protocol {
 	BLKIF_PROTOCOL_X86_64 = 3,
 };
 
-enum blkif_backend_type {
-	BLKIF_BACKEND_PHY  = 1,
-	BLKIF_BACKEND_FILE = 2,
-};
-
 struct xen_vbd {
 	/* What the domain refers to this vbd as. */
 	blkif_vdev_t		handle;
@@ -177,7 +172,6 @@ struct xen_blkif {
 	unsigned int		irq;
 	/* Comms information. */
 	enum blkif_protocol	blk_protocol;
-	enum blkif_backend_type blk_backend_type;
 	union blkif_back_rings	blk_rings;
 	void			*blk_ring;
 	/* The VBD attached to this interface. */

commit 5ea42986694a96542644f9cae8b122d3a00c508f
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Wed Oct 12 16:23:30 2011 -0400

    xen/blk[front|back]: Enhance discard support with secure erasing support.
    
    Part of the blkdev_issue_discard(xx) operation is that it can also
    issue a secure discard operation that will permanantly remove the
    sectors in question. We advertise that we can support that via the
    'discard-secure' attribute and on the request, if the 'secure' bit
    is set, we will attempt to pass in REQ_DISCARD | REQ_SECURE.
    
    CC: Li Dongyang <lidongyang@novell.com>
    [v1: Used 'flag' instead of 'secure:1' bit]
    [v2: Use 'reserved' uint8_t instead of adding a new value]
    [v3: Check for nseg when mapping instead of operation]
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/drivers/block/xen-blkback/common.h b/drivers/block/xen-blkback/common.h
index dbfe7b3b0737..d0ee7edc9be8 100644
--- a/drivers/block/xen-blkback/common.h
+++ b/drivers/block/xen-blkback/common.h
@@ -69,7 +69,7 @@ struct blkif_x86_32_request_rw {
 } __attribute__((__packed__));
 
 struct blkif_x86_32_request_discard {
-	uint8_t        nr_segments;  /* number of segments                   */
+	uint8_t        flag;         /* BLKIF_DISCARD_SECURE or zero         */
 	blkif_vdev_t   _pad1;        /* was "handle" for read/write requests */
 	uint64_t       id;           /* private guest value, echoed in resp  */
 	blkif_sector_t sector_number;/* start sector idx on disk (r/w only)  */
@@ -104,7 +104,7 @@ struct blkif_x86_64_request_rw {
 } __attribute__((__packed__));
 
 struct blkif_x86_64_request_discard {
-	uint8_t        nr_segments;  /* number of segments                   */
+	uint8_t        flag;         /* BLKIF_DISCARD_SECURE or zero         */
 	blkif_vdev_t   _pad1;        /* was "handle" for read/write requests */
         uint32_t       _pad2;        /* offsetof(blkif_..,u.discard.id)==8   */
 	uint64_t       id;
@@ -164,6 +164,7 @@ struct xen_vbd {
 	/* Cached size parameter. */
 	sector_t		size;
 	bool			flush_support;
+	bool			discard_secure;
 };
 
 struct backend_info;
@@ -261,6 +262,7 @@ static inline void blkif_get_x86_32_req(struct blkif_request *dst,
 			dst->u.rw.seg[i] = src->u.rw.seg[i];
 		break;
 	case BLKIF_OP_DISCARD:
+		dst->u.discard.flag = src->u.discard.flag;
 		dst->u.discard.sector_number = src->u.discard.sector_number;
 		dst->u.discard.nr_sectors = src->u.discard.nr_sectors;
 		break;
@@ -290,6 +292,7 @@ static inline void blkif_get_x86_64_req(struct blkif_request *dst,
 			dst->u.rw.seg[i] = src->u.rw.seg[i];
 		break;
 	case BLKIF_OP_DISCARD:
+		dst->u.discard.flag = src->u.discard.flag;
 		dst->u.discard.sector_number = src->u.discard.sector_number;
 		dst->u.discard.nr_sectors = src->u.discard.nr_sectors;
 		break;

commit 97e36834f5a106459ab1b290e663a4eb6264639e
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Wed Oct 12 12:12:36 2011 -0400

    xen/blk[front|back]: Squash blkif_request_rw and blkif_request_discard together
    
    In a union type structure to deal with the overlapping
    attributes in a easier manner.
    
    Suggested-by: Ian Campbell <Ian.Campbell@citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/drivers/block/xen-blkback/common.h b/drivers/block/xen-blkback/common.h
index dfb1b3a43a5d..dbfe7b3b0737 100644
--- a/drivers/block/xen-blkback/common.h
+++ b/drivers/block/xen-blkback/common.h
@@ -60,58 +60,66 @@ struct blkif_common_response {
 	char dummy;
 };
 
-/* i386 protocol version */
-#pragma pack(push, 4)
-
 struct blkif_x86_32_request_rw {
+	uint8_t        nr_segments;  /* number of segments                   */
+	blkif_vdev_t   handle;       /* only for read/write requests         */
+	uint64_t       id;           /* private guest value, echoed in resp  */
 	blkif_sector_t sector_number;/* start sector idx on disk (r/w only)  */
 	struct blkif_request_segment seg[BLKIF_MAX_SEGMENTS_PER_REQUEST];
-};
+} __attribute__((__packed__));
 
 struct blkif_x86_32_request_discard {
+	uint8_t        nr_segments;  /* number of segments                   */
+	blkif_vdev_t   _pad1;        /* was "handle" for read/write requests */
+	uint64_t       id;           /* private guest value, echoed in resp  */
 	blkif_sector_t sector_number;/* start sector idx on disk (r/w only)  */
-	uint64_t nr_sectors;
-};
+	uint64_t       nr_sectors;
+} __attribute__((__packed__));
 
 struct blkif_x86_32_request {
 	uint8_t        operation;    /* BLKIF_OP_???                         */
-	uint8_t        nr_segments;  /* number of segments                   */
-	blkif_vdev_t   handle;       /* only for read/write requests         */
-	uint64_t       id;           /* private guest value, echoed in resp  */
 	union {
 		struct blkif_x86_32_request_rw rw;
 		struct blkif_x86_32_request_discard discard;
 	} u;
-};
+} __attribute__((__packed__));
+
+/* i386 protocol version */
+#pragma pack(push, 4)
 struct blkif_x86_32_response {
 	uint64_t        id;              /* copied from request */
 	uint8_t         operation;       /* copied from request */
 	int16_t         status;          /* BLKIF_RSP_???       */
 };
 #pragma pack(pop)
-
 /* x86_64 protocol version */
 
 struct blkif_x86_64_request_rw {
+	uint8_t        nr_segments;  /* number of segments                   */
+	blkif_vdev_t   handle;       /* only for read/write requests         */
+	uint32_t       _pad1;        /* offsetof(blkif_reqest..,u.rw.id)==8  */
+	uint64_t       id;
 	blkif_sector_t sector_number;/* start sector idx on disk (r/w only)  */
 	struct blkif_request_segment seg[BLKIF_MAX_SEGMENTS_PER_REQUEST];
-};
+} __attribute__((__packed__));
 
 struct blkif_x86_64_request_discard {
+	uint8_t        nr_segments;  /* number of segments                   */
+	blkif_vdev_t   _pad1;        /* was "handle" for read/write requests */
+        uint32_t       _pad2;        /* offsetof(blkif_..,u.discard.id)==8   */
+	uint64_t       id;
 	blkif_sector_t sector_number;/* start sector idx on disk (r/w only)  */
-	uint64_t nr_sectors;
-};
+	uint64_t       nr_sectors;
+} __attribute__((__packed__));
 
 struct blkif_x86_64_request {
 	uint8_t        operation;    /* BLKIF_OP_???                         */
-	uint8_t        nr_segments;  /* number of segments                   */
-	blkif_vdev_t   handle;       /* only for read/write requests         */
-	uint64_t       __attribute__((__aligned__(8))) id;
 	union {
 		struct blkif_x86_64_request_rw rw;
 		struct blkif_x86_64_request_discard discard;
 	} u;
-};
+} __attribute__((__packed__));
+
 struct blkif_x86_64_response {
 	uint64_t       __attribute__((__aligned__(8))) id;
 	uint8_t         operation;       /* copied from request */
@@ -237,18 +245,18 @@ static inline void blkif_get_x86_32_req(struct blkif_request *dst,
 {
 	int i, n = BLKIF_MAX_SEGMENTS_PER_REQUEST;
 	dst->operation = src->operation;
-	dst->nr_segments = src->nr_segments;
-	dst->handle = src->handle;
-	dst->id = src->id;
 	switch (src->operation) {
 	case BLKIF_OP_READ:
 	case BLKIF_OP_WRITE:
 	case BLKIF_OP_WRITE_BARRIER:
 	case BLKIF_OP_FLUSH_DISKCACHE:
+		dst->u.rw.nr_segments = src->u.rw.nr_segments;
+		dst->u.rw.handle = src->u.rw.handle;
+		dst->u.rw.id = src->u.rw.id;
 		dst->u.rw.sector_number = src->u.rw.sector_number;
 		barrier();
-		if (n > dst->nr_segments)
-			n = dst->nr_segments;
+		if (n > dst->u.rw.nr_segments)
+			n = dst->u.rw.nr_segments;
 		for (i = 0; i < n; i++)
 			dst->u.rw.seg[i] = src->u.rw.seg[i];
 		break;
@@ -266,18 +274,18 @@ static inline void blkif_get_x86_64_req(struct blkif_request *dst,
 {
 	int i, n = BLKIF_MAX_SEGMENTS_PER_REQUEST;
 	dst->operation = src->operation;
-	dst->nr_segments = src->nr_segments;
-	dst->handle = src->handle;
-	dst->id = src->id;
 	switch (src->operation) {
 	case BLKIF_OP_READ:
 	case BLKIF_OP_WRITE:
 	case BLKIF_OP_WRITE_BARRIER:
 	case BLKIF_OP_FLUSH_DISKCACHE:
+		dst->u.rw.nr_segments = src->u.rw.nr_segments;
+		dst->u.rw.handle = src->u.rw.handle;
+		dst->u.rw.id = src->u.rw.id;
 		dst->u.rw.sector_number = src->u.rw.sector_number;
 		barrier();
-		if (n > dst->nr_segments)
-			n = dst->nr_segments;
+		if (n > dst->u.rw.nr_segments)
+			n = dst->u.rw.nr_segments;
 		for (i = 0; i < n; i++)
 			dst->u.rw.seg[i] = src->u.rw.seg[i];
 		break;

commit 06d381484fe8fb1ba2996c22e89595a273e3634c
Merge: 5d5a8d2d9d6c c9d636997841
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Nov 6 18:31:36 2011 -0800

    Merge branch 'stable/vmalloc-3.2' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen
    
    * 'stable/vmalloc-3.2' of git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen:
      net: xen-netback: use API provided by xenbus module to map rings
      block: xen-blkback: use API provided by xenbus module to map rings
      xen: use generic functions instead of xen_{alloc, free}_vm_area()

commit 3d0a8d10cfb4cc3d1877c29a866ee7d8a46aa2fa
Merge: b4fdcb02f1e3 a0eda62552eb
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Nov 4 17:22:14 2011 -0700

    Merge branch 'for-3.2/drivers' of git://git.kernel.dk/linux-block
    
    * 'for-3.2/drivers' of git://git.kernel.dk/linux-block: (30 commits)
      virtio-blk: use ida to allocate disk index
      hpsa: add small delay when using PCI Power Management to reset for kump
      cciss: add small delay when using PCI Power Management to reset for kump
      xen/blkback: Fix two races in the handling of barrier requests.
      xen/blkback: Check for proper operation.
      xen/blkback: Fix the inhibition to map pages when discarding sector ranges.
      xen/blkback: Report VBD_WSECT (wr_sect) properly.
      xen/blkback: Support 'feature-barrier' aka old-style BARRIER requests.
      xen-blkfront: plug device number leak in xlblk_init() error path
      xen-blkfront: If no barrier or flush is supported, use invalid operation.
      xen-blkback: use kzalloc() in favor of kmalloc()+memset()
      xen-blkback: fixed indentation and comments
      xen-blkfront: fix a deadlock while handling discard response
      xen-blkfront: Handle discard requests.
      xen-blkback: Implement discard requests ('feature-discard')
      xen-blkfront: add BLKIF_OP_DISCARD and discard request struct
      drivers/block/loop.c: remove unnecessary bdev argument from loop_clr_fd()
      drivers/block/loop.c: emit uevent on auto release
      drivers/block/cpqarray.c: use pci_dev->revision
      loop: always allow userspace partitions and optionally support automatic scanning
      ...
    
    Fic up trivial header file includsion conflict in drivers/block/loop.c

commit 2d073846b891c3f49c4ea03c5db3ac92f92742f1
Author: David Vrabel <david.vrabel@citrix.com>
Date:   Thu Sep 29 16:53:30 2011 +0100

    block: xen-blkback: use API provided by xenbus module to map rings
    
    The xenbus module provides xenbus_map_ring_valloc() and
    xenbus_map_ring_vfree().  Use these to map the ring pages granted by
    the frontend.
    
    Acked-by: Jens Axboe <jaxboe@fusionio.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/drivers/block/xen-blkback/common.h b/drivers/block/xen-blkback/common.h
index 00c57c90e2d6..7ec0e8896a78 100644
--- a/drivers/block/xen-blkback/common.h
+++ b/drivers/block/xen-blkback/common.h
@@ -139,7 +139,7 @@ struct xen_blkif {
 	/* Comms information. */
 	enum blkif_protocol	blk_protocol;
 	union blkif_back_rings	blk_rings;
-	struct vm_struct	*blk_ring_area;
+	void			*blk_ring;
 	/* The VBD attached to this interface. */
 	struct xen_vbd		vbd;
 	/* Back pointer to the backend_info. */
@@ -163,9 +163,6 @@ struct xen_blkif {
 	int			st_wr_sect;
 
 	wait_queue_head_t	waiting_to_free;
-
-	grant_handle_t		shmem_handle;
-	grant_ref_t		shmem_ref;
 };
 
 

commit 59e52534172d845ebffb0d7e85fc56fb7b857051
Merge: 73692d9bb58e 0d89e54c8249
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Tue Oct 25 12:11:02 2011 +0200

    Merge branch 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/jikos/trivial
    
    * 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/jikos/trivial: (59 commits)
      MAINTAINERS: linux-m32r is moderated for non-subscribers
      linux@lists.openrisc.net is moderated for non-subscribers
      Drop default from "DM365 codec select" choice
      parisc: Kconfig: cleanup Kernel page size default
      Kconfig: remove redundant CONFIG_ prefix on two symbols
      cris: remove arch/cris/arch-v32/lib/nand_init.S
      microblaze: add missing CONFIG_ prefixes
      h8300: drop puzzling Kconfig dependencies
      MAINTAINERS: microblaze-uclinux@itee.uq.edu.au is moderated for non-subscribers
      tty: drop superfluous dependency in Kconfig
      ARM: mxc: fix Kconfig typo 'i.MX51'
      Fix file references in Kconfig files
      aic7xxx: fix Kconfig references to READMEs
      Fix file references in drivers/ide/
      thinkpad_acpi: Fix printk typo 'bluestooth'
      bcmring: drop commented out line in Kconfig
      btmrvl_sdio: fix typo 'btmrvl_sdio_sd6888'
      doc: raw1394: Trivial typo fix
      CIFS: Don't free volume_info->UNC until we are entirely done with it.
      treewide: Correct spelling of successfully in comments
      ...

commit 29bde093787f3bdf7b9b4270ada6be7c8076e36b
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Mon Oct 10 00:42:22 2011 -0400

    xen/blkback: Support 'feature-barrier' aka old-style BARRIER requests.
    
    We emulate the barrier requests by draining the outstanding bio's
    and then sending the WRITE_FLUSH command. To drain the I/Os
    we use the refcnt that is used during disconnect to wait for all
    the I/Os before disconnecting from the frontend. We latch on its
    value and if it reaches either the threshold for disconnect or when
    there are no more outstanding I/Os, then we have drained all I/Os.
    
    Suggested-by: Christopher Hellwig <hch@infradead.org>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/drivers/block/xen-blkback/common.h b/drivers/block/xen-blkback/common.h
index 1b1bc4458685..e638457d9de4 100644
--- a/drivers/block/xen-blkback/common.h
+++ b/drivers/block/xen-blkback/common.h
@@ -181,6 +181,9 @@ struct xen_blkif {
 	atomic_t		refcnt;
 
 	wait_queue_head_t	wq;
+	/* for barrier (drain) requests */
+	struct completion	drain_complete;
+	atomic_t		drain;
 	/* One thread per one blkif. */
 	struct task_struct	*xenblkd;
 	unsigned int		waiting_reqs;
@@ -229,6 +232,8 @@ int xen_blkif_schedule(void *arg);
 int xen_blkbk_flush_diskcache(struct xenbus_transaction xbt,
 			      struct backend_info *be, int state);
 
+int xen_blkbk_barrier(struct xenbus_transaction xbt,
+		      struct backend_info *be, int state);
 struct xenbus_device *xen_blkbk_xenbus(struct backend_info *be);
 
 static inline void blkif_get_x86_32_req(struct blkif_request *dst,

commit c555aab97de139ac8762c922248bb68f43a8c488
Author: Joe Jin <joe.jin@oracle.com>
Date:   Mon Aug 15 12:57:07 2011 +0800

    xen-blkback: fixed indentation and comments
    
    This patch fixes belows:
    
    1. Fix code style issue.
    2. Fix incorrect functions name in comments.
    
    Signed-off-by: Joe Jin <joe.jin@oracle.com>
    Cc: Jens Axboe <jaxboe@fusionio.com>
    Cc: Ian Campbell <Ian.Campbell@eu.citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/drivers/block/xen-blkback/common.h b/drivers/block/xen-blkback/common.h
index bfb532ea5b1b..1b1bc4458685 100644
--- a/drivers/block/xen-blkback/common.h
+++ b/drivers/block/xen-blkback/common.h
@@ -46,7 +46,7 @@
 
 #define DRV_PFX "xen-blkback:"
 #define DPRINTK(fmt, args...)				\
-	pr_debug(DRV_PFX "(%s:%d) " fmt ".\n",	\
+	pr_debug(DRV_PFX "(%s:%d) " fmt ".\n",		\
 		 __func__, __LINE__, ##args)
 
 

commit b3cb0d6adc4bbc70b5e37e49a6068e973545ead7
Author: Li Dongyang <lidongyang@novell.com>
Date:   Thu Sep 1 18:39:10 2011 +0800

    xen-blkback: Implement discard requests ('feature-discard')
    
    ..aka ATA TRIM/SCSI UNMAP command to be passed through the frontend
    and used as appropiately by the backend. We also advertise
    certain granulity parameters to the frontend so it can plug them in.
    If the backend is a realy device - we just end up using
    'blkdev_issue_discard' while for loopback devices - we just punch
    a hole in the image file.
    
    Signed-off-by: Li Dongyang <lidongyang@novell.com>
    [v1: Fixed up pr_debug and commit description]
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/drivers/block/xen-blkback/common.h b/drivers/block/xen-blkback/common.h
index 9e40b283a468..bfb532ea5b1b 100644
--- a/drivers/block/xen-blkback/common.h
+++ b/drivers/block/xen-blkback/common.h
@@ -63,13 +63,26 @@ struct blkif_common_response {
 
 /* i386 protocol version */
 #pragma pack(push, 4)
+
+struct blkif_x86_32_request_rw {
+	blkif_sector_t sector_number;/* start sector idx on disk (r/w only)  */
+	struct blkif_request_segment seg[BLKIF_MAX_SEGMENTS_PER_REQUEST];
+};
+
+struct blkif_x86_32_request_discard {
+	blkif_sector_t sector_number;/* start sector idx on disk (r/w only)  */
+	uint64_t nr_sectors;
+};
+
 struct blkif_x86_32_request {
 	uint8_t        operation;    /* BLKIF_OP_???                         */
 	uint8_t        nr_segments;  /* number of segments                   */
 	blkif_vdev_t   handle;       /* only for read/write requests         */
 	uint64_t       id;           /* private guest value, echoed in resp  */
-	blkif_sector_t sector_number;/* start sector idx on disk (r/w only)  */
-	struct blkif_request_segment seg[BLKIF_MAX_SEGMENTS_PER_REQUEST];
+	union {
+		struct blkif_x86_32_request_rw rw;
+		struct blkif_x86_32_request_discard discard;
+	} u;
 };
 struct blkif_x86_32_response {
 	uint64_t        id;              /* copied from request */
@@ -79,13 +92,26 @@ struct blkif_x86_32_response {
 #pragma pack(pop)
 
 /* x86_64 protocol version */
+
+struct blkif_x86_64_request_rw {
+	blkif_sector_t sector_number;/* start sector idx on disk (r/w only)  */
+	struct blkif_request_segment seg[BLKIF_MAX_SEGMENTS_PER_REQUEST];
+};
+
+struct blkif_x86_64_request_discard {
+	blkif_sector_t sector_number;/* start sector idx on disk (r/w only)  */
+	uint64_t nr_sectors;
+};
+
 struct blkif_x86_64_request {
 	uint8_t        operation;    /* BLKIF_OP_???                         */
 	uint8_t        nr_segments;  /* number of segments                   */
 	blkif_vdev_t   handle;       /* only for read/write requests         */
 	uint64_t       __attribute__((__aligned__(8))) id;
-	blkif_sector_t sector_number;/* start sector idx on disk (r/w only)  */
-	struct blkif_request_segment seg[BLKIF_MAX_SEGMENTS_PER_REQUEST];
+	union {
+		struct blkif_x86_64_request_rw rw;
+		struct blkif_x86_64_request_discard discard;
+	} u;
 };
 struct blkif_x86_64_response {
 	uint64_t       __attribute__((__aligned__(8))) id;
@@ -113,6 +139,11 @@ enum blkif_protocol {
 	BLKIF_PROTOCOL_X86_64 = 3,
 };
 
+enum blkif_backend_type {
+	BLKIF_BACKEND_PHY  = 1,
+	BLKIF_BACKEND_FILE = 2,
+};
+
 struct xen_vbd {
 	/* What the domain refers to this vbd as. */
 	blkif_vdev_t		handle;
@@ -138,6 +169,7 @@ struct xen_blkif {
 	unsigned int		irq;
 	/* Comms information. */
 	enum blkif_protocol	blk_protocol;
+	enum blkif_backend_type blk_backend_type;
 	union blkif_back_rings	blk_rings;
 	struct vm_struct	*blk_ring_area;
 	/* The VBD attached to this interface. */
@@ -159,6 +191,7 @@ struct xen_blkif {
 	int			st_wr_req;
 	int			st_oo_req;
 	int			st_f_req;
+	int			st_ds_req;
 	int			st_rd_sect;
 	int			st_wr_sect;
 
@@ -182,7 +215,7 @@ struct xen_blkif {
 
 struct phys_req {
 	unsigned short		dev;
-	unsigned short		nr_sects;
+	blkif_sector_t		nr_sects;
 	struct block_device	*bdev;
 	blkif_sector_t		sector_number;
 };
@@ -206,12 +239,25 @@ static inline void blkif_get_x86_32_req(struct blkif_request *dst,
 	dst->nr_segments = src->nr_segments;
 	dst->handle = src->handle;
 	dst->id = src->id;
-	dst->u.rw.sector_number = src->sector_number;
-	barrier();
-	if (n > dst->nr_segments)
-		n = dst->nr_segments;
-	for (i = 0; i < n; i++)
-		dst->u.rw.seg[i] = src->seg[i];
+	switch (src->operation) {
+	case BLKIF_OP_READ:
+	case BLKIF_OP_WRITE:
+	case BLKIF_OP_WRITE_BARRIER:
+	case BLKIF_OP_FLUSH_DISKCACHE:
+		dst->u.rw.sector_number = src->u.rw.sector_number;
+		barrier();
+		if (n > dst->nr_segments)
+			n = dst->nr_segments;
+		for (i = 0; i < n; i++)
+			dst->u.rw.seg[i] = src->u.rw.seg[i];
+		break;
+	case BLKIF_OP_DISCARD:
+		dst->u.discard.sector_number = src->u.discard.sector_number;
+		dst->u.discard.nr_sectors = src->u.discard.nr_sectors;
+		break;
+	default:
+		break;
+	}
 }
 
 static inline void blkif_get_x86_64_req(struct blkif_request *dst,
@@ -222,12 +268,25 @@ static inline void blkif_get_x86_64_req(struct blkif_request *dst,
 	dst->nr_segments = src->nr_segments;
 	dst->handle = src->handle;
 	dst->id = src->id;
-	dst->u.rw.sector_number = src->sector_number;
-	barrier();
-	if (n > dst->nr_segments)
-		n = dst->nr_segments;
-	for (i = 0; i < n; i++)
-		dst->u.rw.seg[i] = src->seg[i];
+	switch (src->operation) {
+	case BLKIF_OP_READ:
+	case BLKIF_OP_WRITE:
+	case BLKIF_OP_WRITE_BARRIER:
+	case BLKIF_OP_FLUSH_DISKCACHE:
+		dst->u.rw.sector_number = src->u.rw.sector_number;
+		barrier();
+		if (n > dst->nr_segments)
+			n = dst->nr_segments;
+		for (i = 0; i < n; i++)
+			dst->u.rw.seg[i] = src->u.rw.seg[i];
+		break;
+	case BLKIF_OP_DISCARD:
+		dst->u.discard.sector_number = src->u.discard.sector_number;
+		dst->u.discard.nr_sectors = src->u.discard.nr_sectors;
+		break;
+	default:
+		break;
+	}
 }
 
 #endif /* __XEN_BLKIF__BACKEND__COMMON_H__ */

commit e5de063016ce838aff08683ce38ac40211c247d9
Author: Jesper Juhl <jj@chaosbits.net>
Date:   Mon Aug 1 23:16:06 2011 +0200

    Remove unneeded version.h includes from drivers/block/
    
    It was pointed out by 'make versioncheck' that some includes of
    linux/version.h are not needed in drivers/block/.
    This patch removes them.
    
    Signed-off-by: Jesper Juhl <jj@chaosbits.net>
    Signed-off-by: Jiri Kosina <jkosina@suse.cz>

diff --git a/drivers/block/xen-blkback/common.h b/drivers/block/xen-blkback/common.h
index 9e40b283a468..0d04c7fb7cee 100644
--- a/drivers/block/xen-blkback/common.h
+++ b/drivers/block/xen-blkback/common.h
@@ -27,7 +27,6 @@
 #ifndef __XEN_BLKIF__BACKEND__COMMON_H__
 #define __XEN_BLKIF__BACKEND__COMMON_H__
 
-#include <linux/version.h>
 #include <linux/module.h>
 #include <linux/interrupt.h>
 #include <linux/slab.h>

commit 1bc05b0ae6448b20d46076899e0cc12ad999e50e
Author: Joe Jin <joe.jin@oracle.com>
Date:   Mon Aug 15 12:57:07 2011 +0800

    xen-blkback: fixed indentation and comments
    
    This patch fixes belows:
    
    1. Fix code style issue.
    2. Fix incorrect functions name in comments.
    
    Signed-off-by: Joe Jin <joe.jin@oracle.com>
    Cc: Jens Axboe <jaxboe@fusionio.com>
    Cc: Ian Campbell <Ian.Campbell@eu.citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/drivers/block/xen-blkback/common.h b/drivers/block/xen-blkback/common.h
index 9e40b283a468..00c57c90e2d6 100644
--- a/drivers/block/xen-blkback/common.h
+++ b/drivers/block/xen-blkback/common.h
@@ -46,7 +46,7 @@
 
 #define DRV_PFX "xen-blkback:"
 #define DPRINTK(fmt, args...)				\
-	pr_debug(DRV_PFX "(%s:%d) " fmt ".\n",	\
+	pr_debug(DRV_PFX "(%s:%d) " fmt ".\n",		\
 		 __func__, __LINE__, ##args)
 
 

commit 5a577e38724226e06337bc8361f492b6bb76b9a5
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Thu May 12 16:58:21 2011 -0400

    xen/blkback: Add the prefix XEN in the common.h.
    
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/drivers/block/xen-blkback/common.h b/drivers/block/xen-blkback/common.h
index 1e4ccdeadef3..9e40b283a468 100644
--- a/drivers/block/xen-blkback/common.h
+++ b/drivers/block/xen-blkback/common.h
@@ -24,8 +24,8 @@
  * IN THE SOFTWARE.
  */
 
-#ifndef __BLKIF__BACKEND__COMMON_H__
-#define __BLKIF__BACKEND__COMMON_H__
+#ifndef __XEN_BLKIF__BACKEND__COMMON_H__
+#define __XEN_BLKIF__BACKEND__COMMON_H__
 
 #include <linux/version.h>
 #include <linux/module.h>
@@ -230,4 +230,4 @@ static inline void blkif_get_x86_64_req(struct blkif_request *dst,
 		dst->u.rw.seg[i] = src->seg[i];
 }
 
-#endif /* __BLKIF__BACKEND__COMMON_H__ */
+#endif /* __XEN_BLKIF__BACKEND__COMMON_H__ */

commit 3d814731ba67f9514bdf380c1b95dd852ac82a2f
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Thu May 12 16:53:56 2011 -0400

    xen/blkback: Prefix 'vbd' with 'xen' in structs and functions.
    
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/drivers/block/xen-blkback/common.h b/drivers/block/xen-blkback/common.h
index 722c048663d6..1e4ccdeadef3 100644
--- a/drivers/block/xen-blkback/common.h
+++ b/drivers/block/xen-blkback/common.h
@@ -113,7 +113,7 @@ enum blkif_protocol {
 	BLKIF_PROTOCOL_X86_64 = 3,
 };
 
-struct vbd {
+struct xen_vbd {
 	/* What the domain refers to this vbd as. */
 	blkif_vdev_t		handle;
 	/* Non-zero -> read-only */
@@ -141,7 +141,7 @@ struct xen_blkif {
 	union blkif_back_rings	blk_rings;
 	struct vm_struct	*blk_ring_area;
 	/* The VBD attached to this interface. */
-	struct vbd		vbd;
+	struct xen_vbd		vbd;
 	/* Back pointer to the backend_info. */
 	struct backend_info	*be;
 	/* Private fields. */

commit 30fd150202fb2d08a62f9c2966a4b1fcf2e861e7
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Thu May 12 16:47:48 2011 -0400

    xen/blkback: Change structure name blkif_st to xen_blkif.
    
    No need for that '_st' and xen_blkif is more apt.
    
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/drivers/block/xen-blkback/common.h b/drivers/block/xen-blkback/common.h
index b0db6aabb5b8..722c048663d6 100644
--- a/drivers/block/xen-blkback/common.h
+++ b/drivers/block/xen-blkback/common.h
@@ -130,7 +130,7 @@ struct vbd {
 
 struct backend_info;
 
-struct blkif_st {
+struct xen_blkif {
 	/* Unique identifier for this interface. */
 	domid_t			domid;
 	unsigned int		handle;

commit 325a64860472765ecaeaa0081e9ddd67671183d4
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Thu May 12 16:37:04 2011 -0400

    xen/blkback: Remove the unused typedefs.
    
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/drivers/block/xen-blkback/common.h b/drivers/block/xen-blkback/common.h
index b8856fe2568f..b0db6aabb5b8 100644
--- a/drivers/block/xen-blkback/common.h
+++ b/drivers/block/xen-blkback/common.h
@@ -76,8 +76,6 @@ struct blkif_x86_32_response {
 	uint8_t         operation;       /* copied from request */
 	int16_t         status;          /* BLKIF_RSP_???       */
 };
-typedef struct blkif_x86_32_request blkif_x86_32_request_t;
-typedef struct blkif_x86_32_response blkif_x86_32_response_t;
 #pragma pack(pop)
 
 /* x86_64 protocol version */
@@ -94,8 +92,6 @@ struct blkif_x86_64_response {
 	uint8_t         operation;       /* copied from request */
 	int16_t         status;          /* BLKIF_RSP_???       */
 };
-typedef struct blkif_x86_64_request blkif_x86_64_request_t;
-typedef struct blkif_x86_64_response blkif_x86_64_response_t;
 
 DEFINE_RING_TYPES(blkif_common, struct blkif_common_request,
 		  struct blkif_common_response);

commit 452a6b2bb6de677acdd2ccb8b39cf6e8fe06f306
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Thu May 12 16:31:51 2011 -0400

    xen/blkback: Move include/xen/blkif.h into drivers/block/xen-blkback/common.h
    
    Not point of the blkif.h file. It is not used by the frontend.
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/drivers/block/xen-blkback/common.h b/drivers/block/xen-blkback/common.h
index 629546558a47..b8856fe2568f 100644
--- a/drivers/block/xen-blkback/common.h
+++ b/drivers/block/xen-blkback/common.h
@@ -38,15 +38,85 @@
 #include <asm/setup.h>
 #include <asm/pgalloc.h>
 #include <asm/hypervisor.h>
-#include <xen/blkif.h>
 #include <xen/grant_table.h>
 #include <xen/xenbus.h>
+#include <xen/interface/io/ring.h>
+#include <xen/interface/io/blkif.h>
+#include <xen/interface/io/protocols.h>
 
 #define DRV_PFX "xen-blkback:"
 #define DPRINTK(fmt, args...)				\
 	pr_debug(DRV_PFX "(%s:%d) " fmt ".\n",	\
 		 __func__, __LINE__, ##args)
 
+
+/* Not a real protocol.  Used to generate ring structs which contain
+ * the elements common to all protocols only.  This way we get a
+ * compiler-checkable way to use common struct elements, so we can
+ * avoid using switch(protocol) in a number of places.  */
+struct blkif_common_request {
+	char dummy;
+};
+struct blkif_common_response {
+	char dummy;
+};
+
+/* i386 protocol version */
+#pragma pack(push, 4)
+struct blkif_x86_32_request {
+	uint8_t        operation;    /* BLKIF_OP_???                         */
+	uint8_t        nr_segments;  /* number of segments                   */
+	blkif_vdev_t   handle;       /* only for read/write requests         */
+	uint64_t       id;           /* private guest value, echoed in resp  */
+	blkif_sector_t sector_number;/* start sector idx on disk (r/w only)  */
+	struct blkif_request_segment seg[BLKIF_MAX_SEGMENTS_PER_REQUEST];
+};
+struct blkif_x86_32_response {
+	uint64_t        id;              /* copied from request */
+	uint8_t         operation;       /* copied from request */
+	int16_t         status;          /* BLKIF_RSP_???       */
+};
+typedef struct blkif_x86_32_request blkif_x86_32_request_t;
+typedef struct blkif_x86_32_response blkif_x86_32_response_t;
+#pragma pack(pop)
+
+/* x86_64 protocol version */
+struct blkif_x86_64_request {
+	uint8_t        operation;    /* BLKIF_OP_???                         */
+	uint8_t        nr_segments;  /* number of segments                   */
+	blkif_vdev_t   handle;       /* only for read/write requests         */
+	uint64_t       __attribute__((__aligned__(8))) id;
+	blkif_sector_t sector_number;/* start sector idx on disk (r/w only)  */
+	struct blkif_request_segment seg[BLKIF_MAX_SEGMENTS_PER_REQUEST];
+};
+struct blkif_x86_64_response {
+	uint64_t       __attribute__((__aligned__(8))) id;
+	uint8_t         operation;       /* copied from request */
+	int16_t         status;          /* BLKIF_RSP_???       */
+};
+typedef struct blkif_x86_64_request blkif_x86_64_request_t;
+typedef struct blkif_x86_64_response blkif_x86_64_response_t;
+
+DEFINE_RING_TYPES(blkif_common, struct blkif_common_request,
+		  struct blkif_common_response);
+DEFINE_RING_TYPES(blkif_x86_32, struct blkif_x86_32_request,
+		  struct blkif_x86_32_response);
+DEFINE_RING_TYPES(blkif_x86_64, struct blkif_x86_64_request,
+		  struct blkif_x86_64_response);
+
+union blkif_back_rings {
+	struct blkif_back_ring        native;
+	struct blkif_common_back_ring common;
+	struct blkif_x86_32_back_ring x86_32;
+	struct blkif_x86_64_back_ring x86_64;
+};
+
+enum blkif_protocol {
+	BLKIF_PROTOCOL_NATIVE = 1,
+	BLKIF_PROTOCOL_X86_32 = 2,
+	BLKIF_PROTOCOL_X86_64 = 3,
+};
+
 struct vbd {
 	/* What the domain refers to this vbd as. */
 	blkif_vdev_t		handle;

commit b0f801273f7359a7d91fc94f5c6bf216bc17aaa1
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Thu May 12 16:23:06 2011 -0400

    xen/blkback: Fixing some more of the cleanpatch.pl warnings.
    
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/drivers/block/xen-blkback/common.h b/drivers/block/xen-blkback/common.h
index 647d974da392..629546558a47 100644
--- a/drivers/block/xen-blkback/common.h
+++ b/drivers/block/xen-blkback/common.h
@@ -132,7 +132,7 @@ int xen_blkbk_flush_diskcache(struct xenbus_transaction xbt,
 
 struct xenbus_device *xen_blkbk_xenbus(struct backend_info *be);
 
-static void inline blkif_get_x86_32_req(struct blkif_request *dst,
+static inline void blkif_get_x86_32_req(struct blkif_request *dst,
 					struct blkif_x86_32_request *src)
 {
 	int i, n = BLKIF_MAX_SEGMENTS_PER_REQUEST;
@@ -148,7 +148,7 @@ static void inline blkif_get_x86_32_req(struct blkif_request *dst,
 		dst->u.rw.seg[i] = src->seg[i];
 }
 
-static void inline blkif_get_x86_64_req(struct blkif_request *dst,
+static inline void blkif_get_x86_64_req(struct blkif_request *dst,
 					struct blkif_x86_64_request *src)
 {
 	int i, n = BLKIF_MAX_SEGMENTS_PER_REQUEST;

commit 68c88dd7d3caf1737112238fbe91cccd8e7a69fc
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Wed May 11 16:23:39 2011 -0400

    xen/blkback: Move blkif_get_x86_[32|64]_req to common.h in block/xen-blkback dir.
    
    From the blkif.h header, which was exposed to the frontend.
    
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/drivers/block/xen-blkback/common.h b/drivers/block/xen-blkback/common.h
index da96e3eaa641..647d974da392 100644
--- a/drivers/block/xen-blkback/common.h
+++ b/drivers/block/xen-blkback/common.h
@@ -132,4 +132,36 @@ int xen_blkbk_flush_diskcache(struct xenbus_transaction xbt,
 
 struct xenbus_device *xen_blkbk_xenbus(struct backend_info *be);
 
+static void inline blkif_get_x86_32_req(struct blkif_request *dst,
+					struct blkif_x86_32_request *src)
+{
+	int i, n = BLKIF_MAX_SEGMENTS_PER_REQUEST;
+	dst->operation = src->operation;
+	dst->nr_segments = src->nr_segments;
+	dst->handle = src->handle;
+	dst->id = src->id;
+	dst->u.rw.sector_number = src->sector_number;
+	barrier();
+	if (n > dst->nr_segments)
+		n = dst->nr_segments;
+	for (i = 0; i < n; i++)
+		dst->u.rw.seg[i] = src->seg[i];
+}
+
+static void inline blkif_get_x86_64_req(struct blkif_request *dst,
+					struct blkif_x86_64_request *src)
+{
+	int i, n = BLKIF_MAX_SEGMENTS_PER_REQUEST;
+	dst->operation = src->operation;
+	dst->nr_segments = src->nr_segments;
+	dst->handle = src->handle;
+	dst->id = src->id;
+	dst->u.rw.sector_number = src->sector_number;
+	barrier();
+	if (n > dst->nr_segments)
+		n = dst->nr_segments;
+	for (i = 0; i < n; i++)
+		dst->u.rw.seg[i] = src->seg[i];
+}
+
 #endif /* __BLKIF__BACKEND__COMMON_H__ */

commit 22b20f2dffd09edd66127f2022c26d0039bad88e
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Thu May 12 16:43:12 2011 -0400

    xen/blkback: Use the DRV_PFX in the pr_.. macros.
    
    To make it easier to read.
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/drivers/block/xen-blkback/common.h b/drivers/block/xen-blkback/common.h
index 46e5d0630440..da96e3eaa641 100644
--- a/drivers/block/xen-blkback/common.h
+++ b/drivers/block/xen-blkback/common.h
@@ -42,8 +42,9 @@
 #include <xen/grant_table.h>
 #include <xen/xenbus.h>
 
+#define DRV_PFX "xen-blkback:"
 #define DPRINTK(fmt, args...)				\
-	pr_debug("xen-blkback: (%s:%d) " fmt ".\n",	\
+	pr_debug(DRV_PFX "(%s:%d) " fmt ".\n",	\
 		 __func__, __LINE__, ##args)
 
 struct vbd {

commit 1afbd730a33c6e4ca780a70351e8929dd4c40636
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Wed May 11 16:15:24 2011 -0400

    xen/blkback: Make the DPRINTK uniform.
    
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/drivers/block/xen-blkback/common.h b/drivers/block/xen-blkback/common.h
index e37dcf7f6b8e..46e5d0630440 100644
--- a/drivers/block/xen-blkback/common.h
+++ b/drivers/block/xen-blkback/common.h
@@ -42,9 +42,9 @@
 #include <xen/grant_table.h>
 #include <xen/xenbus.h>
 
-#define DPRINTK(_f, _a...)			\
-	pr_debug("(file=%s, line=%d) " _f,	\
-		 __FILE__ , __LINE__ , ## _a)
+#define DPRINTK(fmt, args...)				\
+	pr_debug("xen-blkback: (%s:%d) " fmt ".\n",	\
+		 __func__, __LINE__, ##args)
 
 struct vbd {
 	/* What the domain refers to this vbd as. */

commit 01f37f2d53e14a05b7fc3601d182f31ac3b35847
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Wed May 11 15:57:09 2011 -0400

    xen/blkback: Fixed up comments and converted spaces to tabs.
    
    Suggested-by: Ian Campbell <Ian.Campbell@eu.citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/drivers/block/xen-blkback/common.h b/drivers/block/xen-blkback/common.h
index af93837e1295..e37dcf7f6b8e 100644
--- a/drivers/block/xen-blkback/common.h
+++ b/drivers/block/xen-blkback/common.h
@@ -47,53 +47,58 @@
 		 __FILE__ , __LINE__ , ## _a)
 
 struct vbd {
-	blkif_vdev_t   handle;      /* what the domain refers to this vbd as */
-	unsigned char  readonly;    /* Non-zero -> read-only */
-	unsigned char  type;        /* VDISK_xxx */
-	u32            pdevice;     /* phys device that this vbd maps to */
-	struct block_device *bdev;
-	sector_t       size;        /* Cached size parameter */
-	bool           flush_support;
+	/* What the domain refers to this vbd as. */
+	blkif_vdev_t		handle;
+	/* Non-zero -> read-only */
+	unsigned char		readonly;
+	/* VDISK_xxx */
+	unsigned char		type;
+	/* phys device that this vbd maps to. */
+	u32			pdevice;
+	struct block_device	*bdev;
+	/* Cached size parameter. */
+	sector_t		size;
+	bool			flush_support;
 };
 
 struct backend_info;
 
 struct blkif_st {
 	/* Unique identifier for this interface. */
-	domid_t           domid;
-	unsigned int      handle;
+	domid_t			domid;
+	unsigned int		handle;
 	/* Physical parameters of the comms window. */
-	unsigned int      irq;
+	unsigned int		irq;
 	/* Comms information. */
-	enum blkif_protocol blk_protocol;
-	union blkif_back_rings blk_rings;
-	struct vm_struct *blk_ring_area;
+	enum blkif_protocol	blk_protocol;
+	union blkif_back_rings	blk_rings;
+	struct vm_struct	*blk_ring_area;
 	/* The VBD attached to this interface. */
-	struct vbd        vbd;
+	struct vbd		vbd;
 	/* Back pointer to the backend_info. */
-	struct backend_info *be;
+	struct backend_info	*be;
 	/* Private fields. */
-	spinlock_t       blk_ring_lock;
-	atomic_t         refcnt;
+	spinlock_t		blk_ring_lock;
+	atomic_t		refcnt;
 
-	wait_queue_head_t   wq;
+	wait_queue_head_t	wq;
 	/* One thread per one blkif. */
-	struct task_struct  *xenblkd;
-	unsigned int        waiting_reqs;
+	struct task_struct	*xenblkd;
+	unsigned int		waiting_reqs;
 
 	/* statistics */
-	unsigned long       st_print;
-	int                 st_rd_req;
-	int                 st_wr_req;
-	int                 st_oo_req;
-	int                 st_f_req;
-	int                 st_rd_sect;
-	int                 st_wr_sect;
-
-	wait_queue_head_t waiting_to_free;
-
-	grant_handle_t shmem_handle;
-	grant_ref_t    shmem_ref;
+	unsigned long		st_print;
+	int			st_rd_req;
+	int			st_wr_req;
+	int			st_oo_req;
+	int			st_f_req;
+	int			st_rd_sect;
+	int			st_wr_sect;
+
+	wait_queue_head_t	waiting_to_free;
+
+	grant_handle_t		shmem_handle;
+	grant_ref_t		shmem_ref;
 };
 
 
@@ -109,10 +114,10 @@ struct blkif_st {
 	} while (0)
 
 struct phys_req {
-	unsigned short       dev;
-	unsigned short       nr_sects;
-	struct block_device *bdev;
-	blkif_sector_t       sector_number;
+	unsigned short		dev;
+	unsigned short		nr_sects;
+	struct block_device	*bdev;
+	blkif_sector_t		sector_number;
 };
 int xen_blkif_interface_init(void);
 

commit 24f567f952aa308c3352f3340b9d296fc72bd066
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Wed May 4 17:07:27 2011 -0400

    xen/blkback: Add support for BLKIF_OP_FLUSH_DISKCACHE and drop BLKIF_OP_WRITE_BARRIER.
    
    We drop the support for 'feature-barrier' and add in the support
    for the 'feature-flush-cache' if the real backend storage supports
    flushing.
    
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/drivers/block/xen-blkback/common.h b/drivers/block/xen-blkback/common.h
index 16af388268e7..af93837e1295 100644
--- a/drivers/block/xen-blkback/common.h
+++ b/drivers/block/xen-blkback/common.h
@@ -53,6 +53,7 @@ struct vbd {
 	u32            pdevice;     /* phys device that this vbd maps to */
 	struct block_device *bdev;
 	sector_t       size;        /* Cached size parameter */
+	bool           flush_support;
 };
 
 struct backend_info;
@@ -85,7 +86,7 @@ struct blkif_st {
 	int                 st_rd_req;
 	int                 st_wr_req;
 	int                 st_oo_req;
-	int                 st_br_req;
+	int                 st_f_req;
 	int                 st_rd_sect;
 	int                 st_wr_sect;
 
@@ -120,8 +121,8 @@ int xen_blkif_xenbus_init(void);
 irqreturn_t xen_blkif_be_int(int irq, void *dev_id);
 int xen_blkif_schedule(void *arg);
 
-int xen_blkbk_barrier(struct xenbus_transaction xbt,
-			struct backend_info *be, int state);
+int xen_blkbk_flush_diskcache(struct xenbus_transaction xbt,
+			      struct backend_info *be, int state);
 
 struct xenbus_device *xen_blkbk_xenbus(struct backend_info *be);
 

commit 8b6bf747d70e5bac1a34c8fd773230e1cfdd7546
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Wed Apr 20 11:50:43 2011 -0400

    xen/blkback: Prefix exposed functions with xen_
    
    And also shorten the name if it has blkback to blkbk.
    
    This results in the symbol table (if compiled in the kernel)
    to be much shorter, prettier,  and also easier to search for.
    
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/drivers/block/xen-blkback/common.h b/drivers/block/xen-blkback/common.h
index 4b5acb3e8b24..16af388268e7 100644
--- a/drivers/block/xen-blkback/common.h
+++ b/drivers/block/xen-blkback/common.h
@@ -100,8 +100,8 @@ struct blkif_st {
 			 (_v)->bdev->bd_part->nr_sects : \
 			  get_capacity((_v)->bdev->bd_disk))
 
-#define blkif_get(_b) (atomic_inc(&(_b)->refcnt))
-#define blkif_put(_b)					\
+#define xen_blkif_get(_b) (atomic_inc(&(_b)->refcnt))
+#define xen_blkif_put(_b)				\
 	do {						\
 		if (atomic_dec_and_test(&(_b)->refcnt))	\
 			wake_up(&(_b)->waiting_to_free);\
@@ -113,16 +113,16 @@ struct phys_req {
 	struct block_device *bdev;
 	blkif_sector_t       sector_number;
 };
-int blkif_interface_init(void);
+int xen_blkif_interface_init(void);
 
-int blkif_xenbus_init(void);
+int xen_blkif_xenbus_init(void);
 
-irqreturn_t blkif_be_int(int irq, void *dev_id);
-int blkif_schedule(void *arg);
+irqreturn_t xen_blkif_be_int(int irq, void *dev_id);
+int xen_blkif_schedule(void *arg);
 
-int blkback_barrier(struct xenbus_transaction xbt,
-		    struct backend_info *be, int state);
+int xen_blkbk_barrier(struct xenbus_transaction xbt,
+			struct backend_info *be, int state);
 
-struct xenbus_device *blkback_xenbus(struct backend_info *be);
+struct xenbus_device *xen_blkbk_xenbus(struct backend_info *be);
 
 #endif /* __BLKIF__BACKEND__COMMON_H__ */

commit 42c7841d171a2fe32005738dfebd724a90921496
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Wed Apr 20 11:21:43 2011 -0400

    xen-blkback: Inline some of the functions that were moved from vbd/interface.c
    
    Shuffling code around.
    
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/drivers/block/xen-blkback/common.h b/drivers/block/xen-blkback/common.h
index 6257c1106591..4b5acb3e8b24 100644
--- a/drivers/block/xen-blkback/common.h
+++ b/drivers/block/xen-blkback/common.h
@@ -95,12 +95,10 @@ struct blkif_st {
 	grant_ref_t    shmem_ref;
 };
 
-struct blkif_st *blkif_alloc(domid_t domid);
-void blkif_disconnect(struct blkif_st *blkif);
-void blkif_free(struct blkif_st *blkif);
-int blkif_map(struct blkif_st *blkif, unsigned long shared_page,
-	      unsigned int evtchn);
-void vbd_resize(struct blkif_st *blkif);
+
+#define vbd_sz(_v)	((_v)->bdev->bd_part ? \
+			 (_v)->bdev->bd_part->nr_sects : \
+			  get_capacity((_v)->bdev->bd_disk))
 
 #define blkif_get(_b) (atomic_inc(&(_b)->refcnt))
 #define blkif_put(_b)					\
@@ -109,24 +107,12 @@ void vbd_resize(struct blkif_st *blkif);
 			wake_up(&(_b)->waiting_to_free);\
 	} while (0)
 
-/* Create a vbd. */
-int vbd_create(struct blkif_st *blkif, blkif_vdev_t vdevice, unsigned major,
-	       unsigned minor, int readonly, int cdrom);
-void vbd_free(struct vbd *vbd);
-
-unsigned long long vbd_size(struct vbd *vbd);
-unsigned int vbd_info(struct vbd *vbd);
-unsigned long vbd_secsize(struct vbd *vbd);
-
 struct phys_req {
 	unsigned short       dev;
 	unsigned short       nr_sects;
 	struct block_device *bdev;
 	blkif_sector_t       sector_number;
 };
-
-int vbd_translate(struct phys_req *req, struct blkif_st *blkif, int operation);
-
 int blkif_interface_init(void);
 
 int blkif_xenbus_init(void);

commit dfc07b13dcacefda6ebdea14584ed8724dc980ef
Author: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
Date:   Mon Apr 18 14:24:23 2011 -0400

    xen/blkback: Move it from drivers/xen to drivers/block
    
    .. and modify the Makefile and Kconfig files appropriately.
    
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/drivers/block/xen-blkback/common.h b/drivers/block/xen-blkback/common.h
new file mode 100644
index 000000000000..6257c1106591
--- /dev/null
+++ b/drivers/block/xen-blkback/common.h
@@ -0,0 +1,142 @@
+/*
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License version 2
+ * as published by the Free Software Foundation; or, when distributed
+ * separately from the Linux kernel or incorporated into other
+ * software packages, subject to the following license:
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this source file (the "Software"), to deal in the Software without
+ * restriction, including without limitation the rights to use, copy, modify,
+ * merge, publish, distribute, sublicense, and/or sell copies of the Software,
+ * and to permit persons to whom the Software is furnished to do so, subject to
+ * the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
+ * IN THE SOFTWARE.
+ */
+
+#ifndef __BLKIF__BACKEND__COMMON_H__
+#define __BLKIF__BACKEND__COMMON_H__
+
+#include <linux/version.h>
+#include <linux/module.h>
+#include <linux/interrupt.h>
+#include <linux/slab.h>
+#include <linux/blkdev.h>
+#include <linux/vmalloc.h>
+#include <linux/wait.h>
+#include <linux/io.h>
+#include <asm/setup.h>
+#include <asm/pgalloc.h>
+#include <asm/hypervisor.h>
+#include <xen/blkif.h>
+#include <xen/grant_table.h>
+#include <xen/xenbus.h>
+
+#define DPRINTK(_f, _a...)			\
+	pr_debug("(file=%s, line=%d) " _f,	\
+		 __FILE__ , __LINE__ , ## _a)
+
+struct vbd {
+	blkif_vdev_t   handle;      /* what the domain refers to this vbd as */
+	unsigned char  readonly;    /* Non-zero -> read-only */
+	unsigned char  type;        /* VDISK_xxx */
+	u32            pdevice;     /* phys device that this vbd maps to */
+	struct block_device *bdev;
+	sector_t       size;        /* Cached size parameter */
+};
+
+struct backend_info;
+
+struct blkif_st {
+	/* Unique identifier for this interface. */
+	domid_t           domid;
+	unsigned int      handle;
+	/* Physical parameters of the comms window. */
+	unsigned int      irq;
+	/* Comms information. */
+	enum blkif_protocol blk_protocol;
+	union blkif_back_rings blk_rings;
+	struct vm_struct *blk_ring_area;
+	/* The VBD attached to this interface. */
+	struct vbd        vbd;
+	/* Back pointer to the backend_info. */
+	struct backend_info *be;
+	/* Private fields. */
+	spinlock_t       blk_ring_lock;
+	atomic_t         refcnt;
+
+	wait_queue_head_t   wq;
+	/* One thread per one blkif. */
+	struct task_struct  *xenblkd;
+	unsigned int        waiting_reqs;
+
+	/* statistics */
+	unsigned long       st_print;
+	int                 st_rd_req;
+	int                 st_wr_req;
+	int                 st_oo_req;
+	int                 st_br_req;
+	int                 st_rd_sect;
+	int                 st_wr_sect;
+
+	wait_queue_head_t waiting_to_free;
+
+	grant_handle_t shmem_handle;
+	grant_ref_t    shmem_ref;
+};
+
+struct blkif_st *blkif_alloc(domid_t domid);
+void blkif_disconnect(struct blkif_st *blkif);
+void blkif_free(struct blkif_st *blkif);
+int blkif_map(struct blkif_st *blkif, unsigned long shared_page,
+	      unsigned int evtchn);
+void vbd_resize(struct blkif_st *blkif);
+
+#define blkif_get(_b) (atomic_inc(&(_b)->refcnt))
+#define blkif_put(_b)					\
+	do {						\
+		if (atomic_dec_and_test(&(_b)->refcnt))	\
+			wake_up(&(_b)->waiting_to_free);\
+	} while (0)
+
+/* Create a vbd. */
+int vbd_create(struct blkif_st *blkif, blkif_vdev_t vdevice, unsigned major,
+	       unsigned minor, int readonly, int cdrom);
+void vbd_free(struct vbd *vbd);
+
+unsigned long long vbd_size(struct vbd *vbd);
+unsigned int vbd_info(struct vbd *vbd);
+unsigned long vbd_secsize(struct vbd *vbd);
+
+struct phys_req {
+	unsigned short       dev;
+	unsigned short       nr_sects;
+	struct block_device *bdev;
+	blkif_sector_t       sector_number;
+};
+
+int vbd_translate(struct phys_req *req, struct blkif_st *blkif, int operation);
+
+int blkif_interface_init(void);
+
+int blkif_xenbus_init(void);
+
+irqreturn_t blkif_be_int(int irq, void *dev_id);
+int blkif_schedule(void *arg);
+
+int blkback_barrier(struct xenbus_transaction xbt,
+		    struct backend_info *be, int state);
+
+struct xenbus_device *blkback_xenbus(struct backend_info *be);
+
+#endif /* __BLKIF__BACKEND__COMMON_H__ */
