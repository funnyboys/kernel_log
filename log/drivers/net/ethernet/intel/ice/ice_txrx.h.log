commit 22bef5e78f1193b664f59834361704cb22f9d5d7
Author: Jesse Brandeburg <jesse.brandeburg@intel.com>
Date:   Fri May 15 17:36:38 2020 -0700

    ice: fix signed vs unsigned comparisons
    
    Fix the remaining signed vs unsigned issues, which appear
    when compiling with -Werror=sign-compare.
    
    Many of these are because there is an external interface that is passing
    an int to us (which we can't change) but that we (rightfully) store
    and compare against as an unsigned in our data structures.
    
    Signed-off-by: Jesse Brandeburg <jesse.brandeburg@intel.com>
    Signed-off-by: Bruce Allan <bruce.w.allan@intel.com>
    Signed-off-by: Tony Nguyen <anthony.l.nguyen@intel.com>
    Tested-by: Andrew Bowers <andrewx.bowers@intel.com>
    Signed-off-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>

diff --git a/drivers/net/ethernet/intel/ice/ice_txrx.h b/drivers/net/ethernet/intel/ice/ice_txrx.h
index cf21b4fe928a..e70c4619edc3 100644
--- a/drivers/net/ethernet/intel/ice/ice_txrx.h
+++ b/drivers/net/ethernet/intel/ice/ice_txrx.h
@@ -38,7 +38,8 @@
  */
 #if (PAGE_SIZE < 8192)
 #define ICE_2K_TOO_SMALL_WITH_PADDING \
-((NET_SKB_PAD + ICE_RXBUF_1536) > SKB_WITH_OVERHEAD(ICE_RXBUF_2048))
+	((unsigned int)(NET_SKB_PAD + ICE_RXBUF_1536) > \
+			SKB_WITH_OVERHEAD(ICE_RXBUF_2048))
 
 /**
  * ice_compute_pad - compute the padding
@@ -107,8 +108,8 @@ static inline int ice_skb_pad(void)
 #define DESC_NEEDED (MAX_SKB_FRAGS + ICE_DESCS_FOR_CTX_DESC + \
 		     ICE_DESCS_PER_CACHE_LINE + ICE_DESCS_FOR_SKB_DATA_PTR)
 #define ICE_DESC_UNUSED(R)	\
-	((((R)->next_to_clean > (R)->next_to_use) ? 0 : (R)->count) + \
-	(R)->next_to_clean - (R)->next_to_use - 1)
+	(u16)((((R)->next_to_clean > (R)->next_to_use) ? 0 : (R)->count) + \
+	      (R)->next_to_clean - (R)->next_to_use - 1)
 
 #define ICE_TX_FLAGS_TSO	BIT(0)
 #define ICE_TX_FLAGS_HW_VLAN	BIT(1)

commit 2b1a7f741a95cd6ab0554a3942e215c3eeb8a108
Merge: 098205f3c688 c1e0883012a7
Author: David S. Miller <davem@davemloft.net>
Date:   Sat May 23 16:51:26 2020 -0700

    Merge branch '100GbE' of git://git.kernel.org/pub/scm/linux/kernel/git/jkirsher/next-queue
    
    Jeff Kirsher says:
    
    ====================
    100GbE Intel Wired LAN Driver Updates 2020-05-22
    
    This series contains updates to virtchnl and the ice driver.
    
    Geert Uytterhoeven fixes a data structure alignment issue in the
    virtchnl structures.
    
    Henry adds Flow Director support which allows for the redirection on
    ntuple rules over six patches.  Initially Henry adds the initial
    infrastructure for Flow Director, and then later adds IPv4 and IPv6
    support, as well as being able to display the ntuple rules.
    
    Bret add Accelerated Receive Flow Steering (aRFS) support which is used
    to steer receive flows to a specific queue.  Fixes a transmit timeout
    when the VF link transitions from up/down/up because the transmit and
    receive queue interrupts are not enabled as part of VF's link up.  Fixed
    an issue when the default VF LAN address is changed and after reset the
    PF will attempt to add the new MAC, which fails because it already
    exists. This causes the VF to be disabled completely until it is removed
    and enabled via sysfs.
    
    Anirudh (Ani) makes a fix where the ice driver needs to call set_mac_cfg
    to enable jumbo frames, so ensure it gets called during initialization
    and after reset.  Fix bad register reads during a register dump in
    ethtool by removing the bad registers.
    
    Paul fixes an issue where the receive Malicious Driver Detection (MDD)
    auto reset message was not being logged because it occurred after the VF
    reset.
    
    Victor adds a check for compatibility between the Dynamic Device
    Personalization (DDP) package and the NIC firmware to ensure that
    everything aligns.
    
    Jesse fixes a administrative queue string call with the appropriate
    error reporting variable.  Also fixed the loop variables that are
    comparing or assigning signed against unsigned values.
    ====================
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit cac2a27cd9ab1638e21df11ec583d2bc919a3ae3
Author: Henry Tieman <henry.w.tieman@intel.com>
Date:   Mon May 11 18:01:42 2020 -0700

    ice: Support IPv4 Flow Director filters
    
    Support the addition and deletion of IPv4 filters.
    
    Supported fields are: src-ip, dst-ip, src-port, and dst-port
    Supported flow-types are: tcp4, udp4, sctp4, ip4
    
    Example usage:
    
    ethtool -N eth0 flow-type tcp4 src-ip 192.168.0.55 dst-ip 172.16.0.55 \
    src-port 16 dst-port 12 action 32
    
    Signed-off-by: Henry Tieman <henry.w.tieman@intel.com>
    Signed-off-by: Tony Nguyen <anthony.l.nguyen@intel.com>
    Tested-by: Andrew Bowers <andrewx.bowers@intel.com>
    Signed-off-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>

diff --git a/drivers/net/ethernet/intel/ice/ice_txrx.h b/drivers/net/ethernet/intel/ice/ice_txrx.h
index 2209583c993e..7c4030caeea4 100644
--- a/drivers/net/ethernet/intel/ice/ice_txrx.h
+++ b/drivers/net/ethernet/intel/ice/ice_txrx.h
@@ -380,6 +380,9 @@ int ice_setup_rx_ring(struct ice_ring *rx_ring);
 void ice_free_tx_ring(struct ice_ring *tx_ring);
 void ice_free_rx_ring(struct ice_ring *rx_ring);
 int ice_napi_poll(struct napi_struct *napi, int budget);
+int
+ice_prgm_fdir_fltr(struct ice_vsi *vsi, struct ice_fltr_desc *fdir_desc,
+		   u8 *raw_packet);
 int ice_clean_rx_irq(struct ice_ring *rx_ring, int budget);
 void ice_clean_ctrl_tx_irq(struct ice_ring *tx_ring);
 #endif /* _ICE_TXRX_H_ */

commit 148beb612031255156d68b342170140524afb36e
Author: Henry Tieman <henry.w.tieman@intel.com>
Date:   Mon May 11 18:01:40 2020 -0700

    ice: Initialize Flow Director resources
    
    Flow Director allows for redirection based on ntuple rules. Rules are
    programmed using the ethtool set-ntuple interface. Supported actions are
    redirect to queue and drop.
    
    Setup the initial framework to process Flow Director filters. Create and
    allocate resources to manage and program filters to the hardware. Filters
    are processed via a sideband interface; a control VSI is created to manage
    communication and process requests through the sideband. Upon allocation of
    resources, update the hardware tables to accept perfect filters.
    
    Signed-off-by: Henry Tieman <henry.w.tieman@intel.com>
    Signed-off-by: Tony Nguyen <anthony.l.nguyen@intel.com>
    Tested-by: Andrew Bowers <andrewx.bowers@intel.com>
    Signed-off-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>

diff --git a/drivers/net/ethernet/intel/ice/ice_txrx.h b/drivers/net/ethernet/intel/ice/ice_txrx.h
index 025dd642cf28..2209583c993e 100644
--- a/drivers/net/ethernet/intel/ice/ice_txrx.h
+++ b/drivers/net/ethernet/intel/ice/ice_txrx.h
@@ -113,6 +113,10 @@ static inline int ice_skb_pad(void)
 #define ICE_TX_FLAGS_TSO	BIT(0)
 #define ICE_TX_FLAGS_HW_VLAN	BIT(1)
 #define ICE_TX_FLAGS_SW_VLAN	BIT(2)
+/* ICE_TX_FLAGS_DUMMY_PKT is used to mark dummy packets that should be
+ * freed instead of returned like skb packets.
+ */
+#define ICE_TX_FLAGS_DUMMY_PKT	BIT(3)
 #define ICE_TX_FLAGS_IPV4	BIT(5)
 #define ICE_TX_FLAGS_IPV6	BIT(6)
 #define ICE_TX_FLAGS_TUNNEL	BIT(7)
@@ -376,5 +380,6 @@ int ice_setup_rx_ring(struct ice_ring *rx_ring);
 void ice_free_tx_ring(struct ice_ring *tx_ring);
 void ice_free_rx_ring(struct ice_ring *rx_ring);
 int ice_napi_poll(struct napi_struct *napi, int budget);
-
+int ice_clean_rx_irq(struct ice_ring *rx_ring, int budget);
+void ice_clean_ctrl_tx_irq(struct ice_ring *tx_ring);
 #endif /* _ICE_TXRX_H_ */

commit a152b85984a03e7f83b9d8bcf908c29597d898fc
Merge: 1e6a70526640 a5dfaa2ab940
Author: David S. Miller <davem@davemloft.net>
Date:   Fri May 22 18:30:34 2020 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/bpf/bpf-next
    
    Daniel Borkmann says:
    
    ====================
    pull-request: bpf-next 2020-05-23
    
    The following pull-request contains BPF updates for your *net-next* tree.
    
    We've added 50 non-merge commits during the last 8 day(s) which contain
    a total of 109 files changed, 2776 insertions(+), 2887 deletions(-).
    
    The main changes are:
    
    1) Add a new AF_XDP buffer allocation API to the core in order to help
       lowering the bar for drivers adopting AF_XDP support. i40e, ice, ixgbe
       as well as mlx5 have been moved over to the new API and also gained a
       small improvement in performance, from Björn Töpel and Magnus Karlsson.
    
    2) Add getpeername()/getsockname() attach types for BPF sock_addr programs
       in order to allow for e.g. reverse translation of load-balancer backend
       to service address/port tuple from a connected peer, from Daniel Borkmann.
    
    3) Improve the BPF verifier is_branch_taken() logic to evaluate pointers
       being non-NULL, e.g. if after an initial test another non-NULL test on
       that pointer follows in a given path, then it can be pruned right away,
       from John Fastabend.
    
    4) Larger rework of BPF sockmap selftests to make output easier to understand
       and to reduce overall runtime as well as adding new BPF kTLS selftests
       that run in combination with sockmap, also from John Fastabend.
    
    5) Batch of misc updates to BPF selftests including fixing up test_align
       to match verifier output again and moving it under test_progs, allowing
       bpf_iter selftest to compile on machines with older vmlinux.h, and
       updating config options for lirc and v6 segment routing helpers, from
       Stanislav Fomichev, Andrii Nakryiko and Alan Maguire.
    
    6) Conversion of BPF tracing samples outdated internal BPF loader to use
       libbpf API instead, from Daniel T. Lee.
    
    7) Follow-up to BPF kernel test infrastructure in order to fix a flake in
       the XDP selftests, from Jesper Dangaard Brouer.
    
    8) Minor improvements to libbpf's internal hashmap implementation, from
       Ian Rogers.
    ====================
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit a4e82a81f57387803f950cc3d9d112bcc5553a3d
Author: Tony Nguyen <anthony.l.nguyen@intel.com>
Date:   Wed May 6 09:32:30 2020 -0700

    ice: Add support for tunnel offloads
    
    Create a boost TCAM entry for each tunnel port in order to get a tunnel
    PTYPE. Update netdev feature flags and implement the appropriate logic to
    get and set values for hardware offloads.
    
    Signed-off-by: Tony Nguyen <anthony.l.nguyen@intel.com>
    Signed-off-by: Henry Tieman <henry.w.tieman@intel.com>
    Tested-by: Andrew Bowers <andrewx.bowers@intel.com>
    Signed-off-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>

diff --git a/drivers/net/ethernet/intel/ice/ice_txrx.h b/drivers/net/ethernet/intel/ice/ice_txrx.h
index 7ee00a128663..025dd642cf28 100644
--- a/drivers/net/ethernet/intel/ice/ice_txrx.h
+++ b/drivers/net/ethernet/intel/ice/ice_txrx.h
@@ -113,6 +113,9 @@ static inline int ice_skb_pad(void)
 #define ICE_TX_FLAGS_TSO	BIT(0)
 #define ICE_TX_FLAGS_HW_VLAN	BIT(1)
 #define ICE_TX_FLAGS_SW_VLAN	BIT(2)
+#define ICE_TX_FLAGS_IPV4	BIT(5)
+#define ICE_TX_FLAGS_IPV6	BIT(6)
+#define ICE_TX_FLAGS_TUNNEL	BIT(7)
 #define ICE_TX_FLAGS_VLAN_M	0xffff0000
 #define ICE_TX_FLAGS_VLAN_PR_M	0xe0000000
 #define ICE_TX_FLAGS_VLAN_PR_S	29

commit 175fc430670be92c00317b9aada8bf39b47b717e
Author: Björn Töpel <bjorn.topel@intel.com>
Date:   Wed May 20 21:20:57 2020 +0200

    ice, xsk: Migrate to new MEM_TYPE_XSK_BUFF_POOL
    
    Remove MEM_TYPE_ZERO_COPY in favor of the new MEM_TYPE_XSK_BUFF_POOL
    APIs.
    
    v4->v5: Fixed "warning: Excess function parameter 'alloc' description
            in 'ice_alloc_rx_bufs_zc'" and "warning: Excess function
            parameter 'xdp' description in
            'ice_construct_skb_zc'". (Jakub)
    
    Signed-off-by: Maciej Fijalkowski <maciej.fijalkowski@intel.com>
    Signed-off-by: Björn Töpel <bjorn.topel@intel.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Cc: intel-wired-lan@lists.osuosl.org
    Link: https://lore.kernel.org/bpf/20200520192103.355233-10-bjorn.topel@gmail.com

diff --git a/drivers/net/ethernet/intel/ice/ice_txrx.h b/drivers/net/ethernet/intel/ice/ice_txrx.h
index 7ee00a128663..d0fd2173854f 100644
--- a/drivers/net/ethernet/intel/ice/ice_txrx.h
+++ b/drivers/net/ethernet/intel/ice/ice_txrx.h
@@ -155,17 +155,16 @@ struct ice_tx_offload_params {
 };
 
 struct ice_rx_buf {
-	struct sk_buff *skb;
-	dma_addr_t dma;
 	union {
 		struct {
+			struct sk_buff *skb;
+			dma_addr_t dma;
 			struct page *page;
 			unsigned int page_offset;
 			u16 pagecnt_bias;
 		};
 		struct {
-			void *addr;
-			u64 handle;
+			struct xdp_buff *xdp;
 		};
 	};
 };
@@ -289,7 +288,6 @@ struct ice_ring {
 	struct rcu_head rcu;		/* to avoid race on free */
 	struct bpf_prog *xdp_prog;
 	struct xdp_umem *xsk_umem;
-	struct zero_copy_allocator zca;
 	/* CL3 - 3rd cacheline starts here */
 	struct xdp_rxq_info xdp_rxq;
 	/* CLX - the below items are only accessed infrequently and should be

commit 840f8ad0aaf20044e2fb099095bbce27c02f58da
Author: Brett Creeley <brett.creeley@intel.com>
Date:   Thu Feb 13 13:31:23 2020 -0800

    ice: Don't reject odd values of usecs set by user
    
    Currently if a user sets an odd [tx|rx]-usecs value through ethtool,
    the request is denied because the hardware is set to have an ITR
    granularity of 2us. This caused poor customer experience. Fix this by
    aligning to a register allowed value, which results in rounding down.
    Also, print a once per ring container type message to be clear about
    our intentions.
    
    Also, change the ITR_TO_REG define to be the bitwise and of the ITR
    setting and the ICE_ITR_MASK. This makes the purpose of ITR_TO_REG more
    obvious.
    
    Signed-off-by: Brett Creeley <brett.creeley@intel.com>
    Tested-by: Andrew Bowers <andrewx.bowers@intel.com>
    Signed-off-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>

diff --git a/drivers/net/ethernet/intel/ice/ice_txrx.h b/drivers/net/ethernet/intel/ice/ice_txrx.h
index 14a1bf445889..7ee00a128663 100644
--- a/drivers/net/ethernet/intel/ice/ice_txrx.h
+++ b/drivers/net/ethernet/intel/ice/ice_txrx.h
@@ -222,7 +222,7 @@ enum ice_rx_dtype {
 #define ICE_ITR_GRAN_S		1	/* ITR granularity is always 2us */
 #define ICE_ITR_GRAN_US		BIT(ICE_ITR_GRAN_S)
 #define ICE_ITR_MASK		0x1FFE	/* ITR register value alignment mask */
-#define ITR_REG_ALIGN(setting)	__ALIGN_MASK(setting, ~ICE_ITR_MASK)
+#define ITR_REG_ALIGN(setting)	((setting) & ICE_ITR_MASK)
 
 #define ICE_ITR_ADAPTIVE_MIN_INC	0x0002
 #define ICE_ITR_ADAPTIVE_MIN_USECS	0x0002

commit 4ee656bba8013929bcc050bcebc39a47fe763ee9
Author: Tony Nguyen <anthony.l.nguyen@intel.com>
Date:   Thu Feb 6 01:20:13 2020 -0800

    ice: Trivial fixes
    
    This is a collection of trivial fixes including fixing whitespace, typos,
    function headers, reverse Christmas tree, etc.
    
    Signed-off-by: Tony Nguyen <anthony.l.nguyen@intel.com>
    Tested-by: Andrew Bowers <andrewx.bowers@intel.com>
    Signed-off-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>

diff --git a/drivers/net/ethernet/intel/ice/ice_txrx.h b/drivers/net/ethernet/intel/ice/ice_txrx.h
index a86270696df1..14a1bf445889 100644
--- a/drivers/net/ethernet/intel/ice/ice_txrx.h
+++ b/drivers/net/ethernet/intel/ice/ice_txrx.h
@@ -33,8 +33,8 @@
  * frame.
  *
  * Note: For cache line sizes 256 or larger this value is going to end
- *       up negative.  In these cases we should fall back to the legacy
- *       receive path.
+ *	 up negative.  In these cases we should fall back to the legacy
+ *	 receive path.
  */
 #if (PAGE_SIZE < 8192)
 #define ICE_2K_TOO_SMALL_WITH_PADDING \

commit 61dc79ced7aaff5420a4d2f7f63925c607fa9395
Author: Michal Swiatkowski <michal.swiatkowski@intel.com>
Date:   Thu Dec 12 03:12:58 2019 -0800

    ice: Restore interrupt throttle settings after VSI rebuild
    
    After each rebuild driver deallocates q_vectors, so the interrupt
    throttle rate (ITR) settings get lost.
    
    Create a function to save and restore ITR for each queue. If a user
    increases the number of queues, restore all the previous queue
    settings for each existing queue, and the additional queues will
    get the default setting.
    
    Signed-off-by: Michal Swiatkowski <michal.swiatkowski@intel.com>
    Tested-by: Andrew Bowers <andrewx.bowers@intel.com>
    Signed-off-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>

diff --git a/drivers/net/ethernet/intel/ice/ice_txrx.h b/drivers/net/ethernet/intel/ice/ice_txrx.h
index a84cc0e6dd27..a86270696df1 100644
--- a/drivers/net/ethernet/intel/ice/ice_txrx.h
+++ b/drivers/net/ethernet/intel/ice/ice_txrx.h
@@ -341,6 +341,12 @@ struct ice_ring_container {
 	u16 itr_setting;
 };
 
+struct ice_coalesce_stored {
+	u16 itr_tx;
+	u16 itr_rx;
+	u8 intrl;
+};
+
 /* iterator for handling rings in ring container */
 #define ice_for_each_ring(pos, head) \
 	for (pos = (head).ring; pos; pos = pos->next)

commit 59bb08080557589aaf577a99d329ccea38b55c95
Author: Maciej Fijalkowski <maciej.fijalkowski@intel.com>
Date:   Thu Oct 24 01:11:23 2019 -0700

    ice: introduce frame padding computation logic
    
    Take into account the underlying architecture specific settings and
    based on that calculate the possible padding that can be supplied.
    Typically, for x86 and standard MTU size we will end up with 192 bytes
    of headroom. This is the same behavior as our other drivers have and we
    can dedicate it for XDP purposes.
    
    Furthermore, introduce the Rx ring flag for indicating whether build_skb
    is used on particular. Based on that invoke the routines for padding
    calculation.
    
    Signed-off-by: Maciej Fijalkowski <maciej.fijalkowski@intel.com>
    Signed-off-by: Tony Nguyen <anthony.l.nguyen@intel.com>
    Tested-by: Andrew Bowers <andrewx.bowers@intel.com>
    Signed-off-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>

diff --git a/drivers/net/ethernet/intel/ice/ice_txrx.h b/drivers/net/ethernet/intel/ice/ice_txrx.h
index 6a6e3d2339ba..a84cc0e6dd27 100644
--- a/drivers/net/ethernet/intel/ice/ice_txrx.h
+++ b/drivers/net/ethernet/intel/ice/ice_txrx.h
@@ -26,6 +26,71 @@
 #define ICE_RX_BUF_WRITE	16	/* Must be power of 2 */
 #define ICE_MAX_TXQ_PER_TXQG	128
 
+/* Attempt to maximize the headroom available for incoming frames. We use a 2K
+ * buffer for MTUs <= 1500 and need 1536/1534 to store the data for the frame.
+ * This leaves us with 512 bytes of room.  From that we need to deduct the
+ * space needed for the shared info and the padding needed to IP align the
+ * frame.
+ *
+ * Note: For cache line sizes 256 or larger this value is going to end
+ *       up negative.  In these cases we should fall back to the legacy
+ *       receive path.
+ */
+#if (PAGE_SIZE < 8192)
+#define ICE_2K_TOO_SMALL_WITH_PADDING \
+((NET_SKB_PAD + ICE_RXBUF_1536) > SKB_WITH_OVERHEAD(ICE_RXBUF_2048))
+
+/**
+ * ice_compute_pad - compute the padding
+ * rx_buf_len: buffer length
+ *
+ * Figure out the size of half page based on given buffer length and
+ * then subtract the skb_shared_info followed by subtraction of the
+ * actual buffer length; this in turn results in the actual space that
+ * is left for padding usage
+ */
+static inline int ice_compute_pad(int rx_buf_len)
+{
+	int half_page_size;
+
+	half_page_size = ALIGN(rx_buf_len, PAGE_SIZE / 2);
+	return SKB_WITH_OVERHEAD(half_page_size) - rx_buf_len;
+}
+
+/**
+ * ice_skb_pad - determine the padding that we can supply
+ *
+ * Figure out the right Rx buffer size and based on that calculate the
+ * padding
+ */
+static inline int ice_skb_pad(void)
+{
+	int rx_buf_len;
+
+	/* If a 2K buffer cannot handle a standard Ethernet frame then
+	 * optimize padding for a 3K buffer instead of a 1.5K buffer.
+	 *
+	 * For a 3K buffer we need to add enough padding to allow for
+	 * tailroom due to NET_IP_ALIGN possibly shifting us out of
+	 * cache-line alignment.
+	 */
+	if (ICE_2K_TOO_SMALL_WITH_PADDING)
+		rx_buf_len = ICE_RXBUF_3072 + SKB_DATA_ALIGN(NET_IP_ALIGN);
+	else
+		rx_buf_len = ICE_RXBUF_1536;
+
+	/* if needed make room for NET_IP_ALIGN */
+	rx_buf_len -= NET_IP_ALIGN;
+
+	return ice_compute_pad(rx_buf_len);
+}
+
+#define ICE_SKB_PAD ice_skb_pad()
+#else
+#define ICE_2K_TOO_SMALL_WITH_PADDING false
+#define ICE_SKB_PAD (NET_SKB_PAD + NET_IP_ALIGN)
+#endif
+
 /* We are assuming that the cache line is always 64 Bytes here for ice.
  * In order to make sure that is a correct assumption there is a check in probe
  * to print a warning if the read from GLPCI_CNF2 tells us that the cache line
@@ -231,6 +296,7 @@ struct ice_ring {
 	 * in their own cache line if possible
 	 */
 #define ICE_TX_FLAGS_RING_XDP		BIT(0)
+#define ICE_RX_FLAGS_RING_BUILD_SKB	BIT(1)
 	u8 flags;
 	dma_addr_t dma;			/* physical address of ring */
 	unsigned int size;		/* length of descriptor ring in bytes */
@@ -239,6 +305,21 @@ struct ice_ring {
 	u8 dcb_tc;			/* Traffic class of ring */
 } ____cacheline_internodealigned_in_smp;
 
+static inline bool ice_ring_uses_build_skb(struct ice_ring *ring)
+{
+	return !!(ring->flags & ICE_RX_FLAGS_RING_BUILD_SKB);
+}
+
+static inline void ice_set_ring_build_skb_ena(struct ice_ring *ring)
+{
+	ring->flags |= ICE_RX_FLAGS_RING_BUILD_SKB;
+}
+
+static inline void ice_clear_ring_build_skb_ena(struct ice_ring *ring)
+{
+	ring->flags &= ~ICE_RX_FLAGS_RING_BUILD_SKB;
+}
+
 static inline bool ice_ring_is_xdp(struct ice_ring *ring)
 {
 	return !!(ring->flags & ICE_TX_FLAGS_RING_XDP);

commit 7237f5b0dba443756e190bdbecd83f9b1377a912
Author: Maciej Fijalkowski <maciej.fijalkowski@intel.com>
Date:   Thu Oct 24 01:11:22 2019 -0700

    ice: introduce legacy Rx flag
    
    Add an ethtool "legacy-rx" priv flag for toggling the Rx path. This
    control knob will be mainly used for build_skb usage as well as buffer
    size/MTU manipulation.
    
    In preparation for adding build_skb support in a way that it takes
    care of how we set the values of max_frame and rx_buf_len fields of
    struct ice_vsi. Specifically, in this patch mentioned fields are set to
    values that will allow us to provide headroom and tailroom in-place.
    
    This can be mostly broken down onto following:
    - for legacy-rx "on" ethtool control knob, old behaviour is kept;
    - for standard 1500 MTU size configure the buffer of size 1536, as
      network stack is expecting the NET_SKB_PAD to be provided and
      NET_IP_ALIGN can have a non-zero value (these can be typically equal
      to 32 and 2, respectively);
    - for larger MTUs go with max_frame set to 9k and configure the 3k
      buffer in case when PAGE_SIZE of underlying arch is less than 8k; 3k
      buffer is implying the need for order 1 page, so that our page
      recycling scheme can still be applied;
    
    With that said, substitute the hardcoded ICE_RXBUF_2048 and PAGE_SIZE
    values in DMA API that we're making use of with rx_ring->rx_buf_len and
    ice_rx_pg_size(rx_ring). The latter is an introduced helper for
    determining the page size based on its order (which was figured out via
    ice_rx_pg_order). Last but not least, take care of truesize calculation.
    
    In the followup patch the headroom/tailroom computation logic will be
    introduced.
    
    This change aligns the buffer and frame configuration with other Intel
    drivers, most importantly with iavf.
    
    Signed-off-by: Maciej Fijalkowski <maciej.fijalkowski@intel.com>
    Signed-off-by: Tony Nguyen <anthony.l.nguyen@intel.com>
    Tested-by: Andrew Bowers <andrewx.bowers@intel.com>
    Signed-off-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>

diff --git a/drivers/net/ethernet/intel/ice/ice_txrx.h b/drivers/net/ethernet/intel/ice/ice_txrx.h
index d5d243b8e69f..6a6e3d2339ba 100644
--- a/drivers/net/ethernet/intel/ice/ice_txrx.h
+++ b/drivers/net/ethernet/intel/ice/ice_txrx.h
@@ -7,7 +7,9 @@
 #include "ice_type.h"
 
 #define ICE_DFLT_IRQ_WORK	256
+#define ICE_RXBUF_3072		3072
 #define ICE_RXBUF_2048		2048
+#define ICE_RXBUF_1536		1536
 #define ICE_MAX_CHAINED_RX_BUFS	5
 #define ICE_MAX_BUF_TXD		8
 #define ICE_MIN_TX_LEN		17
@@ -262,6 +264,17 @@ struct ice_ring_container {
 #define ice_for_each_ring(pos, head) \
 	for (pos = (head).ring; pos; pos = pos->next)
 
+static inline unsigned int ice_rx_pg_order(struct ice_ring *ring)
+{
+#if (PAGE_SIZE < 8192)
+	if (ring->rx_buf_len > (PAGE_SIZE / 2))
+		return 1;
+#endif
+	return 0;
+}
+
+#define ice_rx_pg_size(_ring) (PAGE_SIZE << ice_rx_pg_order(_ring))
+
 union ice_32b_rx_flex_desc;
 
 bool ice_alloc_rx_bufs(struct ice_ring *rxr, u16 cleaned_count);

commit 2d4238f5569722197612656163d824098208519c
Author: Krzysztof Kazimierczak <krzysztof.kazimierczak@intel.com>
Date:   Mon Nov 4 09:38:56 2019 -0800

    ice: Add support for AF_XDP
    
    Add zero copy AF_XDP support.  This patch adds zero copy support for
    Tx and Rx; code for zero copy is added to ice_xsk.h and ice_xsk.c.
    
    For Tx, implement ndo_xsk_wakeup. As with other drivers, reuse
    existing XDP Tx queues for this task, since XDP_REDIRECT guarantees
    mutual exclusion between different NAPI contexts based on CPU ID. In
    turn, a netdev can XDP_REDIRECT to another netdev with a different
    NAPI context, since the operation is bound to a specific core and each
    core has its own hardware ring.
    
    For Rx, allocate frames as MEM_TYPE_ZERO_COPY on queues that AF_XDP is
    enabled.
    
    Signed-off-by: Krzysztof Kazimierczak <krzysztof.kazimierczak@intel.com>
    Co-developed-by: Maciej Fijalkowski <maciej.fijalkowski@intel.com>
    Signed-off-by: Maciej Fijalkowski <maciej.fijalkowski@intel.com>
    Signed-off-by: Tony Nguyen <anthony.l.nguyen@intel.com>
    Signed-off-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>

diff --git a/drivers/net/ethernet/intel/ice/ice_txrx.h b/drivers/net/ethernet/intel/ice/ice_txrx.h
index a07101b13226..d5d243b8e69f 100644
--- a/drivers/net/ethernet/intel/ice/ice_txrx.h
+++ b/drivers/net/ethernet/intel/ice/ice_txrx.h
@@ -4,6 +4,8 @@
 #ifndef _ICE_TXRX_H_
 #define _ICE_TXRX_H_
 
+#include "ice_type.h"
+
 #define ICE_DFLT_IRQ_WORK	256
 #define ICE_RXBUF_2048		2048
 #define ICE_MAX_CHAINED_RX_BUFS	5
@@ -88,9 +90,17 @@ struct ice_tx_offload_params {
 struct ice_rx_buf {
 	struct sk_buff *skb;
 	dma_addr_t dma;
-	struct page *page;
-	unsigned int page_offset;
-	u16 pagecnt_bias;
+	union {
+		struct {
+			struct page *page;
+			unsigned int page_offset;
+			u16 pagecnt_bias;
+		};
+		struct {
+			void *addr;
+			u64 handle;
+		};
+	};
 };
 
 struct ice_q_stats {
@@ -211,6 +221,8 @@ struct ice_ring {
 
 	struct rcu_head rcu;		/* to avoid race on free */
 	struct bpf_prog *xdp_prog;
+	struct xdp_umem *xsk_umem;
+	struct zero_copy_allocator zca;
 	/* CL3 - 3rd cacheline starts here */
 	struct xdp_rxq_info xdp_rxq;
 	/* CLX - the below items are only accessed infrequently and should be
@@ -250,6 +262,8 @@ struct ice_ring_container {
 #define ice_for_each_ring(pos, head) \
 	for (pos = (head).ring; pos; pos = pos->next)
 
+union ice_32b_rx_flex_desc;
+
 bool ice_alloc_rx_bufs(struct ice_ring *rxr, u16 cleaned_count);
 netdev_tx_t ice_start_xmit(struct sk_buff *skb, struct net_device *netdev);
 void ice_clean_tx_ring(struct ice_ring *tx_ring);

commit 0891d6d4b1fe1becf1a77353b03449955820084b
Author: Krzysztof Kazimierczak <krzysztof.kazimierczak@intel.com>
Date:   Mon Nov 4 09:38:56 2019 -0800

    ice: Move common functions to ice_txrx_lib.c
    
    In preparation of AF XDP, move functions that will be used both by skb and
    zero-copy paths to a new file called ice_txrx_lib.c.  This allows us to
    avoid using ifdefs to control the staticness of said functions.
    
    Move other functions (ice_rx_csum, ice_rx_hash and ice_ptype_to_htype)
    called only by the moved ones to the new file as well.
    
    Signed-off-by: Krzysztof Kazimierczak <krzysztof.kazimierczak@intel.com>
    Signed-off-by: Tony Nguyen <anthony.l.nguyen@intel.com>
    Tested-by: Andrew Bowers <andrewx.bowers@intel.com>
    Signed-off-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>

diff --git a/drivers/net/ethernet/intel/ice/ice_txrx.h b/drivers/net/ethernet/intel/ice/ice_txrx.h
index e40b4cb54ce3..a07101b13226 100644
--- a/drivers/net/ethernet/intel/ice/ice_txrx.h
+++ b/drivers/net/ethernet/intel/ice/ice_txrx.h
@@ -22,16 +22,6 @@
 #define ICE_RX_BUF_WRITE	16	/* Must be power of 2 */
 #define ICE_MAX_TXQ_PER_TXQG	128
 
-static inline __le64
-build_ctob(u64 td_cmd, u64 td_offset, unsigned int size, u64 td_tag)
-{
-	return cpu_to_le64(ICE_TX_DESC_DTYPE_DATA |
-			   (td_cmd    << ICE_TXD_QW1_CMD_S) |
-			   (td_offset << ICE_TXD_QW1_OFFSET_S) |
-			   ((u64)size << ICE_TXD_QW1_TX_BUF_SZ_S) |
-			   (td_tag    << ICE_TXD_QW1_L2TAG1_S));
-}
-
 /* We are assuming that the cache line is always 64 Bytes here for ice.
  * In order to make sure that is a correct assumption there is a check in probe
  * to print a warning if the read from GLPCI_CNF2 tells us that the cache line

commit efc2214b6047b6f5b4ca53151eba62521b9452d6
Author: Maciej Fijalkowski <maciej.fijalkowski@intel.com>
Date:   Mon Nov 4 09:38:56 2019 -0800

    ice: Add support for XDP
    
    Add support for XDP. Implement ndo_bpf and ndo_xdp_xmit.  Upon load of
    an XDP program, allocate additional Tx rings for dedicated XDP use.
    The following actions are supported: XDP_TX, XDP_DROP, XDP_REDIRECT,
    XDP_PASS, and XDP_ABORTED.
    
    Signed-off-by: Maciej Fijalkowski <maciej.fijalkowski@intel.com>
    Signed-off-by: Tony Nguyen <anthony.l.nguyen@intel.com>
    Tested-by: Andrew Bowers <andrewx.bowers@intel.com>
    Signed-off-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>

diff --git a/drivers/net/ethernet/intel/ice/ice_txrx.h b/drivers/net/ethernet/intel/ice/ice_txrx.h
index a914e603b2ed..e40b4cb54ce3 100644
--- a/drivers/net/ethernet/intel/ice/ice_txrx.h
+++ b/drivers/net/ethernet/intel/ice/ice_txrx.h
@@ -22,6 +22,16 @@
 #define ICE_RX_BUF_WRITE	16	/* Must be power of 2 */
 #define ICE_MAX_TXQ_PER_TXQG	128
 
+static inline __le64
+build_ctob(u64 td_cmd, u64 td_offset, unsigned int size, u64 td_tag)
+{
+	return cpu_to_le64(ICE_TX_DESC_DTYPE_DATA |
+			   (td_cmd    << ICE_TXD_QW1_CMD_S) |
+			   (td_offset << ICE_TXD_QW1_OFFSET_S) |
+			   ((u64)size << ICE_TXD_QW1_TX_BUF_SZ_S) |
+			   (td_tag    << ICE_TXD_QW1_L2TAG1_S));
+}
+
 /* We are assuming that the cache line is always 64 Bytes here for ice.
  * In order to make sure that is a correct assumption there is a check in probe
  * to print a warning if the read from GLPCI_CNF2 tells us that the cache line
@@ -49,12 +59,24 @@
 #define ICE_TX_FLAGS_VLAN_PR_S	29
 #define ICE_TX_FLAGS_VLAN_S	16
 
+#define ICE_XDP_PASS		0
+#define ICE_XDP_CONSUMED	BIT(0)
+#define ICE_XDP_TX		BIT(1)
+#define ICE_XDP_REDIR		BIT(2)
+
 #define ICE_RX_DMA_ATTR \
 	(DMA_ATTR_SKIP_CPU_SYNC | DMA_ATTR_WEAK_ORDERING)
 
+#define ICE_ETH_PKT_HDR_PAD	(ETH_HLEN + ETH_FCS_LEN + (VLAN_HLEN * 2))
+
+#define ICE_TXD_LAST_DESC_CMD (ICE_TX_DESC_CMD_EOP | ICE_TX_DESC_CMD_RS)
+
 struct ice_tx_buf {
 	struct ice_tx_desc *next_to_watch;
-	struct sk_buff *skb;
+	union {
+		struct sk_buff *skb;
+		void *raw_buf; /* used for XDP */
+	};
 	unsigned int bytecount;
 	unsigned short gso_segs;
 	u32 tx_flags;
@@ -198,9 +220,14 @@ struct ice_ring {
 	};
 
 	struct rcu_head rcu;		/* to avoid race on free */
+	struct bpf_prog *xdp_prog;
+	/* CL3 - 3rd cacheline starts here */
+	struct xdp_rxq_info xdp_rxq;
 	/* CLX - the below items are only accessed infrequently and should be
 	 * in their own cache line if possible
 	 */
+#define ICE_TX_FLAGS_RING_XDP		BIT(0)
+	u8 flags;
 	dma_addr_t dma;			/* physical address of ring */
 	unsigned int size;		/* length of descriptor ring in bytes */
 	u32 txq_teid;			/* Added Tx queue TEID */
@@ -208,6 +235,11 @@ struct ice_ring {
 	u8 dcb_tc;			/* Traffic class of ring */
 } ____cacheline_internodealigned_in_smp;
 
+static inline bool ice_ring_is_xdp(struct ice_ring *ring)
+{
+	return !!(ring->flags & ICE_TX_FLAGS_RING_XDP);
+}
+
 struct ice_ring_container {
 	/* head of linked-list of rings */
 	struct ice_ring *ring;

commit eff380aaffedb279b69d160061e2c01f9df5da96
Author: Anirudh Venkataramanan <anirudh.venkataramanan@intel.com>
Date:   Thu Oct 24 01:11:17 2019 -0700

    ice: Introduce ice_base.c
    
    Remove a few uses of kernel configuration flags from ice_lib.c by
    introducing a new source file ice_base.c. Also move corresponding
    function prototypes from ice_lib.h to ice_base.h and include ice_base.h
    where required.
    
    Signed-off-by: Anirudh Venkataramanan <anirudh.venkataramanan@intel.com>
    Signed-off-by: Tony Nguyen <anthony.l.nguyen@intel.com>
    Tested-by: Andrew Bowers <andrewx.bowers@intel.com>
    Signed-off-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>

diff --git a/drivers/net/ethernet/intel/ice/ice_txrx.h b/drivers/net/ethernet/intel/ice/ice_txrx.h
index 94a9280193e2..a914e603b2ed 100644
--- a/drivers/net/ethernet/intel/ice/ice_txrx.h
+++ b/drivers/net/ethernet/intel/ice/ice_txrx.h
@@ -205,9 +205,7 @@ struct ice_ring {
 	unsigned int size;		/* length of descriptor ring in bytes */
 	u32 txq_teid;			/* Added Tx queue TEID */
 	u16 rx_buf_len;
-#ifdef CONFIG_DCB
 	u8 dcb_tc;			/* Traffic class of ring */
-#endif /* CONFIG_DCB */
 } ____cacheline_internodealigned_in_smp;
 
 struct ice_ring_container {

commit 2ab28bb04ce6432f15cd5086e7c7384f3deab639
Author: Brett Creeley <brett.creeley@intel.com>
Date:   Thu Jul 25 01:55:32 2019 -0700

    ice: Set WB_ON_ITR when we don't re-enable interrupts
    
    Currently when busy polling is enabled we aren't setting/enabling
    WB_ON_ITR in the driver. This doesn't break the driver, but it does
    cause issues. If we don't enable WB_ON_ITR mode we will still get
    write-backs from hardware during polling when a cache line has been
    filled, but if a cache line is not filled we will not get the
    write-back because WB_ON_ITR is not set. Fix this by enabling
    WB_ON_ITR in the driver when interrupts are disabled.
    
    Signed-off-by: Brett Creeley <brett.creeley@intel.com>
    Signed-off-by: Tony Nguyen <anthony.l.nguyen@intel.com>
    Tested-by: Andrew Bowers <andrewx.bowers@intel.com>
    Signed-off-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>

diff --git a/drivers/net/ethernet/intel/ice/ice_txrx.h b/drivers/net/ethernet/intel/ice/ice_txrx.h
index ec76aba347b9..94a9280193e2 100644
--- a/drivers/net/ethernet/intel/ice/ice_txrx.h
+++ b/drivers/net/ethernet/intel/ice/ice_txrx.h
@@ -144,6 +144,19 @@ enum ice_rx_dtype {
 #define ICE_DFLT_INTRL	0
 #define ICE_MAX_INTRL	236
 
+#define ICE_WB_ON_ITR_USECS	2
+#define ICE_IN_WB_ON_ITR_MODE	255
+/* Sets WB_ON_ITR and assumes INTENA bit is already cleared, which allows
+ * setting the MSK_M bit to tell hardware to ignore the INTENA_M bit. Also,
+ * set the write-back latency to the usecs passed in.
+ */
+#define ICE_GLINT_DYN_CTL_WB_ON_ITR(usecs, itr_idx)	\
+	((((usecs) << (GLINT_DYN_CTL_INTERVAL_S - ICE_ITR_GRAN_S)) & \
+	  GLINT_DYN_CTL_INTERVAL_M) | \
+	 (((itr_idx) << GLINT_DYN_CTL_ITR_INDX_S) & \
+	  GLINT_DYN_CTL_ITR_INDX_M) | GLINT_DYN_CTL_INTENA_MSK_M | \
+	 GLINT_DYN_CTL_WB_ON_ITR_M)
+
 /* Legacy or Advanced Mode Queue */
 #define ICE_TX_ADVANCED	0
 #define ICE_TX_LEGACY	1

commit 0ab54c5f2fe8929d04d965bc3e05325e37de45ba
Author: Jesse Brandeburg <jesse.brandeburg@intel.com>
Date:   Tue Apr 16 10:24:35 2019 -0700

    ice: Use bitfields when possible
    
    We can use bit fields to store boolean values and when the
    bit fields are next to each other, the compiler will combine them
    (as long as the size holds enough).
    
    Signed-off-by: Jesse Brandeburg <jesse.brandeburg@intel.com>
    Signed-off-by: Anirudh Venkataramanan <anirudh.venkataramanan@intel.com>
    Tested-by: Andrew Bowers <andrewx.bowers@intel.com>
    Signed-off-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>

diff --git a/drivers/net/ethernet/intel/ice/ice_txrx.h b/drivers/net/ethernet/intel/ice/ice_txrx.h
index 4cff52ffefe0..ec76aba347b9 100644
--- a/drivers/net/ethernet/intel/ice/ice_txrx.h
+++ b/drivers/net/ethernet/intel/ice/ice_txrx.h
@@ -166,7 +166,7 @@ struct ice_ring {
 	u16 q_index;			/* Queue number of ring */
 	u16 q_handle;			/* Queue handle per TC */
 
-	u8 ring_active;			/* is ring online or not */
+	u8 ring_active:1;		/* is ring online or not */
 
 	u16 count;			/* Number of descriptors */
 	u16 reg_idx;			/* HW register index of the ring */

commit 65124bbf980c6256e3385e7d002a5db102d30d3a
Author: Jesse Brandeburg <jesse.brandeburg@intel.com>
Date:   Tue Apr 16 10:24:34 2019 -0700

    ice: Reorganize tx_buf and ring structs
    
    Use more efficient structure ordering by using the pahole tool
    and a lot of code inspection to get hot cache lines to have
    packed data (no holes if possible) and adjacent warm data.
    
    ice_ring prior to this change:
      /* size: 192, cachelines: 3, members: 23 */
      /* sum members: 158, holes: 4, sum holes: 12 */
      /* padding: 22 */
    
    ice_ring after this change:
      /* size: 192, cachelines: 3, members: 25 */
      /* sum members: 162, holes: 1, sum holes: 1 */
      /* padding: 29 */
    
    ice_tx_buf prior to this change:
      /* size: 48, cachelines: 1, members: 7 */
      /* sum members: 38, holes: 2, sum holes: 6 */
      /* padding: 4 */
      /* last cacheline: 48 bytes */
    
    ice_tx_buf after this change:
      /* size: 40, cachelines: 1, members: 7 */
      /* sum members: 38, holes: 1, sum holes: 2 */
      /* last cacheline: 40 bytes */
    
    Signed-off-by: Jesse Brandeburg <jesse.brandeburg@intel.com>
    Signed-off-by: Anirudh Venkataramanan <anirudh.venkataramanan@intel.com>
    Tested-by: Andrew Bowers <andrewx.bowers@intel.com>
    Signed-off-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>

diff --git a/drivers/net/ethernet/intel/ice/ice_txrx.h b/drivers/net/ethernet/intel/ice/ice_txrx.h
index 66e05032ee56..4cff52ffefe0 100644
--- a/drivers/net/ethernet/intel/ice/ice_txrx.h
+++ b/drivers/net/ethernet/intel/ice/ice_txrx.h
@@ -58,19 +58,19 @@ struct ice_tx_buf {
 	unsigned int bytecount;
 	unsigned short gso_segs;
 	u32 tx_flags;
-	DEFINE_DMA_UNMAP_ADDR(dma);
 	DEFINE_DMA_UNMAP_LEN(len);
+	DEFINE_DMA_UNMAP_ADDR(dma);
 };
 
 struct ice_tx_offload_params {
-	u8 header_len;
+	u64 cd_qw1;
+	struct ice_ring *tx_ring;
 	u32 td_cmd;
 	u32 td_offset;
 	u32 td_l2tag1;
-	u16 cd_l2tag2;
 	u32 cd_tunnel_params;
-	u64 cd_qw1;
-	struct ice_ring *tx_ring;
+	u16 cd_l2tag2;
+	u8 header_len;
 };
 
 struct ice_rx_buf {
@@ -150,6 +150,7 @@ enum ice_rx_dtype {
 
 /* descriptor ring, associated with a VSI */
 struct ice_ring {
+	/* CL1 - 1st cacheline starts here */
 	struct ice_ring *next;		/* pointer to next ring in q_vector */
 	void *desc;			/* Descriptor ring memory */
 	struct device *dev;		/* Used for DMA mapping */
@@ -161,11 +162,11 @@ struct ice_ring {
 		struct ice_tx_buf *tx_buf;
 		struct ice_rx_buf *rx_buf;
 	};
+	/* CL2 - 2nd cacheline starts here */
 	u16 q_index;			/* Queue number of ring */
-	u32 txq_teid;			/* Added Tx queue TEID */
-#ifdef CONFIG_DCB
-	u8 dcb_tc;		/* Traffic class of ring */
-#endif /* CONFIG_DCB */
+	u16 q_handle;			/* Queue handle per TC */
+
+	u8 ring_active;			/* is ring online or not */
 
 	u16 count;			/* Number of descriptors */
 	u16 reg_idx;			/* HW register index of the ring */
@@ -173,8 +174,7 @@ struct ice_ring {
 	/* used in interrupt processing */
 	u16 next_to_use;
 	u16 next_to_clean;
-
-	u8 ring_active;			/* is ring online or not */
+	u16 next_to_alloc;
 
 	/* stats structs */
 	struct ice_q_stats	stats;
@@ -184,10 +184,17 @@ struct ice_ring {
 		struct ice_rxq_stats rx_stats;
 	};
 
-	unsigned int size;		/* length of descriptor ring in bytes */
-	dma_addr_t dma;			/* physical address of ring */
 	struct rcu_head rcu;		/* to avoid race on free */
-	u16 next_to_alloc;
+	/* CLX - the below items are only accessed infrequently and should be
+	 * in their own cache line if possible
+	 */
+	dma_addr_t dma;			/* physical address of ring */
+	unsigned int size;		/* length of descriptor ring in bytes */
+	u32 txq_teid;			/* Added Tx queue TEID */
+	u16 rx_buf_len;
+#ifdef CONFIG_DCB
+	u8 dcb_tc;			/* Traffic class of ring */
+#endif /* CONFIG_DCB */
 } ____cacheline_internodealigned_in_smp;
 
 struct ice_ring_container {

commit b9c8bb06b53d28c83c47988f645b6cf4543c2685
Author: Brett Creeley <brett.creeley@intel.com>
Date:   Thu Feb 28 15:25:55 2019 -0800

    ice: Add ability to update rx-usecs-high
    
    Currently the driver allows rx-usecs-high values to be set,
    but when querying the device for rx-usecs-high the value
    does not stick. This is because it was not yet implemented.
    Add code to allow the user to change rx-usecs-high and
    use this to set the q_vector's intrl value.
    
    Signed-off-by: Brett Creeley <brett.creeley@intel.com>
    Signed-off-by: Anirudh Venkataramanan <anirudh.venkataramanan@intel.com>
    Tested-by: Andrew Bowers <andrewx.bowers@intel.com>
    Signed-off-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>

diff --git a/drivers/net/ethernet/intel/ice/ice_txrx.h b/drivers/net/ethernet/intel/ice/ice_txrx.h
index c75d9fd12a68..66e05032ee56 100644
--- a/drivers/net/ethernet/intel/ice/ice_txrx.h
+++ b/drivers/net/ethernet/intel/ice/ice_txrx.h
@@ -142,6 +142,7 @@ enum ice_rx_dtype {
 #define ICE_ITR_ADAPTIVE_BULK		0x0000
 
 #define ICE_DFLT_INTRL	0
+#define ICE_MAX_INTRL	236
 
 /* Legacy or Advanced Mode Queue */
 #define ICE_TX_ADVANCED	0

commit 5f6aa50e4ece6b9464130d819a2caa75c783c603
Author: Anirudh Venkataramanan <anirudh.venkataramanan@intel.com>
Date:   Thu Feb 28 15:24:28 2019 -0800

    ice: Add priority information into VLAN header
    
    This patch introduces a new function ice_tx_prepare_vlan_flags_dcb to
    insert 802.1p priority information into the VLAN header
    
    Signed-off-by: Anirudh Venkataramanan <anirudh.venkataramanan@intel.com>
    Tested-by: Andrew Bowers <andrewx.bowers@intel.com>
    Signed-off-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>

diff --git a/drivers/net/ethernet/intel/ice/ice_txrx.h b/drivers/net/ethernet/intel/ice/ice_txrx.h
index 53903f846834..c75d9fd12a68 100644
--- a/drivers/net/ethernet/intel/ice/ice_txrx.h
+++ b/drivers/net/ethernet/intel/ice/ice_txrx.h
@@ -45,6 +45,8 @@
 #define ICE_TX_FLAGS_HW_VLAN	BIT(1)
 #define ICE_TX_FLAGS_SW_VLAN	BIT(2)
 #define ICE_TX_FLAGS_VLAN_M	0xffff0000
+#define ICE_TX_FLAGS_VLAN_PR_M	0xe0000000
+#define ICE_TX_FLAGS_VLAN_PR_S	29
 #define ICE_TX_FLAGS_VLAN_S	16
 
 #define ICE_RX_DMA_ATTR \

commit a629cf0a018b8d80b65bfd2b7f0d209a52834315
Author: Anirudh Venkataramanan <anirudh.venkataramanan@intel.com>
Date:   Thu Feb 28 15:24:27 2019 -0800

    ice: Update rings based on TC information
    
    This patch adds a new function ice_vsi_cfg_dcb_rings which updates a
    VSI's rings based on DCB traffic class information.
    
    Signed-off-by: Anirudh Venkataramanan <anirudh.venkataramanan@intel.com>
    Tested-by: Andrew Bowers <andrewx.bowers@intel.com>
    Signed-off-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>

diff --git a/drivers/net/ethernet/intel/ice/ice_txrx.h b/drivers/net/ethernet/intel/ice/ice_txrx.h
index 60131b84b021..53903f846834 100644
--- a/drivers/net/ethernet/intel/ice/ice_txrx.h
+++ b/drivers/net/ethernet/intel/ice/ice_txrx.h
@@ -160,6 +160,9 @@ struct ice_ring {
 	};
 	u16 q_index;			/* Queue number of ring */
 	u32 txq_teid;			/* Added Tx queue TEID */
+#ifdef CONFIG_DCB
+	u8 dcb_tc;		/* Traffic class of ring */
+#endif /* CONFIG_DCB */
 
 	u16 count;			/* Number of descriptors */
 	u16 reg_idx;			/* HW register index of the ring */

commit 92414f329262c9240223b8279aa9f544a754d78f
Author: Brett Creeley <brett.creeley@intel.com>
Date:   Tue Feb 19 15:04:10 2019 -0800

    ice: Update comment regarding the ITR_GRAN_S
    
    Since the driver now hard codes the ITR granularity to 2 us in the
    GLINT_CTL register the comment next to ITR_GRAN_S needs to be updated.
    
    Signed-off-by: Brett Creeley <brett.creeley@intel.com>
    Signed-off-by: Anirudh Venkataramanan <anirudh.venkataramanan@intel.com>
    Tested-by: Andrew Bowers <andrewx.bowers@intel.com>
    Signed-off-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>

diff --git a/drivers/net/ethernet/intel/ice/ice_txrx.h b/drivers/net/ethernet/intel/ice/ice_txrx.h
index 2c8af98ff640..60131b84b021 100644
--- a/drivers/net/ethernet/intel/ice/ice_txrx.h
+++ b/drivers/net/ethernet/intel/ice/ice_txrx.h
@@ -128,7 +128,7 @@ enum ice_rx_dtype {
 #define ICE_ITR_DYNAMIC	0x8000  /* used as flag for itr_setting */
 #define ITR_IS_DYNAMIC(setting) (!!((setting) & ICE_ITR_DYNAMIC))
 #define ITR_TO_REG(setting)	((setting) & ~ICE_ITR_DYNAMIC)
-#define ICE_ITR_GRAN_S		1	/* Assume ITR granularity is 2us */
+#define ICE_ITR_GRAN_S		1	/* ITR granularity is always 2us */
 #define ICE_ITR_GRAN_US		BIT(ICE_ITR_GRAN_S)
 #define ICE_ITR_MASK		0x1FFE	/* ITR register value alignment mask */
 #define ITR_REG_ALIGN(setting)	__ALIGN_MASK(setting, ~ICE_ITR_MASK)

commit 8244dd2d23b251dcba3238e42216e9277beb5729
Author: Brett Creeley <brett.creeley@intel.com>
Date:   Tue Feb 19 15:04:05 2019 -0800

    ice: Audit hotpath structures with pahole
    
    Currently the ice_q_vector structure and ice_ring_container structure
    are taking up more space than necessary due to cache alignment holes
    and unnecessary variables respectively. This is not helping the
    driver's performance. The following fixes were done to improve cache
    alignment, reduce wasted space, and increase performance.
    
    1. Remove the ice_latency_range enum as it is unused.
    2. Remove the latency_range variable in the ice_ring_container structure.
    3. Change the size of the itr_idx in the ice_ring_container structure
       from an int to an u16. This reduced the size of ice_ring_container
       structure to 32 Bytes so it has no holes or padding.
    4. Re-arrange the ice_q_vector structure using pahole to align
       members as best as possible in regards to 64 Byte cache line size.
    
    Signed-off-by: Brett Creeley <brett.creeley@intel.com>
    Signed-off-by: Anirudh Venkataramanan <anirudh.venkataramanan@intel.com>
    Tested-by: Andrew Bowers <andrewx.bowers@intel.com>
    Signed-off-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>

diff --git a/drivers/net/ethernet/intel/ice/ice_txrx.h b/drivers/net/ethernet/intel/ice/ice_txrx.h
index 69625857c482..2c8af98ff640 100644
--- a/drivers/net/ethernet/intel/ice/ice_txrx.h
+++ b/drivers/net/ethernet/intel/ice/ice_txrx.h
@@ -184,21 +184,13 @@ struct ice_ring {
 	u16 next_to_alloc;
 } ____cacheline_internodealigned_in_smp;
 
-enum ice_latency_range {
-	ICE_LOWEST_LATENCY = 0,
-	ICE_LOW_LATENCY = 1,
-	ICE_BULK_LATENCY = 2,
-	ICE_ULTRA_LATENCY = 3,
-};
-
 struct ice_ring_container {
 	/* head of linked-list of rings */
 	struct ice_ring *ring;
 	unsigned long next_update;	/* jiffies value of next queue update */
 	unsigned int total_bytes;	/* total bytes processed this int */
 	unsigned int total_pkts;	/* total packets processed this int */
-	enum ice_latency_range latency_range;
-	int itr_idx;		/* index in the interrupt vector */
+	u16 itr_idx;		/* index in the interrupt vector */
 	u16 target_itr;		/* value in usecs divided by the hw->itr_gran */
 	u16 current_itr;	/* value in usecs divided by the hw->itr_gran */
 	/* high bit set means dynamic ITR, rest is used to store user

commit 64a59d05a4b3ddb37eb5ad3a3be0f17148f449f5
Author: Anirudh Venkataramanan <anirudh.venkataramanan@intel.com>
Date:   Tue Feb 19 15:04:01 2019 -0800

    ice: Fix for adaptive interrupt moderation
    
    commit 63f545ed1285 ("ice: Add support for adaptive interrupt moderation")
    was meant to add support for adaptive interrupt moderation but there was
    an error on my part while formatting the patch, and thus only part of the
    patch ended up being submitted.
    
    This patch rectifies the error by adding the rest of the code.
    
    Fixes: 63f545ed1285 ("ice: Add support for adaptive interrupt moderation")
    Signed-off-by: Anirudh Venkataramanan <anirudh.venkataramanan@intel.com>
    Tested-by: Andrew Bowers <andrewx.bowers@intel.com>
    Signed-off-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>

diff --git a/drivers/net/ethernet/intel/ice/ice_txrx.h b/drivers/net/ethernet/intel/ice/ice_txrx.h
index bd446ed423e5..69625857c482 100644
--- a/drivers/net/ethernet/intel/ice/ice_txrx.h
+++ b/drivers/net/ethernet/intel/ice/ice_txrx.h
@@ -133,6 +133,12 @@ enum ice_rx_dtype {
 #define ICE_ITR_MASK		0x1FFE	/* ITR register value alignment mask */
 #define ITR_REG_ALIGN(setting)	__ALIGN_MASK(setting, ~ICE_ITR_MASK)
 
+#define ICE_ITR_ADAPTIVE_MIN_INC	0x0002
+#define ICE_ITR_ADAPTIVE_MIN_USECS	0x0002
+#define ICE_ITR_ADAPTIVE_MAX_USECS	0x00FA
+#define ICE_ITR_ADAPTIVE_LATENCY	0x8000
+#define ICE_ITR_ADAPTIVE_BULK		0x0000
+
 #define ICE_DFLT_INTRL	0
 
 /* Legacy or Advanced Mode Queue */

commit a65f71fed5add2ec5713fcc605842f5f2dff22a3
Author: Maciej Fijalkowski <maciej.fijalkowski@intel.com>
Date:   Wed Feb 13 10:51:07 2019 -0800

    ice: map Rx buffer pages with DMA attributes
    
    Provide DMA_ATTR_WEAK_ORDERING and DMA_ATTR_SKIP_CPU_SYNC attributes to
    the DMA API during the mapping operations on Rx side. With this change
    the non-x86 platforms will be able to sync only with what is being used
    (2k buffer) instead of entire page. This should yield a slight
    performance improvement.
    
    Furthermore, DMA unmap may destroy the changes that were made to the
    buffer by CPU when platform is not a x86 one. DMA_ATTR_SKIP_CPU_SYNC
    attribute usage fixes this issue.
    
    Also add a sync_single_for_device call during the Rx buffer assignment,
    to make sure that the cache lines are cleared before device attempting
    to write to the buffer.
    
    Signed-off-by: Maciej Fijalkowski <maciej.fijalkowski@intel.com>
    Signed-off-by: Anirudh Venkataramanan <anirudh.venkataramanan@intel.com>
    Tested-by: Andrew Bowers <andrewx.bowers@intel.com>
    Signed-off-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>

diff --git a/drivers/net/ethernet/intel/ice/ice_txrx.h b/drivers/net/ethernet/intel/ice/ice_txrx.h
index 43b39e7ce470..bd446ed423e5 100644
--- a/drivers/net/ethernet/intel/ice/ice_txrx.h
+++ b/drivers/net/ethernet/intel/ice/ice_txrx.h
@@ -47,6 +47,9 @@
 #define ICE_TX_FLAGS_VLAN_M	0xffff0000
 #define ICE_TX_FLAGS_VLAN_S	16
 
+#define ICE_RX_DMA_ATTR \
+	(DMA_ATTR_SKIP_CPU_SYNC | DMA_ATTR_WEAK_ORDERING)
+
 struct ice_tx_buf {
 	struct ice_tx_desc *next_to_watch;
 	struct sk_buff *skb;

commit 03c66a1376616015b04b6783feadfcf02ba37c3f
Author: Maciej Fijalkowski <maciej.fijalkowski@intel.com>
Date:   Wed Feb 13 10:51:04 2019 -0800

    ice: Introduce bulk update for page count
    
    {get,put}_page are atomic operations which we use for page count
    handling. The current logic for refcount handling is that we increment
    it when passing a skb with the data from the first half of page up to
    netstack and recycle the second half of page. This operation protects us
    from losing a page since the network stack can decrement the refcount of
    page from skb.
    
    The performance can be gently improved by doing the bulk updates of
    refcount instead of doing it one by one. During the buffer initialization,
    maximize the page's refcount and don't allow the refcount to become
    less than two.
    
    Signed-off-by: Maciej Fijalkowski <maciej.fijalkowski@intel.com>
    Signed-off-by: Anirudh Venkataramanan <anirudh.venkataramanan@intel.com>
    Tested-by: Andrew Bowers <andrewx.bowers@intel.com>
    Signed-off-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>

diff --git a/drivers/net/ethernet/intel/ice/ice_txrx.h b/drivers/net/ethernet/intel/ice/ice_txrx.h
index b7ff0ff82517..43b39e7ce470 100644
--- a/drivers/net/ethernet/intel/ice/ice_txrx.h
+++ b/drivers/net/ethernet/intel/ice/ice_txrx.h
@@ -73,6 +73,7 @@ struct ice_rx_buf {
 	dma_addr_t dma;
 	struct page *page;
 	unsigned int page_offset;
+	u16 pagecnt_bias;
 };
 
 struct ice_q_stats {

commit 70457520bab82bd758307837964ef7bbd5dd9dc8
Author: Brett Creeley <brett.creeley@intel.com>
Date:   Fri Feb 8 12:50:55 2019 -0800

    ice: configure GLINT_ITR to always have an ITR gran of 2
    
    Instead of hoping that our ITR granularity will be 2 usec program the
    GLINT_CTL register to make sure the ITR granularity is always 2 usecs.
    
    Now that we know what the ITR granularity will be get rid of the check
    in ice_probe() to verify our previous assumption.
    
    Signed-off-by: Brett Creeley <brett.creeley@intel.com>
    Signed-off-by: Anirudh Venkataramanan <anirudh.venkataramanan@intel.com>
    Tested-by: Andrew Bowers <andrewx.bowers@intel.com>
    Signed-off-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>

diff --git a/drivers/net/ethernet/intel/ice/ice_txrx.h b/drivers/net/ethernet/intel/ice/ice_txrx.h
index fc358ea81816..b7ff0ff82517 100644
--- a/drivers/net/ethernet/intel/ice/ice_txrx.h
+++ b/drivers/net/ethernet/intel/ice/ice_txrx.h
@@ -125,6 +125,7 @@ enum ice_rx_dtype {
 #define ITR_IS_DYNAMIC(setting) (!!((setting) & ICE_ITR_DYNAMIC))
 #define ITR_TO_REG(setting)	((setting) & ~ICE_ITR_DYNAMIC)
 #define ICE_ITR_GRAN_S		1	/* Assume ITR granularity is 2us */
+#define ICE_ITR_GRAN_US		BIT(ICE_ITR_GRAN_S)
 #define ICE_ITR_MASK		0x1FFE	/* ITR register value alignment mask */
 #define ITR_REG_ALIGN(setting)	__ALIGN_MASK(setting, ~ICE_ITR_MASK)
 

commit 67fe64d78c437d3f5a280a074e8467fa9b16216d
Author: Brett Creeley <brett.creeley@intel.com>
Date:   Wed Dec 19 10:03:30 2018 -0800

    ice: Implement getting and setting ethtool coalesce
    
    This patch includes the following ethtool operations:
    
    1. get_coalesce
    2. set_coalesce
    3. get_per_q_coalesce
    4. set_per_q_coalesce
    
    Each ITR value (current_itr/target_itr) are stored on a per
    ice_ring_container basis. This is because each valid ice_ring_container
    can have 1 or more rings that are tied to the same q_vector ITR index.
    
    Signed-off-by: Brett Creeley <brett.creeley@intel.com>
    Signed-off-by: Anirudh Venkataramanan <anirudh.venkataramanan@intel.com>
    Tested-by: Andrew Bowers <andrewx.bowers@intel.com>
    Signed-off-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>

diff --git a/drivers/net/ethernet/intel/ice/ice_txrx.h b/drivers/net/ethernet/intel/ice/ice_txrx.h
index aa646002d653..fc358ea81816 100644
--- a/drivers/net/ethernet/intel/ice/ice_txrx.h
+++ b/drivers/net/ethernet/intel/ice/ice_txrx.h
@@ -118,9 +118,11 @@ enum ice_rx_dtype {
 #define ICE_TX_ITR	ICE_IDX_ITR1
 #define ICE_ITR_8K	124
 #define ICE_ITR_20K	50
+#define ICE_ITR_MAX	8160
 #define ICE_DFLT_TX_ITR	(ICE_ITR_20K | ICE_ITR_DYNAMIC)
 #define ICE_DFLT_RX_ITR	(ICE_ITR_20K | ICE_ITR_DYNAMIC)
 #define ICE_ITR_DYNAMIC	0x8000  /* used as flag for itr_setting */
+#define ITR_IS_DYNAMIC(setting) (!!((setting) & ICE_ITR_DYNAMIC))
 #define ITR_TO_REG(setting)	((setting) & ~ICE_ITR_DYNAMIC)
 #define ICE_ITR_GRAN_S		1	/* Assume ITR granularity is 2us */
 #define ICE_ITR_MASK		0x1FFE	/* ITR register value alignment mask */

commit 63f545ed1285a5f904c260ff22c958609c3c11c5
Author: Brett Creeley <brett.creeley@intel.com>
Date:   Wed Dec 19 10:03:29 2018 -0800

    ice: Add support for adaptive interrupt moderation
    
    Currently the driver does not support adaptive/dynamic interrupt
    moderation. This patch adds support for this. Also, adaptive/dynamic
    interrupt moderation is turned on by default upon driver load.
    
    In order to support adaptive interrupt moderation, two functions were
    added, ice_update_itr() and ice_itr_divisor(). These are used to
    determine the current packet load and to determine a divisor based
    on link speed respectively.
    
    This patch also adds the ICE_ITR_GRAN_S define that is used in the
    hot-path when setting a new ITR value. The shift is used to pet two
    birds with one hand, set the ITR value while re-enabling the
    interrupt. Also, the ICE_ITR_GRAN_S is defined as 1 because the device
    has a ITR granularity of 2usecs.
    
    Signed-off-by: Brett Creeley <brett.creeley@intel.com>
    Signed-off-by: Anirudh Venkataramanan <anirudh.venkataramanan@intel.com>
    Tested-by: Andrew Bowers <andrewx.bowers@intel.com>
    Signed-off-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>

diff --git a/drivers/net/ethernet/intel/ice/ice_txrx.h b/drivers/net/ethernet/intel/ice/ice_txrx.h
index 75d0eaf6c9dd..aa646002d653 100644
--- a/drivers/net/ethernet/intel/ice/ice_txrx.h
+++ b/drivers/net/ethernet/intel/ice/ice_txrx.h
@@ -116,16 +116,15 @@ enum ice_rx_dtype {
 /* indices into GLINT_ITR registers */
 #define ICE_RX_ITR	ICE_IDX_ITR0
 #define ICE_TX_ITR	ICE_IDX_ITR1
-#define ICE_ITR_DYNAMIC	0x8000  /* use top bit as a flag */
-#define ICE_ITR_8K	125
+#define ICE_ITR_8K	124
 #define ICE_ITR_20K	50
-#define ICE_DFLT_TX_ITR	ICE_ITR_20K
-#define ICE_DFLT_RX_ITR	ICE_ITR_20K
-/* apply ITR granularity translation to program the register. itr_gran is either
- * 2 or 4 usecs so we need to divide by 2 first then shift by that value
- */
-#define ITR_TO_REG(val, itr_gran) (((val) & ~ICE_ITR_DYNAMIC) >> \
-				   ((itr_gran) / 2))
+#define ICE_DFLT_TX_ITR	(ICE_ITR_20K | ICE_ITR_DYNAMIC)
+#define ICE_DFLT_RX_ITR	(ICE_ITR_20K | ICE_ITR_DYNAMIC)
+#define ICE_ITR_DYNAMIC	0x8000  /* used as flag for itr_setting */
+#define ITR_TO_REG(setting)	((setting) & ~ICE_ITR_DYNAMIC)
+#define ICE_ITR_GRAN_S		1	/* Assume ITR granularity is 2us */
+#define ICE_ITR_MASK		0x1FFE	/* ITR register value alignment mask */
+#define ITR_REG_ALIGN(setting)	__ALIGN_MASK(setting, ~ICE_ITR_MASK)
 
 #define ICE_DFLT_INTRL	0
 
@@ -180,13 +179,20 @@ enum ice_latency_range {
 };
 
 struct ice_ring_container {
-	/* array of pointers to rings */
+	/* head of linked-list of rings */
 	struct ice_ring *ring;
+	unsigned long next_update;	/* jiffies value of next queue update */
 	unsigned int total_bytes;	/* total bytes processed this int */
 	unsigned int total_pkts;	/* total packets processed this int */
 	enum ice_latency_range latency_range;
-	int itr_idx;	/* index in the interrupt vector */
-	u16 itr;
+	int itr_idx;		/* index in the interrupt vector */
+	u16 target_itr;		/* value in usecs divided by the hw->itr_gran */
+	u16 current_itr;	/* value in usecs divided by the hw->itr_gran */
+	/* high bit set means dynamic ITR, rest is used to store user
+	 * readable ITR value in usecs and must be converted before programming
+	 * to a register.
+	 */
+	u16 itr_setting;
 };
 
 /* iterator for handling rings in ring container */

commit c585ea42ec75e8d3afa278b7095d9f0dd6ee515b
Author: Brett Creeley <brett.creeley@intel.com>
Date:   Fri Oct 26 10:40:58 2018 -0700

    ice: Fix tx_timeout in PF driver
    
    Prior to this commit the driver was running into tx_timeouts when a
    queue was stressed enough. This was happening because the HW tail
    and SW tail (NTU) were incorrectly out of sync. Consequently this was
    causing the HW head to collide with the HW tail, which to the hardware
    means that all descriptors posted for Tx have been processed.
    
    Due to the Tx logic used in the driver SW tail and HW tail are allowed
    to be out of sync. This is done as an optimization because it allows the
    driver to write HW tail as infrequently as possible, while still
    updating the SW tail index to keep track. However, there are situations
    where this results in the tail never getting updated, resulting in Tx
    timeouts.
    
    Tx HW tail write condition:
            if (netif_xmit_stopped(txring_txq(tx_ring) || !skb->xmit_more)
                    writel(sw_tail, tx_ring->tail);
    
    An issue was found in the Tx logic that was causing the afore mentioned
    condition for updating HW tail to never happen, causing tx_timeouts.
    
    In ice_xmit_frame_ring we calculate how many descriptors we need for the
    Tx transaction based on the skb the kernel hands us. This is then passed
    into ice_maybe_stop_tx along with some extra padding to determine if we
    have enough descriptors available for this transaction. If we don't then
    we return -EBUSY to the stack, otherwise we move on and eventually
    prepare the Tx descriptors accordingly in ice_tx_map and set
    next_to_watch. In ice_tx_map we make another call to ice_maybe_stop_tx
    with a value of MAX_SKB_FRAGS + 4. The key here is that this value is
    possibly less than the value we sent in the first call to
    ice_maybe_stop_tx in ice_xmit_frame_ring. Now, if the number of unused
    descriptors is between MAX_SKB_FRAGS + 4 and the value used in the first
    call to ice_maybe_stop_tx in ice_xmit_frame_ring then we do not update
    the HW tail because of the "Tx HW tail write condition" above. This is
    because in ice_maybe_stop_tx we return success from ice_maybe_stop_tx
    instead of calling __ice_maybe_stop_tx and subsequently calling
    netif_stop_subqueue, which sets the __QUEUE_STATE_DEV_XOFF bit. This
    bit is then checked in the "Tx HW tail write condition" by calling
    netif_xmit_stopped and subsequently updating HW tail if the
    afore mentioned bit is set.
    
    In ice_clean_tx_irq, if next_to_watch is not NULL, we end up cleaning
    the descriptors that HW sets the DD bit on and we have the budget. The
    HW head will eventually run into the HW tail in response to the
    description in the paragraph above.
    
    The next time through ice_xmit_frame_ring we make the initial call to
    ice_maybe_stop_tx with another skb from the stack. This time we do not
    have enough descriptors available and we return NETDEV_TX_BUSY to the
    stack and end up setting next_to_watch to NULL.
    
    This is where we are stuck. In ice_clean_tx_irq we never clean anything
    because next_to_watch is always NULL and in ice_xmit_frame_ring we never
    update HW tail because we already return NETDEV_TX_BUSY to the stack and
    eventually we hit a tx_timeout.
    
    This issue was fixed by making sure that the second call to
    ice_maybe_stop_tx in ice_tx_map is passed a value that is >= the value
    that was used on the initial call to ice_maybe_stop_tx in
    ice_xmit_frame_ring. This was done by adding the following defines to
    make the logic more clear and to reduce the chance of mucking this up
    again:
    
    ICE_CACHE_LINE_BYTES            64
    ICE_DESCS_PER_CACHE_LINE        (ICE_CACHE_LINE_BYTES / \
                                     sizeof(struct ice_tx_desc))
    ICE_DESCS_FOR_CTX_DESC          1
    ICE_DESCS_FOR_SKB_DATA_PTR      1
    
    The ICE_CACHE_LINE_BYTES being 64 is an assumption being made so we
    don't have to figure this out on every pass through the Tx path. Instead
    I added a sanity check in ice_probe to verify cache line size and print
    a message if it's not 64 Bytes. This will make it easier to file issues
    if they are seen when the cache line size is not 64 Bytes when reading
    from the GLPCI_CNF2 register.
    
    Signed-off-by: Brett Creeley <brett.creeley@intel.com>
    Signed-off-by: Anirudh Venkataramanan <anirudh.venkataramanan@intel.com>
    Tested-by: Andrew Bowers <andrewx.bowers@intel.com>
    Signed-off-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>

diff --git a/drivers/net/ethernet/intel/ice/ice_txrx.h b/drivers/net/ethernet/intel/ice/ice_txrx.h
index 1d0f58bd389b..75d0eaf6c9dd 100644
--- a/drivers/net/ethernet/intel/ice/ice_txrx.h
+++ b/drivers/net/ethernet/intel/ice/ice_txrx.h
@@ -22,8 +22,21 @@
 #define ICE_RX_BUF_WRITE	16	/* Must be power of 2 */
 #define ICE_MAX_TXQ_PER_TXQG	128
 
-/* Tx Descriptors needed, worst case */
-#define DESC_NEEDED (MAX_SKB_FRAGS + 4)
+/* We are assuming that the cache line is always 64 Bytes here for ice.
+ * In order to make sure that is a correct assumption there is a check in probe
+ * to print a warning if the read from GLPCI_CNF2 tells us that the cache line
+ * size is 128 bytes. We do it this way because we do not want to read the
+ * GLPCI_CNF2 register or a variable containing the value on every pass through
+ * the Tx path.
+ */
+#define ICE_CACHE_LINE_BYTES		64
+#define ICE_DESCS_PER_CACHE_LINE	(ICE_CACHE_LINE_BYTES / \
+					 sizeof(struct ice_tx_desc))
+#define ICE_DESCS_FOR_CTX_DESC		1
+#define ICE_DESCS_FOR_SKB_DATA_PTR	1
+/* Tx descriptors needed, worst case */
+#define DESC_NEEDED (MAX_SKB_FRAGS + ICE_DESCS_FOR_CTX_DESC + \
+		     ICE_DESCS_PER_CACHE_LINE + ICE_DESCS_FOR_SKB_DATA_PTR)
 #define ICE_DESC_UNUSED(R)	\
 	((((R)->next_to_clean > (R)->next_to_use) ? 0 : (R)->count) + \
 	(R)->next_to_clean - (R)->next_to_use - 1)

commit d2b464a7ff6cda3e1d4eb070ed6558f0cd152d1c
Author: Brett Creeley <brett.creeley@intel.com>
Date:   Wed Sep 19 17:43:05 2018 -0700

    ice: Add more flexibility on how we assign an ITR index
    
    This issue came about when looking at the VF function
    ice_vc_cfg_irq_map_msg. Currently we are assigning the itr_setting value
    to the itr_idx received from the AVF driver, which is not correct and is
    not used for the VF flow anyway. Currently the only way we set the ITR
    index for both the PF and VF driver is by hard coding ICE_TX_ITR or
    ICE_RX_ITR for the ITR index on each q_vector.
    
    To fix this, add the member itr_idx in struct ice_ring_container. This
    can then be used to dynamically program the correct ITR index. This change
    also affected the PF driver so make the necessary changes there as well.
    
    Also, removed the itr_setting member in struct ice_ring because it is not
    being used meaningfully and is going to be removed in a future patch that
    includes dynamic ITR.
    
    On another note, this will be useful moving forward if we decide to split
    Rx/Tx rings on different q_vectors instead of sharing them as queue pairs.
    
    Signed-off-by: Brett Creeley <brett.creeley@intel.com>
    Signed-off-by: Anirudh Venkataramanan <anirudh.venkataramanan@intel.com>
    Tested-by: Andrew Bowers <andrewx.bowers@intel.com>
    Signed-off-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>

diff --git a/drivers/net/ethernet/intel/ice/ice_txrx.h b/drivers/net/ethernet/intel/ice/ice_txrx.h
index a9b92974e041..1d0f58bd389b 100644
--- a/drivers/net/ethernet/intel/ice/ice_txrx.h
+++ b/drivers/net/ethernet/intel/ice/ice_txrx.h
@@ -105,8 +105,9 @@ enum ice_rx_dtype {
 #define ICE_TX_ITR	ICE_IDX_ITR1
 #define ICE_ITR_DYNAMIC	0x8000  /* use top bit as a flag */
 #define ICE_ITR_8K	125
-#define ICE_DFLT_TX_ITR	ICE_ITR_8K
-#define ICE_DFLT_RX_ITR	ICE_ITR_8K
+#define ICE_ITR_20K	50
+#define ICE_DFLT_TX_ITR	ICE_ITR_20K
+#define ICE_DFLT_RX_ITR	ICE_ITR_20K
 /* apply ITR granularity translation to program the register. itr_gran is either
  * 2 or 4 usecs so we need to divide by 2 first then shift by that value
  */
@@ -135,13 +136,6 @@ struct ice_ring {
 	u16 q_index;			/* Queue number of ring */
 	u32 txq_teid;			/* Added Tx queue TEID */
 
-	/* high bit set means dynamic, use accessor routines to read/write.
-	 * hardware supports 4us/2us resolution for the ITR registers.
-	 * these values always store the USER setting, and must be converted
-	 * before programming to a register.
-	 */
-	u16 itr_setting;
-
 	u16 count;			/* Number of descriptors */
 	u16 reg_idx;			/* HW register index of the ring */
 
@@ -178,6 +172,7 @@ struct ice_ring_container {
 	unsigned int total_bytes;	/* total bytes processed this int */
 	unsigned int total_pkts;	/* total packets processed this int */
 	enum ice_latency_range latency_range;
+	int itr_idx;	/* index in the interrupt vector */
 	u16 itr;
 };
 

commit 9e4ab4c29a62d2ccbf4be42707669be2f42d391c
Author: Brett Creeley <brett.creeley@intel.com>
Date:   Wed Sep 19 17:23:19 2018 -0700

    ice: Add support for dynamic interrupt moderation
    
    Currently there is no support for dynamic interrupt moderation. This
    patch adds some initial code to support this. The following changes
    were made:
    
    1. Currently we are using multiple members to store the interrupt
       granularity (itr_gran_25/50/100/200). This is not necessary because
       we can query the device to determine what the interrupt granularity
       should be set to, done by a new function ice_get_itr_intrl_gran.
    
    2. Added intrl to ice_q_vector structure to support interrupt rate
       limiting.
    
    3. Added the function ice_intrl_usecs_to_reg for converting to a value
       in usecs that the device understands.
    
    4. Added call to write to the GLINT_RATE register. Disable intrl by
       default for now.
    
    5. Changed rx/tx_itr_setting to itr_setting because having both seems
       redundant because a ring is either Tx or Rx.
    
    6. Initialize itr_setting for both Tx/Rx rings in ice_vsi_alloc_rings()
    
    Signed-off-by: Brett Creeley <brett.creeley@intel.com>
    Signed-off-by: Anirudh Venkataramanan <anirudh.venkataramanan@intel.com>
    Tested-by: Andrew Bowers <andrewx.bowers@intel.com>
    Signed-off-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>

diff --git a/drivers/net/ethernet/intel/ice/ice_txrx.h b/drivers/net/ethernet/intel/ice/ice_txrx.h
index 839fd9ff6043..a9b92974e041 100644
--- a/drivers/net/ethernet/intel/ice/ice_txrx.h
+++ b/drivers/net/ethernet/intel/ice/ice_txrx.h
@@ -104,10 +104,16 @@ enum ice_rx_dtype {
 #define ICE_RX_ITR	ICE_IDX_ITR0
 #define ICE_TX_ITR	ICE_IDX_ITR1
 #define ICE_ITR_DYNAMIC	0x8000  /* use top bit as a flag */
-#define ICE_ITR_8K	0x003E
+#define ICE_ITR_8K	125
+#define ICE_DFLT_TX_ITR	ICE_ITR_8K
+#define ICE_DFLT_RX_ITR	ICE_ITR_8K
+/* apply ITR granularity translation to program the register. itr_gran is either
+ * 2 or 4 usecs so we need to divide by 2 first then shift by that value
+ */
+#define ITR_TO_REG(val, itr_gran) (((val) & ~ICE_ITR_DYNAMIC) >> \
+				   ((itr_gran) / 2))
 
-/* apply ITR HW granularity translation to program the HW registers */
-#define ITR_TO_REG(val, itr_gran) (((val) & ~ICE_ITR_DYNAMIC) >> (itr_gran))
+#define ICE_DFLT_INTRL	0
 
 /* Legacy or Advanced Mode Queue */
 #define ICE_TX_ADVANCED	0
@@ -130,12 +136,11 @@ struct ice_ring {
 	u32 txq_teid;			/* Added Tx queue TEID */
 
 	/* high bit set means dynamic, use accessor routines to read/write.
-	 * hardware supports 2us/1us resolution for the ITR registers.
+	 * hardware supports 4us/2us resolution for the ITR registers.
 	 * these values always store the USER setting, and must be converted
 	 * before programming to a register.
 	 */
-	u16 rx_itr_setting;
-	u16 tx_itr_setting;
+	u16 itr_setting;
 
 	u16 count;			/* Number of descriptors */
 	u16 reg_idx;			/* HW register index of the ring */

commit b3969fd727aa1f2ace4794f2a180f9769128027c
Author: Sudheer Mogilappagari <sudheer.mogilappagari@intel.com>
Date:   Thu Aug 9 06:29:53 2018 -0700

    ice: Add support for Tx hang, Tx timeout and malicious driver detection
    
    When a malicious operation is detected, the firmware triggers an
    interrupt, which is then picked up by the service task (specifically by
    ice_handle_mdd_event). A reset is scheduled if required.
    
    Tx hang detection works in a similar way, except the logic here monitors
    the VSI's Tx queues and tries to revive them if stalled. If the hang is
    not resolved, the kernel eventually calls ndo_tx_timeout, which is
    handled by ice_tx_timeout.
    
    Signed-off-by: Sudheer Mogilappagari <sudheer.mogilappagari@intel.com>
    Signed-off-by: Anirudh Venkataramanan <anirudh.venkataramanan@intel.com>
    Tested-by: Tony Brelinski <tonyx.brelinski@intel.com>
    Signed-off-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>

diff --git a/drivers/net/ethernet/intel/ice/ice_txrx.h b/drivers/net/ethernet/intel/ice/ice_txrx.h
index 31bc998fe200..839fd9ff6043 100644
--- a/drivers/net/ethernet/intel/ice/ice_txrx.h
+++ b/drivers/net/ethernet/intel/ice/ice_txrx.h
@@ -71,6 +71,7 @@ struct ice_txq_stats {
 	u64 restart_q;
 	u64 tx_busy;
 	u64 tx_linearize;
+	int prev_pkt; /* negative if no pending Tx descriptors */
 };
 
 struct ice_rxq_stats {

commit 43f8b22450f0a721915f1d2c7e3db6dd9573e76b
Author: Bruce Allan <bruce.w.allan@intel.com>
Date:   Thu Aug 9 06:29:02 2018 -0700

    ice: Change struct members from bool to u8
    
    Recent versions of checkpatch have a new warning based on a documented
    preference of Linus to not use bool in structures due to wasted space and
    the size of bool is implementation dependent.  For more information, see
    the email thread at https://lkml.org/lkml/2017/11/21/384.
    
    Signed-off-by: Bruce Allan <bruce.w.allan@intel.com>
    Signed-off-by: Anirudh Venkataramanan <anirudh.venkataramanan@intel.com>
    Tested-by: Tony Brelinski <tonyx.brelinski@intel.com>
    Signed-off-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>

diff --git a/drivers/net/ethernet/intel/ice/ice_txrx.h b/drivers/net/ethernet/intel/ice/ice_txrx.h
index 567067b650c4..31bc998fe200 100644
--- a/drivers/net/ethernet/intel/ice/ice_txrx.h
+++ b/drivers/net/ethernet/intel/ice/ice_txrx.h
@@ -143,7 +143,7 @@ struct ice_ring {
 	u16 next_to_use;
 	u16 next_to_clean;
 
-	bool ring_active;		/* is ring online or not */
+	u8 ring_active;			/* is ring online or not */
 
 	/* stats structs */
 	struct ice_q_stats	stats;

commit d76a60ba7afb89523c88cf2ed3a044ce4180289e
Author: Anirudh Venkataramanan <anirudh.venkataramanan@intel.com>
Date:   Tue Mar 20 07:58:15 2018 -0700

    ice: Add support for VLANs and offloads
    
    This patch adds support for VLANs. When a VLAN is created a switch filter
    is added to direct the VLAN traffic to the corresponding VSI. When a VLAN
    is deleted, the filter is deleted as well.
    
    This patch also adds support for the following hardware offloads.
        1) VLAN tag insertion/stripping
        2) Receive Side Scaling (RSS)
        3) Tx checksum and TCP segmentation
        4) Rx checksum
    
    Signed-off-by: Anirudh Venkataramanan <anirudh.venkataramanan@intel.com>
    Tested-by: Tony Brelinski <tonyx.brelinski@intel.com>
    Signed-off-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>

diff --git a/drivers/net/ethernet/intel/ice/ice_txrx.h b/drivers/net/ethernet/intel/ice/ice_txrx.h
index 8fa4450514b4..567067b650c4 100644
--- a/drivers/net/ethernet/intel/ice/ice_txrx.h
+++ b/drivers/net/ethernet/intel/ice/ice_txrx.h
@@ -28,6 +28,12 @@
 	((((R)->next_to_clean > (R)->next_to_use) ? 0 : (R)->count) + \
 	(R)->next_to_clean - (R)->next_to_use - 1)
 
+#define ICE_TX_FLAGS_TSO	BIT(0)
+#define ICE_TX_FLAGS_HW_VLAN	BIT(1)
+#define ICE_TX_FLAGS_SW_VLAN	BIT(2)
+#define ICE_TX_FLAGS_VLAN_M	0xffff0000
+#define ICE_TX_FLAGS_VLAN_S	16
+
 struct ice_tx_buf {
 	struct ice_tx_desc *next_to_watch;
 	struct sk_buff *skb;
@@ -38,6 +44,17 @@ struct ice_tx_buf {
 	DEFINE_DMA_UNMAP_LEN(len);
 };
 
+struct ice_tx_offload_params {
+	u8 header_len;
+	u32 td_cmd;
+	u32 td_offset;
+	u32 td_l2tag1;
+	u16 cd_l2tag2;
+	u32 cd_tunnel_params;
+	u64 cd_qw1;
+	struct ice_ring *tx_ring;
+};
+
 struct ice_rx_buf {
 	struct sk_buff *skb;
 	dma_addr_t dma;

commit 2b245cb29421abbad508e93cdfedf81adc12edf1
Author: Anirudh Venkataramanan <anirudh.venkataramanan@intel.com>
Date:   Tue Mar 20 07:58:14 2018 -0700

    ice: Implement transmit and NAPI support
    
    This patch implements ice_start_xmit (the handler for ndo_start_xmit) and
    related functions. ice_start_xmit ultimately calls ice_tx_map, where the
    Tx descriptor is built and posted to the hardware by bumping the ring tail.
    
    This patch also implements ice_napi_poll, which is invoked when there's an
    interrupt on the VSI's queues. The interrupt can be due to either a
    completed Tx or an Rx event. In case of a completed Tx/Rx event, resources
    are reclaimed. Additionally, in case of an Rx event, the skb is fetched
    and passed up to the network stack.
    
    Signed-off-by: Anirudh Venkataramanan <anirudh.venkataramanan@intel.com>
    Tested-by: Tony Brelinski <tonyx.brelinski@intel.com>
    Signed-off-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>

diff --git a/drivers/net/ethernet/intel/ice/ice_txrx.h b/drivers/net/ethernet/intel/ice/ice_txrx.h
index b7015cfad2d7..8fa4450514b4 100644
--- a/drivers/net/ethernet/intel/ice/ice_txrx.h
+++ b/drivers/net/ethernet/intel/ice/ice_txrx.h
@@ -7,8 +7,23 @@
 #define ICE_DFLT_IRQ_WORK	256
 #define ICE_RXBUF_2048		2048
 #define ICE_MAX_CHAINED_RX_BUFS	5
+#define ICE_MAX_BUF_TXD		8
+#define ICE_MIN_TX_LEN		17
+
+/* The size limit for a transmit buffer in a descriptor is (16K - 1).
+ * In order to align with the read requests we will align the value to
+ * the nearest 4K which represents our maximum read request size.
+ */
+#define ICE_MAX_READ_REQ_SIZE	4096
+#define ICE_MAX_DATA_PER_TXD	(16 * 1024 - 1)
+#define ICE_MAX_DATA_PER_TXD_ALIGNED \
+	(~(ICE_MAX_READ_REQ_SIZE - 1) & ICE_MAX_DATA_PER_TXD)
+
+#define ICE_RX_BUF_WRITE	16	/* Must be power of 2 */
 #define ICE_MAX_TXQ_PER_TXQG	128
 
+/* Tx Descriptors needed, worst case */
+#define DESC_NEEDED (MAX_SKB_FRAGS + 4)
 #define ICE_DESC_UNUSED(R)	\
 	((((R)->next_to_clean > (R)->next_to_use) ? 0 : (R)->count) + \
 	(R)->next_to_clean - (R)->next_to_use - 1)
@@ -30,6 +45,24 @@ struct ice_rx_buf {
 	unsigned int page_offset;
 };
 
+struct ice_q_stats {
+	u64 pkts;
+	u64 bytes;
+};
+
+struct ice_txq_stats {
+	u64 restart_q;
+	u64 tx_busy;
+	u64 tx_linearize;
+};
+
+struct ice_rxq_stats {
+	u64 non_eop_descs;
+	u64 alloc_page_failed;
+	u64 alloc_buf_failed;
+	u64 page_reuse_count;
+};
+
 /* this enum matches hardware bits and is meant to be used by DYN_CTLN
  * registers and QINT registers or more generally anywhere in the manual
  * mentioning ITR_INDX, ITR_NONE cannot be used as an index 'n' into any
@@ -94,6 +127,15 @@ struct ice_ring {
 	u16 next_to_clean;
 
 	bool ring_active;		/* is ring online or not */
+
+	/* stats structs */
+	struct ice_q_stats	stats;
+	struct u64_stats_sync syncp;
+	union {
+		struct ice_txq_stats tx_stats;
+		struct ice_rxq_stats rx_stats;
+	};
+
 	unsigned int size;		/* length of descriptor ring in bytes */
 	dma_addr_t dma;			/* physical address of ring */
 	struct rcu_head rcu;		/* to avoid race on free */
@@ -121,10 +163,13 @@ struct ice_ring_container {
 	for (pos = (head).ring; pos; pos = pos->next)
 
 bool ice_alloc_rx_bufs(struct ice_ring *rxr, u16 cleaned_count);
+netdev_tx_t ice_start_xmit(struct sk_buff *skb, struct net_device *netdev);
 void ice_clean_tx_ring(struct ice_ring *tx_ring);
 void ice_clean_rx_ring(struct ice_ring *rx_ring);
 int ice_setup_tx_ring(struct ice_ring *tx_ring);
 int ice_setup_rx_ring(struct ice_ring *rx_ring);
 void ice_free_tx_ring(struct ice_ring *tx_ring);
 void ice_free_rx_ring(struct ice_ring *rx_ring);
+int ice_napi_poll(struct napi_struct *napi, int budget);
+
 #endif /* _ICE_TXRX_H_ */

commit cdedef59deb020e78721d820a5692100128c8c73
Author: Anirudh Venkataramanan <anirudh.venkataramanan@intel.com>
Date:   Tue Mar 20 07:58:13 2018 -0700

    ice: Configure VSIs for Tx/Rx
    
    This patch configures the VSIs to be able to send and receive
    packets by doing the following:
    
    1) Initialize flexible parser to extract and include certain
       fields in the Rx descriptor.
    
    2) Add Tx queues by programming the Tx queue context (implemented in
       ice_vsi_cfg_txqs). Note that adding the queues also enables (starts)
       the queues.
    
    3) Add Rx queues by programming Rx queue context (implemented in
       ice_vsi_cfg_rxqs). Note that this only adds queues but doesn't start
       them. The rings will be started by calling ice_vsi_start_rx_rings on
       interface up.
    
    4) Configure interrupts for VSI queues.
    
    5) Implement ice_open and ice_stop.
    
    Signed-off-by: Anirudh Venkataramanan <anirudh.venkataramanan@intel.com>
    Tested-by: Tony Brelinski <tonyx.brelinski@intel.com>
    Signed-off-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>

diff --git a/drivers/net/ethernet/intel/ice/ice_txrx.h b/drivers/net/ethernet/intel/ice/ice_txrx.h
index 2dd2232127a7..b7015cfad2d7 100644
--- a/drivers/net/ethernet/intel/ice/ice_txrx.h
+++ b/drivers/net/ethernet/intel/ice/ice_txrx.h
@@ -5,6 +5,30 @@
 #define _ICE_TXRX_H_
 
 #define ICE_DFLT_IRQ_WORK	256
+#define ICE_RXBUF_2048		2048
+#define ICE_MAX_CHAINED_RX_BUFS	5
+#define ICE_MAX_TXQ_PER_TXQG	128
+
+#define ICE_DESC_UNUSED(R)	\
+	((((R)->next_to_clean > (R)->next_to_use) ? 0 : (R)->count) + \
+	(R)->next_to_clean - (R)->next_to_use - 1)
+
+struct ice_tx_buf {
+	struct ice_tx_desc *next_to_watch;
+	struct sk_buff *skb;
+	unsigned int bytecount;
+	unsigned short gso_segs;
+	u32 tx_flags;
+	DEFINE_DMA_UNMAP_ADDR(dma);
+	DEFINE_DMA_UNMAP_LEN(len);
+};
+
+struct ice_rx_buf {
+	struct sk_buff *skb;
+	dma_addr_t dma;
+	struct page *page;
+	unsigned int page_offset;
+};
 
 /* this enum matches hardware bits and is meant to be used by DYN_CTLN
  * registers and QINT registers or more generally anywhere in the manual
@@ -18,33 +42,77 @@ enum ice_dyn_idx_t {
 	ICE_ITR_NONE = 3	/* ITR_NONE must not be used as an index */
 };
 
+/* Header split modes defined by DTYPE field of Rx RLAN context */
+enum ice_rx_dtype {
+	ICE_RX_DTYPE_NO_SPLIT		= 0,
+	ICE_RX_DTYPE_HEADER_SPLIT	= 1,
+	ICE_RX_DTYPE_SPLIT_ALWAYS	= 2,
+};
+
 /* indices into GLINT_ITR registers */
 #define ICE_RX_ITR	ICE_IDX_ITR0
+#define ICE_TX_ITR	ICE_IDX_ITR1
 #define ICE_ITR_DYNAMIC	0x8000  /* use top bit as a flag */
 #define ICE_ITR_8K	0x003E
 
 /* apply ITR HW granularity translation to program the HW registers */
 #define ITR_TO_REG(val, itr_gran) (((val) & ~ICE_ITR_DYNAMIC) >> (itr_gran))
 
+/* Legacy or Advanced Mode Queue */
+#define ICE_TX_ADVANCED	0
+#define ICE_TX_LEGACY	1
+
 /* descriptor ring, associated with a VSI */
 struct ice_ring {
 	struct ice_ring *next;		/* pointer to next ring in q_vector */
+	void *desc;			/* Descriptor ring memory */
 	struct device *dev;		/* Used for DMA mapping */
 	struct net_device *netdev;	/* netdev ring maps to */
 	struct ice_vsi *vsi;		/* Backreference to associated VSI */
 	struct ice_q_vector *q_vector;	/* Backreference to associated vector */
+	u8 __iomem *tail;
+	union {
+		struct ice_tx_buf *tx_buf;
+		struct ice_rx_buf *rx_buf;
+	};
 	u16 q_index;			/* Queue number of ring */
+	u32 txq_teid;			/* Added Tx queue TEID */
+
+	/* high bit set means dynamic, use accessor routines to read/write.
+	 * hardware supports 2us/1us resolution for the ITR registers.
+	 * these values always store the USER setting, and must be converted
+	 * before programming to a register.
+	 */
+	u16 rx_itr_setting;
+	u16 tx_itr_setting;
+
 	u16 count;			/* Number of descriptors */
 	u16 reg_idx;			/* HW register index of the ring */
+
+	/* used in interrupt processing */
+	u16 next_to_use;
+	u16 next_to_clean;
+
 	bool ring_active;		/* is ring online or not */
+	unsigned int size;		/* length of descriptor ring in bytes */
+	dma_addr_t dma;			/* physical address of ring */
 	struct rcu_head rcu;		/* to avoid race on free */
+	u16 next_to_alloc;
 } ____cacheline_internodealigned_in_smp;
 
+enum ice_latency_range {
+	ICE_LOWEST_LATENCY = 0,
+	ICE_LOW_LATENCY = 1,
+	ICE_BULK_LATENCY = 2,
+	ICE_ULTRA_LATENCY = 3,
+};
+
 struct ice_ring_container {
 	/* array of pointers to rings */
 	struct ice_ring *ring;
 	unsigned int total_bytes;	/* total bytes processed this int */
 	unsigned int total_pkts;	/* total packets processed this int */
+	enum ice_latency_range latency_range;
 	u16 itr;
 };
 
@@ -52,4 +120,11 @@ struct ice_ring_container {
 #define ice_for_each_ring(pos, head) \
 	for (pos = (head).ring; pos; pos = pos->next)
 
+bool ice_alloc_rx_bufs(struct ice_ring *rxr, u16 cleaned_count);
+void ice_clean_tx_ring(struct ice_ring *tx_ring);
+void ice_clean_rx_ring(struct ice_ring *rx_ring);
+int ice_setup_tx_ring(struct ice_ring *tx_ring);
+int ice_setup_rx_ring(struct ice_ring *rx_ring);
+void ice_free_tx_ring(struct ice_ring *tx_ring);
+void ice_free_rx_ring(struct ice_ring *rx_ring);
 #endif /* _ICE_TXRX_H_ */

commit 3a858ba392c3b19986c40a4c170ddc37b144115f
Author: Anirudh Venkataramanan <anirudh.venkataramanan@intel.com>
Date:   Tue Mar 20 07:58:11 2018 -0700

    ice: Add support for VSI allocation and deallocation
    
    This patch introduces data structures and functions to alloc/free
    VSIs. The driver represents a VSI using the ice_vsi structure.
    
    Some noteworthy points about VSI allocation:
    
    1) A VSI is allocated in the firmware using the "add VSI" admin queue
       command (implemented as ice_aq_add_vsi). The firmware returns an
       identifier for the allocated VSI. The VSI context is used to program
       certain aspects (loopback, queue map, etc.) of the VSI's configuration.
    
    2) A VSI is deleted using the "free VSI" admin queue command (implemented
       as ice_aq_free_vsi).
    
    3) The driver represents a VSI using struct ice_vsi. This is allocated
       and initialized as part of the ice_vsi_alloc flow, and deallocated
       as part of the ice_vsi_delete flow.
    
    4) Once the VSI is created, a netdev is allocated and associated with it.
       The VSI's ring and vector related data structures are also allocated
       and initialized.
    
    5) A VSI's queues can either be contiguous or scattered. To do this, the
       driver maintains a bitmap (vsi->avail_txqs) which is kept in sync with
       the firmware's VSI queue allocation imap. If the VSI can't get a
       contiguous queue allocation, it will fallback to scatter. This is
       implemented in ice_vsi_get_qs which is called as part of the VSI setup
       flow. In the release flow, the VSI's queues are released and the bitmap
       is updated to reflect this by ice_vsi_put_qs.
    
    CC: Shannon Nelson <shannon.nelson@oracle.com>
    Signed-off-by: Anirudh Venkataramanan <anirudh.venkataramanan@intel.com>
    Acked-by: Shannon Nelson <shannon.nelson@oracle.com>
    Tested-by: Tony Brelinski <tonyx.brelinski@intel.com>
    Signed-off-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>

diff --git a/drivers/net/ethernet/intel/ice/ice_txrx.h b/drivers/net/ethernet/intel/ice/ice_txrx.h
index 7ceb69ea745e..2dd2232127a7 100644
--- a/drivers/net/ethernet/intel/ice/ice_txrx.h
+++ b/drivers/net/ethernet/intel/ice/ice_txrx.h
@@ -26,4 +26,30 @@ enum ice_dyn_idx_t {
 /* apply ITR HW granularity translation to program the HW registers */
 #define ITR_TO_REG(val, itr_gran) (((val) & ~ICE_ITR_DYNAMIC) >> (itr_gran))
 
+/* descriptor ring, associated with a VSI */
+struct ice_ring {
+	struct ice_ring *next;		/* pointer to next ring in q_vector */
+	struct device *dev;		/* Used for DMA mapping */
+	struct net_device *netdev;	/* netdev ring maps to */
+	struct ice_vsi *vsi;		/* Backreference to associated VSI */
+	struct ice_q_vector *q_vector;	/* Backreference to associated vector */
+	u16 q_index;			/* Queue number of ring */
+	u16 count;			/* Number of descriptors */
+	u16 reg_idx;			/* HW register index of the ring */
+	bool ring_active;		/* is ring online or not */
+	struct rcu_head rcu;		/* to avoid race on free */
+} ____cacheline_internodealigned_in_smp;
+
+struct ice_ring_container {
+	/* array of pointers to rings */
+	struct ice_ring *ring;
+	unsigned int total_bytes;	/* total bytes processed this int */
+	unsigned int total_pkts;	/* total packets processed this int */
+	u16 itr;
+};
+
+/* iterator for handling rings in ring container */
+#define ice_for_each_ring(pos, head) \
+	for (pos = (head).ring; pos; pos = pos->next)
+
 #endif /* _ICE_TXRX_H_ */

commit 940b61af02f497fcd911b9e2d75c6b8cf76b92fd
Author: Anirudh Venkataramanan <anirudh.venkataramanan@intel.com>
Date:   Tue Mar 20 07:58:10 2018 -0700

    ice: Initialize PF and setup miscellaneous interrupt
    
    This patch continues the initialization flow as follows:
    
    1) Allocate and initialize necessary fields (like vsi, num_alloc_vsi,
       irq_tracker, etc) in the ice_pf instance.
    
    2) Setup the miscellaneous interrupt handler. This also known as the
       "other interrupt causes" (OIC) handler and is used to handle non
       hotpath interrupts (like control queue events, link events,
       exceptions, etc.
    
    3) Implement a background task to process admin queue receive (ARQ)
       events received by the driver.
    
    CC: Shannon Nelson <shannon.nelson@oracle.com>
    Signed-off-by: Anirudh Venkataramanan <anirudh.venkataramanan@intel.com>
    Acked-by: Shannon Nelson <shannon.nelson@oracle.com>
    Tested-by: Tony Brelinski <tonyx.brelinski@intel.com>
    Signed-off-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>

diff --git a/drivers/net/ethernet/intel/ice/ice_txrx.h b/drivers/net/ethernet/intel/ice/ice_txrx.h
new file mode 100644
index 000000000000..7ceb69ea745e
--- /dev/null
+++ b/drivers/net/ethernet/intel/ice/ice_txrx.h
@@ -0,0 +1,29 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/* Copyright (c) 2018, Intel Corporation. */
+
+#ifndef _ICE_TXRX_H_
+#define _ICE_TXRX_H_
+
+#define ICE_DFLT_IRQ_WORK	256
+
+/* this enum matches hardware bits and is meant to be used by DYN_CTLN
+ * registers and QINT registers or more generally anywhere in the manual
+ * mentioning ITR_INDX, ITR_NONE cannot be used as an index 'n' into any
+ * register but instead is a special value meaning "don't update" ITR0/1/2.
+ */
+enum ice_dyn_idx_t {
+	ICE_IDX_ITR0 = 0,
+	ICE_IDX_ITR1 = 1,
+	ICE_IDX_ITR2 = 2,
+	ICE_ITR_NONE = 3	/* ITR_NONE must not be used as an index */
+};
+
+/* indices into GLINT_ITR registers */
+#define ICE_RX_ITR	ICE_IDX_ITR0
+#define ICE_ITR_DYNAMIC	0x8000  /* use top bit as a flag */
+#define ICE_ITR_8K	0x003E
+
+/* apply ITR HW granularity translation to program the HW registers */
+#define ITR_TO_REG(val, itr_gran) (((val) & ~ICE_ITR_DYNAMIC) >> (itr_gran))
+
+#endif /* _ICE_TXRX_H_ */
