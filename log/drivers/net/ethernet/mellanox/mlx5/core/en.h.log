commit 1806c13dc2532090d742ce03847b22367fb20ad6
Merge: 1079a34c56c5 bdc48fa11e46
Author: David S. Miller <davem@davemloft.net>
Date:   Sun May 31 17:48:46 2020 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/netdev/net
    
    xdp_umem.c had overlapping changes between the 64-bit math fix
    for the calculation of npgs and the removal of the zerocopy
    memory type which got rid of the chunk_size_nohdr member.
    
    The mlx5 Kconfig conflict is a case where we just take the
    net-next copy of the Kconfig entry dependency as it takes on
    the ESWITCH dependency by one level of indirection which is
    what the 'net' conflicting change is trying to ensure.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit ebeaf084ad5c0eeaf8ea3314f62cc28cb79d529f
Author: Tal Gilboa <talgi@mellanox.com>
Date:   Thu Apr 23 13:23:06 2020 +0300

    net/mlx5e: Properly set default values when disabling adaptive moderation
    
    Add a call to mlx5e_reset_rx/tx_moderation() when enabling/disabling
    adaptive moderation, in order to select the proper default values.
    
    In order to do so, we separate the logic of selecting the moderation values
    and setting moderion mode (CQE/EQE based).
    
    Fixes: 0088cbbc4b66 ("net/mlx5e: Enable CQE based moderation on TX CQ")
    Fixes: 9908aa292971 ("net/mlx5e: CQE based moderation")
    Signed-off-by: Tal Gilboa <talgi@mellanox.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 59745402747b..0a5aada0f50f 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -1068,10 +1068,12 @@ void mlx5e_deactivate_priv_channels(struct mlx5e_priv *priv);
 
 void mlx5e_build_default_indir_rqt(u32 *indirection_rqt, int len,
 				   int num_channels);
-void mlx5e_set_tx_cq_mode_params(struct mlx5e_params *params,
-				 u8 cq_period_mode);
-void mlx5e_set_rx_cq_mode_params(struct mlx5e_params *params,
-				 u8 cq_period_mode);
+
+void mlx5e_reset_tx_moderation(struct mlx5e_params *params, u8 cq_period_mode);
+void mlx5e_reset_rx_moderation(struct mlx5e_params *params, u8 cq_period_mode);
+void mlx5e_set_tx_cq_mode_params(struct mlx5e_params *params, u8 cq_period_mode);
+void mlx5e_set_rx_cq_mode_params(struct mlx5e_params *params, u8 cq_period_mode);
+
 void mlx5e_set_rq_type(struct mlx5_core_dev *mdev, struct mlx5e_params *params);
 void mlx5e_init_rq_type_params(struct mlx5_core_dev *mdev,
 			       struct mlx5e_params *params);

commit 13209a8f7304a34158f4366e8ea07a1965c05ac7
Merge: 316107119f47 98790bbac4db
Author: David S. Miller <davem@davemloft.net>
Date:   Sun May 24 13:47:27 2020 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/netdev/net
    
    The MSCC bug fix in 'net' had to be slightly adjusted because the
    register accesses are done slightly differently in net-next.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit a152b85984a03e7f83b9d8bcf908c29597d898fc
Merge: 1e6a70526640 a5dfaa2ab940
Author: David S. Miller <davem@davemloft.net>
Date:   Fri May 22 18:30:34 2020 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/bpf/bpf-next
    
    Daniel Borkmann says:
    
    ====================
    pull-request: bpf-next 2020-05-23
    
    The following pull-request contains BPF updates for your *net-next* tree.
    
    We've added 50 non-merge commits during the last 8 day(s) which contain
    a total of 109 files changed, 2776 insertions(+), 2887 deletions(-).
    
    The main changes are:
    
    1) Add a new AF_XDP buffer allocation API to the core in order to help
       lowering the bar for drivers adopting AF_XDP support. i40e, ice, ixgbe
       as well as mlx5 have been moved over to the new API and also gained a
       small improvement in performance, from Björn Töpel and Magnus Karlsson.
    
    2) Add getpeername()/getsockname() attach types for BPF sock_addr programs
       in order to allow for e.g. reverse translation of load-balancer backend
       to service address/port tuple from a connected peer, from Daniel Borkmann.
    
    3) Improve the BPF verifier is_branch_taken() logic to evaluate pointers
       being non-NULL, e.g. if after an initial test another non-NULL test on
       that pointer follows in a given path, then it can be pruned right away,
       from John Fastabend.
    
    4) Larger rework of BPF sockmap selftests to make output easier to understand
       and to reduce overall runtime as well as adding new BPF kTLS selftests
       that run in combination with sockmap, also from John Fastabend.
    
    5) Batch of misc updates to BPF selftests including fixing up test_align
       to match verifier output again and moving it under test_progs, allowing
       bpf_iter selftest to compile on machines with older vmlinux.h, and
       updating config options for lirc and v6 segment routing helpers, from
       Stanislav Fomichev, Andrii Nakryiko and Alan Maguire.
    
    6) Conversion of BPF tracing samples outdated internal BPF loader to use
       libbpf API instead, from Daniel T. Lee.
    
    7) Follow-up to BPF kernel test infrastructure in order to fix a flake in
       the XDP selftests, from Jesper Dangaard Brouer.
    
    8) Minor improvements to libbpf's internal hashmap implementation, from
       Ian Rogers.
    ====================
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit a16b8e0dcf7043bee46174bed0553cc9e36b63a5
Author: Roi Dayan <roid@mellanox.com>
Date:   Thu Apr 30 09:16:01 2020 +0300

    net/mlx5e: Fix inner tirs handling
    
    In the cited commit inner_tirs argument was added to create and destroy
    inner tirs, and no indication was added to mlx5e_modify_tirs_hash()
    function. In order to have a consistent handling, use
    inner_indir_tir[0].tirn in tirs destroy/modify function as an indication
    to whether inner tirs are created.
    Inner tirs are not created for representors and before this commit,
    a call to mlx5e_modify_tirs_hash() was sending HW commands to
    modify non-existent inner tirs.
    
    Fixes: 46dc933cee82 ("net/mlx5e: Provide explicit directive if to create inner indirect tirs")
    Signed-off-by: Roi Dayan <roid@mellanox.com>
    Reviewed-by: Vlad Buslov <vladbu@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 23701c0e36ec..59745402747b 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -1121,7 +1121,7 @@ void mlx5e_close_drop_rq(struct mlx5e_rq *drop_rq);
 int mlx5e_create_indirect_rqt(struct mlx5e_priv *priv);
 
 int mlx5e_create_indirect_tirs(struct mlx5e_priv *priv, bool inner_ttc);
-void mlx5e_destroy_indirect_tirs(struct mlx5e_priv *priv, bool inner_ttc);
+void mlx5e_destroy_indirect_tirs(struct mlx5e_priv *priv);
 
 int mlx5e_create_direct_rqts(struct mlx5e_priv *priv, struct mlx5e_tir *tirs);
 void mlx5e_destroy_direct_rqts(struct mlx5e_priv *priv, struct mlx5e_tir *tirs);

commit 39d6443c8daf9fefcfcf89de7ae87240956a0b84
Author: Björn Töpel <bjorn.topel@intel.com>
Date:   Wed May 20 21:20:59 2020 +0200

    mlx5, xsk: Migrate to new MEM_TYPE_XSK_BUFF_POOL
    
    Use the new MEM_TYPE_XSK_BUFF_POOL API in lieu of MEM_TYPE_ZERO_COPY in
    mlx5e. It allows to drop a lot of code from the driver (which is now
    common in AF_XDP core and was related to XSK RX frame allocation, DMA
    mapping, etc.) and slightly improve performance (RX +0.8 Mpps, TX +0.4
    Mpps).
    
    rfc->v1: Put back the sanity check for XSK params, use XSK API to get
             the total headroom size. (Maxim)
    
    v1->v2: Fix DMA address handling, set XDP metadata to invalid. (Maxim)
    
    v2->v3: Handle frame_sz, use xsk_buff_xdp_get_frame_dma, use xsk_buff
            API for DMA sync on TX, add performance numbers. (Maxim)
    
    v3->v4: Remove unused variable num_xsk_frames. (Jakub)
    
    Signed-off-by: Björn Töpel <bjorn.topel@intel.com>
    Signed-off-by: Maxim Mikityanskiy <maximmi@mellanox.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Link: https://lore.kernel.org/bpf/20200520192103.355233-12-bjorn.topel@gmail.com

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 26911b15f8fe..0a02b804b2fe 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -407,10 +407,7 @@ struct mlx5e_dma_info {
 	dma_addr_t addr;
 	union {
 		struct page *page;
-		struct {
-			u64 handle;
-			void *data;
-		} xsk;
+		struct xdp_buff *xsk;
 	};
 };
 
@@ -623,7 +620,6 @@ struct mlx5e_rq {
 		} mpwqe;
 	};
 	struct {
-		u16            umem_headroom;
 		u16            headroom;
 		u32            frame0_sz;
 		u8             map_dir;   /* dma map direction */
@@ -656,7 +652,6 @@ struct mlx5e_rq {
 	struct page_pool      *page_pool;
 
 	/* AF_XDP zero-copy */
-	struct zero_copy_allocator zca;
 	struct xdp_umem       *umem;
 
 	struct work_struct     recover_work;

commit 3f3ab178c71b12295b5950792b72d2198f0e77c7
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Wed Oct 23 15:00:52 2019 +0300

    net/mlx5e: Take DCBNL-related definitions into dedicated files
    
    Take DCBNL-related definitions out of the common en.h header,
    Use a dedicated header file for exposing them.
    Some need not to be exposed, use them locally in the .c file.
    Use stubs to eliminate use of CONFIG_MLX5_CORE_EN_DCB in the
    generic control flows.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index ac385ac93fe5..81fd53569463 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -52,6 +52,7 @@
 #include "wq.h"
 #include "mlx5_core.h"
 #include "en_stats.h"
+#include "en/dcbnl.h"
 #include "en/fs.h"
 #include "lib/hv_vhca.h"
 
@@ -68,8 +69,6 @@ struct page_pool;
 #define MLX5E_HW2SW_MTU(params, hwmtu) ((hwmtu) - ((params)->hard_mtu))
 #define MLX5E_SW2HW_MTU(params, swmtu) ((swmtu) + ((params)->hard_mtu))
 
-#define MLX5E_MAX_PRIORITY      8
-#define MLX5E_MAX_DSCP          64
 #define MLX5E_MAX_NUM_TC	8
 
 #define MLX5_RX_HEADROOM NET_SKB_PAD
@@ -242,10 +241,6 @@ enum mlx5e_priv_flag {
 
 #define MLX5E_GET_PFLAG(params, pflag) (!!((params)->pflags & (BIT(pflag))))
 
-#ifdef CONFIG_MLX5_CORE_EN_DCB
-#define MLX5E_MAX_BW_ALLOC 100 /* Max percentage of BW allocation */
-#endif
-
 struct mlx5e_params {
 	u8  log_sq_size;
 	u8  rq_wq_type;
@@ -270,42 +265,6 @@ struct mlx5e_params {
 	int hard_mtu;
 };
 
-#ifdef CONFIG_MLX5_CORE_EN_DCB
-struct mlx5e_cee_config {
-	/* bw pct for priority group */
-	u8                         pg_bw_pct[CEE_DCBX_MAX_PGS];
-	u8                         prio_to_pg_map[CEE_DCBX_MAX_PRIO];
-	bool                       pfc_setting[CEE_DCBX_MAX_PRIO];
-	bool                       pfc_enable;
-};
-
-enum {
-	MLX5_DCB_CHG_RESET,
-	MLX5_DCB_NO_CHG,
-	MLX5_DCB_CHG_NO_RESET,
-};
-
-struct mlx5e_dcbx {
-	enum mlx5_dcbx_oper_mode   mode;
-	struct mlx5e_cee_config    cee_cfg; /* pending configuration */
-	u8                         dscp_app_cnt;
-
-	/* The only setting that cannot be read from FW */
-	u8                         tc_tsa[IEEE_8021QAZ_MAX_TCS];
-	u8                         cap;
-
-	/* Buffer configuration */
-	bool                       manual_buffer;
-	u32                        cable_len;
-	u32                        xoff;
-};
-
-struct mlx5e_dcbx_dp {
-	u8                         dscp2prio[MLX5E_MAX_DSCP];
-	u8                         trust_state;
-};
-#endif
-
 enum {
 	MLX5E_RQ_STATE_ENABLED,
 	MLX5E_RQ_STATE_RECOVERING,
@@ -1068,13 +1027,6 @@ static inline bool mlx5_tx_swp_supported(struct mlx5_core_dev *mdev)
 }
 
 extern const struct ethtool_ops mlx5e_ethtool_ops;
-#ifdef CONFIG_MLX5_CORE_EN_DCB
-extern const struct dcbnl_rtnl_ops mlx5e_dcbnl_ops;
-int mlx5e_dcbnl_ieee_setets_core(struct mlx5e_priv *priv, struct ieee_ets *ets);
-void mlx5e_dcbnl_initialize(struct mlx5e_priv *priv);
-void mlx5e_dcbnl_init_app(struct mlx5e_priv *priv);
-void mlx5e_dcbnl_delete_app(struct mlx5e_priv *priv);
-#endif
 
 int mlx5e_create_tir(struct mlx5_core_dev *mdev, struct mlx5e_tir *tir,
 		     u32 *in);

commit 80639b199c9ca87444da218ba0e7511946452dd4
Author: Erez Shitrit <erezsh@mellanox.com>
Date:   Sun May 3 13:01:37 2020 +0300

    net/mlx5e: IPoIB, Enable loopback packets for IPoIB interfaces
    
    Enable loopback of unicast and multicast traffic for IPoIB enhanced
    mode.
    This will allow interfaces with the same pkey to communicate between
    them e.g cloned interfaces that located in different namespaces.
    
    Signed-off-by: Erez Shitrit <erezsh@mellanox.com>
    Reviewed-by: Alex Vesker <valex@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 195162b9b245..ac385ac93fe5 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -1082,7 +1082,8 @@ void mlx5e_destroy_tir(struct mlx5_core_dev *mdev,
 		       struct mlx5e_tir *tir);
 int mlx5e_create_mdev_resources(struct mlx5_core_dev *mdev);
 void mlx5e_destroy_mdev_resources(struct mlx5_core_dev *mdev);
-int mlx5e_refresh_tirs(struct mlx5e_priv *priv, bool enable_uc_lb);
+int mlx5e_refresh_tirs(struct mlx5e_priv *priv, bool enable_uc_lb,
+		       bool enable_mc_lb);
 
 /* common netdev helpers */
 void mlx5e_create_q_counters(struct mlx5e_priv *priv);

commit 90bf1c8dbdc52d08a18c9c1fb2b847999159e3a2
Author: Eran Ben Elisha <eranbe@mellanox.com>
Date:   Thu May 7 14:13:33 2020 +0300

    net/mlx5: Move internal timer read function to clock library
    
    Move mlx5_read_internal_timer() into lib/clock.c file as it is being
    used there. As such, make this function a static one.
    
    In addition, rearrange headers include to support function move.
    
    Signed-off-by: Eran Ben Elisha <eranbe@mellanox.com>
    Reviewed-by: Aya Levin <ayal@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 26911b15f8fe..195162b9b245 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -36,7 +36,6 @@
 #include <linux/etherdevice.h>
 #include <linux/timecounter.h>
 #include <linux/net_tstamp.h>
-#include <linux/ptp_clock_kernel.h>
 #include <linux/crash_dump.h>
 #include <linux/mlx5/driver.h>
 #include <linux/mlx5/qp.h>

commit d628ee4fef1dbd6f2fa11e3548322c7839319537
Author: Jesper Dangaard Brouer <brouer@redhat.com>
Date:   Thu May 14 12:51:20 2020 +0200

    mlx5: Rx queue setup time determine frame_sz for XDP
    
    The mlx5 driver have multiple memory models, which are also changed
    according to whether a XDP bpf_prog is attached.
    
    The 'rx_striding_rq' setting is adjusted via ethtool priv-flags e.g.:
     # ethtool --set-priv-flags mlx5p2 rx_striding_rq off
    
    On the general case with 4K page_size and regular MTU packet, then
    the frame_sz is 2048 and 4096 when XDP is enabled, in both modes.
    
    The info on the given frame size is stored differently depending on the
    RQ-mode and encoded in a union in struct mlx5e_rq union wqe/mpwqe.
    In rx striding mode rq->mpwqe.log_stride_sz is either 11 or 12, which
    corresponds to 2048 or 4096 (MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ).
    In non-striding mode (MLX5_WQ_TYPE_CYCLIC) the frag_stride is stored
    in rq->wqe.info.arr[0].frag_stride, for the first fragment, which is
    what the XDP case cares about.
    
    To reduce effect on fast-path, this patch determine the frame_sz at
    setup time, to avoid determining the memory model runtime. Variable
    is named frame0_sz to make it clear that this is only the frame
    size of the first fragment.
    
    This mlx5 driver does a DMA-sync on XDP_TX action, but grow is safe
    as it have done a DMA-map on the entire PAGE_SIZE. The driver also
    already does a XDP length check against sq->hw_mtu on the possible
    XDP xmit paths mlx5e_xmit_xdp_frame() + mlx5e_xmit_xdp_frame_mpwqe().
    
    V3+4: Change variable name first_frame_sz to frame0_sz
    
    V2: Fix that frag_size need to be recalc before creating SKB.
    
    Signed-off-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Acked-by: Tariq Toukan <tariqt@mellanox.com>
    Cc: Saeed Mahameed <saeedm@mellanox.com>
    Link: https://lore.kernel.org/bpf/158945348021.97035.12295039384250022883.stgit@firesoul

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 3bd64c63865b..26911b15f8fe 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -625,6 +625,7 @@ struct mlx5e_rq {
 	struct {
 		u16            umem_headroom;
 		u16            headroom;
+		u32            frame0_sz;
 		u8             map_dir;   /* dma map direction */
 	} buff;
 

commit 05dfd570826f1ae408e1a3faeddb753ff06fed14
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Thu Apr 9 13:43:43 2020 +0300

    net/mlx5e: Take TX WQE info structures out of general EN header
    
    Into the txrx header file.
    The mlx5e_sq_wqe_info structure describes WQE info for the ICOSQ,
    rename it to better reflect this.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Reviewed-by: Maxim Mikityanskiy <maximmi@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index da7fe6aafeed..3bd64c63865b 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -339,16 +339,6 @@ struct mlx5e_cq_decomp {
 	u16                        wqe_counter;
 } ____cacheline_aligned_in_smp;
 
-struct mlx5e_tx_wqe_info {
-	struct sk_buff *skb;
-	u32 num_bytes;
-	u8  num_wqebbs;
-	u8  num_dma;
-#ifdef CONFIG_MLX5_EN_TLS
-	struct page *resync_dump_frag_page;
-#endif
-};
-
 enum mlx5e_dma_map_type {
 	MLX5E_DMA_MAP_SINGLE,
 	MLX5E_DMA_MAP_PAGE
@@ -370,18 +360,6 @@ enum {
 	MLX5E_SQ_STATE_PENDING_XSK_TX,
 };
 
-struct mlx5e_icosq_wqe_info {
-	u8  opcode;
-	u8 num_wqebbs;
-
-	/* Auxiliary data for different opcodes. */
-	union {
-		struct {
-			struct mlx5e_rq *rq;
-		} umr;
-	};
-};
-
 struct mlx5e_txqsq {
 	/* data path */
 
@@ -484,11 +462,6 @@ struct mlx5e_xdp_info_fifo {
 	u32 mask;
 };
 
-struct mlx5e_xdp_wqe_info {
-	u8 num_wqebbs;
-	u8 num_pkts;
-};
-
 struct mlx5e_xdp_mpwqe {
 	/* Current MPWQE session */
 	struct mlx5e_tx_wqe *wqe;

commit 3df711db05b545f3c9b610bee62d33f4e67f64b7
Author: Maxim Mikityanskiy <maximmi@mellanox.com>
Date:   Thu Dec 5 18:11:16 2019 +0200

    net/mlx5e: Return void from mlx5e_sq_xmit and mlx5i_sq_xmit
    
    mlx5e_sq_xmit and mlx5i_sq_xmit always return NETDEV_TX_OK. Drop the
    return value to simplify the code.
    
    Signed-off-by: Maxim Mikityanskiy <maximmi@mellanox.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 0864b76ca2c0..da7fe6aafeed 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -919,8 +919,8 @@ void mlx5e_build_ptys2ethtool_map(void);
 u16 mlx5e_select_queue(struct net_device *dev, struct sk_buff *skb,
 		       struct net_device *sb_dev);
 netdev_tx_t mlx5e_xmit(struct sk_buff *skb, struct net_device *dev);
-netdev_tx_t mlx5e_sq_xmit(struct mlx5e_txqsq *sq, struct sk_buff *skb,
-			  struct mlx5e_tx_wqe *wqe, u16 pi, bool xmit_more);
+void mlx5e_sq_xmit(struct mlx5e_txqsq *sq, struct sk_buff *skb,
+		   struct mlx5e_tx_wqe *wqe, u16 pi, bool xmit_more);
 
 void mlx5e_trigger_irq(struct mlx5e_icosq *sq);
 void mlx5e_completion_event(struct mlx5_core_cq *mcq, struct mlx5_eqe *eqe);

commit 7d42c8e9ab50091e4dda29066975fbb7aa0f1585
Author: Maxim Mikityanskiy <maximmi@mellanox.com>
Date:   Thu Apr 16 11:32:42 2020 +0300

    net/mlx5e: Rename ICOSQ WQE info struct and field
    
    Structs mlx5e_txqsq and mlx5e_xdpsq contain wqe_info arrays to store
    supplementary information corresponding to WQEs in the queue. Struct
    mlx5e_icosq also has such an array, but it's called differently -
    ico_wqe. This patch renames it to unify with the other SQs.
    
    In addition, rename the struct to emphasize its specific usage.
    
    Signed-off-by: Maxim Mikityanskiy <maximmi@mellanox.com>
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index e8508c74eaa8..0864b76ca2c0 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -370,7 +370,7 @@ enum {
 	MLX5E_SQ_STATE_PENDING_XSK_TX,
 };
 
-struct mlx5e_sq_wqe_info {
+struct mlx5e_icosq_wqe_info {
 	u8  opcode;
 	u8 num_wqebbs;
 
@@ -552,7 +552,7 @@ struct mlx5e_icosq {
 
 	/* write@xmit, read@completion */
 	struct {
-		struct mlx5e_sq_wqe_info *ico_wqe;
+		struct mlx5e_icosq_wqe_info *wqe_info;
 	} db;
 
 	/* read only */

commit a6b1b936057e874db52d3e5f0caeb42f11449acf
Merge: 323e395f197f 2dc8b5246d2c
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Thu Apr 30 09:46:19 2020 -0700

    Merge branch 'mlx5-next' of git://git.kernel.org/pub/scm/linux/kernel/git/mellanox/linux
    
    mlx5 updates for both net-next and rdma-next:
    
    1) HW bits and definitions for TLS and IPsec offlaods
    2) Release all pages capability bits
    3) New command interface helpers and some code cleanup as a result
    4) Move qp.c out of mlx5 core driver into mlx5_ib rdma driver
    
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

commit e0b4b4722dfac09658d1519b296cf8dc349a2451
Author: Leon Romanovsky <leonro@mellanox.com>
Date:   Thu Apr 9 21:03:33 2020 +0300

    net/mlx5: Update transobj.c new cmd interface
    
    Do mass update of transobj.c to reuse newly introduced
    mlx5_cmd_exec_in*() interfaces.
    
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 12a61bf82c14..1599b05f3c5a 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -1012,7 +1012,7 @@ int mlx5e_redirect_rqt(struct mlx5e_priv *priv, u32 rqtn, int sz,
 void mlx5e_build_indir_tir_ctx_hash(struct mlx5e_rss_params *rss_params,
 				    const struct mlx5e_tirc_config *ttconfig,
 				    void *tirc, bool inner);
-void mlx5e_modify_tirs_hash(struct mlx5e_priv *priv, void *in, int inlen);
+void mlx5e_modify_tirs_hash(struct mlx5e_priv *priv, void *in);
 struct mlx5e_tirc_config mlx5e_tirc_get_default_config(enum mlx5e_traffic_types tt);
 
 struct mlx5e_xsk_param;
@@ -1102,8 +1102,8 @@ void mlx5e_dcbnl_init_app(struct mlx5e_priv *priv);
 void mlx5e_dcbnl_delete_app(struct mlx5e_priv *priv);
 #endif
 
-int mlx5e_create_tir(struct mlx5_core_dev *mdev,
-		     struct mlx5e_tir *tir, u32 *in, int inlen);
+int mlx5e_create_tir(struct mlx5_core_dev *mdev, struct mlx5e_tir *tir,
+		     u32 *in);
 void mlx5e_destroy_tir(struct mlx5_core_dev *mdev,
 		       struct mlx5e_tir *tir);
 int mlx5e_create_mdev_resources(struct mlx5_core_dev *mdev);

commit e7e0004abdd6f83ae4be5613b29ed396beff576c
Author: Maxim Mikityanskiy <maximmi@mellanox.com>
Date:   Tue Feb 11 16:02:35 2020 +0200

    net/mlx5e: Don't trigger IRQ multiple times on XSK wakeup to avoid WQ overruns
    
    XSK wakeup function triggers NAPI by posting a NOP WQE to a special XSK
    ICOSQ. When the application floods the driver with wakeup requests by
    calling sendto() in a certain pattern that ends up in mlx5e_trigger_irq,
    the XSK ICOSQ may overflow.
    
    Multiple NOPs are not required and won't accelerate the process, so
    avoid posting a second NOP if there is one already on the way. This way
    we also avoid increasing the queue size (which might not help anyway).
    
    Fixes: db05815b36cb ("net/mlx5e: Add XSK zero-copy support")
    Signed-off-by: Maxim Mikityanskiy <maximmi@mellanox.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 12a61bf82c14..23701c0e36ec 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -367,6 +367,7 @@ enum {
 	MLX5E_SQ_STATE_AM,
 	MLX5E_SQ_STATE_TLS,
 	MLX5E_SQ_STATE_VLAN_NEED_L2_INLINE,
+	MLX5E_SQ_STATE_PENDING_XSK_TX,
 };
 
 struct mlx5e_sq_wqe_info {
@@ -960,7 +961,7 @@ void mlx5e_page_release_dynamic(struct mlx5e_rq *rq,
 void mlx5e_handle_rx_cqe(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe);
 void mlx5e_handle_rx_cqe_mpwrq(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe);
 bool mlx5e_post_rx_wqes(struct mlx5e_rq *rq);
-void mlx5e_poll_ico_cq(struct mlx5e_cq *cq);
+int mlx5e_poll_ico_cq(struct mlx5e_cq *cq);
 bool mlx5e_post_rx_mpwqes(struct mlx5e_rq *rq);
 void mlx5e_dealloc_rx_wqe(struct mlx5e_rq *rq, u16 ix);
 void mlx5e_dealloc_rx_mpwqe(struct mlx5e_rq *rq, u16 ix);

commit 9fb16955fb661945ddffce4504dcffbe55cd518a
Merge: 1f074e677a34 1b649e0bcae7
Author: David S. Miller <davem@davemloft.net>
Date:   Wed Mar 25 18:58:11 2020 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/netdev/net
    
    Overlapping header include additions in macsec.c
    
    A bug fix in 'net' overlapping with the removal of 'version'
    string in ena_netdev.c
    
    Overlapping test additions in selftests Makefile
    
    Overlapping PCI ID table adjustments in iwlwifi driver.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit e239c6d686e1c37fb2ab143162dfb57471a8643f
Author: Aya Levin <ayal@mellanox.com>
Date:   Mon Mar 16 16:53:10 2020 +0200

    net/mlx5e: Fix ICOSQ recovery flow with Striding RQ
    
    In striding RQ mode, the buffers of an RX WQE are first
    prepared and posted to the HW using a UMR WQEs via the ICOSQ.
    We maintain the state of these in-progress WQEs in the RQ
    SW struct.
    
    In the flow of ICOSQ recovery, the corresponding RQ is not
    in error state, hence:
    
    - The buffers of the in-progress WQEs must be released
      and the RQ metadata should reflect it.
    - Existing RX WQEs in the RQ should not be affected.
    
    For this, wrap the dealloc of the in-progress WQEs in
    a function, and use it in the ICOSQ recovery flow
    instead of mlx5e_free_rx_descs().
    
    Fixes: be5323c8379f ("net/mlx5e: Report and recover from CQE error on ICOSQ")
    Signed-off-by: Aya Levin <ayal@mellanox.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 571ac5976549..c9606b8ab6ef 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -1060,6 +1060,7 @@ int mlx5e_modify_rq_state(struct mlx5e_rq *rq, int curr_state, int next_state);
 void mlx5e_activate_rq(struct mlx5e_rq *rq);
 void mlx5e_deactivate_rq(struct mlx5e_rq *rq);
 void mlx5e_free_rx_descs(struct mlx5e_rq *rq);
+void mlx5e_free_rx_in_progress_descs(struct mlx5e_rq *rq);
 void mlx5e_activate_icosq(struct mlx5e_icosq *icosq);
 void mlx5e_deactivate_icosq(struct mlx5e_icosq *icosq);
 

commit 1de0306c3a05d305e45b1f1fabe2f4e94222eb6b
Author: Aya Levin <ayal@mellanox.com>
Date:   Mon Mar 9 09:44:18 2020 +0200

    net/mlx5e: Enhance ICOSQ WQE info fields
    
    Add number of WQEBBs (WQE's Basic Block) to WQE info struct. Set the
    number of WQEBBs on WQE post, and increment the consumer counter (cc)
    on completion.
    
    In case of error completions, the cc was mistakenly not incremented,
    keeping a gap between cc and pc (producer counter). This failed the
    recovery flow on the ICOSQ from a CQE error which timed-out waiting for
    the cc and pc to meet.
    
    Fixes: be5323c8379f ("net/mlx5e: Report and recover from CQE error on ICOSQ")
    Signed-off-by: Aya Levin <ayal@mellanox.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 220ef9f06f84..571ac5976549 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -371,6 +371,7 @@ enum {
 
 struct mlx5e_sq_wqe_info {
 	u8  opcode;
+	u8 num_wqebbs;
 
 	/* Auxiliary data for different opcodes. */
 	union {

commit b63293e759a1dd1d105f4c6c32d7ed150b6af8d2
Author: Vlad Buslov <vladbu@mellanox.com>
Date:   Fri May 10 20:42:49 2019 +0300

    net/mlx5e: Show/set Rx network flow classification rules on ul rep
    
    Reuse infrastructure that already exists for pf in legacy mode to show/set
    Rx network flow classification rules for uplink representors.
    
    Signed-off-by: Vlad Buslov <vladbu@mellanox.com>
    Reviewed-by: Roi Dayan <roid@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 0410cdaab475..6c4b45c2a8d6 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -1172,6 +1172,9 @@ int mlx5e_ethtool_set_link_ksettings(struct mlx5e_priv *priv,
 int mlx5e_get_rxfh(struct net_device *netdev, u32 *indir, u8 *key, u8 *hfunc);
 int mlx5e_set_rxfh(struct net_device *dev, const u32 *indir, const u8 *key,
 		   const u8 hfunc);
+int mlx5e_get_rxnfc(struct net_device *dev, struct ethtool_rxnfc *info,
+		    u32 *rule_locs);
+int mlx5e_set_rxnfc(struct net_device *dev, struct ethtool_rxnfc *cmd);
 u32 mlx5e_ethtool_get_rxfh_key_size(struct mlx5e_priv *priv);
 u32 mlx5e_ethtool_get_rxfh_indir_size(struct mlx5e_priv *priv);
 int mlx5e_ethtool_get_ts_info(struct mlx5e_priv *priv,

commit 01013ad355d6b167e9665b1c5ec3b89089de5caa
Author: Vlad Buslov <vladbu@mellanox.com>
Date:   Fri May 10 15:19:35 2019 +0300

    net/mlx5e: Show/set Rx flow indir table and RSS hash key on ul rep
    
    Reuse infrastructure that already exists for pf in legacy mode to show/set
    Rx flow hash indirection table and RSS hash key for uplink representors.
    
    Signed-off-by: Vlad Buslov <vladbu@mellanox.com>
    Reviewed-by: Roi Dayan <roid@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 02b91aa896b0..0410cdaab475 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -1169,6 +1169,9 @@ int mlx5e_ethtool_get_link_ksettings(struct mlx5e_priv *priv,
 				     struct ethtool_link_ksettings *link_ksettings);
 int mlx5e_ethtool_set_link_ksettings(struct mlx5e_priv *priv,
 				     const struct ethtool_link_ksettings *link_ksettings);
+int mlx5e_get_rxfh(struct net_device *netdev, u32 *indir, u8 *key, u8 *hfunc);
+int mlx5e_set_rxfh(struct net_device *dev, const u32 *indir, const u8 *key,
+		   const u8 hfunc);
 u32 mlx5e_ethtool_get_rxfh_key_size(struct mlx5e_priv *priv);
 u32 mlx5e_ethtool_get_rxfh_indir_size(struct mlx5e_priv *priv);
 int mlx5e_ethtool_get_ts_info(struct mlx5e_priv *priv,

commit a70ed9d8ecf395ca7d15c2d13782cc0055398ed5
Merge: 34a568a244be e0ebd8eb36ed
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Mon Mar 9 13:01:26 2020 -0700

    Merge branch 'mlx5-next' of git://git.kernel.org/pub/scm/linux/kernel/git/mellanox/linux
    
    This series adds some HW bits and definitions for mlx5 driver, to be
    used by downstream features in both rdma and netdev branches.
    
    * 'mlx5-next' of git://git.kernel.org/pub/scm/linux/kernel/git/mellanox/linux:
      net/mlx5: HW bit for goto chain offload support
      net/mlx5: Expose link speed directly
      net/mlx5: Introduce TLS and IPSec objects enums
      net/mlx5: Introduce egress acl forward-to-vport capability
      net/mlx5: Expose raw packet pacing APIs
      net/mlx5e: Replace zero-length array with flexible-array member
      net/mlx5: fix spelling mistake "reserverd" -> "reserved"
    
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

commit 162add8cbae4635cf0598c640a24d5ed2849774f
Author: Parav Pandit <parav@mellanox.com>
Date:   Tue Mar 3 08:12:43 2020 -0600

    net/mlx5e: Use devlink virtual flavour for VF devlink port
    
    Use newly introduce 'virtual' port flavour for devlink
    port of PCI VF devlink device in non-representors mode.
    
    While at it, remove recently introduced empty lines at end of the file.
    
    Reviewed-by: Jiri Pirko <jiri@mellanox.com>
    Signed-off-by: Parav Pandit <parav@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 93ca9ea5a96e..23032b4ddedd 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -880,7 +880,7 @@ struct mlx5e_priv {
 #endif
 	struct devlink_health_reporter *tx_reporter;
 	struct devlink_health_reporter *rx_reporter;
-	struct devlink_port             dl_phy_port;
+	struct devlink_port            dl_port;
 	struct mlx5e_xsk           xsk;
 #if IS_ENABLED(CONFIG_PCI_HYPERV_INTERFACE)
 	struct mlx5e_hv_vhca_stats_agent stats_agent;

commit c6acd629eec754a9679f922d51f90e44c769b80c
Author: Vladyslav Tarasiuk <vladyslavt@mellanox.com>
Date:   Wed Nov 13 17:19:47 2019 +0200

    net/mlx5e: Add support for devlink-port in non-representors mode
    
    Added devlink_port field to mlx5e_priv structure and a callback to
    netdev ops to enable devlink to get info about the port. The port
    registration happens at driver initialization.
    
    Signed-off-by: Vladyslav Tarasiuk <vladyslavt@mellanox.com>
    Reviewed-by: Moshe Shemesh <moshe@mellanox.com>
    Reviewed-by: Jiri Pirko <jiri@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 3cc439ab3253..93ca9ea5a96e 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -880,6 +880,7 @@ struct mlx5e_priv {
 #endif
 	struct devlink_health_reporter *tx_reporter;
 	struct devlink_health_reporter *rx_reporter;
+	struct devlink_port             dl_phy_port;
 	struct mlx5e_xsk           xsk;
 #if IS_ENABLED(CONFIG_PCI_HYPERV_INTERFACE)
 	struct mlx5e_hv_vhca_stats_agent stats_agent;

commit b9ab5d0ecf426a1bf16d706e7c284e00998d00be
Author: Maxim Mikityanskiy <maximmi@mellanox.com>
Date:   Mon Dec 2 15:48:25 2019 +0200

    net/mlx5e: Add context to the preactivate hook
    
    Sometimes the preactivate hook of mlx5e_safe_switch_channels needs more
    parameters than just struct mlx5e_priv *. For such cases, a new
    parameter (void *context) is added to preactivate hooks.
    
    Some of the existing normal functions are currently used as preactivate
    callbacks. To avoid adding an extra unused parameter, they are wrapped
    in an automatic way using the MLX5E_DEFINE_PREACTIVATE_WRAPPER_CTX
    macro.
    
    Signed-off-by: Maxim Mikityanskiy <maximmi@mellanox.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 6d725d2acd3d..3cc439ab3253 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -1047,12 +1047,19 @@ void mlx5e_close_channels(struct mlx5e_channels *chs);
 /* Function pointer to be used to modify HW or kernel settings while
  * switching channels
  */
-typedef int (*mlx5e_fp_preactivate)(struct mlx5e_priv *priv);
+typedef int (*mlx5e_fp_preactivate)(struct mlx5e_priv *priv, void *context);
+#define MLX5E_DEFINE_PREACTIVATE_WRAPPER_CTX(fn) \
+int fn##_ctx(struct mlx5e_priv *priv, void *context) \
+{ \
+	return fn(priv); \
+}
 int mlx5e_safe_reopen_channels(struct mlx5e_priv *priv);
 int mlx5e_safe_switch_channels(struct mlx5e_priv *priv,
 			       struct mlx5e_channels *new_chs,
-			       mlx5e_fp_preactivate preactivate);
+			       mlx5e_fp_preactivate preactivate,
+			       void *context);
 int mlx5e_num_channels_changed(struct mlx5e_priv *priv);
+int mlx5e_num_channels_changed_ctx(struct mlx5e_priv *priv, void *context);
 void mlx5e_activate_priv_channels(struct mlx5e_priv *priv);
 void mlx5e_deactivate_priv_channels(struct mlx5e_priv *priv);
 
@@ -1132,10 +1139,10 @@ void mlx5e_update_ndo_stats(struct mlx5e_priv *priv);
 void mlx5e_queue_update_stats(struct mlx5e_priv *priv);
 int mlx5e_bits_invert(unsigned long a, int size);
 
-typedef int (*change_hw_mtu_cb)(struct mlx5e_priv *priv);
 int mlx5e_set_dev_port_mtu(struct mlx5e_priv *priv);
+int mlx5e_set_dev_port_mtu_ctx(struct mlx5e_priv *priv, void *context);
 int mlx5e_change_mtu(struct net_device *netdev, int new_mtu,
-		     change_hw_mtu_cb set_mtu_cb);
+		     mlx5e_fp_preactivate preactivate);
 
 /* ethtool helpers */
 void mlx5e_ethtool_get_drvinfo(struct mlx5e_priv *priv,

commit 3909a12e79135a66a797041ab337a8c7cb387bdf
Author: Maxim Mikityanskiy <maximmi@mellanox.com>
Date:   Tue Sep 3 17:55:45 2019 +0300

    net/mlx5e: Fix configuration of XPS cpumasks and netdev queues in corner cases
    
    Currently, mlx5e notifies the kernel about the number of queues and sets
    the default XPS cpumasks when channels are activated. This
    implementation has several corner cases, in which the kernel may not be
    updated on time, or XPS cpumasks may be reset when not directly touched
    by the user.
    
    This commit fixes these corner cases to match the following expected
    behavior:
    
    1. The number of queues always corresponds to the number of channels
    configured.
    
    2. XPS cpumasks are set to driver's defaults on netdev attach.
    
    3. XPS cpumasks set by user are not reset, unless the number of channels
    changes. If the number of channels changes, they are reset to driver's
    defaults. (In general case, when the number of channels increases or
    decreases, it's not possible to guess how to convert the current XPS
    cpumasks to work with the new number of channels, so we let the user
    reconfigure it if they change the number of channels.)
    
    XPS cpumasks are no longer stored per channel. Only one temporary
    cpumask is used. The old stored cpumasks didn't reflect the user's
    changes and were not used after applying them.
    
    A scratchpad area is added to struct mlx5e_priv. As cpumask_var_t
    requires allocation, and the preactivate hook can't fail, we need to
    preallocate the temporary cpumask in advance. It's stored in the
    scratchpad.
    
    Fixes: 149e566fef81 ("net/mlx5e: Expand XPS cpumask to cover all online cpus")
    Signed-off-by: Maxim Mikityanskiy <maximmi@mellanox.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 4ddccab02a4b..6d725d2acd3d 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -737,7 +737,6 @@ struct mlx5e_channel {
 	DECLARE_BITMAP(state, MLX5E_CHANNEL_NUM_STATES);
 	int                        ix;
 	int                        cpu;
-	cpumask_var_t              xps_cpumask;
 };
 
 struct mlx5e_channels {
@@ -813,6 +812,15 @@ struct mlx5e_xsk {
 	bool ever_used;
 };
 
+/* Temporary storage for variables that are allocated when struct mlx5e_priv is
+ * initialized, and used where we can't allocate them because that functions
+ * must not fail. Use with care and make sure the same variable is not used
+ * simultaneously by multiple users.
+ */
+struct mlx5e_scratchpad {
+	cpumask_var_t cpumask;
+};
+
 struct mlx5e_priv {
 	/* priv data path fields - start */
 	struct mlx5e_txqsq *txq2sq[MLX5E_MAX_NUM_CHANNELS * MLX5E_MAX_NUM_TC];
@@ -876,6 +884,7 @@ struct mlx5e_priv {
 #if IS_ENABLED(CONFIG_PCI_HYPERV_INTERFACE)
 	struct mlx5e_hv_vhca_stats_agent stats_agent;
 #endif
+	struct mlx5e_scratchpad    scratchpad;
 };
 
 struct mlx5e_profile {

commit fe867cac9e1967c553e4ac2aece5fc8675258010
Author: Maxim Mikityanskiy <maximmi@mellanox.com>
Date:   Mon Nov 4 12:02:14 2019 +0200

    net/mlx5e: Use preactivate hook to set the indirection table
    
    mlx5e_ethtool_set_channels updates the indirection table before
    switching to the new channels. If the switch fails, the indirection
    table is new, but the channels are old, which is wrong. Fix it by using
    the preactivate hook of mlx5e_safe_switch_channels to update the
    indirection table at the stage when nothing can fail anymore.
    
    As the code that updates the indirection table is now encapsulated into
    a new function, use that function in the attach flow when the driver has
    to reduce the number of channels, and prepare the code for the next
    commit.
    
    Fixes: 85082dba0a ("net/mlx5e: Correctly handle RSS indirection table when changing number of channels")
    Signed-off-by: Maxim Mikityanskiy <maximmi@mellanox.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index bc2c96b34de1..4ddccab02a4b 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -1043,6 +1043,7 @@ int mlx5e_safe_reopen_channels(struct mlx5e_priv *priv);
 int mlx5e_safe_switch_channels(struct mlx5e_priv *priv,
 			       struct mlx5e_channels *new_chs,
 			       mlx5e_fp_preactivate preactivate);
+int mlx5e_num_channels_changed(struct mlx5e_priv *priv);
 void mlx5e_activate_priv_channels(struct mlx5e_priv *priv);
 void mlx5e_deactivate_priv_channels(struct mlx5e_priv *priv);
 

commit dca147b3dce5abb5284ff747211960fd2db5ec2e
Author: Maxim Mikityanskiy <maximmi@mellanox.com>
Date:   Thu Oct 31 09:39:34 2019 +0200

    net/mlx5e: Rename hw_modify to preactivate
    
    mlx5e_safe_switch_channels accepts a callback to be called before
    activating new channels. It is intended to configure some hardware
    parameters in cases where channels are recreated because some
    configuration has changed.
    
    Recently, this callback has started being used to update the driver's
    internal MLX5E_STATE_XDP_OPEN flag, and the following patches also
    intend to use this callback for software preparations. This patch
    renames the hw_modify callback to preactivate, so that the name fits
    better.
    
    Signed-off-by: Maxim Mikityanskiy <maximmi@mellanox.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 220ef9f06f84..bc2c96b34de1 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -1035,14 +1035,14 @@ int mlx5e_open_channels(struct mlx5e_priv *priv,
 			struct mlx5e_channels *chs);
 void mlx5e_close_channels(struct mlx5e_channels *chs);
 
-/* Function pointer to be used to modify WH settings while
+/* Function pointer to be used to modify HW or kernel settings while
  * switching channels
  */
-typedef int (*mlx5e_fp_hw_modify)(struct mlx5e_priv *priv);
+typedef int (*mlx5e_fp_preactivate)(struct mlx5e_priv *priv);
 int mlx5e_safe_reopen_channels(struct mlx5e_priv *priv);
 int mlx5e_safe_switch_channels(struct mlx5e_priv *priv,
 			       struct mlx5e_channels *new_chs,
-			       mlx5e_fp_hw_modify hw_modify);
+			       mlx5e_fp_preactivate preactivate);
 void mlx5e_activate_priv_channels(struct mlx5e_priv *priv);
 void mlx5e_deactivate_priv_channels(struct mlx5e_priv *priv);
 

commit 339ffae598ed74a0220a92c9b8d1cecb2e9a0dc8
Author: Gustavo A. R. Silva <gustavo@embeddedor.com>
Date:   Tue Feb 18 14:31:14 2020 -0600

    net/mlx5e: Replace zero-length array with flexible-array member
    
    The current codebase makes use of the zero-length array language
    extension to the C90 standard, but the preferred mechanism to declare
    variable-length types such as these ones is a flexible array member[1][2],
    introduced in C99:
    
    struct foo {
            int stuff;
            struct boo array[];
    };
    
    By making use of the mechanism above, we will get a compiler warning
    in case the flexible array does not occur last in the structure, which
    will help us prevent some kind of undefined behavior bugs from being
    inadvertently introduced[3] to the codebase from now on.
    
    Also, notice that, dynamic memory allocations won't be affected by
    this change:
    
    "Flexible array members have incomplete type, and so the sizeof operator
    may not be applied. As a quirk of the original implementation of
    zero-length arrays, sizeof evaluates to zero."[1]
    
    This issue was found with the help of Coccinelle.
    
    [1] https://gcc.gnu.org/onlinedocs/gcc/Zero-Length.html
    [2] https://github.com/KSPP/linux/issues/21
    [3] commit 76497732932f ("cxgb3/l2t: Fix undefined behaviour")
    
    Signed-off-by: Gustavo A. R. Silva <gustavo@embeddedor.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 220ef9f06f84..a960099cd7aa 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -204,7 +204,7 @@ struct mlx5e_tx_wqe {
 
 struct mlx5e_rx_wqe_ll {
 	struct mlx5_wqe_srq_next_seg  next;
-	struct mlx5_wqe_data_seg      data[0];
+	struct mlx5_wqe_data_seg      data[];
 };
 
 struct mlx5e_rx_wqe_cyc {

commit f0ff8e8c7206886a5360eee7d3f664480ed473c9
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Tue Jan 21 00:54:00 2020 -0800

    net/mlx5e: Convert stats groups array to array of group pointers
    
    Convert stats groups array to array of "stats group" pointers to allow
    sharing and individual selection of groups per profile as illustrated in
    the next patches.
    
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index f85d99d601a6..220ef9f06f84 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -893,7 +893,7 @@ struct mlx5e_profile {
 	void	(*update_stats)(struct mlx5e_priv *priv);
 	void	(*update_carrier)(struct mlx5e_priv *priv);
 	unsigned int (*stats_grps_num)(struct mlx5e_priv *priv);
-	const struct mlx5e_stats_grp *stats_grps;
+	mlx5e_stats_grp_t *stats_grps;
 	struct {
 		mlx5e_fp_handle_rx_cqe handle_rx_cqe;
 		mlx5e_fp_handle_rx_cqe handle_rx_cqe_mpwqe;

commit 3460c184dfb0b41118b5000ad78dd997c034b2b5
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Mon Jan 20 22:32:12 2020 -0800

    net/mlx5e: Profile specific stats groups
    
    Attach stats groups array to the profiles and make the stats utility
    functions (get_num, update, fill, fill_strings) generic and use the
    profile->stats_grps rather the hardcoded NIC stats groups.
    
    This will allow future extension to have per profile stats groups.
    
    In this patch mlx5e NIC and IPoIB will still share the same stats
    groups.
    
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index fc80b59db9a8..f85d99d601a6 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -892,6 +892,8 @@ struct mlx5e_profile {
 	int	(*update_rx)(struct mlx5e_priv *priv);
 	void	(*update_stats)(struct mlx5e_priv *priv);
 	void	(*update_carrier)(struct mlx5e_priv *priv);
+	unsigned int (*stats_grps_num)(struct mlx5e_priv *priv);
+	const struct mlx5e_stats_grp *stats_grps;
 	struct {
 		mlx5e_fp_handle_rx_cqe handle_rx_cqe;
 		mlx5e_fp_handle_rx_cqe handle_rx_cqe_mpwqe;
@@ -964,7 +966,6 @@ struct sk_buff *
 mlx5e_skb_from_cqe_nonlinear(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe,
 			     struct mlx5e_wqe_frag_info *wi, u32 cqe_bcnt);
 
-void mlx5e_update_stats(struct mlx5e_priv *priv);
 void mlx5e_get_stats(struct net_device *dev, struct rtnl_link_stats64 *stats);
 void mlx5e_fold_sw_stats64(struct mlx5e_priv *priv, struct rtnl_link_stats64 *s);
 

commit 57c7fce14b1ad512a42abe33cb721a2ea3520d4b
Author: Fan Li <fanl@mellanox.com>
Date:   Mon Dec 16 14:46:15 2019 +0200

    net/mlx5: Increase the max number of channels to 128
    
    Currently the max number of channels is limited to 64, which is half of
    the indirection table size to allow some flexibility. But on servers
    with more than 64 cores, users may want to utilize more queues.
    
    This patch increases the advertised max number of channels to 128 by
    changing the ratio between channels and indirection table slots to 1:1.
    At the same time, the driver still enable no more than 64 channels at
    loading. Users can change it by ethtool afterwards.
    
    Signed-off-by: Fan Li <fanl@mellanox.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 9c8427698238..fc80b59db9a8 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -135,7 +135,7 @@ struct page_pool;
 #define MLX5E_LOG_INDIR_RQT_SIZE       0x7
 #define MLX5E_INDIR_RQT_SIZE           BIT(MLX5E_LOG_INDIR_RQT_SIZE)
 #define MLX5E_MIN_NUM_CHANNELS         0x1
-#define MLX5E_MAX_NUM_CHANNELS         (MLX5E_INDIR_RQT_SIZE >> 1)
+#define MLX5E_MAX_NUM_CHANNELS         MLX5E_INDIR_RQT_SIZE
 #define MLX5E_MAX_NUM_SQS              (MLX5E_MAX_NUM_CHANNELS * MLX5E_MAX_NUM_TC)
 #define MLX5E_TX_CQ_POLL_BUDGET        128
 #define MLX5E_TX_XSK_POLL_BUDGET       64
@@ -1175,11 +1175,11 @@ int mlx5e_attach_netdev(struct mlx5e_priv *priv);
 void mlx5e_detach_netdev(struct mlx5e_priv *priv);
 void mlx5e_destroy_netdev(struct mlx5e_priv *priv);
 void mlx5e_set_netdev_mtu_boundaries(struct mlx5e_priv *priv);
-void mlx5e_build_nic_params(struct mlx5_core_dev *mdev,
+void mlx5e_build_nic_params(struct mlx5e_priv *priv,
 			    struct mlx5e_xsk *xsk,
 			    struct mlx5e_rss_params *rss_params,
 			    struct mlx5e_params *params,
-			    u16 max_channels, u16 mtu);
+			    u16 mtu);
 void mlx5e_build_rq_params(struct mlx5_core_dev *mdev,
 			   struct mlx5e_params *params);
 void mlx5e_build_rss_params(struct mlx5e_rss_params *rss_params,

commit 9cf88808ad6a0f1e958e00abd9a081295fe6da0c
Author: Maxim Mikityanskiy <maximmi@mellanox.com>
Date:   Tue Dec 17 16:20:44 2019 +0000

    net/mlx5e: Fix concurrency issues between config flow and XSK
    
    After disabling resources necessary for XSK (the XDP program, channels,
    XSK queues), use synchronize_rcu to wait until the XSK wakeup function
    finishes, before freeing the resources.
    
    Suspend XSK wakeups during switching channels. If the XDP program is
    being removed, synchronize_rcu before closing the old channels to allow
    XSK wakeup to complete.
    
    Signed-off-by: Maxim Mikityanskiy <maximmi@mellanox.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Link: https://lore.kernel.org/bpf/20191217162023.16011-3-maximmi@mellanox.com

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 2c16add0b642..9c8427698238 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -760,7 +760,7 @@ enum {
 	MLX5E_STATE_OPENED,
 	MLX5E_STATE_DESTROYING,
 	MLX5E_STATE_XDP_TX_ENABLED,
-	MLX5E_STATE_XDP_OPEN,
+	MLX5E_STATE_XDP_ACTIVE,
 };
 
 struct mlx5e_rqt {

commit c55d8b108caa2ec1ae8dddd02cb9d3a740f7c838
Author: Eran Ben Elisha <eranbe@mellanox.com>
Date:   Mon Nov 25 12:11:49 2019 +0200

    net/mlx5e: Fix TXQ indices to be sequential
    
    Cited patch changed (channel index, tc) => (TXQ index) mapping to be a
    static one, in order to keep indices consistent when changing number of
    channels or TCs.
    
    For 32 channels (OOB) and 8 TCs, real num of TXQs is 256.
    When reducing the amount of channels to 8, the real num of TXQs will be
    changed to 64.
    This indices method is buggy:
    - Channel #0, TC 3, the TXQ index is 96.
    - Index 8 is not valid, as there is no such TXQ from driver perspective
      (As it represents channel #8, TC 0, which is not valid with the above
      configuration).
    
    As part of driver's select queue, it calls netdev_pick_tx which returns an
    index in the range of real number of TXQs. Depends on the return value,
    with the examples above, driver could have returned index larger than the
    real number of tx queues, or crash the kernel as it tries to read invalid
    address of SQ which was not allocated.
    
    Fix that by allocating sequential TXQ indices, and hold a new mapping
    between (channel index, tc) => (real TXQ index). This mapping will be
    updated as part of priv channels activation, and is used in
    mlx5e_select_queue to find the selected queue index.
    
    The existing indices mapping (channel_tc2txq) is no longer needed, as it
    is used only for statistics structures and can be calculated on run time.
    Delete its definintion and updates.
    
    Fixes: 8bfaf07f7806 ("net/mlx5e: Present SW stats when state is not opened")
    Signed-off-by: Eran Ben Elisha <eranbe@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index f1a7bc46f1c0..2c16add0b642 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -816,7 +816,7 @@ struct mlx5e_xsk {
 struct mlx5e_priv {
 	/* priv data path fields - start */
 	struct mlx5e_txqsq *txq2sq[MLX5E_MAX_NUM_CHANNELS * MLX5E_MAX_NUM_TC];
-	int channel_tc2txq[MLX5E_MAX_NUM_CHANNELS][MLX5E_MAX_NUM_TC];
+	int channel_tc2realtxq[MLX5E_MAX_NUM_CHANNELS][MLX5E_MAX_NUM_TC];
 #ifdef CONFIG_MLX5_CORE_EN_DCB
 	struct mlx5e_dcbx_dp       dcbx_dp;
 #endif

commit 84d1bb2b139e0184b1754aa1b5776186b475fce8
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Mon Oct 7 14:01:29 2019 +0300

    net/mlx5e: kTLS, Limit DUMP wqe size
    
    HW expects the data size in DUMP WQEs to be up to MTU.
    Make sure they are in range.
    
    We elevate the frag page refcount by 'n-1', in addition to the
    one obtained in tx_sync_info_get(), having an overall of 'n'
    references. We bulk increments by using a single page_ref_add()
    command, to optimize perfermance.
    The refcounts are released one by one, by the corresponding completions.
    
    Fixes: d2ead1f360e8 ("net/mlx5e: Add kTLS TX HW offload support")
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Reviewed-by: Eran Ben Elisha <eranbe@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index cb6f7b87e38f..f1a7bc46f1c0 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -410,6 +410,7 @@ struct mlx5e_txqsq {
 	struct device             *pdev;
 	__be32                     mkey_be;
 	unsigned long              state;
+	unsigned int               hw_mtu;
 	struct hwtstamp_config    *tstamp;
 	struct mlx5_clock         *clock;
 

commit f45da3716fb2fb09e301a1b6edf200ff343dc06e
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Wed Sep 18 13:50:32 2019 +0300

    net/mlx5e: kTLS, Save only the frag page to release at completion
    
    In TX resync flow where DUMP WQEs are posted, keep a pointer to
    the fragment page to unref it upon completion, instead of saving
    the whole fragment.
    
    In addition, move it the end of the arguments list in tx_fill_wi().
    
    Fixes: d2ead1f360e8 ("net/mlx5e: Add kTLS TX HW offload support")
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Reviewed-by: Eran Ben Elisha <eranbe@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 8d76452cacdc..cb6f7b87e38f 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -345,7 +345,7 @@ struct mlx5e_tx_wqe_info {
 	u8  num_wqebbs;
 	u8  num_dma;
 #ifdef CONFIG_MLX5_EN_TLS
-	skb_frag_t *resync_dump_frag;
+	struct page *resync_dump_frag_page;
 #endif
 };
 

commit a49e1f31ae155d64355d0cd0e0afa5b2bc8544cd
Author: Aya Levin <ayal@mellanox.com>
Date:   Thu Aug 8 16:16:28 2019 +0300

    net/mlx5e: Change function's position to a more fitting file
    
    Move function which indicates whether tunnel inner flow table is
    supported from en.h to en_fs.c. It fits better right after tunnel
    protocol rules definitions.
    
    Signed-off-by: Aya Levin <ayal@mellanox.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index e03f973c962f..8d76452cacdc 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -1065,12 +1065,6 @@ int mlx5e_modify_sq(struct mlx5_core_dev *mdev, u32 sqn,
 void mlx5e_activate_txqsq(struct mlx5e_txqsq *sq);
 void mlx5e_tx_disable_queue(struct netdev_queue *txq);
 
-static inline bool mlx5e_tunnel_inner_ft_supported(struct mlx5_core_dev *mdev)
-{
-	return (MLX5_CAP_ETH(mdev, tunnel_stateless_gre) &&
-		MLX5_CAP_FLOWTABLE_NIC_RX(mdev, ft_field_support.inner_ip_version));
-}
-
 static inline bool mlx5_tx_swp_supported(struct mlx5_core_dev *mdev)
 {
 	return MLX5_CAP_ETH(mdev, swp) &&

commit 45f171b1182b9c4ab6d854d6f7fd7dd771fed591
Author: Maxim Mikityanskiy <maximmi@mellanox.com>
Date:   Wed Aug 7 17:46:15 2019 +0300

    net/mlx5e: Support LAG TX port affinity distribution
    
    When the VF LAG is in use, round-robin the TX affinity of channels among
    the different ports, if supported by the firmware. Create a set of TISes
    per port, while doing round-robin of the channels over the different
    sets. Let all SQs of a channel share the same set of TISes.
    
    If lag_tx_port_affinity HCA cap bit is supported, num_lag_ports > 1 and
    we aren't the LACP owner (PF in the regular use), assign the affinities,
    otherwise use tx_affinity == 0 in TIS context to let the FW assign the
    affinities itself. The TISes of the LACP owner are mapped only to the
    native physical port.
    
    For VFs, the starting port for round-robin is determined by its vhca_id,
    because a VF may have only one channel if attached to a single-core VM.
    
    Signed-off-by: Maxim Mikityanskiy <maximmi@mellanox.com>
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Mark Bloch <markb@mellanox.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 491c281416d0..e03f973c962f 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -163,6 +163,14 @@ enum mlx5e_rq_group {
 #define MLX5E_NUM_RQ_GROUPS(g) (1 + MLX5E_RQ_GROUP_##g)
 };
 
+static inline u8 mlx5e_get_num_lag_ports(struct mlx5_core_dev *mdev)
+{
+	if (mlx5_lag_is_lacp_owner(mdev))
+		return 1;
+
+	return clamp_t(u8, MLX5_CAP_GEN(mdev, num_lag_ports), 1, MLX5_MAX_PORTS);
+}
+
 static inline u16 mlx5_min_rx_wqes(int wq_type, u32 wq_size)
 {
 	switch (wq_type) {
@@ -705,6 +713,7 @@ struct mlx5e_channel {
 	struct net_device         *netdev;
 	__be32                     mkey_be;
 	u8                         num_tc;
+	u8                         lag_port;
 
 	/* XDP_REDIRECT */
 	struct mlx5e_xdpsq         xdpsq;
@@ -818,7 +827,7 @@ struct mlx5e_priv {
 	struct mlx5e_rq            drop_rq;
 
 	struct mlx5e_channels      channels;
-	u32                        tisn[MLX5E_MAX_NUM_TC];
+	u32                        tisn[MLX5_MAX_PORTS][MLX5E_MAX_NUM_TC];
 	struct mlx5e_rqt           indir_rqt;
 	struct mlx5e_tir           indir_tir[MLX5E_NUM_INDIR_TIRS];
 	struct mlx5e_tir           inner_indir_tir[MLX5E_NUM_INDIR_TIRS];

commit 3c14562663c603bc523b6619a2b19a411e1cdc8e
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Mon Jun 24 12:03:02 2019 +0300

    net/mlx5e: Expose new function for TIS destroy loop
    
    For better modularity and code sharing.
    Function internal change to be introduced in the next patches.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 446792799125..491c281416d0 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -1107,6 +1107,7 @@ int mlx5e_create_tis(struct mlx5_core_dev *mdev, void *in, u32 *tisn);
 void mlx5e_destroy_tis(struct mlx5_core_dev *mdev, u32 tisn);
 
 int mlx5e_create_tises(struct mlx5e_priv *priv);
+void mlx5e_destroy_tises(struct mlx5e_priv *priv);
 int mlx5e_update_nic_rx(struct mlx5e_priv *priv);
 void mlx5e_update_carrier(struct mlx5e_priv *priv);
 int mlx5e_close(struct net_device *netdev);

commit cef35af34d6dc3792333075115c7deb7062b6e18
Author: Eran Ben Elisha <eranbe@mellanox.com>
Date:   Thu Aug 22 05:06:00 2019 +0000

    net/mlx5e: Add mlx5e HV VHCA stats agent
    
    HV VHCA stats agent is responsible on running a preiodic rx/tx
    packets/bytes stats update. Currently the supported format is version
    MLX5_HV_VHCA_STATS_VERSION. Block ID 1 is dedicated for statistics data
    transfer from the VF to the PF.
    
    The reporter fetch the statistics data from all opened channels, fill it
    in a buffer and send it to mlx5_hv_vhca_write_agent.
    
    As the stats layer should include some metadata per block (sequence and
    offset), the HV VHCA layer shall modify the buffer before actually send it
    over block 1.
    
    Signed-off-by: Eran Ben Elisha <eranbe@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: Haiyang Zhang <haiyangz@microsoft.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 7316571a4df5..446792799125 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -54,6 +54,7 @@
 #include "mlx5_core.h"
 #include "en_stats.h"
 #include "en/fs.h"
+#include "lib/hv_vhca.h"
 
 extern const struct net_device_ops mlx5e_netdev_ops;
 struct page_pool;
@@ -782,6 +783,15 @@ struct mlx5e_modify_sq_param {
 	int rl_index;
 };
 
+#if IS_ENABLED(CONFIG_PCI_HYPERV_INTERFACE)
+struct mlx5e_hv_vhca_stats_agent {
+	struct mlx5_hv_vhca_agent *agent;
+	struct delayed_work        work;
+	u16                        delay;
+	void                      *buf;
+};
+#endif
+
 struct mlx5e_xsk {
 	/* UMEMs are stored separately from channels, because we don't want to
 	 * lose them when channels are recreated. The kernel also stores UMEMs,
@@ -853,6 +863,9 @@ struct mlx5e_priv {
 	struct devlink_health_reporter *tx_reporter;
 	struct devlink_health_reporter *rx_reporter;
 	struct mlx5e_xsk           xsk;
+#if IS_ENABLED(CONFIG_PCI_HYPERV_INTERFACE)
+	struct mlx5e_hv_vhca_stats_agent stats_agent;
+#endif
 };
 
 struct mlx5e_profile {

commit 8276ea1353a4968a212f04ddf16659223e5408d9
Author: Aya Levin <ayal@mellanox.com>
Date:   Wed Jun 26 23:21:40 2019 +0300

    net/mlx5e: Report and recover from CQE with error on RQ
    
    Add support for report and recovery from error on completion on RQ by
    setting the queue back to ready state. Handle only errors with a
    syndrome indicating the RQ might enter error state and could be
    recovered.
    
    Signed-off-by: Aya Levin <ayal@mellanox.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>
    Acked-by: Jiri Pirko <jiri@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 842adf3719ff..7316571a4df5 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -300,6 +300,7 @@ struct mlx5e_dcbx_dp {
 
 enum {
 	MLX5E_RQ_STATE_ENABLED,
+	MLX5E_RQ_STATE_RECOVERING,
 	MLX5E_RQ_STATE_AM,
 	MLX5E_RQ_STATE_NO_CSUM_COMPLETE,
 	MLX5E_RQ_STATE_CSUM_FULL, /* cqe_csum_full hw bit is set */
@@ -672,6 +673,8 @@ struct mlx5e_rq {
 	struct zero_copy_allocator zca;
 	struct xdp_umem       *umem;
 
+	struct work_struct     recover_work;
+
 	/* control */
 	struct mlx5_wq_ctrl    wq_ctrl;
 	__be32                 mkey_be;

commit be5323c8379f488f1de53206edeaf80fc20d7686
Author: Aya Levin <ayal@mellanox.com>
Date:   Tue Jun 25 17:44:28 2019 +0300

    net/mlx5e: Report and recover from CQE error on ICOSQ
    
    Add support for report and recovery from error on completion on ICOSQ.
    Deactivate RQ and flush, then deactivate ICOSQ. Set the queue back to
    ready state (firmware) and reset the ICOSQ and the RQ (software
    resources). Finally, activate the ICOSQ and the RQ.
    
    Signed-off-by: Aya Levin <ayal@mellanox.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>
    Acked-by: Jiri Pirko <jiri@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 7381d57b4b82..842adf3719ff 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -551,6 +551,8 @@ struct mlx5e_icosq {
 	/* control path */
 	struct mlx5_wq_ctrl        wq_ctrl;
 	struct mlx5e_channel      *channel;
+
+	struct work_struct         recover_work;
 } ____cacheline_aligned_in_smp;
 
 struct mlx5e_wqe_frag_info {
@@ -1026,6 +1028,12 @@ void mlx5e_set_rx_cq_mode_params(struct mlx5e_params *params,
 void mlx5e_set_rq_type(struct mlx5_core_dev *mdev, struct mlx5e_params *params);
 void mlx5e_init_rq_type_params(struct mlx5_core_dev *mdev,
 			       struct mlx5e_params *params);
+int mlx5e_modify_rq_state(struct mlx5e_rq *rq, int curr_state, int next_state);
+void mlx5e_activate_rq(struct mlx5e_rq *rq);
+void mlx5e_deactivate_rq(struct mlx5e_rq *rq);
+void mlx5e_free_rx_descs(struct mlx5e_rq *rq);
+void mlx5e_activate_icosq(struct mlx5e_icosq *icosq);
+void mlx5e_deactivate_icosq(struct mlx5e_icosq *icosq);
 
 int mlx5e_modify_sq(struct mlx5_core_dev *mdev, u32 sqn,
 		    struct mlx5e_modify_sq_param *p);

commit 9032e7192eac8e657b52cf1c89fe730308b72c2a
Author: Aya Levin <ayal@mellanox.com>
Date:   Tue Jun 25 16:26:46 2019 +0300

    net/mlx5e: Add support to rx reporter diagnose
    
    Add rx reporter, which supports diagnose call-back. Diagnostics output
    include: information common to all RQs: RQ type, RQ size, RQ stride
    size, CQ size and CQ stride size. In addition advertise information per
    RQ and its related icosq and attached CQ.
    
    $ devlink health diagnose pci/0000:00:0b.0 reporter rx
     Common config:
       RQ:
         type: 2 stride size: 2048 size: 8
       CQ:
         stride size: 64 size: 1024
     RQs:
       channel ix: 0 rqn: 4308 HW state: 1 SW state: 3 posted WQEs: 7 cc: 7 ICOSQ HW state: 1
       CQ:
         cqn: 1032 HW status: 0
       channel ix: 1 rqn: 4313 HW state: 1 SW state: 3 posted WQEs: 7 cc: 7 ICOSQ HW state: 1
       CQ:
         cqn: 1036 HW status: 0
       channel ix: 2 rqn: 4318 HW state: 1 SW state: 3 posted WQEs: 7 cc: 7 ICOSQ HW state: 1
       CQ:
         cqn: 1040 HW status: 0
       channel ix: 3 rqn: 4323 HW state: 1 SW state: 3 posted WQEs: 7 cc: 7 ICOSQ HW state: 1
       CQ:
         cqn: 1044 HW status: 0
    
    $ devlink health diagnose pci/0000:00:0b.0 reporter rx -jp
    {
        "Common config": {
            "RQ": {
                "type": 2,
                "stride size": 2048,
                "size": 8
            },
            "CQ": {
                "stride size": 64,
                "size": 1024
            }
        },
        "RQs": [ {
                "channel ix": 0,
                "rqn": 4308,
                "HW state": 1,
                "SW state": 3,
                "posted WQEs": 7,
                "cc": 7,
                "ICOSQ HW state": 1,
                "CQ": {
                    "cqn": 1032,
                    "HW status": 0
                }
            },{
                "channel ix": 1,
                "rqn": 4313,
                "HW state": 1,
                "SW state": 3,
                "posted WQEs": 7,
                "cc": 7,
                "ICOSQ HW state": 1,
                "CQ": {
                    "cqn": 1036,
                    "HW status": 0
                }
            },{
                "channel ix": 2,
                "rqn": 4318,
                "HW state": 1,
                "SW state": 3,
                "posted WQEs": 7,
                "cc": 7,
                "ICOSQ HW state": 1,
                "CQ": {
                    "cqn": 1040,
                    "HW status": 0
                }
            },{
                "channel ix": 3,
                "rqn": 4323,
                "HW state": 1,
                "SW state": 3,
                "posted WQEs": 7,
                "cc": 7,
                "ICOSQ HW state": 1,
                "CQ": {
                    "cqn": 1044,
                    "HW status": 0
                }
            } ]
    }
    
    Signed-off-by: Aya Levin <ayal@mellanox.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>
    Acked-by: Jiri Pirko <jiri@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 8cf548c7ad9c..7381d57b4b82 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -846,6 +846,7 @@ struct mlx5e_priv {
 	struct mlx5e_tls          *tls;
 #endif
 	struct devlink_health_reporter *tx_reporter;
+	struct devlink_health_reporter *rx_reporter;
 	struct mlx5e_xsk           xsk;
 };
 
@@ -887,6 +888,26 @@ bool mlx5e_poll_tx_cq(struct mlx5e_cq *cq, int napi_budget);
 int mlx5e_poll_rx_cq(struct mlx5e_cq *cq, int budget);
 void mlx5e_free_txqsq_descs(struct mlx5e_txqsq *sq);
 
+static inline u32 mlx5e_rqwq_get_size(struct mlx5e_rq *rq)
+{
+	switch (rq->wq_type) {
+	case MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ:
+		return mlx5_wq_ll_get_size(&rq->mpwqe.wq);
+	default:
+		return mlx5_wq_cyc_get_size(&rq->wqe.wq);
+	}
+}
+
+static inline u32 mlx5e_rqwq_get_cur_sz(struct mlx5e_rq *rq)
+{
+	switch (rq->wq_type) {
+	case MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ:
+		return rq->mpwqe.wq.cur_sz;
+	default:
+		return rq->wqe.wq.cur_sz;
+	}
+}
+
 bool mlx5e_check_fragmented_striding_rq_cap(struct mlx5_core_dev *mdev);
 bool mlx5e_striding_rq_possible(struct mlx5_core_dev *mdev,
 				struct mlx5e_params *params);

commit 446bf64b613c4433dac4b15f4eaf326beaad3c8e
Merge: 20e79a0a2cfd 06821504fd47
Author: David S. Miller <davem@davemloft.net>
Date:   Mon Aug 19 11:54:03 2019 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/netdev/net
    
    Merge conflict of mlx5 resolved using instructions in merge
    commit 9566e650bf7fdf58384bb06df634f7531ca3a97e.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit f43d48d10a42787c1de1d3facd7db210c91db1da
Author: Eran Ben Elisha <eranbe@mellanox.com>
Date:   Thu Aug 1 14:27:30 2019 +0300

    net/mlx5e: Fix compatibility issue with ethtool flash device
    
    Cited patch deleted ethtool flash device support, as ethtool core can
    fallback into devlink flash callback. However, this is supported only if
    there is a devlink port registered over the corresponding netdevice.
    
    As mlx5e do not have devlink port support over native netdevice, it broke
    the ability to flash device via ethtool.
    
    This patch re-add the ethtool callback to avoid user functionality breakage
    when trying to flash device via ethtool.
    
    Fixes: 9c8bca2637b8 ("mlx5: Move firmware flash implementation to devlink")
    Signed-off-by: Eran Ben Elisha <eranbe@mellanox.com>
    Acked-by: Jiri Pirko <jiri@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index f6b64a03cd06..65bec19a438f 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -1105,6 +1105,8 @@ u32 mlx5e_ethtool_get_rxfh_key_size(struct mlx5e_priv *priv);
 u32 mlx5e_ethtool_get_rxfh_indir_size(struct mlx5e_priv *priv);
 int mlx5e_ethtool_get_ts_info(struct mlx5e_priv *priv,
 			      struct ethtool_ts_info *info);
+int mlx5e_ethtool_flash_device(struct mlx5e_priv *priv,
+			       struct ethtool_flash *flash);
 void mlx5e_ethtool_get_pauseparam(struct mlx5e_priv *priv,
 				  struct ethtool_pauseparam *pauseparam);
 int mlx5e_ethtool_set_pauseparam(struct mlx5e_priv *priv,

commit a9bc3390327317345dd4683b70970c83ab400ea3
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Tue Jul 30 11:55:25 2019 +0300

    net/mlx5e: kTLS, Fix progress params context WQE layout
    
    The TLS progress params context WQE should not include an
    Eth segment, drop it.
    In addition, align the tls_progress_params layout with the
    HW specification document:
    - fix the tisn field name.
    - remove the valid bit.
    
    Fixes: a12ff35e0fb7 ("net/mlx5: Introduce TLS TX offload hardware bits and structures")
    Fixes: d2ead1f360e8 ("net/mlx5e: Add kTLS TX HW offload support")
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index ce1be2a84231..f6b64a03cd06 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -184,8 +184,13 @@ static inline int mlx5e_get_max_num_channels(struct mlx5_core_dev *mdev)
 
 struct mlx5e_tx_wqe {
 	struct mlx5_wqe_ctrl_seg ctrl;
-	struct mlx5_wqe_eth_seg  eth;
-	struct mlx5_wqe_data_seg data[0];
+	union {
+		struct {
+			struct mlx5_wqe_eth_seg  eth;
+			struct mlx5_wqe_data_seg data[0];
+		};
+		u8 tls_progress_params_ctx[0];
+	};
 };
 
 struct mlx5e_rx_wqe_ll {

commit 13dfb3fa494361ea9a5950f27c9cd8b06d28c04f
Merge: 05bb520376af 33920f1ec5bf
Author: David S. Miller <davem@davemloft.net>
Date:   Tue Aug 6 18:44:57 2019 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/netdev/net
    
    Just minor overlapping changes in the conflicts here.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit b431302e92f00b7acd5617a4d289f8006394bfc2
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Mon Jul 1 12:08:08 2019 +0300

    net/mlx5e: Tx, Soften inline mode VLAN dependencies
    
    If capable, use zero inline mode in TX WQE for non-VLAN packets.
    For VLAN ones, keep the enforcement of at least L2 inline mode,
    unless the WQE VLAN insertion offload cap is on.
    
    Performance:
    Tested single core packet rate of 64Bytes.
    
    NIC: ConnectX-5
    CPU: Intel(R) Xeon(R) Gold 6154 CPU @ 3.00GHz
    
    pktgen:
    Before: 12.46 Mpps
    After:  14.65 Mpps (+17.5%)
    
    XDP_TX:
    The MPWQE flow is not affected, as it already has this optimization.
    So we test with priv-flag xdp_tx_mpwqe: off.
    
    Before:  9.90 Mpps
    After:  10.20 Mpps (+3%)
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Tested-by: Noam Stolero <noams@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 745bcc25c6f8..30f13f81c965 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -359,6 +359,7 @@ enum {
 	MLX5E_SQ_STATE_IPSEC,
 	MLX5E_SQ_STATE_AM,
 	MLX5E_SQ_STATE_TLS,
+	MLX5E_SQ_STATE_VLAN_NEED_L2_INLINE,
 };
 
 struct mlx5e_sq_wqe_info {
@@ -1132,7 +1133,6 @@ void mlx5e_build_rq_params(struct mlx5_core_dev *mdev,
 			   struct mlx5e_params *params);
 void mlx5e_build_rss_params(struct mlx5e_rss_params *rss_params,
 			    u16 num_channels);
-u8 mlx5e_params_calculate_tx_min_inline(struct mlx5_core_dev *mdev);
 void mlx5e_rx_dim_work(struct work_struct *work);
 void mlx5e_tx_dim_work(struct work_struct *work);
 

commit 6c085a8aab5183d8658c9a692bcfda3e24195b7a
Author: Shay Agroskin <shayag@mellanox.com>
Date:   Sun May 12 18:28:27 2019 +0300

    net/mlx5e: XDP, Close TX MPWQE session when no room for inline packet left
    
    In MPWQE mode, when transmitting packets with XDP, a packet that is smaller
    than a certain size (set to 256 bytes) would be sent inline within its WQE
    TX descriptor (mem-copied), in case the hardware tx queue is congested
    beyond a pre-defined water-mark.
    
    If a MPWQE cannot contain an additional inline packet, we close this
    MPWQE session, and send the packet inlined within the next MPWQE.
    To save some MPWQE session close+open operations, we don't open MPWQE
    sessions that are contiguously smaller than certain size (set to the
    HW MPWQE maximum size). If there isn't enough contiguous room in the
    send queue, we fill it with NOPs and wrap the send queue index around.
    
    This way, qualified packets are always sent inline.
    
    Perf tests:
    Tested packet rate for UDP 64Byte multi-stream
    over two dual port ConnectX-5 100Gbps NICs.
    CPU: Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz
    
    XDP_TX:
    
    With 24 channels:
    | ------ | bounced packets | inlined packets | inline ratio |
    | before | 113.6Mpps       | 96.3Mpps        | 84%          |
    | after  |   115Mpps       | 99.5Mpps        | 86%          |
    
    With one channel:
    
    | ------ | bounced packets | inlined packets | inline ratio |
    | before | 6.7Mpps         | 0pps            | 0%           |
    | after  | 6.8Mpps         | 0pps            | 0%           |
    
    As we can see, there is improvement in both inline ratio and overall
    packet rate for 24 channels. Also, we see no degradation for the
    one-channel case.
    
    Signed-off-by: Shay Agroskin <shayag@mellanox.com>
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 79d93d6c7d7a..745bcc25c6f8 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -483,8 +483,6 @@ struct mlx5e_xdp_mpwqe {
 	struct mlx5e_tx_wqe *wqe;
 	u8                   ds_count;
 	u8                   pkt_count;
-	u8                   max_ds_count;
-	u8                   complete;
 	u8                   inline_on;
 };
 

commit 694826e366349d5b27599f591d3bd3a53512306e
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Sun Jul 14 11:43:43 2019 +0300

    net/mlx5e: Fix wrong max num channels indication
    
    No XSK support in the enhanced IPoIB driver and representors.
    Add a profile property to specify this, and enhance the logic
    that calculates the max number of channels to take it into
    account.
    
    Fixes: db05815b36cb ("net/mlx5e: Add XSK zero-copy support")
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 79d93d6c7d7a..ce1be2a84231 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -159,7 +159,7 @@ do {                                                            \
 enum mlx5e_rq_group {
 	MLX5E_RQ_GROUP_REGULAR,
 	MLX5E_RQ_GROUP_XSK,
-	MLX5E_NUM_RQ_GROUPS /* Keep last. */
+#define MLX5E_NUM_RQ_GROUPS(g) (1 + MLX5E_RQ_GROUP_##g)
 };
 
 static inline u16 mlx5_min_rx_wqes(int wq_type, u32 wq_size)
@@ -182,14 +182,6 @@ static inline int mlx5e_get_max_num_channels(struct mlx5_core_dev *mdev)
 		min_t(int, mlx5_comp_vectors_count(mdev), MLX5E_MAX_NUM_CHANNELS);
 }
 
-/* Use this function to get max num channels after netdev was created */
-static inline int mlx5e_get_netdev_max_channels(struct net_device *netdev)
-{
-	return min_t(unsigned int,
-		     netdev->num_rx_queues / MLX5E_NUM_RQ_GROUPS,
-		     netdev->num_tx_queues);
-}
-
 struct mlx5e_tx_wqe {
 	struct mlx5_wqe_ctrl_seg ctrl;
 	struct mlx5_wqe_eth_seg  eth;
@@ -830,6 +822,7 @@ struct mlx5e_priv {
 	struct net_device         *netdev;
 	struct mlx5e_stats         stats;
 	struct mlx5e_channel_stats channel_stats[MLX5E_MAX_NUM_CHANNELS];
+	u16                        max_nch;
 	u8                         max_opened_tc;
 	struct hwtstamp_config     tstamp;
 	u16                        q_counter;
@@ -871,6 +864,7 @@ struct mlx5e_profile {
 		mlx5e_fp_handle_rx_cqe handle_rx_cqe_mpwqe;
 	} rx_handlers;
 	int	max_tc;
+	u8	rq_groups;
 };
 
 void mlx5e_build_ptys2ethtool_map(void);

commit 114a5c3240155fdb01bf821c9d326d7bb05bd464
Merge: 08d14c49ac94 ef1ce7d7b67b
Author: David S. Miller <davem@davemloft.net>
Date:   Thu Jul 11 15:06:37 2019 -0700

    Merge tag 'mlx5-fixes-2019-07-11' of git://git.kernel.org/pub/scm/linux/kernel/git/saeed/linux
    
    Saeed Mahameed says:
    
    ====================
    Mellanox, mlx5 fixes 2019-07-11
    
    This series introduces some fixes to mlx5 driver.
    
    Please pull and let me know if there is any problem.
    
    For -stable v4.15
    ('net/mlx5e: IPoIB, Add error path in mlx5_rdma_setup_rn')
    
    For -stable v5.1
    ('net/mlx5e: Fix port tunnel GRE entropy control')
    ('net/mlx5e: Rx, Fix checksum calculation for new hardware')
    ('net/mlx5e: Fix return value from timeout recover function')
    ('net/mlx5e: Fix error flow in tx reporter diagnose')
    
    For -stable v5.2
    ('net/mlx5: E-Switch, Fix default encap mode')
    
    Conflict note: This pull request will produce a small conflict when
    merged with net-next.
    In drivers/net/ethernet/mellanox/mlx5/core/eswitch_offloads.c
    Take the hunk from net and replace:
    esw_offloads_steering_init(esw, vf_nvports, total_nvports);
    with:
    esw_offloads_steering_init(esw);
    ====================
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit db849faa9bef993a1379dc510623f750a72fa7ce
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Fri May 3 13:14:59 2019 -0700

    net/mlx5e: Rx, Fix checksum calculation for new hardware
    
    CQE checksum full mode in new HW, provides a full checksum of rx frame.
    Covering bytes starting from eth protocol up to last byte in the received
    frame (frame_size - ETH_HLEN), as expected by the stack.
    
    Fixing up skb->csum by the driver is not required in such case. This fix
    is to avoid wrong checksum calculation in drivers which already support
    the new hardware with the new checksum mode.
    
    Fixes: 85327a9c4150 ("net/mlx5: Update the list of the PCI supported devices")
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index cc6797e24571..cc227a7aa79f 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -294,6 +294,7 @@ enum {
 	MLX5E_RQ_STATE_ENABLED,
 	MLX5E_RQ_STATE_AM,
 	MLX5E_RQ_STATE_NO_CSUM_COMPLETE,
+	MLX5E_RQ_STATE_CSUM_FULL, /* cqe_csum_full hw bit is set */
 };
 
 struct mlx5e_cq {

commit d2ead1f360e8abd9817dea4f7290ffaf00078ac1
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Fri Jul 5 18:30:22 2019 +0300

    net/mlx5e: Add kTLS TX HW offload support
    
    Add support for transmit side kernel-TLS acceleration.
    Offload the crypto encryption to HW.
    
    Per TLS connection:
    - Use a separate TIS to maintain the HW context.
    - Use a separate encryption key.
    - Maintain static and progress HW contexts by posting the proper
      WQEs at creation time, or upon resync.
    - Use a special DUMP opcode to replay the previous frags and sync
      the HW context.
    
    To make sure the SQ is able to serve an xmit request, increase
    SQ stop room to cover:
    - static params WQE,
    - progress params WQE, and
    - resync DUMP per frag.
    
    Currently supporting TLS 1.2, and key size 128bit.
    
    Tested over SimX simulator.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Eran Ben Elisha <eranbe@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index d3d2733917ff..263558875f20 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -209,7 +209,10 @@ struct mlx5e_umr_wqe {
 	struct mlx5_wqe_ctrl_seg       ctrl;
 	struct mlx5_wqe_umr_ctrl_seg   uctrl;
 	struct mlx5_mkey_seg           mkc;
-	struct mlx5_mtt                inline_mtts[0];
+	union {
+		struct mlx5_mtt        inline_mtts[0];
+		u8                     tls_static_params_ctx[0];
+	};
 };
 
 extern const char mlx5e_self_tests[][ETH_GSTRING_LEN];
@@ -333,6 +336,9 @@ struct mlx5e_tx_wqe_info {
 	u32 num_bytes;
 	u8  num_wqebbs;
 	u8  num_dma;
+#ifdef CONFIG_MLX5_EN_TLS
+	skb_frag_t *resync_dump_frag;
+#endif
 };
 
 enum mlx5e_dma_map_type {

commit 2b257a6e736f5af17c796087b628cb320cdec062
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Fri Jul 5 18:30:20 2019 +0300

    net/mlx5e: Re-work TIS creation functions
    
    Let the EN TIS creation function (mlx5e_create_tis) be responsible
    for applying common mdev related fields.
    Other specific fields must be set by the caller and passed within
    the inbox.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Reviewed-by: Boris Pismenny <borisp@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 09c43c9f3b4a..d3d2733917ff 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -1053,8 +1053,7 @@ int mlx5e_create_direct_tirs(struct mlx5e_priv *priv, struct mlx5e_tir *tirs);
 void mlx5e_destroy_direct_tirs(struct mlx5e_priv *priv, struct mlx5e_tir *tirs);
 void mlx5e_destroy_rqt(struct mlx5e_priv *priv, struct mlx5e_rqt *rqt);
 
-int mlx5e_create_tis(struct mlx5_core_dev *mdev, int tc,
-		     u32 underlay_qpn, u32 *tisn);
+int mlx5e_create_tis(struct mlx5_core_dev *mdev, void *in, u32 *tisn);
 void mlx5e_destroy_tis(struct mlx5_core_dev *mdev, u32 tisn);
 
 int mlx5e_create_tises(struct mlx5e_priv *priv);

commit 01614d4f603ed651098b7741f4742e7caf6d0155
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Fri Jul 5 18:30:19 2019 +0300

    net/mlx5e: Tx, Unconstify SQ stop room
    
    Use an SQ field for stop_room, and use the larger value only if TLS
    is supported.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 6e31b7c07f8e..09c43c9f3b4a 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -390,6 +390,7 @@ struct mlx5e_txqsq {
 	void __iomem              *uar_map;
 	struct netdev_queue       *txq;
 	u32                        sqn;
+	u16                        stop_room;
 	u8                         min_inline_mode;
 	struct device             *pdev;
 	__be32                     mkey_be;

commit 542578c6793698a98cd0b7c77a96b8c6bfdfcb88
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Fri Jul 5 18:30:15 2019 +0300

    net/mlx5e: Move helper functions to a new txrx datapath header
    
    Take datapath helper functions to a new header file en/txrx.h.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Eran Ben Elisha <eranbe@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index cd5afc6ef50b..6e31b7c07f8e 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -549,12 +549,6 @@ struct mlx5e_icosq {
 	struct mlx5e_channel      *channel;
 } ____cacheline_aligned_in_smp;
 
-static inline bool
-mlx5e_wqc_has_room_for(struct mlx5_wq_cyc *wq, u16 cc, u16 pc, u16 n)
-{
-	return (mlx5_wq_cyc_ctr2ix(wq, cc - pc) >= n) || (cc == pc);
-}
-
 struct mlx5e_wqe_frag_info {
 	struct mlx5e_dma_info *di;
 	u32 offset;
@@ -1023,102 +1017,6 @@ static inline bool mlx5_tx_swp_supported(struct mlx5_core_dev *mdev)
 		MLX5_CAP_ETH(mdev, swp_csum) && MLX5_CAP_ETH(mdev, swp_lso);
 }
 
-struct mlx5e_swp_spec {
-	__be16 l3_proto;
-	u8 l4_proto;
-	u8 is_tun;
-	__be16 tun_l3_proto;
-	u8 tun_l4_proto;
-};
-
-static inline void
-mlx5e_set_eseg_swp(struct sk_buff *skb, struct mlx5_wqe_eth_seg *eseg,
-		   struct mlx5e_swp_spec *swp_spec)
-{
-	/* SWP offsets are in 2-bytes words */
-	eseg->swp_outer_l3_offset = skb_network_offset(skb) / 2;
-	if (swp_spec->l3_proto == htons(ETH_P_IPV6))
-		eseg->swp_flags |= MLX5_ETH_WQE_SWP_OUTER_L3_IPV6;
-	if (swp_spec->l4_proto) {
-		eseg->swp_outer_l4_offset = skb_transport_offset(skb) / 2;
-		if (swp_spec->l4_proto == IPPROTO_UDP)
-			eseg->swp_flags |= MLX5_ETH_WQE_SWP_OUTER_L4_UDP;
-	}
-
-	if (swp_spec->is_tun) {
-		eseg->swp_inner_l3_offset = skb_inner_network_offset(skb) / 2;
-		if (swp_spec->tun_l3_proto == htons(ETH_P_IPV6))
-			eseg->swp_flags |= MLX5_ETH_WQE_SWP_INNER_L3_IPV6;
-	} else { /* typically for ipsec when xfrm mode != XFRM_MODE_TUNNEL */
-		eseg->swp_inner_l3_offset = skb_network_offset(skb) / 2;
-		if (swp_spec->l3_proto == htons(ETH_P_IPV6))
-			eseg->swp_flags |= MLX5_ETH_WQE_SWP_INNER_L3_IPV6;
-	}
-	switch (swp_spec->tun_l4_proto) {
-	case IPPROTO_UDP:
-		eseg->swp_flags |= MLX5_ETH_WQE_SWP_INNER_L4_UDP;
-		/* fall through */
-	case IPPROTO_TCP:
-		eseg->swp_inner_l4_offset = skb_inner_transport_offset(skb) / 2;
-		break;
-	}
-}
-
-static inline void mlx5e_sq_fetch_wqe(struct mlx5e_txqsq *sq,
-				      struct mlx5e_tx_wqe **wqe,
-				      u16 *pi)
-{
-	struct mlx5_wq_cyc *wq = &sq->wq;
-
-	*pi  = mlx5_wq_cyc_ctr2ix(wq, sq->pc);
-	*wqe = mlx5_wq_cyc_get_wqe(wq, *pi);
-	memset(*wqe, 0, sizeof(**wqe));
-}
-
-static inline
-struct mlx5e_tx_wqe *mlx5e_post_nop(struct mlx5_wq_cyc *wq, u32 sqn, u16 *pc)
-{
-	u16                         pi   = mlx5_wq_cyc_ctr2ix(wq, *pc);
-	struct mlx5e_tx_wqe        *wqe  = mlx5_wq_cyc_get_wqe(wq, pi);
-	struct mlx5_wqe_ctrl_seg   *cseg = &wqe->ctrl;
-
-	memset(cseg, 0, sizeof(*cseg));
-
-	cseg->opmod_idx_opcode = cpu_to_be32((*pc << 8) | MLX5_OPCODE_NOP);
-	cseg->qpn_ds           = cpu_to_be32((sqn << 8) | 0x01);
-
-	(*pc)++;
-
-	return wqe;
-}
-
-static inline
-void mlx5e_notify_hw(struct mlx5_wq_cyc *wq, u16 pc,
-		     void __iomem *uar_map,
-		     struct mlx5_wqe_ctrl_seg *ctrl)
-{
-	ctrl->fm_ce_se = MLX5_WQE_CTRL_CQ_UPDATE;
-	/* ensure wqe is visible to device before updating doorbell record */
-	dma_wmb();
-
-	*wq->db = cpu_to_be32(pc);
-
-	/* ensure doorbell record is visible to device before ringing the
-	 * doorbell
-	 */
-	wmb();
-
-	mlx5_write64((__be32 *)ctrl, uar_map);
-}
-
-static inline void mlx5e_cq_arm(struct mlx5e_cq *cq)
-{
-	struct mlx5_core_cq *mcq;
-
-	mcq = &cq->mcq;
-	mlx5_cq_arm(mcq, MLX5_CQ_DB_REQ_NOT, mcq->uar->map, cq->wq.cc);
-}
-
 extern const struct ethtool_ops mlx5e_ethtool_ops;
 #ifdef CONFIG_MLX5_CORE_EN_DCB
 extern const struct dcbnl_rtnl_ops mlx5e_dcbnl_ops;

commit 61c2491db779c94d29446cea38b95d0c72bc1a9e
Merge: f654e676702e 8338d9378895
Author: David S. Miller <davem@davemloft.net>
Date:   Fri Jul 5 16:24:27 2019 -0700

    Merge tag 'mlx5-updates-2019-07-04-v2' of git://git.kernel.org/pub/scm/linux/kernel/git/saeed/linux
    
    Saeed Mahameed says:
    
    ====================
    mlx5-update-2019-07-04
    
    This series adds mlx5 support for devlink fw versions query.
    
    1) Implement the required low level firmware commands
    2) Implement the devlink knobs and callbacks for fw versions query.
    ====================
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit e08a976a16cafc20931db1d17aed9183202bfa8d
Merge: e2c746944e26 f8efee08dd9d
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Thu Jul 4 16:40:32 2019 -0400

    Merge branch 'mlx5-next' of git://git.kernel.org/pub/scm/linux/kernel/git/mellanox/linux
    
    Misc updates from mlx5-next branch:
    
    1) Add the required HW definitions and structures for upcoming TLS
       support.
    2) Add support for MCQI and MCQS hardware registers for fw version query.
    3) Added hardware bits and structures definitions for sub-functions
    4) Small code cleanup and improvement for PF pci driver.
    5) Bluefield (ECPF) updates and refactoring for better E-Switch
       management on ECPF embedded CPU NIC:
       5.1) Consolidate querying eswitch number of VFs
       5.2) Register event handler at the correct E-Switch init stage
       5.3) Setup PF's inline mode and vlan pop when the ECPF is the
            E-Swtich manager ( the host PF is basically a VF ).
       5.4) Handle Vport UC address changes in switchdev mode.
    
    6) Cleanup the rep and netdev reference when unloading IB rep.
    
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    
    i# All conflicts fixed but you are still merging.

commit c4cde5804d512a2f8934017dbf7df642dfbdf2ad
Merge: e2c746944e26 e5a3e259ef23
Author: David S. Miller <davem@davemloft.net>
Date:   Thu Jul 4 12:48:21 2019 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/bpf/bpf-next
    
    Daniel Borkmann says:
    
    ====================
    pull-request: bpf-next 2019-07-03
    
    The following pull-request contains BPF updates for your *net-next* tree.
    
    There is a minor merge conflict in mlx5 due to 8960b38932be ("linux/dim:
    Rename externally used net_dim members") which has been pulled into your
    tree in the meantime, but resolution seems not that bad ... getting current
    bpf-next out now before there's coming more on mlx5. ;) I'm Cc'ing Saeed
    just so he's aware of the resolution below:
    
    ** First conflict in drivers/net/ethernet/mellanox/mlx5/core/en_main.c:
    
      <<<<<<< HEAD
      static int mlx5e_open_cq(struct mlx5e_channel *c,
                               struct dim_cq_moder moder,
                               struct mlx5e_cq_param *param,
                               struct mlx5e_cq *cq)
      =======
      int mlx5e_open_cq(struct mlx5e_channel *c, struct net_dim_cq_moder moder,
                        struct mlx5e_cq_param *param, struct mlx5e_cq *cq)
      >>>>>>> e5a3e259ef239f443951d401db10db7d426c9497
    
    Resolution is to take the second chunk and rename net_dim_cq_moder into
    dim_cq_moder. Also the signature for mlx5e_open_cq() in ...
    
      drivers/net/ethernet/mellanox/mlx5/core/en.h +977
    
    ... and in mlx5e_open_xsk() ...
    
      drivers/net/ethernet/mellanox/mlx5/core/en/xsk/setup.c +64
    
    ... needs the same rename from net_dim_cq_moder into dim_cq_moder.
    
    ** Second conflict in drivers/net/ethernet/mellanox/mlx5/core/en_main.c:
    
      <<<<<<< HEAD
              int cpu = cpumask_first(mlx5_comp_irq_get_affinity_mask(priv->mdev, ix));
              struct dim_cq_moder icocq_moder = {0, 0};
              struct net_device *netdev = priv->netdev;
              struct mlx5e_channel *c;
              unsigned int irq;
      =======
              struct net_dim_cq_moder icocq_moder = {0, 0};
      >>>>>>> e5a3e259ef239f443951d401db10db7d426c9497
    
    Take the second chunk and rename net_dim_cq_moder into dim_cq_moder
    as well.
    
    Let me know if you run into any issues. Anyway, the main changes are:
    
    1) Long-awaited AF_XDP support for mlx5e driver, from Maxim.
    
    2) Addition of two new per-cgroup BPF hooks for getsockopt and
       setsockopt along with a new sockopt program type which allows more
       fine-grained pass/reject settings for containers. Also add a sock_ops
       callback that can be selectively enabled on a per-socket basis and is
       executed for every RTT to help tracking TCP statistics, both features
       from Stanislav.
    
    3) Follow-up fix from loops in precision tracking which was not propagating
       precision marks and as a result verifier assumed that some branches were
       not taken and therefore wrongly removed as dead code, from Alexei.
    
    4) Fix BPF cgroup release synchronization race which could lead to a
       double-free if a leaf's cgroup_bpf object is released and a new BPF
       program is attached to the one of ancestor cgroups in parallel, from Roman.
    
    5) Support for bulking XDP_TX on veth devices which improves performance
       in some cases by around 9%, from Toshiaki.
    
    6) Allow for lookups into BPF devmap and improve feedback when calling into
       bpf_redirect_map() as lookup is now performed right away in the helper
       itself, from Toke.
    
    7) Add support for fq's Earliest Departure Time to the Host Bandwidth
       Manager (HBM) sample BPF program, from Lawrence.
    
    8) Various cleanups and minor fixes all over the place from many others.
    ====================
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 4e0e2ea1886afe8c001971ff767f6670312a9b04
Author: Yishai Hadas <yishaih@mellanox.com>
Date:   Sun Jun 30 19:23:27 2019 +0300

    net/mlx5: Report EQE data upon CQ completion
    
    Report EQE data upon CQ completion to let upper layers use this data.
    
    Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
    Acked-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 3a183d690e23..16753f263079 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -780,7 +780,7 @@ netdev_tx_t mlx5e_sq_xmit(struct mlx5e_txqsq *sq, struct sk_buff *skb,
 			  struct mlx5e_tx_wqe *wqe, u16 pi, bool xmit_more);
 
 void mlx5e_trigger_irq(struct mlx5e_icosq *sq);
-void mlx5e_completion_event(struct mlx5_core_cq *mcq);
+void mlx5e_completion_event(struct mlx5_core_cq *mcq, struct mlx5_eqe *eqe);
 void mlx5e_cq_error_event(struct mlx5_core_cq *mcq, enum mlx5_event event);
 int mlx5e_napi_poll(struct napi_struct *napi, int budget);
 bool mlx5e_poll_tx_cq(struct mlx5e_cq *cq, int napi_budget);

commit a90f88fe557d7f2318b724a93875dcaac45b5f05
Author: Gavi Teitz <gavi@mellanox.com>
Date:   Thu May 23 09:58:56 2019 +0300

    net/mlx5e: Don't refresh TIRs when updating representor SQs
    
    Refreshing TIRs is done in order to update the TIRs with the current
    state of SQs in the transport domain, so that the TIRs can filter out
    undesired self-loopback packets based on the source SQ of the packet.
    
    Representor TIRs will only receive packets that originate from their
    associated vport, due to dedicated steering, and therefore will never
    receive self-loopback packets, whose source vport will be the vport of
    the E-Switch manager, and therefore not the vport associated with the
    representor. As such, it is not necessary to refresh the representors'
    TIRs, since self-loopback packets can't reach them.
    
    Since representors only exist in switchdev mode, and there is no
    scenario in which a representor will exist in the transport domain
    alongside a non-representor, it is not necessary to refresh the
    transport domain's TIRs upon changing the state of a representor's
    queues. Therefore, do not refresh TIRs upon such a change. Achieve
    this by adding an update_rx callback to the mlx5e_profile, which
    refreshes TIRs for non-representors and does nothing for representors,
    and replace instances of mlx5e_refresh_tirs() upon changing the state
    of the queues with update_rx().
    
    Signed-off-by: Gavi Teitz <gavi@mellanox.com>
    Reviewed-by: Roi Dayan <roid@mellanox.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 3fe3fe128256..3281ace5e126 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -763,6 +763,7 @@ struct mlx5e_profile {
 	void	(*cleanup_tx)(struct mlx5e_priv *priv);
 	void	(*enable)(struct mlx5e_priv *priv);
 	void	(*disable)(struct mlx5e_priv *priv);
+	int	(*update_rx)(struct mlx5e_priv *priv);
 	void	(*update_stats)(struct mlx5e_priv *priv);
 	void	(*update_carrier)(struct mlx5e_priv *priv);
 	struct {
@@ -1034,6 +1035,7 @@ int mlx5e_create_tis(struct mlx5_core_dev *mdev, int tc,
 void mlx5e_destroy_tis(struct mlx5_core_dev *mdev, u32 tisn);
 
 int mlx5e_create_tises(struct mlx5e_priv *priv);
+int mlx5e_update_nic_rx(struct mlx5e_priv *priv);
 void mlx5e_update_carrier(struct mlx5e_priv *priv);
 int mlx5e_close(struct net_device *netdev);
 int mlx5e_open(struct net_device *netdev);

commit db05815b36cbd486c86fd002dfa81c9af6245e25
Author: Maxim Mikityanskiy <maximmi@mellanox.com>
Date:   Wed Jun 26 17:35:38 2019 +0300

    net/mlx5e: Add XSK zero-copy support
    
    This commit adds support for AF_XDP zero-copy RX and TX.
    
    We create a dedicated XSK RQ inside the channel, it means that two
    RQs are running simultaneously: one for non-XSK traffic and the other
    for XSK traffic. The regular and XSK RQs use a single ID namespace split
    into two halves: the lower half is regular RQs, and the upper half is
    XSK RQs. When any zero-copy AF_XDP socket is active, changing the number
    of channels is not allowed, because it would break to mapping between
    XSK RQ IDs and channels.
    
    XSK requires different page allocation and release routines. Such
    functions as mlx5e_{alloc,free}_rx_mpwqe and mlx5e_{get,put}_rx_frag are
    generic enough to be used for both regular and XSK RQs, and they use the
    mlx5e_page_{alloc,release} wrappers around the real allocation
    functions. Function pointers are not used to avoid losing the
    performance with retpolines. Wherever it's certain that the regular
    (non-XSK) page release function should be used, it's called directly.
    
    Only the stats that could be meaningful for XSK are exposed to the
    userspace. Those that don't take part in the XSK flow are not
    considered.
    
    Note that we don't wait for WQEs on the XSK RQ (unlike the regular RQ),
    because the newer xdpsock sample doesn't provide any Fill Ring entries
    at the setup stage.
    
    We create a dedicated XSK SQ in the channel. This separation has its
    advantages:
    
    1. When the UMEM is closed, the XSK SQ can also be closed and stop
    receiving completions. If an existing SQ was used for XSK, it would
    continue receiving completions for the packets of the closed socket. If
    a new UMEM was opened at that point, it would start getting completions
    that don't belong to it.
    
    2. Calculating statistics separately.
    
    When the userspace kicks the TX, the driver triggers a hardware
    interrupt by posting a NOP to a dedicated XSK ICO (internal control
    operations) SQ, in order to trigger NAPI on the right CPU core. This XSK
    ICO SQ is protected by a spinlock, as the userspace application may kick
    the TX from any core.
    
    Store the pointers to the UMEMs in the net device private context,
    independently from the kernel. This way the driver can distinguish
    between the zero-copy and non-zero-copy UMEMs. The kernel function
    xdp_get_umem_from_qid does not care about this difference, but the
    driver is only interested in zero-copy UMEMs, particularly, on the
    cleanup it determines whether to close the XSK RQ and SQ or not by
    looking at the presence of the UMEM. Use state_lock to protect the
    access to this area of UMEM pointers.
    
    LRO isn't compatible with XDP, but there may be active UMEMs while
    XDP is off. If this is the case, don't allow LRO to ensure XDP can
    be reenabled at any time.
    
    The validation of XSK parameters typically happens when XSK queues
    open. However, when the interface is down or the XDP program isn't
    set, it's still possible to have active AF_XDP sockets and even to
    open new, but the XSK queues will be closed. To cover these cases,
    perform the validation also in these flows:
    
    1. A new UMEM is registered, but the XSK queues aren't going to be
    created due to missing XDP program or interface being down.
    
    2. MTU changes while there are UMEMs registered.
    
    Having this early check prevents mlx5e_open_channels from failing
    at a later stage, where recovery is impossible and the application
    has no chance to handle the error, because it got the successful
    return value for an MTU change or XSK open operation.
    
    The performance testing was performed on a machine with the following
    configuration:
    
    - 24 cores of Intel Xeon E5-2620 v3 @ 2.40 GHz
    - Mellanox ConnectX-5 Ex with 100 Gbit/s link
    
    The results with retpoline disabled, single stream:
    
    txonly: 33.3 Mpps (21.5 Mpps with queue and app pinned to the same CPU)
    rxdrop: 12.2 Mpps
    l2fwd: 9.4 Mpps
    
    The results with retpoline enabled, single stream:
    
    txonly: 21.3 Mpps (14.1 Mpps with queue and app pinned to the same CPU)
    rxdrop: 9.9 Mpps
    l2fwd: 6.8 Mpps
    
    Signed-off-by: Maxim Mikityanskiy <maximmi@mellanox.com>
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Acked-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 010140753e73..cb00e622c006 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -137,6 +137,7 @@ struct page_pool;
 #define MLX5E_MAX_NUM_CHANNELS         (MLX5E_INDIR_RQT_SIZE >> 1)
 #define MLX5E_MAX_NUM_SQS              (MLX5E_MAX_NUM_CHANNELS * MLX5E_MAX_NUM_TC)
 #define MLX5E_TX_CQ_POLL_BUDGET        128
+#define MLX5E_TX_XSK_POLL_BUDGET       64
 #define MLX5E_SQ_RECOVER_MIN_INTERVAL  500 /* msecs */
 
 #define MLX5E_UMR_WQE_INLINE_SZ \
@@ -155,6 +156,11 @@ do {                                                            \
 			    ##__VA_ARGS__);                     \
 } while (0)
 
+enum mlx5e_rq_group {
+	MLX5E_RQ_GROUP_REGULAR,
+	MLX5E_RQ_GROUP_XSK,
+	MLX5E_NUM_RQ_GROUPS /* Keep last. */
+};
 
 static inline u16 mlx5_min_rx_wqes(int wq_type, u32 wq_size)
 {
@@ -179,7 +185,8 @@ static inline int mlx5e_get_max_num_channels(struct mlx5_core_dev *mdev)
 /* Use this function to get max num channels after netdev was created */
 static inline int mlx5e_get_netdev_max_channels(struct net_device *netdev)
 {
-	return min_t(unsigned int, netdev->num_rx_queues,
+	return min_t(unsigned int,
+		     netdev->num_rx_queues / MLX5E_NUM_RQ_GROUPS,
 		     netdev->num_tx_queues);
 }
 
@@ -250,6 +257,7 @@ struct mlx5e_params {
 	u32 lro_timeout;
 	u32 pflags;
 	struct bpf_prog *xdp_prog;
+	struct mlx5e_xsk *xsk;
 	unsigned int sw_mtu;
 	int hard_mtu;
 };
@@ -399,8 +407,14 @@ struct mlx5e_txqsq {
 } ____cacheline_aligned_in_smp;
 
 struct mlx5e_dma_info {
-	struct page     *page;
-	dma_addr_t      addr;
+	dma_addr_t addr;
+	union {
+		struct page *page;
+		struct {
+			u64 handle;
+			void *data;
+		} xsk;
+	};
 };
 
 /* XDP packets can be transmitted in different ways. On completion, we need to
@@ -467,9 +481,11 @@ struct mlx5e_xdp_mpwqe {
 };
 
 struct mlx5e_xdpsq;
+typedef int (*mlx5e_fp_xmit_xdp_frame_check)(struct mlx5e_xdpsq *);
 typedef bool (*mlx5e_fp_xmit_xdp_frame)(struct mlx5e_xdpsq *,
 					struct mlx5e_xdp_xmit_data *,
-					struct mlx5e_xdp_info *);
+					struct mlx5e_xdp_info *,
+					int);
 
 struct mlx5e_xdpsq {
 	/* data path */
@@ -487,8 +503,10 @@ struct mlx5e_xdpsq {
 	struct mlx5e_cq            cq;
 
 	/* read only */
+	struct xdp_umem           *umem;
 	struct mlx5_wq_cyc         wq;
 	struct mlx5e_xdpsq_stats  *stats;
+	mlx5e_fp_xmit_xdp_frame_check xmit_xdp_frame_check;
 	mlx5e_fp_xmit_xdp_frame    xmit_xdp_frame;
 	struct {
 		struct mlx5e_xdp_wqe_info *wqe_info;
@@ -619,6 +637,7 @@ struct mlx5e_rq {
 		} mpwqe;
 	};
 	struct {
+		u16            umem_headroom;
 		u16            headroom;
 		u8             map_dir;   /* dma map direction */
 	} buff;
@@ -649,6 +668,10 @@ struct mlx5e_rq {
 	DECLARE_BITMAP(flags, 8);
 	struct page_pool      *page_pool;
 
+	/* AF_XDP zero-copy */
+	struct zero_copy_allocator zca;
+	struct xdp_umem       *umem;
+
 	/* control */
 	struct mlx5_wq_ctrl    wq_ctrl;
 	__be32                 mkey_be;
@@ -661,6 +684,11 @@ struct mlx5e_rq {
 	struct xdp_rxq_info    xdp_rxq;
 } ____cacheline_aligned_in_smp;
 
+enum mlx5e_channel_state {
+	MLX5E_CHANNEL_STATE_XSK,
+	MLX5E_CHANNEL_NUM_STATES
+};
+
 struct mlx5e_channel {
 	/* data path */
 	struct mlx5e_rq            rq;
@@ -677,6 +705,13 @@ struct mlx5e_channel {
 	/* XDP_REDIRECT */
 	struct mlx5e_xdpsq         xdpsq;
 
+	/* AF_XDP zero-copy */
+	struct mlx5e_rq            xskrq;
+	struct mlx5e_xdpsq         xsksq;
+	struct mlx5e_icosq         xskicosq;
+	/* xskicosq can be accessed from any CPU - the spinlock protects it. */
+	spinlock_t                 xskicosq_lock;
+
 	/* data path - accessed per napi poll */
 	struct irq_desc *irq_desc;
 	struct mlx5e_ch_stats     *stats;
@@ -685,6 +720,7 @@ struct mlx5e_channel {
 	struct mlx5e_priv         *priv;
 	struct mlx5_core_dev      *mdev;
 	struct hwtstamp_config    *tstamp;
+	DECLARE_BITMAP(state, MLX5E_CHANNEL_NUM_STATES);
 	int                        ix;
 	int                        cpu;
 	cpumask_var_t              xps_cpumask;
@@ -700,14 +736,17 @@ struct mlx5e_channel_stats {
 	struct mlx5e_ch_stats ch;
 	struct mlx5e_sq_stats sq[MLX5E_MAX_NUM_TC];
 	struct mlx5e_rq_stats rq;
+	struct mlx5e_rq_stats xskrq;
 	struct mlx5e_xdpsq_stats rq_xdpsq;
 	struct mlx5e_xdpsq_stats xdpsq;
+	struct mlx5e_xdpsq_stats xsksq;
 } ____cacheline_aligned_in_smp;
 
 enum {
 	MLX5E_STATE_OPENED,
 	MLX5E_STATE_DESTROYING,
 	MLX5E_STATE_XDP_TX_ENABLED,
+	MLX5E_STATE_XDP_OPEN,
 };
 
 struct mlx5e_rqt {
@@ -740,6 +779,17 @@ struct mlx5e_modify_sq_param {
 	int rl_index;
 };
 
+struct mlx5e_xsk {
+	/* UMEMs are stored separately from channels, because we don't want to
+	 * lose them when channels are recreated. The kernel also stores UMEMs,
+	 * but it doesn't distinguish between zero-copy and non-zero-copy UMEMs,
+	 * so rely on our mechanism.
+	 */
+	struct xdp_umem **umems;
+	u16 refcnt;
+	bool ever_used;
+};
+
 struct mlx5e_priv {
 	/* priv data path fields - start */
 	struct mlx5e_txqsq *txq2sq[MLX5E_MAX_NUM_CHANNELS * MLX5E_MAX_NUM_TC];
@@ -760,6 +810,7 @@ struct mlx5e_priv {
 	struct mlx5e_tir           indir_tir[MLX5E_NUM_INDIR_TIRS];
 	struct mlx5e_tir           inner_indir_tir[MLX5E_NUM_INDIR_TIRS];
 	struct mlx5e_tir           direct_tir[MLX5E_MAX_NUM_CHANNELS];
+	struct mlx5e_tir           xsk_tir[MLX5E_MAX_NUM_CHANNELS];
 	struct mlx5e_rss_params    rss_params;
 	u32                        tx_rates[MLX5E_MAX_NUM_SQS];
 
@@ -796,6 +847,7 @@ struct mlx5e_priv {
 	struct mlx5e_tls          *tls;
 #endif
 	struct devlink_health_reporter *tx_reporter;
+	struct mlx5e_xsk           xsk;
 };
 
 struct mlx5e_profile {
@@ -839,8 +891,9 @@ bool mlx5e_striding_rq_possible(struct mlx5_core_dev *mdev,
 				struct mlx5e_params *params);
 
 void mlx5e_page_dma_unmap(struct mlx5e_rq *rq, struct mlx5e_dma_info *dma_info);
-void mlx5e_page_release(struct mlx5e_rq *rq, struct mlx5e_dma_info *dma_info,
-			bool recycle);
+void mlx5e_page_release_dynamic(struct mlx5e_rq *rq,
+				struct mlx5e_dma_info *dma_info,
+				bool recycle);
 void mlx5e_handle_rx_cqe(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe);
 void mlx5e_handle_rx_cqe_mpwrq(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe);
 bool mlx5e_post_rx_wqes(struct mlx5e_rq *rq);
@@ -900,6 +953,30 @@ void mlx5e_build_indir_tir_ctx_hash(struct mlx5e_rss_params *rss_params,
 void mlx5e_modify_tirs_hash(struct mlx5e_priv *priv, void *in, int inlen);
 struct mlx5e_tirc_config mlx5e_tirc_get_default_config(enum mlx5e_traffic_types tt);
 
+struct mlx5e_xsk_param;
+
+struct mlx5e_rq_param;
+int mlx5e_open_rq(struct mlx5e_channel *c, struct mlx5e_params *params,
+		  struct mlx5e_rq_param *param, struct mlx5e_xsk_param *xsk,
+		  struct xdp_umem *umem, struct mlx5e_rq *rq);
+int mlx5e_wait_for_min_rx_wqes(struct mlx5e_rq *rq, int wait_time);
+void mlx5e_deactivate_rq(struct mlx5e_rq *rq);
+void mlx5e_close_rq(struct mlx5e_rq *rq);
+
+struct mlx5e_sq_param;
+int mlx5e_open_icosq(struct mlx5e_channel *c, struct mlx5e_params *params,
+		     struct mlx5e_sq_param *param, struct mlx5e_icosq *sq);
+void mlx5e_close_icosq(struct mlx5e_icosq *sq);
+int mlx5e_open_xdpsq(struct mlx5e_channel *c, struct mlx5e_params *params,
+		     struct mlx5e_sq_param *param, struct xdp_umem *umem,
+		     struct mlx5e_xdpsq *sq, bool is_redirect);
+void mlx5e_close_xdpsq(struct mlx5e_xdpsq *sq);
+
+struct mlx5e_cq_param;
+int mlx5e_open_cq(struct mlx5e_channel *c, struct net_dim_cq_moder moder,
+		  struct mlx5e_cq_param *param, struct mlx5e_cq *cq);
+void mlx5e_close_cq(struct mlx5e_cq *cq);
+
 int mlx5e_open_locked(struct net_device *netdev);
 int mlx5e_close_locked(struct net_device *netdev);
 
@@ -1070,10 +1147,10 @@ int mlx5e_create_indirect_rqt(struct mlx5e_priv *priv);
 int mlx5e_create_indirect_tirs(struct mlx5e_priv *priv, bool inner_ttc);
 void mlx5e_destroy_indirect_tirs(struct mlx5e_priv *priv, bool inner_ttc);
 
-int mlx5e_create_direct_rqts(struct mlx5e_priv *priv);
-void mlx5e_destroy_direct_rqts(struct mlx5e_priv *priv);
-int mlx5e_create_direct_tirs(struct mlx5e_priv *priv);
-void mlx5e_destroy_direct_tirs(struct mlx5e_priv *priv);
+int mlx5e_create_direct_rqts(struct mlx5e_priv *priv, struct mlx5e_tir *tirs);
+void mlx5e_destroy_direct_rqts(struct mlx5e_priv *priv, struct mlx5e_tir *tirs);
+int mlx5e_create_direct_tirs(struct mlx5e_priv *priv, struct mlx5e_tir *tirs);
+void mlx5e_destroy_direct_tirs(struct mlx5e_priv *priv, struct mlx5e_tir *tirs);
 void mlx5e_destroy_rqt(struct mlx5e_priv *priv, struct mlx5e_rqt *rqt);
 
 int mlx5e_create_tis(struct mlx5_core_dev *mdev, int tc,
@@ -1142,6 +1219,7 @@ void mlx5e_detach_netdev(struct mlx5e_priv *priv);
 void mlx5e_destroy_netdev(struct mlx5e_priv *priv);
 void mlx5e_set_netdev_mtu_boundaries(struct mlx5e_priv *priv);
 void mlx5e_build_nic_params(struct mlx5_core_dev *mdev,
+			    struct mlx5e_xsk *xsk,
 			    struct mlx5e_rss_params *rss_params,
 			    struct mlx5e_params *params,
 			    u16 max_channels, u16 mtu);

commit b9673cf5558c0ae1be787611884d4131633f31a8
Author: Maxim Mikityanskiy <maximmi@mellanox.com>
Date:   Wed Jun 26 17:35:33 2019 +0300

    net/mlx5e: Share the XDP SQ for XDP_TX between RQs
    
    Put the XDP SQ that is used for XDP_TX into the channel. It used to be a
    part of the RQ, but with introduction of AF_XDP there will be one more
    RQ that could share the same XDP SQ. This patch is a preparation for
    that change.
    
    Separate XDP_TX statistics per RQ were implemented in one of the previous
    patches.
    
    Signed-off-by: Maxim Mikityanskiy <maximmi@mellanox.com>
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Acked-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index b3c2d7ef1287..010140753e73 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -432,6 +432,7 @@ struct mlx5e_xdp_info {
 			dma_addr_t dma_addr;
 		} frame;
 		struct {
+			struct mlx5e_rq *rq;
 			struct mlx5e_dma_info di;
 		} page;
 	};
@@ -644,7 +645,7 @@ struct mlx5e_rq {
 
 	/* XDP */
 	struct bpf_prog       *xdp_prog;
-	struct mlx5e_xdpsq     xdpsq;
+	struct mlx5e_xdpsq    *xdpsq;
 	DECLARE_BITMAP(flags, 8);
 	struct page_pool      *page_pool;
 
@@ -663,6 +664,7 @@ struct mlx5e_rq {
 struct mlx5e_channel {
 	/* data path */
 	struct mlx5e_rq            rq;
+	struct mlx5e_xdpsq         rq_xdpsq;
 	struct mlx5e_txqsq         sq[MLX5E_MAX_NUM_TC];
 	struct mlx5e_icosq         icosq;   /* internal control operations */
 	bool                       xdp;

commit d963fa15113076f69d8a021de393ab4613620cd9
Author: Maxim Mikityanskiy <maximmi@mellanox.com>
Date:   Wed Jun 26 17:35:32 2019 +0300

    net/mlx5e: Refactor struct mlx5e_xdp_info
    
    Currently, struct mlx5e_xdp_info has some issues that have to be cleaned
    up before the upcoming AF_XDP support makes things too complicated and
    messy. This structure is used both when sending the packet and on
    completion. Moreover, the cleanup procedure on completion depends on the
    origin of the packet (XDP_REDIRECT, XDP_TX). Adding AF_XDP support will
    add new flows that use this structure even differently. To avoid
    overcomplicating the code, this commit refactors the usage of this
    structure in the following ways:
    
    1. struct mlx5e_xdp_info is split into two different structures. One is
    struct mlx5e_xdp_xmit_data, a transient structure that doesn't need to
    be stored and is only used while sending the packet. The other is still
    struct mlx5e_xdp_info that is stored in a FIFO and contains the fields
    needed on completion.
    
    2. The fields of struct mlx5e_xdp_info that are used in different flows
    are put into a union. A special enum indicates the cleanup mode and
    helps choose the right union member. This approach is clear and
    explicit. Although it could be possible to "guess" the mode by looking
    at the values of the fields and at the XDP SQ type, it wouldn't be that
    clear and extendable and would require looking through the whole chain
    to understand what's going on.
    
    For the reference, there are the fields of struct mlx5e_xdp_info that
    are used in different flows (including AF_XDP ones):
    
    Packet origin          | Fields used on completion | Cleanup steps
    -----------------------+---------------------------+------------------
    XDP_REDIRECT,          | xdpf, dma_addr            | DMA unmap and
    XDP_TX from XSK RQ     |                           | xdp_return_frame.
    -----------------------+---------------------------+------------------
    XDP_TX from regular RQ | di                        | Recycle page.
    -----------------------+---------------------------+------------------
    AF_XDP TX              | (none)                    | Increment the
                           |                           | producer index in
                           |                           | Completion Ring.
    
    On send, the same set of mlx5e_xdp_xmit_data fields is used in all
    flows: DMA and virtual addresses and length.
    
    Signed-off-by: Maxim Mikityanskiy <maximmi@mellanox.com>
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Acked-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index c7676635bc8e..b3c2d7ef1287 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -403,10 +403,44 @@ struct mlx5e_dma_info {
 	dma_addr_t      addr;
 };
 
+/* XDP packets can be transmitted in different ways. On completion, we need to
+ * distinguish between them to clean up things in a proper way.
+ */
+enum mlx5e_xdp_xmit_mode {
+	/* An xdp_frame was transmitted due to either XDP_REDIRECT from another
+	 * device or XDP_TX from an XSK RQ. The frame has to be unmapped and
+	 * returned.
+	 */
+	MLX5E_XDP_XMIT_MODE_FRAME,
+
+	/* The xdp_frame was created in place as a result of XDP_TX from a
+	 * regular RQ. No DMA remapping happened, and the page belongs to us.
+	 */
+	MLX5E_XDP_XMIT_MODE_PAGE,
+
+	/* No xdp_frame was created at all, the transmit happened from a UMEM
+	 * page. The UMEM Completion Ring producer pointer has to be increased.
+	 */
+	MLX5E_XDP_XMIT_MODE_XSK,
+};
+
 struct mlx5e_xdp_info {
-	struct xdp_frame      *xdpf;
-	dma_addr_t            dma_addr;
-	struct mlx5e_dma_info di;
+	enum mlx5e_xdp_xmit_mode mode;
+	union {
+		struct {
+			struct xdp_frame *xdpf;
+			dma_addr_t dma_addr;
+		} frame;
+		struct {
+			struct mlx5e_dma_info di;
+		} page;
+	};
+};
+
+struct mlx5e_xdp_xmit_data {
+	dma_addr_t  dma_addr;
+	void       *data;
+	u32         len;
 };
 
 struct mlx5e_xdp_info_fifo {
@@ -432,8 +466,10 @@ struct mlx5e_xdp_mpwqe {
 };
 
 struct mlx5e_xdpsq;
-typedef bool (*mlx5e_fp_xmit_xdp_frame)(struct mlx5e_xdpsq*,
-					struct mlx5e_xdp_info*);
+typedef bool (*mlx5e_fp_xmit_xdp_frame)(struct mlx5e_xdpsq *,
+					struct mlx5e_xdp_xmit_data *,
+					struct mlx5e_xdp_info *);
+
 struct mlx5e_xdpsq {
 	/* data path */
 

commit ed084fb60429fe1d666402f31e17c394001eecbc
Author: Maxim Mikityanskiy <maximmi@mellanox.com>
Date:   Wed Jun 26 17:35:31 2019 +0300

    net/mlx5e: Allow ICO SQ to be used by multiple RQs
    
    Prepare to creation of the XSK RQ, which will require posting UMRs, too.
    The same ICO SQ will be used for both RQs and also to trigger interrupts
    by posting NOPs. UMR WQEs can't be reused any more. Optimization
    introduced in commit ab966d7e4ff98 ("net/mlx5e: RX, Recycle buffer of
    UMR WQEs") is reverted.
    
    Signed-off-by: Maxim Mikityanskiy <maximmi@mellanox.com>
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Acked-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 992d5cb646b2..c7676635bc8e 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -348,6 +348,13 @@ enum {
 
 struct mlx5e_sq_wqe_info {
 	u8  opcode;
+
+	/* Auxiliary data for different opcodes. */
+	union {
+		struct {
+			struct mlx5e_rq *rq;
+		} umr;
+	};
 };
 
 struct mlx5e_txqsq {
@@ -571,6 +578,7 @@ struct mlx5e_rq {
 			u8                     log_stride_sz;
 			u8                     umr_in_progress;
 			u8                     umr_last_bulk;
+			u8                     umr_completed;
 		} mpwqe;
 	};
 	struct {
@@ -798,6 +806,7 @@ void mlx5e_page_release(struct mlx5e_rq *rq, struct mlx5e_dma_info *dma_info,
 void mlx5e_handle_rx_cqe(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe);
 void mlx5e_handle_rx_cqe_mpwrq(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe);
 bool mlx5e_post_rx_wqes(struct mlx5e_rq *rq);
+void mlx5e_poll_ico_cq(struct mlx5e_cq *cq);
 bool mlx5e_post_rx_mpwqes(struct mlx5e_rq *rq);
 void mlx5e_dealloc_rx_wqe(struct mlx5e_rq *rq, u16 ix);
 void mlx5e_dealloc_rx_mpwqe(struct mlx5e_rq *rq, u16 ix);

commit d7ee287827ef107420871651963770f8705cb56e
Merge: a653f2f538f9 398c2b05bbee
Author: David S. Miller <davem@davemloft.net>
Date:   Thu Jun 27 12:42:51 2019 -0700

    Merge tag 'blk-dim-v2' of git://git.kernel.org/pub/scm/linux/kernel/git/saeed/linux
    
    Saeed Mamameed says:
    
    ====================
    Generic DIM
    
    From: Tal Gilboa and Yamin Fridman
    
    Implement net DIM over a generic DIM library, add RDMA DIM
    
    dim.h lib exposes an implementation of the DIM algorithm for
    dynamically-tuned interrupt moderation for networking interfaces.
    
    We want a similar functionality for other protocols, which might need to
    optimize interrupts differently. Main motivation here is DIM for NVMf
    storage protocol.
    
    Current DIM implementation prioritizes reducing interrupt overhead over
    latency. Also, in order to reduce DIM's own overhead, the algorithm might
    take some time to identify it needs to change profiles. While this is
    acceptable for networking, it might not work well on other scenarios.
    
    Here we propose a new structure to DIM. The idea is to allow a slightly
    modified functionality without the risk of breaking Net DIM behavior for
    netdev. We verified there are no degradations in current DIM behavior with
    the modified solution.
    
    Suggested solution:
    - Common logic is implemented in lib/dim/dim.c
    - Net DIM (existing) logic is implemented in lib/dim/net_dim.c, which uses
      the common logic in dim.c
    - Any new DIM logic will be implemented in "lib/dim/new_dim.c".
      This new implementation will expose modified versions of profiles,
      dim_step() and dim_decision().
    - DIM API is declared in include/linux/dim.h for all implementations.
    
    Pros for this solution are:
    - Zero impact on existing net_dim implementation and usage
    - Relatively more code reuse (compared to two separate solutions)
    - Increased extensibility
    ====================
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 4f75da3666c0c572967729a2401ac650be5581b6
Author: Tal Gilboa <talgi@mellanox.com>
Date:   Thu Jan 10 17:33:17 2019 +0200

    linux/dim: Move implementation to .c files
    
    Moved all logic from dim.h and net_dim.h to dim.c and net_dim.c.
    This is both more structurally appealing and would allow to only
    expose externally used functions.
    
    Signed-off-by: Tal Gilboa <talgi@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 11efd6e4bdc3..abf42d3aabe9 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -48,7 +48,7 @@
 #include <linux/rhashtable.h>
 #include <net/switchdev.h>
 #include <net/xdp.h>
-#include <linux/net_dim.h>
+#include <linux/dim.h>
 #include <linux/bits.h>
 #include "wq.h"
 #include "mlx5_core.h"

commit 8960b38932bee8db0bc9c4d8c135f21df6cdd297
Author: Tal Gilboa <talgi@mellanox.com>
Date:   Thu Jan 31 16:44:48 2019 +0200

    linux/dim: Rename externally used net_dim members
    
    Removed 'net' prefix from functions and structs used by external drivers.
    
    Signed-off-by: Tal Gilboa <talgi@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 3a183d690e23..11efd6e4bdc3 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -238,9 +238,9 @@ struct mlx5e_params {
 	u16 num_channels;
 	u8  num_tc;
 	bool rx_cqe_compress_def;
-	struct net_dim_cq_moder rx_cq_moderation;
-	struct net_dim_cq_moder tx_cq_moderation;
 	bool tunneled_offload_en;
+	struct dim_cq_moder rx_cq_moderation;
+	struct dim_cq_moder tx_cq_moderation;
 	bool lro_en;
 	u8  tx_min_inline_mode;
 	bool vlan_strip_disable;
@@ -356,7 +356,7 @@ struct mlx5e_txqsq {
 	/* dirtied @completion */
 	u16                        cc;
 	u32                        dma_fifo_cc;
-	struct net_dim             dim; /* Adaptive Moderation */
+	struct dim                 dim; /* Adaptive Moderation */
 
 	/* dirtied @xmit */
 	u16                        pc ____cacheline_aligned_in_smp;
@@ -595,7 +595,7 @@ struct mlx5e_rq {
 	int                    ix;
 	unsigned int           hw_mtu;
 
-	struct net_dim         dim; /* Dynamic Interrupt Moderation */
+	struct dim         dim; /* Dynamic Interrupt Moderation */
 
 	/* XDP */
 	struct bpf_prog       *xdp_prog;

commit 13091aa30535b719e269f20a7bc34002bf5afae5
Merge: f97252a8c33f 29f785ff76b6
Author: David S. Miller <davem@davemloft.net>
Date:   Mon Jun 17 19:48:13 2019 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Honestly all the conflicts were simple overlapping changes,
    nothing really interesting to report.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 57c70d8740f740498a52f9c0c0d7295829b944de
Author: Shay Agroskin <shayag@mellanox.com>
Date:   Sun Apr 28 10:14:23 2019 +0300

    net/mlx5e: Replace reciprocal_scale in TX select queue function
    
    The TX queue index returned by the fallback function ranges
    between [0,NUM CHANNELS - 1] if QoS isn't set and
    [0, (NUM CHANNELS)*(NUM TCs) -1] otherwise.
    
    Our HW uses different TC mapping than the fallback function
    (which is denoted as 'up', user priority) so we only need to extract
    a channel number out of the returned value.
    
    Since (NUM CHANNELS)*(NUM TCs) is a relatively small number, using
    reciprocal scale almost always returns zero.
    We instead access the 'txq2sq' table to extract the sq (and with it the
    channel number) associated with the tx queue, thus getting
    a more evenly distributed channel number.
    
    Perf:
    
    Rx/Tx side with Intel(R) Xeon(R) Silver 4108 CPU @ 1.80GHz and ConnectX-5.
    Used 'iperf' UDP traffic, 10 threads, and priority 5.
    
    Before: 0.566Mpps
    After:   2.37Mpps
    
    As expected, releasing the existing bottleneck of steering all traffic
    to TX queue zero significantly improves transmission rates.
    
    Fixes: 7ccdd0841b30 ("net/mlx5e: Fix select queue callback")
    Signed-off-by: Shay Agroskin <shayag@mellanox.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index ab027f57725c..cc6797e24571 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -385,6 +385,7 @@ struct mlx5e_txqsq {
 	/* control path */
 	struct mlx5_wq_ctrl        wq_ctrl;
 	struct mlx5e_channel      *channel;
+	int                        ch_ix;
 	int                        txq_ix;
 	u32                        rate_limit;
 	struct work_struct         recover_work;

commit d3cbd4254df881777e2efb68ee10ede0d9dc0647
Author: Chris Mi <chrism@mellanox.com>
Date:   Thu May 16 17:36:43 2019 +0800

    net/mlx5e: Add ndo_set_feature for uplink representor
    
    After we have a dedicated uplink representor, the new netdev ops
    doesn't support ndo_set_feature. Because of that, we can't change
    some features, eg. rxvlan. Now add it back.
    
    In this patch, I also do a cleanup for the features flag handling,
    eg. remove duplicate NETIF_F_HW_TC flag setting.
    
    Fixes: aec002f6f82c ("net/mlx5e: Uninstantiate esw manager vport netdev on switchdev mode")
    Signed-off-by: Chris Mi <chrism@mellanox.com>
    Reviewed-by: Roi Dayan <roid@mellanox.com>
    Reviewed-by: Vlad Buslov <vladbu@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 3a183d690e23..ab027f57725c 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -1112,6 +1112,7 @@ void mlx5e_del_vxlan_port(struct net_device *netdev, struct udp_tunnel_info *ti)
 netdev_features_t mlx5e_features_check(struct sk_buff *skb,
 				       struct net_device *netdev,
 				       netdev_features_t features);
+int mlx5e_set_features(struct net_device *netdev, netdev_features_t features);
 #ifdef CONFIG_MLX5_ESWITCH
 int mlx5e_set_vf_mac(struct net_device *dev, int vf, u8 *mac);
 int mlx5e_set_vf_rate(struct net_device *dev, int vf, int min_tx_rate, int max_tx_rate);

commit 9c8bca2637b8441a18bb846a1d74293cb990724e
Author: Jiri Pirko <jiri@mellanox.com>
Date:   Tue Jun 4 15:40:38 2019 +0200

    mlx5: Move firmware flash implementation to devlink
    
    Benefit from the devlink flash update implementation and ethtool
    fallback to it and move firmware flash implementation there.
    
    Signed-off-by: Jiri Pirko <jiri@mellanox.com>
    Acked-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 3a183d690e23..4e417dfe4ee5 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -1074,8 +1074,6 @@ u32 mlx5e_ethtool_get_rxfh_key_size(struct mlx5e_priv *priv);
 u32 mlx5e_ethtool_get_rxfh_indir_size(struct mlx5e_priv *priv);
 int mlx5e_ethtool_get_ts_info(struct mlx5e_priv *priv,
 			      struct ethtool_ts_info *info);
-int mlx5e_ethtool_flash_device(struct mlx5e_priv *priv,
-			       struct ethtool_flash *flash);
 void mlx5e_ethtool_get_pauseparam(struct mlx5e_priv *priv,
 				  struct ethtool_pauseparam *pauseparam);
 int mlx5e_ethtool_set_pauseparam(struct mlx5e_priv *priv,

commit 69dad68d1bcf26dde3cc4b08b08c2260ae575ab6
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Sun Jan 20 11:04:34 2019 +0200

    net/mlx5e: Turn on HW tunnel offload in all TIRs
    
    Hardware requires that all TIRs that steer traffic to the same RQ
    should share identical tunneled_offload_en value.
    For that, the tunneled_offload_en bit should be set/unset (according to
    the HW capability) for all TIRs', not only the ones dedicated for
    tunneled (inner) traffic.
    
    Fixes: 1b223dd39162 ("net/mlx5e: Fix checksum handling for non-stripped vlan packets")
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 7e0c3d4de108..3a183d690e23 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -240,6 +240,7 @@ struct mlx5e_params {
 	bool rx_cqe_compress_def;
 	struct net_dim_cq_moder rx_cq_moderation;
 	struct net_dim_cq_moder tx_cq_moderation;
+	bool tunneled_offload_en;
 	bool lro_en;
 	u8  tx_min_inline_mode;
 	bool vlan_strip_disable;

commit 63d26b490b56fe9e1765dc2413d2188cec7f6bf7
Author: Maxim Mikityanskiy <maximmi@mellanox.com>
Date:   Fri Mar 1 12:05:21 2019 +0200

    net/mlx5e: Take HW interrupt trigger into a function
    
    mlx5e_trigger_irq posts a NOP to the ICO SQ just to trigger an IRQ and
    enter the NAPI poll on the right CPU according to the affinity. Use it
    in mlx5e_activate_rq.
    
    Signed-off-by: Maxim Mikityanskiy <maximmi@mellanox.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index c190061763fd..7e0c3d4de108 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -778,6 +778,7 @@ netdev_tx_t mlx5e_xmit(struct sk_buff *skb, struct net_device *dev);
 netdev_tx_t mlx5e_sq_xmit(struct mlx5e_txqsq *sq, struct sk_buff *skb,
 			  struct mlx5e_tx_wqe *wqe, u16 pi, bool xmit_more);
 
+void mlx5e_trigger_irq(struct mlx5e_icosq *sq);
 void mlx5e_completion_event(struct mlx5_core_cq *mcq);
 void mlx5e_cq_error_event(struct mlx5_core_cq *mcq, enum mlx5_event event);
 int mlx5e_napi_poll(struct napi_struct *napi, int budget);

commit c2273219baa5097a4d7c1c162b992623534f34c1
Author: Shay Agroskin <shayag@mellanox.com>
Date:   Thu Mar 14 14:54:07 2019 +0200

    net/mlx5e: XDP, Inline small packets into the TX MPWQE in XDP xmit flow
    
    Upon high packet rate with multiple CPUs TX workloads, much of the HCA's
    resources are spent on prefetching TX descriptors, thus affecting
    transmission rates.
    This patch comes to mitigate this problem by moving some workload to the
    CPU and reducing the HW data prefetch overhead for small packets (<= 256B).
    
    When forwarding packets with XDP, a packet that is smaller
    than a certain size (set to ~256 bytes) would be sent inline within
    its WQE TX descrptor (mem-copied), when the hardware tx queue is congested
    beyond a pre-defined water-mark.
    
    This is added to better utilize the HW resources (which now makes
    one less packet data prefetch) and allow better scalability, on the
    account of CPU usage (which now 'memcpy's the packet into the WQE).
    
    To load balance between HW and CPU and get max packet rate, we use
    watermarks to detect how much the HW is congested and move the work
    loads back and forth between HW and CPU.
    
    Performance:
    Tested packet rate for UDP 64Byte multi-stream
    over two dual port ConnectX-5 100Gbps NICs.
    CPU: Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz
    
    * Tested with hyper-threading disabled
    
    XDP_TX:
    
    |          | before | after   |       |
    | 24 rings | 51Mpps | 116Mpps | +126% |
    | 1 ring   | 12Mpps | 12Mpps  | same  |
    
    XDP_REDIRECT:
    
    ** Below is the transmit rate, not the redirection rate
    which might be larger, and is not affected by this patch.
    
    |          | before  | after   |      |
    | 32 rings | 64Mpps  | 92Mpps  | +43% |
    | 1 ring   | 6.4Mpps | 6.4Mpps | same |
    
    As we can see, feature significantly improves scaling, without
    hurting single ring performance.
    
    Signed-off-by: Shay Agroskin <shayag@mellanox.com>
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index a3700c57b073..c190061763fd 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -409,14 +409,17 @@ struct mlx5e_xdp_info_fifo {
 
 struct mlx5e_xdp_wqe_info {
 	u8 num_wqebbs;
-	u8 num_ds;
+	u8 num_pkts;
 };
 
 struct mlx5e_xdp_mpwqe {
 	/* Current MPWQE session */
 	struct mlx5e_tx_wqe *wqe;
 	u8                   ds_count;
+	u8                   pkt_count;
 	u8                   max_ds_count;
+	u8                   complete;
+	u8                   inline_on;
 };
 
 struct mlx5e_xdpsq;

commit 15143bf51c57a8868d2c4600d99ddaae7aa3d917
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Sun Mar 10 15:35:58 2019 +0200

    net/mlx5e: XDP, Enhance RQ indication for XDP redirect flush
    
    The XDP redirect flush indication belongs to the receive queue,
    not to its XDP send queue.
    
    For this, use a new bit on rq->flags.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Reviewed-by: Shay Agroskin <shayag@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 8b37264ba107..a3700c57b073 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -428,7 +428,6 @@ struct mlx5e_xdpsq {
 	/* dirtied @completion */
 	u32                        xdpi_fifo_cc;
 	u16                        cc;
-	bool                       redirect_flush;
 
 	/* dirtied @xmit */
 	u32                        xdpi_fifo_pc ____cacheline_aligned_in_smp;
@@ -532,6 +531,7 @@ typedef void (*mlx5e_fp_dealloc_wqe)(struct mlx5e_rq*, u16);
 
 enum mlx5e_rq_flag {
 	MLX5E_RQ_FLAG_XDP_XMIT,
+	MLX5E_RQ_FLAG_XDP_REDIRECT,
 };
 
 struct mlx5e_rq_frag_info {

commit f03590f74cc280ce5bcfe7aa0729691df79f7718
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Sun Mar 10 15:29:13 2019 +0200

    net/mlx5e: XDP, Fix shifted flag index in RQ bitmap
    
    Values in enum mlx5e_rq_flag are used as bit indixes.
    Intention was to use them with no BIT(i) wrapping.
    
    No functional bug fix here, as the same (shifted)flag bit
    is used for all set, test, and clear operations.
    
    Fixes: 121e89275471 ("net/mlx5e: Refactor RQ XDP_TX indication")
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Reviewed-by: Shay Agroskin <shayag@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index abd2c67fe419..8b37264ba107 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -531,7 +531,7 @@ typedef bool (*mlx5e_fp_post_rx_wqes)(struct mlx5e_rq *rq);
 typedef void (*mlx5e_fp_dealloc_wqe)(struct mlx5e_rq*, u16);
 
 enum mlx5e_rq_flag {
-	MLX5E_RQ_FLAG_XDP_XMIT = BIT(0),
+	MLX5E_RQ_FLAG_XDP_XMIT,
 };
 
 struct mlx5e_rq_frag_info {

commit fd9b4be8002cae5f854c7acd9e86d31780ebaf16
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Wed Feb 27 12:06:08 2019 +0200

    net/mlx5e: RX, Support multiple outstanding UMR posts
    
    The buffers mapping of the Multi-Packet WQEs (of Striding RQ)
    is done via UMR posts, one UMR WQE per an RX MPWQE.
    
    A single MPWQE is capable of serving many incoming packets,
    usually larger than the budget of a single napi cycle.
    Hence, posting a single UMR WQE per napi cycle (and handling its
    completion in the next cycle) works fine in many common cases,
    but not always.
    
    When an XDP program is loaded, every MPWQE is capable of serving less
    packets, to satisfy the packet-per-page requirement.
    Thus, for the same number of packets more MPWQEs (and UMR posts)
    are needed (twice as much for the default MTU), giving less latency
    room for the UMR completions.
    
    In this patch, we add support for multiple outstanding UMR posts,
    to allow faster gap closure between consuming MPWQEs and reposting
    them back into the WQ.
    
    For better SW and HW locality, we combine the UMR posts in bulks of
    (at least) two.
    
    This is expected to improve packet rate in high CPU scale.
    
    Performance test:
    As expected, huge improvement in large-scale (48 cores).
    
    xdp_redirect_map, 64B UDP multi-stream.
    Redirect from ConnectX-5 100Gbps to ConnectX-6 100Gbps.
    CPU: Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz.
    
    Before: Unstable, 7 to 30 Mpps
    After:  Stable,   at 70.5 Mpps
    
    No degradation in other tested scenarios.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 51e109fdeec1..abd2c67fe419 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -461,10 +461,10 @@ struct mlx5e_xdpsq {
 
 struct mlx5e_icosq {
 	/* data path */
+	u16                        cc;
+	u16                        pc;
 
-	/* dirtied @xmit */
-	u16                        pc ____cacheline_aligned_in_smp;
-
+	struct mlx5_wqe_ctrl_seg  *doorbell_cseg;
 	struct mlx5e_cq            cq;
 
 	/* write@xmit, read@completion */
@@ -562,8 +562,10 @@ struct mlx5e_rq {
 			struct mlx5e_mpw_info *info;
 			mlx5e_fp_skb_from_cqe_mpwrq skb_from_cqe_mpwrq;
 			u16                    num_strides;
+			u16                    actual_wq_head;
 			u8                     log_stride_sz;
-			bool                   umr_in_progress;
+			u8                     umr_in_progress;
+			u8                     umr_last_bulk;
 		} mpwqe;
 	};
 	struct {

commit 6b0a7f84ea1fe248df96ccc4dd86e817e32ef65b
Merge: cea0aa9cbd5a fe5cdef29e41
Author: David S. Miller <davem@davemloft.net>
Date:   Wed Apr 17 11:26:25 2019 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Conflict resolution of af_smc.c from Stephen Rothwell.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 484c1ada0bd2bdcb76f849ae77983e24320a2d1d
Author: Eran Ben Elisha <eranbe@mellanox.com>
Date:   Thu Mar 28 14:26:47 2019 +0200

    net/mlx5e: Use fail-safe channels reopen in tx reporter recover
    
    When requested to recover from error, the tx reporter might open new
    channels and close the existing ones. Use safe channels switch flow in
    order to guarantee opened channels at the end of the recover flow.
    For this purpose, define mlx5e_safe_reopen_channels function and use it
    within those flows.
    
    Fixes: de8650a82071 ("net/mlx5e: Add tx reporter support")
    Signed-off-by: Eran Ben Elisha <eranbe@mellanox.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 71c65cc17904..d3eaf2ceaa39 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -858,6 +858,7 @@ void mlx5e_close_channels(struct mlx5e_channels *chs);
  * switching channels
  */
 typedef int (*mlx5e_fp_hw_modify)(struct mlx5e_priv *priv);
+int mlx5e_safe_reopen_channels(struct mlx5e_priv *priv);
 int mlx5e_safe_switch_channels(struct mlx5e_priv *priv,
 			       struct mlx5e_channels *new_chs,
 			       mlx5e_fp_hw_modify hw_modify);

commit 6d7ee2edaa54d676026b7f0d50ae472e62a84564
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Tue Jan 22 13:42:10 2019 +0200

    net/mlx5e: Unify logic of MTU boundaries
    
    Expose a new helper that wraps the logic for setting the
    netdevice's MTU boundaries.
    Use it for the different components (Eth, rep, IPoIB).
    
    Set the netdevice min MTU to ETH_MIN_MTU, and the max according
    to both the FW capability and the kernel definition.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 7ce5edeaba33..bd855ab8dfe2 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -1086,6 +1086,7 @@ mlx5e_create_netdev(struct mlx5_core_dev *mdev, const struct mlx5e_profile *prof
 int mlx5e_attach_netdev(struct mlx5e_priv *priv);
 void mlx5e_detach_netdev(struct mlx5e_priv *priv);
 void mlx5e_destroy_netdev(struct mlx5e_priv *priv);
+void mlx5e_set_netdev_mtu_boundaries(struct mlx5e_priv *priv);
 void mlx5e_build_nic_params(struct mlx5_core_dev *mdev,
 			    struct mlx5e_rss_params *rss_params,
 			    struct mlx5e_params *params,

commit 0b77f2305f38d63fbf0c501adfd342848d695718
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Thu Jan 17 15:58:09 2019 +0200

    net/mlx5e: Obsolete param field holding a constant value
    
    The LRO WQE size is a constant, obsolete the parameter field
    that holds it.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Reviewed-by: Maxim Mikityanskiy <maximmi@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index b0b68dde3017..7ce5edeaba33 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -241,7 +241,6 @@ struct mlx5e_params {
 	struct net_dim_cq_moder rx_cq_moderation;
 	struct net_dim_cq_moder tx_cq_moderation;
 	bool lro_en;
-	u32 lro_wqe_sz;
 	u8  tx_min_inline_mode;
 	bool vlan_strip_disable;
 	bool scatter_fcs_en;

commit b6460c72c36df973b4935492bacd90b2bbff7028
Merge: 3eed52842b9f aef6c443fe84
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Tue Apr 2 15:43:45 2019 -0700

    Merge branch 'mlx5-next' of git://git.kernel.org/pub/scm/linux/kernel/git/mellanox/linux
    
    This merge commit includes some misc shared code updates from mlx5-next branch needed
    for net-next.
    
    1) From Maxim, Remove un-used macros and spinlock from mlx5 code.
    
    2) From Aya, Expose Management PCIE info register layout and add rate limit
    print macros.
    
    3) From Tariq, Compilation warning fix in fs_core.c
    
    4) From Vu, Huy and Saeed, Improve mlx5 initialization flow:
    The goal is to provide a better logical separation of mlx5 core
    device initialization flow and will help to seamlessly support
    creating different mlx5 device types such as PF, VF and SF
    mlx5 sub-function virtual devices.
    
    Mlx5_core driver needs to separate HCA resources from pci resources.
    Its initialize/load/unload will be broken into stages:
    1. Initialize common data structures
    2. Setup function which initializes pci resources (for PF/VF)
       or some other specific resources for virtual device
    3. Initialize software objects according to hardware capabilities
    4. Load all mlx5_core components
    
    It is also necessary to detach mlx5_core mdev name/message from pci
    device mdev->pdev name/message for a clearer report/debug of
    different mlx5 device types.
    
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

commit bbf29f618e8c5bfd6efdad5fdc050a84bab795ab
Author: Maxim Mikityanskiy <maximmi@mellanox.com>
Date:   Fri Mar 29 15:37:52 2019 -0700

    net/mlx5: Remove spinlock support from mlx5_write64
    
    As there is no user of mlx5_write64 that passes a spinlock to
    mlx5_write64, remove this functionality and simplify the function.
    
    Signed-off-by: Maxim Mikityanskiy <maximmi@mellanox.com>
    Reviewed-by: Eran Ben Elisha <eranbe@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 8fa8fdd30b85..2623d3fb6b96 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -916,7 +916,7 @@ void mlx5e_notify_hw(struct mlx5_wq_cyc *wq, u16 pc,
 	 */
 	wmb();
 
-	mlx5_write64((__be32 *)ctrl, uar_map, NULL);
+	mlx5_write64((__be32 *)ctrl, uar_map);
 }
 
 static inline void mlx5e_cq_arm(struct mlx5e_cq *cq)

commit 3c31ff22b25f15c6a642bb775884a599379a3cb5
Author: Florian Westphal <fw@strlen.de>
Date:   Mon Apr 1 16:42:15 2019 +0200

    drivers: mellanox: use netdev_xmit_more() helper
    
    skb->xmit_more hint is now always 0. This switches the mellanox drivers
    to the netdev_xmit_more() helper.
    
    Cc: Saeed Mahameed <saeedm@mellanox.com>
    Cc: Leon Romanovsky <leon@kernel.org>
    Cc: Boris Pismenny <borisp@mellanox.com>
    Cc: Ilya Lesokhin <ilyal@mellanox.com>
    Cc: Eran Ben Elisha <eranbe@mellanox.com>
    Signed-off-by: Florian Westphal <fw@strlen.de>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 9e71cf03369c..3e1ea8b42c77 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -772,7 +772,7 @@ u16 mlx5e_select_queue(struct net_device *dev, struct sk_buff *skb,
 		       struct net_device *sb_dev);
 netdev_tx_t mlx5e_xmit(struct sk_buff *skb, struct net_device *dev);
 netdev_tx_t mlx5e_sq_xmit(struct mlx5e_txqsq *sq, struct sk_buff *skb,
-			  struct mlx5e_tx_wqe *wqe, u16 pi);
+			  struct mlx5e_tx_wqe *wqe, u16 pi, bool xmit_more);
 
 void mlx5e_completion_event(struct mlx5_core_cq *mcq);
 void mlx5e_cq_error_event(struct mlx5_core_cq *mcq, enum mlx5_event event);

commit e3cfc7e6b7bd59e4ce244deedbad4d1b7a609415
Author: Moshe Shemesh <moshe@mellanox.com>
Date:   Thu Mar 21 15:51:38 2019 -0700

    net/mlx5e: TX, Add geneve tunnel stateless offload support
    
    Currently support only default geneve udp port (6081).
    For the tx side, the HW is assisted by SW parsing, which sets the
    headers offset to offload tunneled LSO and csum. Note that for udp
    tunnels, we don't use special rx offloads, as rss on the outer headers
    is enough, we support checksum complete and GRO takes care of
    aggregation.
    
    Geneve TSO BW and CPU load results (tested using iperf single tcp
    stream).
    In this patch we add TSO support over Geneve, so the "before" result
    doesn't actually get to using the TSO HW offload even when turned on.
    Tested on ConnectX-5, Intel(R) Xeon(R) CPU E5-2660 v2 @2.20GHz.
    
     __________________________________
    | Before         | After           |
    |________________|_________________|
    | 12.6 Gbits/sec | 21.7 Gbits/sec  |
    | 100% CPU load  | 61.5% CPU load  |
    |________________|_________________|
    
    Signed-off-by: Moshe Shemesh <moshe@mellanox.com>
    Acked-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 16a64485a13b..9e71cf03369c 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -884,6 +884,12 @@ static inline bool mlx5e_tunnel_inner_ft_supported(struct mlx5_core_dev *mdev)
 		MLX5_CAP_FLOWTABLE_NIC_RX(mdev, ft_field_support.inner_ip_version));
 }
 
+static inline bool mlx5_tx_swp_supported(struct mlx5_core_dev *mdev)
+{
+	return MLX5_CAP_ETH(mdev, swp) &&
+		MLX5_CAP_ETH(mdev, swp_csum) && MLX5_CAP_ETH(mdev, swp_lso);
+}
+
 struct mlx5e_swp_spec {
 	__be16 l3_proto;
 	u8 l4_proto;

commit cac018b8c7ee8510b00d0d2ffdaef5c38c34129b
Author: Moshe Shemesh <moshe@mellanox.com>
Date:   Thu Mar 21 15:51:37 2019 -0700

    net/mlx5e: Take SW parser code to a separate function
    
    Refactor mlx5e_ipsec_set_swp() code, split the part which sets the eseg
    software parser (SWP) offsets and flags, so it can be used in a
    downstream patch by other mlx5e functionality which needs to set eseg
    SWP.
    The new function mlx5e_set_eseg_swp() is useful for setting swp for both
    outer and inner headers. It also handles the special ipsec case of xfrm
    mode transfer.
    
    Signed-off-by: Moshe Shemesh <moshe@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 5c2e3276d9cc..16a64485a13b 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -884,6 +884,47 @@ static inline bool mlx5e_tunnel_inner_ft_supported(struct mlx5_core_dev *mdev)
 		MLX5_CAP_FLOWTABLE_NIC_RX(mdev, ft_field_support.inner_ip_version));
 }
 
+struct mlx5e_swp_spec {
+	__be16 l3_proto;
+	u8 l4_proto;
+	u8 is_tun;
+	__be16 tun_l3_proto;
+	u8 tun_l4_proto;
+};
+
+static inline void
+mlx5e_set_eseg_swp(struct sk_buff *skb, struct mlx5_wqe_eth_seg *eseg,
+		   struct mlx5e_swp_spec *swp_spec)
+{
+	/* SWP offsets are in 2-bytes words */
+	eseg->swp_outer_l3_offset = skb_network_offset(skb) / 2;
+	if (swp_spec->l3_proto == htons(ETH_P_IPV6))
+		eseg->swp_flags |= MLX5_ETH_WQE_SWP_OUTER_L3_IPV6;
+	if (swp_spec->l4_proto) {
+		eseg->swp_outer_l4_offset = skb_transport_offset(skb) / 2;
+		if (swp_spec->l4_proto == IPPROTO_UDP)
+			eseg->swp_flags |= MLX5_ETH_WQE_SWP_OUTER_L4_UDP;
+	}
+
+	if (swp_spec->is_tun) {
+		eseg->swp_inner_l3_offset = skb_inner_network_offset(skb) / 2;
+		if (swp_spec->tun_l3_proto == htons(ETH_P_IPV6))
+			eseg->swp_flags |= MLX5_ETH_WQE_SWP_INNER_L3_IPV6;
+	} else { /* typically for ipsec when xfrm mode != XFRM_MODE_TUNNEL */
+		eseg->swp_inner_l3_offset = skb_network_offset(skb) / 2;
+		if (swp_spec->l3_proto == htons(ETH_P_IPV6))
+			eseg->swp_flags |= MLX5_ETH_WQE_SWP_INNER_L3_IPV6;
+	}
+	switch (swp_spec->tun_l4_proto) {
+	case IPPROTO_UDP:
+		eseg->swp_flags |= MLX5_ETH_WQE_SWP_INNER_L4_UDP;
+		/* fall through */
+	case IPPROTO_TCP:
+		eseg->swp_inner_l4_offset = skb_inner_transport_offset(skb) / 2;
+		break;
+	}
+}
+
 static inline void mlx5e_sq_fetch_wqe(struct mlx5e_txqsq *sq,
 				      struct mlx5e_tx_wqe **wqe,
 				      u16 *pi)

commit a350eccee5830d9a1f29e393a88dc05a15326d44
Author: Paolo Abeni <pabeni@redhat.com>
Date:   Wed Mar 20 11:02:06 2019 +0100

    net: remove 'fallback' argument from dev->ndo_select_queue()
    
    After the previous patch, all the callers of ndo_select_queue()
    provide as a 'fallback' argument netdev_pick_tx.
    The only exceptions are nested calls to ndo_select_queue(),
    which pass down the 'fallback' available in the current scope
    - still netdev_pick_tx.
    
    We can drop such argument and replace fallback() invocation with
    netdev_pick_tx(). This avoids an indirect call per xmit packet
    in some scenarios (TCP syn, UDP unconnected, XDP generic, pktgen)
    with device drivers implementing such ndo. It also clean the code
    a bit.
    
    Tested with ixgbe and CONFIG_FCOE=m
    
    With pktgen using queue xmit:
    threads         vanilla         patched
                    (kpps)          (kpps)
    1               2334            2428
    2               4166            4278
    4               7895            8100
    
     v1 -> v2:
     - rebased after helper's name change
    
    Signed-off-by: Paolo Abeni <pabeni@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 71c65cc17904..5c2e3276d9cc 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -769,8 +769,7 @@ struct mlx5e_profile {
 void mlx5e_build_ptys2ethtool_map(void);
 
 u16 mlx5e_select_queue(struct net_device *dev, struct sk_buff *skb,
-		       struct net_device *sb_dev,
-		       select_queue_fallback_t fallback);
+		       struct net_device *sb_dev);
 netdev_tx_t mlx5e_xmit(struct sk_buff *skb, struct net_device *dev);
 netdev_tx_t mlx5e_sq_xmit(struct mlx5e_txqsq *sq, struct sk_buff *skb,
 			  struct mlx5e_tx_wqe *wqe, u16 pi);

commit 877662e27276f345ea07e49f153aa88e9a1e313c
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Mon Nov 26 17:22:16 2018 +0200

    net/mlx5e: Wrap the open and apply of channels in one fail-safe function
    
    Take into a function the common code structure of opening
    a side set of channels followed by a call to apply them.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index e9acfa9aa069..71c65cc17904 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -858,9 +858,9 @@ void mlx5e_close_channels(struct mlx5e_channels *chs);
  * switching channels
  */
 typedef int (*mlx5e_fp_hw_modify)(struct mlx5e_priv *priv);
-void mlx5e_switch_priv_channels(struct mlx5e_priv *priv,
-				struct mlx5e_channels *new_chs,
-				mlx5e_fp_hw_modify hw_modify);
+int mlx5e_safe_switch_channels(struct mlx5e_priv *priv,
+			       struct mlx5e_channels *new_chs,
+			       mlx5e_fp_hw_modify hw_modify);
 void mlx5e_activate_priv_channels(struct mlx5e_priv *priv);
 void mlx5e_deactivate_priv_channels(struct mlx5e_priv *priv);
 

commit 3313da8188cc346a205783c22c37e821b4b7016d
Merge: 50f444aa50a4 24f0a48743a2
Author: David S. Miller <davem@davemloft.net>
Date:   Fri Feb 15 12:38:38 2019 -0800

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    The netfilter conflicts were rather simple overlapping
    changes.
    
    However, the cls_tcindex.c stuff was a bit more complex.
    
    On the 'net' side, Cong is fixing several races and memory
    leaks.  Whilst on the 'net-next' side we have Vlad adding
    the rtnl-ness support.
    
    What I've decided to do, in order to resolve this, is revert the
    conversion over to using a workqueue that Cong did, bringing us back
    to pure RCU.  I did it this way because I believe that either Cong's
    races don't apply with have Vlad did things, or Cong will have to
    implement the race fix slightly differently.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 407e17b1a69a51ba9a512a04342da56c1f931df4
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Mon Feb 11 16:27:02 2019 -0800

    net/mlx5e: XDP, fix redirect resources availability check
    
    Currently mlx5 driver creates xdp redirect hw queues unconditionally on
    netdevice open, This is great until someone starts redirecting XDP traffic
    via ndo_xdp_xmit on mlx5 device and changes the device configuration at
    the same time, this might cause crashes, since the other device's napi
    is not aware of the mlx5 state change (resources un-availability).
    
    To fix this we must synchronize with other devices napi's on the system.
    Added a new flag under mlx5e_priv to determine XDP TX resources are
    available, set/clear it up when necessary and use synchronize_rcu()
    when the flag is turned off, so other napi's are in-sync with it, before
    we actually cleanup the hw resources.
    
    The flag is tested prior to committing to transmit on mlx5e_xdp_xmit, and
    it is sufficient to determine if it safe to transmit or not. The other
    two internal flags (MLX5E_STATE_OPENED and MLX5E_SQ_STATE_ENABLED) become
    unnecessary. Thus, they are removed from data path.
    
    Fixes: 58b99ee3e3eb ("net/mlx5e: Add support for XDP_REDIRECT in device-out side")
    Reported-by: Toke Høiland-Jørgensen <toke@redhat.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 8fa8fdd30b85..448a92561567 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -657,6 +657,7 @@ struct mlx5e_channel_stats {
 enum {
 	MLX5E_STATE_OPENED,
 	MLX5E_STATE_DESTROYING,
+	MLX5E_STATE_XDP_TX_ENABLED,
 };
 
 struct mlx5e_rqt {

commit de8650a820715ea619b687b4a315794593be1d59
Author: Eran Ben Elisha <eranbe@mellanox.com>
Date:   Thu Feb 7 11:36:40 2019 +0200

    net/mlx5e: Add tx reporter support
    
    Add mlx5e tx reporter to devlink health reporters. This reporter will be
    responsible for diagnosing, reporting and recovering of tx errors.
    This patch declares the TX reporter operations and creates it using the
    devlink health API. Currently, this reporter supports reporting and
    recovering from send error CQE only. In addition, it adds diagnose
    information for the open SQs.
    
    For a local SQ recover (due to driver error report), in case of SQ recover
    failure, the recover operation will be considered as a failure.
    For a full tx recover, an attempt to close and open the channels will be
    done. If this one passed successfully, it will be considered as a
    successful recover.
    
    The SQ recover from error CQE flow is not a new feature in the driver,
    this patch re-organize the functions and adapt them for the devlink
    health API. For this purpose, move code from en_main.c to a new file
    named reporter_tx.c.
    
    Diagnose output:
    $devlink health diagnose pci/0000:00:09.0 reporter tx -j -p
    {
        "SQs": [ {
                "sqn": 138,
                "HW state": 1,
                "stopped": false
            },{
                "sqn": 142,
                "HW state": 1,
                "stopped": false
            } ]
    }
    
    $devlink health diagnose pci/0000:00:09.0 reporter tx
    SQs:
      sqn: 138 HW state: 1 stopped: false
      sqn: 142 HW state: 1 stopped: false
    
    Signed-off-by: Eran Ben Elisha <eranbe@mellanox.com>
    Reviewed-by: Moshe Shemesh <moshe@mellanox.com>
    Acked-by: Saeed Mahameed <saeedm@mellanox.com>
    Acked-by: Jiri Pirko <jiri@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 6dd74ef69389..66e510b16243 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -387,10 +387,7 @@ struct mlx5e_txqsq {
 	struct mlx5e_channel      *channel;
 	int                        txq_ix;
 	u32                        rate_limit;
-	struct mlx5e_txqsq_recover {
-		struct work_struct         recover_work;
-		u64                        last_recover;
-	} recover;
+	struct work_struct         recover_work;
 } ____cacheline_aligned_in_smp;
 
 struct mlx5e_dma_info {
@@ -683,6 +680,13 @@ struct mlx5e_rss_params {
 	u8	hfunc;
 };
 
+struct mlx5e_modify_sq_param {
+	int curr_state;
+	int next_state;
+	int rl_update;
+	int rl_index;
+};
+
 struct mlx5e_priv {
 	/* priv data path fields - start */
 	struct mlx5e_txqsq *txq2sq[MLX5E_MAX_NUM_CHANNELS * MLX5E_MAX_NUM_TC];
@@ -738,6 +742,7 @@ struct mlx5e_priv {
 #ifdef CONFIG_MLX5_EN_TLS
 	struct mlx5e_tls          *tls;
 #endif
+	struct devlink_health_reporter *tx_reporter;
 };
 
 struct mlx5e_profile {
@@ -868,6 +873,11 @@ void mlx5e_set_rq_type(struct mlx5_core_dev *mdev, struct mlx5e_params *params);
 void mlx5e_init_rq_type_params(struct mlx5_core_dev *mdev,
 			       struct mlx5e_params *params);
 
+int mlx5e_modify_sq(struct mlx5_core_dev *mdev, u32 sqn,
+		    struct mlx5e_modify_sq_param *p);
+void mlx5e_activate_txqsq(struct mlx5e_txqsq *sq);
+void mlx5e_tx_disable_queue(struct netdev_queue *txq);
+
 static inline bool mlx5e_tunnel_inner_ft_supported(struct mlx5_core_dev *mdev)
 {
 	return (MLX5_CAP_ETH(mdev, tunnel_stateless_gre) &&

commit b832d4fdf105b5464d786e321e3c9e012e67cdfb
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Wed Dec 12 01:42:30 2018 -0800

    net/mlx5e: Reuse fold sw stats in representors
    
    Representors software stats are basic, this patch is reusing the
    mlx5e_fold_sw_stats in representors, which sums up the basic stats64 for a
    mlx5e netdevice.
    
    Fixes: 8bfaf07f7806 ("net/mlx5e: Present SW stats when state is not opened")
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 3e35a90cf21a..6dd74ef69389 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -804,6 +804,7 @@ mlx5e_skb_from_cqe_nonlinear(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe,
 
 void mlx5e_update_stats(struct mlx5e_priv *priv);
 void mlx5e_get_stats(struct net_device *dev, struct rtnl_link_stats64 *stats);
+void mlx5e_fold_sw_stats64(struct mlx5e_priv *priv, struct rtnl_link_stats64 *s);
 
 void mlx5e_init_l2_addr(struct mlx5e_priv *priv);
 int mlx5e_self_test_num(struct mlx5e_priv *priv);

commit 149e566fef8120a00c49f24e8f570186200e1e81
Author: Moshe Shemesh <moshe@mellanox.com>
Date:   Thu Oct 25 15:03:38 2018 +0300

    net/mlx5e: Expand XPS cpumask to cover all online cpus
    
    Currently we have one cpu in XPS cpumask per tx queue, this is good
    enough for default configuration where there is a tx queue per cpu.
    However, once configuration changes to use less tx queues, part of the
    cpus are not XPS-mapped and so the select queue decision falls back to
    hash calculation and balancing is not guaranteed.
    
    Expand XPS cpumask to enable using all cpus even when number of tx
    queues is smaller than number of cpus.
    
    Signed-off-by: Moshe Shemesh <moshe@mellanox.com>
    Reviewed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 587f94d68dc9..3e35a90cf21a 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -638,6 +638,7 @@ struct mlx5e_channel {
 	struct hwtstamp_config    *tstamp;
 	int                        ix;
 	int                        cpu;
+	cpumask_var_t              xps_cpumask;
 };
 
 struct mlx5e_channels {

commit 79d356ef2c10b19fdd3269b975579aa0be21710c
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Wed Sep 12 18:32:49 2018 +0300

    net/mlx5e: Take CQ decompress fields into a separate structure
    
    Only the Receive CQ makes use of these fields. Take them
    out into a separate struct and save space in the generic
    CQ structure.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 6def2c972bf0..587f94d68dc9 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -306,16 +306,18 @@ struct mlx5e_cq {
 	struct mlx5_core_cq        mcq;
 	struct mlx5e_channel      *channel;
 
+	/* control */
+	struct mlx5_core_dev      *mdev;
+	struct mlx5_wq_ctrl        wq_ctrl;
+} ____cacheline_aligned_in_smp;
+
+struct mlx5e_cq_decomp {
 	/* cqe decompression */
 	struct mlx5_cqe64          title;
 	struct mlx5_mini_cqe8      mini_arr[MLX5_MINI_CQE_ARRAY_SIZE];
 	u8                         mini_arr_idx;
-	u16                        decmprs_left;
-	u16                        decmprs_wqe_counter;
-
-	/* control */
-	struct mlx5_core_dev      *mdev;
-	struct mlx5_wq_ctrl        wq_ctrl;
+	u16                        left;
+	u16                        wqe_counter;
 } ____cacheline_aligned_in_smp;
 
 struct mlx5e_tx_wqe_info {
@@ -578,6 +580,7 @@ struct mlx5e_rq {
 	struct net_device     *netdev;
 	struct mlx5e_rq_stats *stats;
 	struct mlx5e_cq        cq;
+	struct mlx5e_cq_decomp cqd;
 	struct mlx5e_page_cache page_cache;
 	struct hwtstamp_config *tstamp;
 	struct mlx5_clock      *clock;

commit 94816278383eab18c660652bfb7bc929c0d7aef1
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Tue Sep 25 12:05:22 2018 +0300

    net/mlx5e: RX, Make sure packet header does not cross page boundary
    
    In the non-linear SKB memory scheme of Striding RQ, a packet header
    could cross page boundary. This requires special care in fast path
    that costs LoC, additional runtime instructions and branches.
    
    It could happen when the header (up to 256B) does not fit in
    a single stride. Avoid this by working with a stride size that fits
    the maximum possible header. Stride is increased form 64B to 256B.
    
    Performance:
    Tested packet rate for UDP streams, single ring, on ConnectX-5.
    
    Configuration:
    Set Striding RQ and LRO ON (to enabled the non-linear SKB scheme).
    GRO OFF, early drop by TC rule.
    
    64B: 4x worse memory utilization, no page-crossers headers
    - No degradation (5,887,305 pps).
    - The reduction in memory utilization is compensated by the saving of
      branches tests.
    
    192B: 1.33x worse memory utilization, avoid page-crossers headers
    - Before: 5,727,252. After: 5,777,037. ~1% gain.
    
    256B: Same memory util, no page-crossers
    - Before: 5,691,885. After: 5,748,007. ~1% gain.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 8fa8fdd30b85..6def2c972bf0 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -76,15 +76,14 @@ struct page_pool;
 #define MLX5_SKB_FRAG_SZ(len)	(SKB_DATA_ALIGN(len) +	\
 				 SKB_DATA_ALIGN(sizeof(struct skb_shared_info)))
 
+#define MLX5E_RX_MAX_HEAD (256)
+
 #define MLX5_MPWRQ_MIN_LOG_STRIDE_SZ(mdev) \
 	(6 + MLX5_CAP_GEN(mdev, cache_line_128byte)) /* HW restriction */
 #define MLX5_MPWRQ_LOG_STRIDE_SZ(mdev, req) \
 	max_t(u32, MLX5_MPWRQ_MIN_LOG_STRIDE_SZ(mdev), req)
-#define MLX5_MPWRQ_DEF_LOG_STRIDE_SZ(mdev)       MLX5_MPWRQ_LOG_STRIDE_SZ(mdev, 6)
-#define MLX5_MPWRQ_CQE_CMPRS_LOG_STRIDE_SZ(mdev) MLX5_MPWRQ_LOG_STRIDE_SZ(mdev, 8)
-#define MLX5E_MPWQE_STRIDE_SZ(mdev, cqe_cmprs) \
-	(cqe_cmprs ? MLX5_MPWRQ_CQE_CMPRS_LOG_STRIDE_SZ(mdev) : \
-	MLX5_MPWRQ_DEF_LOG_STRIDE_SZ(mdev))
+#define MLX5_MPWRQ_DEF_LOG_STRIDE_SZ(mdev) \
+	MLX5_MPWRQ_LOG_STRIDE_SZ(mdev, order_base_2(MLX5E_RX_MAX_HEAD))
 
 #define MLX5_MPWRQ_LOG_WQE_SZ			18
 #define MLX5_MPWRQ_WQE_PAGE_ORDER  (MLX5_MPWRQ_LOG_WQE_SZ - PAGE_SHIFT > 0 ? \
@@ -119,8 +118,6 @@ struct page_pool;
 
 #define MLX5E_PARAMS_MINIMUM_LOG_RQ_SIZE_MPW            0x2
 
-#define MLX5E_RX_MAX_HEAD (256)
-
 #define MLX5E_PARAMS_DEFAULT_LRO_WQE_SZ                 (64 * 1024)
 #define MLX5E_DEFAULT_LRO_TIMEOUT                       32
 #define MLX5E_LRO_TIMEOUT_ARR_SIZE                      4

commit 30e5c2c6bf285d93dee4c45f23da95d7d50b125a
Author: David S. Miller <davem@davemloft.net>
Date:   Fri Jan 25 10:53:23 2019 -0800

    net: Revert devlink health changes.
    
    This reverts the devlink health changes from 9/17/2019,
    Jiri wants things to be designed differently and it was
    agreed that the easiest way to do this is start from the
    beginning again.
    
    Commits reverted:
    
    cb5ccfbe73b389470e1dc11061bb185ef4bc9aec
    880ee82f0313453ec5a6cb122866ac057263066b
    c7af343b4e33578b7de91786a3f639c8cfa0d97b
    ff253fedab961b22117a73ab808fcfa9e6852b50
    6f9d56132eb6d2603d4273cfc65bed914ec47acb
    fcd852c69d776c0f46c8f79e8e431e5cc6ddc7b7
    8a66704a13d9713593342e29b4f0c19762f5746b
    12bd0dcefe88782ac1c9fff632958dd1b71d27e5
    aba25279c10094c5c97d09c3491ca86d00b4ad5e
    ce019faa70f81555fa17ebc1d5a03651f2e7e15a
    b8c45a033acc607201588f7665ba84207e5149e0
    
    And the follow-on build fix:
    
    o33a0efa4baecd689da9474ce0e8b673eb6931c60
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 27e276c9bf84..8fa8fdd30b85 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -388,7 +388,10 @@ struct mlx5e_txqsq {
 	struct mlx5e_channel      *channel;
 	int                        txq_ix;
 	u32                        rate_limit;
-	struct work_struct         recover_work;
+	struct mlx5e_txqsq_recover {
+		struct work_struct         recover_work;
+		u64                        last_recover;
+	} recover;
 } ____cacheline_aligned_in_smp;
 
 struct mlx5e_dma_info {
@@ -679,13 +682,6 @@ struct mlx5e_rss_params {
 	u8	hfunc;
 };
 
-struct mlx5e_modify_sq_param {
-	int curr_state;
-	int next_state;
-	int rl_update;
-	int rl_index;
-};
-
 struct mlx5e_priv {
 	/* priv data path fields - start */
 	struct mlx5e_txqsq *txq2sq[MLX5E_MAX_NUM_CHANNELS * MLX5E_MAX_NUM_TC];
@@ -741,7 +737,6 @@ struct mlx5e_priv {
 #ifdef CONFIG_MLX5_EN_TLS
 	struct mlx5e_tls          *tls;
 #endif
-	struct devlink_health_reporter *tx_reporter;
 };
 
 struct mlx5e_profile {
@@ -871,11 +866,6 @@ void mlx5e_set_rq_type(struct mlx5_core_dev *mdev, struct mlx5e_params *params);
 void mlx5e_init_rq_type_params(struct mlx5_core_dev *mdev,
 			       struct mlx5e_params *params);
 
-int mlx5e_modify_sq(struct mlx5_core_dev *mdev, u32 sqn,
-		    struct mlx5e_modify_sq_param *p);
-void mlx5e_activate_txqsq(struct mlx5e_txqsq *sq);
-void mlx5e_tx_disable_queue(struct netdev_queue *txq);
-
 static inline bool mlx5e_tunnel_inner_ft_supported(struct mlx5_core_dev *mdev)
 {
 	return (MLX5_CAP_ETH(mdev, tunnel_stateless_gre) &&

commit aba25279c10094c5c97d09c3491ca86d00b4ad5e
Author: Eran Ben Elisha <eranbe@mellanox.com>
Date:   Thu Jan 17 23:59:18 2019 +0200

    net/mlx5e: Add TX reporter support
    
    Add mlx5e tx reporter to devlink health reporters. This reporter will be
    responsible for diagnosing, reporting and recovering of TX errors.
    This patch declares the TX reporter operations and allocate it using the
    devlink health API. Currently, this reporter supports reporting and
    recovering from send error CQE only. In addition, it adds diagnose
    information for the open SQs.
    
    For a local SQ recover (due to driver error report), in case of SQ recover
    failure, the recover operation will be considered as a failure.
    For a full TX recover, an attempt to close and open the channels will be
    done. If this one passed successfully, it will be considered as a
    successful recover.
    
    The SQ recover from error CQE flow is not a new feature in the driver,
    this patch re-organize the functions and adapt them for the devlink
    health API. For this purpose, move code from en_main.c to a new file
    named reporter_tx.c.
    
    Signed-off-by: Eran Ben Elisha <eranbe@mellanox.com>
    Reviewed-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 8fa8fdd30b85..27e276c9bf84 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -388,10 +388,7 @@ struct mlx5e_txqsq {
 	struct mlx5e_channel      *channel;
 	int                        txq_ix;
 	u32                        rate_limit;
-	struct mlx5e_txqsq_recover {
-		struct work_struct         recover_work;
-		u64                        last_recover;
-	} recover;
+	struct work_struct         recover_work;
 } ____cacheline_aligned_in_smp;
 
 struct mlx5e_dma_info {
@@ -682,6 +679,13 @@ struct mlx5e_rss_params {
 	u8	hfunc;
 };
 
+struct mlx5e_modify_sq_param {
+	int curr_state;
+	int next_state;
+	int rl_update;
+	int rl_index;
+};
+
 struct mlx5e_priv {
 	/* priv data path fields - start */
 	struct mlx5e_txqsq *txq2sq[MLX5E_MAX_NUM_CHANNELS * MLX5E_MAX_NUM_TC];
@@ -737,6 +741,7 @@ struct mlx5e_priv {
 #ifdef CONFIG_MLX5_EN_TLS
 	struct mlx5e_tls          *tls;
 #endif
+	struct devlink_health_reporter *tx_reporter;
 };
 
 struct mlx5e_profile {
@@ -866,6 +871,11 @@ void mlx5e_set_rq_type(struct mlx5_core_dev *mdev, struct mlx5e_params *params);
 void mlx5e_init_rq_type_params(struct mlx5_core_dev *mdev,
 			       struct mlx5e_params *params);
 
+int mlx5e_modify_sq(struct mlx5_core_dev *mdev, u32 sqn,
+		    struct mlx5e_modify_sq_param *p);
+void mlx5e_activate_txqsq(struct mlx5e_txqsq *sq);
+void mlx5e_tx_disable_queue(struct netdev_queue *txq);
+
 static inline bool mlx5e_tunnel_inner_ft_supported(struct mlx5_core_dev *mdev)
 {
 	return (MLX5_CAP_ETH(mdev, tunnel_stateless_gre) &&

commit 6277053afa34253eb8d8d147388e4426e615bae4
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Tue Nov 20 11:50:30 2018 +0200

    net/mlx5e: XDP, Add user control for XDP TX MPWQE feature
    
    Add ethtool private flag 'xdp_tx_mpwqe' to control the feature
    from userspace.
    Feature is set ON by default, if supported.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 8f5545d317ba..8fa8fdd30b85 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -216,6 +216,7 @@ enum mlx5e_priv_flag {
 	MLX5E_PFLAG_RX_CQE_COMPRESS,
 	MLX5E_PFLAG_RX_STRIDING_RQ,
 	MLX5E_PFLAG_RX_NO_CSUM_COMPLETE,
+	MLX5E_PFLAG_XDP_TX_MPWQE,
 	MLX5E_NUM_PFLAGS, /* Keep last */
 };
 

commit 5e0d2eef771ee78b092bf93d040eac02a0965fea
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Wed Nov 21 14:08:06 2018 +0200

    net/mlx5e: XDP, Support Enhanced Multi-Packet TX WQE
    
    Add support for the HW feature of multi-packet WQE in XDP
    xmit flow.
    
    The conventional TX descriptor (WQE, Work Queue Element) serves
    a single packet. Our HW has support for multi-packet WQE (MPWQE)
    in which a single descriptor serves multiple TX packets.
    
    This reduces both the PCI overhead and the CPU cycles wasted on
    writing them.
    
    In this patch we add support for the HW feature, which is supported
    starting from ConnectX-5.
    
    Performance:
    Tested packet rate for UDP 64Byte multi-stream over ConnectX-5 NICs.
    CPU: Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz
    
    XDP_TX:
    We see a huge gain on single port ConnectX-5, and reach the 100 Mpps
    milestone.
    * Single-port HCA:
            Before:   70 Mpps
            After:   100 Mpps (+42.8%)
    
    * Dual-port HCA:
            Before: 51.7 Mpps
            After:  57.3 Mpps (+10.8%)
    
    * In both cases we tested traffic on one port and for now On Dual-port HCAs
      we see only small gain, we are working to overcome this bottleneck, but
      for the moment only with experimental firmware on dual port HCAs we can
      reach the wanted numbers as seen on Single-port HCAs.
    
    XDP_REDIRECT:
    Redirect from (A) ConnectX-5 to (B) ConnectX-5.
    Due to a setup limitation, (A) and (B) are on different NUMA nodes,
    so absolute performance numbers are not optimal.
    Note:
      Below is the transmit rate of (B), not the redirect rate of (A)
      which is in some cases higher.
    
    * (B) is single-port:
            Before:   77 Mpps
            After:    90 Mpps (+16.8%)
    
    * (B) is dual-port:
            Before:  61 Mpps
            After:   72 Mpps (+18%)
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 20b3432b35de..8f5545d317ba 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -416,6 +416,16 @@ struct mlx5e_xdp_wqe_info {
 	u8 num_ds;
 };
 
+struct mlx5e_xdp_mpwqe {
+	/* Current MPWQE session */
+	struct mlx5e_tx_wqe *wqe;
+	u8                   ds_count;
+	u8                   max_ds_count;
+};
+
+struct mlx5e_xdpsq;
+typedef bool (*mlx5e_fp_xmit_xdp_frame)(struct mlx5e_xdpsq*,
+					struct mlx5e_xdp_info*);
 struct mlx5e_xdpsq {
 	/* data path */
 
@@ -428,12 +438,14 @@ struct mlx5e_xdpsq {
 	u32                        xdpi_fifo_pc ____cacheline_aligned_in_smp;
 	u16                        pc;
 	struct mlx5_wqe_ctrl_seg   *doorbell_cseg;
+	struct mlx5e_xdp_mpwqe     mpwqe;
 
 	struct mlx5e_cq            cq;
 
 	/* read only */
 	struct mlx5_wq_cyc         wq;
 	struct mlx5e_xdpsq_stats  *stats;
+	mlx5e_fp_xmit_xdp_frame    xmit_xdp_frame;
 	struct {
 		struct mlx5e_xdp_wqe_info *wqe_info;
 		struct mlx5e_xdp_info_fifo xdpi_fifo;

commit 1feeab80078078186b5080fe15c51a5cce724a0e
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Sun Oct 14 14:46:57 2018 +0300

    net/mlx5e: XDP, Add array for WQE info descriptors
    
    Each xdp_wqe_info instance describes the number of data-segments
    and WQEBBs of the WQE.
    This is useful for a downstream patch that adds support for
    Multi-Packet TX WQE feature.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index fab49cbdb85d..20b3432b35de 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -411,6 +411,11 @@ struct mlx5e_xdp_info_fifo {
 	u32 mask;
 };
 
+struct mlx5e_xdp_wqe_info {
+	u8 num_wqebbs;
+	u8 num_ds;
+};
+
 struct mlx5e_xdpsq {
 	/* data path */
 
@@ -430,6 +435,7 @@ struct mlx5e_xdpsq {
 	struct mlx5_wq_cyc         wq;
 	struct mlx5e_xdpsq_stats  *stats;
 	struct {
+		struct mlx5e_xdp_wqe_info *wqe_info;
 		struct mlx5e_xdp_info_fifo xdpi_fifo;
 	} db;
 	void __iomem              *uar_map;

commit fea28dd6a281045e18c1412ab5bba54436c11088
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Sun Oct 14 14:37:48 2018 +0300

    net/mlx5e: XDP, Maintain a FIFO structure for xdp_info instances
    
    This provides infrastructure to have multiple xdp_info instances
    for the same consumer index.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 35cd4099feda..fab49cbdb85d 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -404,15 +404,24 @@ struct mlx5e_xdp_info {
 	struct mlx5e_dma_info di;
 };
 
+struct mlx5e_xdp_info_fifo {
+	struct mlx5e_xdp_info *xi;
+	u32 *cc;
+	u32 *pc;
+	u32 mask;
+};
+
 struct mlx5e_xdpsq {
 	/* data path */
 
 	/* dirtied @completion */
+	u32                        xdpi_fifo_cc;
 	u16                        cc;
 	bool                       redirect_flush;
 
 	/* dirtied @xmit */
-	u16                        pc ____cacheline_aligned_in_smp;
+	u32                        xdpi_fifo_pc ____cacheline_aligned_in_smp;
+	u16                        pc;
 	struct mlx5_wqe_ctrl_seg   *doorbell_cseg;
 
 	struct mlx5e_cq            cq;
@@ -421,7 +430,7 @@ struct mlx5e_xdpsq {
 	struct mlx5_wq_cyc         wq;
 	struct mlx5e_xdpsq_stats  *stats;
 	struct {
-		struct mlx5e_xdp_info     *xdpi;
+		struct mlx5e_xdp_info_fifo xdpi_fifo;
 	} db;
 	void __iomem              *uar_map;
 	u32                        sqn;

commit b8180392edd97cd5bfdf12270315a72fe601cf7e
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Wed Nov 21 14:06:02 2018 +0200

    net/mlx5e: XDP, Replace boolean doorbell indication with segment pointer
    
    Instead of calculating the control segment to be used upon an
    XDP xmit doorbell, save it in SQ structure.
    Nullify when no pending doorbell.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 15cbf0c6e309..35cd4099feda 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -413,7 +413,7 @@ struct mlx5e_xdpsq {
 
 	/* dirtied @xmit */
 	u16                        pc ____cacheline_aligned_in_smp;
-	bool                       doorbell;
+	struct mlx5_wqe_ctrl_seg   *doorbell_cseg;
 
 	struct mlx5e_cq            cq;
 

commit feb2ff9d74f76a174e284255b7e537b6c090c13c
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Wed Nov 21 14:04:45 2018 +0200

    net/mlx5e: XDP, Change the XDP SQ redirect indication
    
    Do not maintain an SQ state bit to indicate whether an
    XDP SQ serves redirect operations.
    
    Instead, rely on the fact that such an XDP SQ doesn't reside
    in an RQ instance, while the others do.
    This info is not known to the XDP SQ functions themselves,
    and they rely on their callers to distinguish between the cases.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 72cc204f9a34..15cbf0c6e309 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -344,7 +344,6 @@ enum {
 	MLX5E_SQ_STATE_IPSEC,
 	MLX5E_SQ_STATE_AM,
 	MLX5E_SQ_STATE_TLS,
-	MLX5E_SQ_STATE_REDIRECT,
 };
 
 struct mlx5e_sq_wqe_info {

commit 371289b61ac1ac890c7e98b1bfd3b8f32e38efae
Author: Or Gerlitz <ogerlitz@mellanox.com>
Date:   Tue Nov 6 19:31:10 2018 +0200

    net/mlx5e: Expose ethtool pause and link functions to mlx5e callers
    
    Towards supporting set/get of global pause for the port
    and get of the port link ksetting from the uplink representor,
    expose the relevant entries to other mlx5 callers.
    
    Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 16fe9e9a14c6..72cc204f9a34 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -973,12 +973,20 @@ int mlx5e_ethtool_get_coalesce(struct mlx5e_priv *priv,
 			       struct ethtool_coalesce *coal);
 int mlx5e_ethtool_set_coalesce(struct mlx5e_priv *priv,
 			       struct ethtool_coalesce *coal);
+int mlx5e_ethtool_get_link_ksettings(struct mlx5e_priv *priv,
+				     struct ethtool_link_ksettings *link_ksettings);
+int mlx5e_ethtool_set_link_ksettings(struct mlx5e_priv *priv,
+				     const struct ethtool_link_ksettings *link_ksettings);
 u32 mlx5e_ethtool_get_rxfh_key_size(struct mlx5e_priv *priv);
 u32 mlx5e_ethtool_get_rxfh_indir_size(struct mlx5e_priv *priv);
 int mlx5e_ethtool_get_ts_info(struct mlx5e_priv *priv,
 			      struct ethtool_ts_info *info);
 int mlx5e_ethtool_flash_device(struct mlx5e_priv *priv,
 			       struct ethtool_flash *flash);
+void mlx5e_ethtool_get_pauseparam(struct mlx5e_priv *priv,
+				  struct ethtool_pauseparam *pauseparam);
+int mlx5e_ethtool_set_pauseparam(struct mlx5e_priv *priv,
+				 struct ethtool_pauseparam *pauseparam);
 
 /* mlx5e generic netdev management API */
 int mlx5e_netdev_init(struct net_device *netdev,

commit 073caf508822750cde02086a6ac1b9131e69692c
Author: Or Gerlitz <ogerlitz@mellanox.com>
Date:   Thu Nov 1 19:14:21 2018 +0200

    net/mlx5e: Add sriov and udp tunnel ndo support for the uplink rep
    
    Some of the sriov ndo calls are needed also on the switchdev mode -
    e.g setup VF mac and reading vport stats. Add them to the uplink rep
    netdev ops. Same for the UDP tunnel ones, need them there to identify
    offloaded udp tunnel ports.
    
    Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 71c0646c8108..16fe9e9a14c6 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -1004,4 +1004,16 @@ void mlx5e_build_rss_params(struct mlx5e_rss_params *rss_params,
 u8 mlx5e_params_calculate_tx_min_inline(struct mlx5_core_dev *mdev);
 void mlx5e_rx_dim_work(struct work_struct *work);
 void mlx5e_tx_dim_work(struct work_struct *work);
+
+void mlx5e_add_vxlan_port(struct net_device *netdev, struct udp_tunnel_info *ti);
+void mlx5e_del_vxlan_port(struct net_device *netdev, struct udp_tunnel_info *ti);
+netdev_features_t mlx5e_features_check(struct sk_buff *skb,
+				       struct net_device *netdev,
+				       netdev_features_t features);
+#ifdef CONFIG_MLX5_ESWITCH
+int mlx5e_set_vf_mac(struct net_device *dev, int vf, u8 *mac);
+int mlx5e_set_vf_rate(struct net_device *dev, int vf, int min_tx_rate, int max_tx_rate);
+int mlx5e_get_vf_config(struct net_device *dev, int vf, struct ifla_vf_info *ivi);
+int mlx5e_get_vf_stats(struct net_device *dev, int vf, struct ifla_vf_stats *vf_stats);
+#endif
 #endif /* __MLX5_EN_H__ */

commit b36cdb42ad4b114d4b9bbba8770d7a4b8e1a62f6
Author: Or Gerlitz <ogerlitz@mellanox.com>
Date:   Thu Nov 8 20:42:55 2018 +0200

    net/mlx5e: Handle port mtu/link, dcb and lag for uplink reps
    
    Take care of setup/teardown for the port link, dcb, lag as well as
    dealing with port mtu and carrier for e-switch uplink representors.
    
    This is achieved by adding a dedicated profile instance for uplink
    representors which includes the enable/disable and more profile routines
    which are invoked by the general mlx5e code for netdev attach/detach.
    
    Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 31deafecf679..71c0646c8108 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -940,6 +940,7 @@ int mlx5e_create_tis(struct mlx5_core_dev *mdev, int tc,
 void mlx5e_destroy_tis(struct mlx5_core_dev *mdev, u32 tisn);
 
 int mlx5e_create_tises(struct mlx5e_priv *priv);
+void mlx5e_update_carrier(struct mlx5e_priv *priv);
 int mlx5e_close(struct net_device *netdev);
 int mlx5e_open(struct net_device *netdev);
 void mlx5e_update_ndo_stats(struct mlx5e_priv *priv);

commit d9ee0491c2ffbb5bc8bc418dd857a045d0d0130f
Author: Or Gerlitz <ogerlitz@mellanox.com>
Date:   Tue Feb 13 15:48:30 2018 +0200

    net/mlx5e: Use dedicated uplink vport netdev representor
    
    Currently, when running in sriov switchdev mode, we are using the PF
    netdevice as the uplink representor, this is problematic from few aspects:
    
    - will break when the PF isn't eswitch manager (e.g smart NIC env)
    - misalignment with other NIC switchdev drivers
    - makes us have and maintain special code, hurts the driver quality/robustness
    - which in turn opens the door for future bugs
    
    As of each and all of the above, we move to have a dedicated netdev representor
    for the uplink vport in a similar manner done for for the VF vports.
    
    This includes the following:
    
    1. have an uplink rep netdev as we have for VF reps
    2. all reps use same load/unload functions
    3. HW stats for uplink based on physical port counters and not vport counters
    4. link state for the uplink managed through PAOS and not vport state
    5. the uplink rep has sysfs link to the PF PCI function && uses the PF MAC address
    
    Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 1c4e033b7509..31deafecf679 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -775,6 +775,7 @@ mlx5e_skb_from_cqe_nonlinear(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe,
 			     struct mlx5e_wqe_frag_info *wi, u32 cqe_bcnt);
 
 void mlx5e_update_stats(struct mlx5e_priv *priv);
+void mlx5e_get_stats(struct net_device *dev, struct rtnl_link_stats64 *stats);
 
 void mlx5e_init_l2_addr(struct mlx5e_priv *priv);
 int mlx5e_self_test_num(struct mlx5e_priv *priv);
@@ -939,7 +940,6 @@ int mlx5e_create_tis(struct mlx5_core_dev *mdev, int tc,
 void mlx5e_destroy_tis(struct mlx5_core_dev *mdev, u32 tisn);
 
 int mlx5e_create_tises(struct mlx5e_priv *priv);
-void mlx5e_cleanup_nic_tx(struct mlx5e_priv *priv);
 int mlx5e_close(struct net_device *netdev);
 int mlx5e_open(struct net_device *netdev);
 void mlx5e_update_ndo_stats(struct mlx5e_priv *priv);
@@ -948,6 +948,7 @@ void mlx5e_queue_update_stats(struct mlx5e_priv *priv);
 int mlx5e_bits_invert(unsigned long a, int size);
 
 typedef int (*change_hw_mtu_cb)(struct mlx5e_priv *priv);
+int mlx5e_set_dev_port_mtu(struct mlx5e_priv *priv);
 int mlx5e_change_mtu(struct net_device *netdev, int new_mtu,
 		     change_hw_mtu_cb set_mtu_cb);
 

commit 5c7e8bbb025726f9cb9b2bc050509a94262426f8
Author: Eyal Davidovich <eyald@mellanox.com>
Date:   Sat Oct 20 16:18:00 2018 +0300

    net/mlx5e: Use monitor counters for update stats
    
    - Adding new notifier block (struct mlx5_nb) monitor_counters_nb
      for handeling MONITOR_COUNTER new event type.
    - Adding work queue element: monitor_counters_work for re-arm and
      update stats.
    - We re-queue the update stat work, only when working over firmware
      that doesn't support the monitored counters.
    
    Signed-off-by: Eyal Davidovich <eyald@mellanox.com>
    Reviewed-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 9ea0f853c3bb..1c4e033b7509 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -685,6 +685,8 @@ struct mlx5e_priv {
 	struct work_struct         set_rx_mode_work;
 	struct work_struct         tx_timeout_work;
 	struct work_struct         update_stats_work;
+	struct work_struct         monitor_counters_work;
+	struct mlx5_nb             monitor_counters_nb;
 
 	struct mlx5_core_dev      *mdev;
 	struct net_device         *netdev;
@@ -940,6 +942,7 @@ int mlx5e_create_tises(struct mlx5e_priv *priv);
 void mlx5e_cleanup_nic_tx(struct mlx5e_priv *priv);
 int mlx5e_close(struct net_device *netdev);
 int mlx5e_open(struct net_device *netdev);
+void mlx5e_update_ndo_stats(struct mlx5e_priv *priv);
 
 void mlx5e_queue_update_stats(struct mlx5e_priv *priv);
 int mlx5e_bits_invert(unsigned long a, int size);

commit 2f62747c77e2e5a8acb720aaec9ee4860d55118f
Merge: d8ed257f313f 6c22a11957f4
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Mon Dec 10 15:43:47 2018 -0800

    Merge branch 'mlx5-next' of git://git.kernel.org/pub/scm/linux/kernel/git/mellanox/linux
    
    mlx5-next shared branch with rdma subtree to avoid mlx5 rdma v.s. netdev
    conflicts.
    
    Highlights:
    
    1) RDMA ODP  (On Demand Paging) improvements and moving ODP logic to
    mlx5 RDMA driver
    2) Improved mlx5 core driver and device events handling and provided API
    for upper layers to subscribe to device events.
    3) RDMA only code cleanup from mlx5 core
    4) Add helper to get CQE opcode
    5) Rework handling of port module events
    6) shared mlx5_ifc.h updates to avoid conflicts
    
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

commit 8ff57c18e9f6b03722070a372cdcc850b13bcbc8
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Mon Nov 19 15:07:07 2018 +0200

    net/mlx5e: Improve ethtool private-flags code structure
    
    Refactor the code of private-flags setter.
    Replace consecutive calls to mlx5e_handle_pflag with a loop
    that uses a preset set of parameters.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index c6686256a92a..8760d10ad5a1 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -49,6 +49,7 @@
 #include <net/switchdev.h>
 #include <net/xdp.h>
 #include <linux/net_dim.h>
+#include <linux/bits.h>
 #include "wq.h"
 #include "mlx5_core.h"
 #include "en_stats.h"
@@ -211,22 +212,23 @@ struct mlx5e_umr_wqe {
 extern const char mlx5e_self_tests[][ETH_GSTRING_LEN];
 
 enum mlx5e_priv_flag {
-	MLX5E_PFLAG_RX_CQE_BASED_MODER = (1 << 0),
-	MLX5E_PFLAG_TX_CQE_BASED_MODER = (1 << 1),
-	MLX5E_PFLAG_RX_CQE_COMPRESS = (1 << 2),
-	MLX5E_PFLAG_RX_STRIDING_RQ = (1 << 3),
-	MLX5E_PFLAG_RX_NO_CSUM_COMPLETE = (1 << 4),
+	MLX5E_PFLAG_RX_CQE_BASED_MODER,
+	MLX5E_PFLAG_TX_CQE_BASED_MODER,
+	MLX5E_PFLAG_RX_CQE_COMPRESS,
+	MLX5E_PFLAG_RX_STRIDING_RQ,
+	MLX5E_PFLAG_RX_NO_CSUM_COMPLETE,
+	MLX5E_NUM_PFLAGS, /* Keep last */
 };
 
 #define MLX5E_SET_PFLAG(params, pflag, enable)			\
 	do {							\
 		if (enable)					\
-			(params)->pflags |= (pflag);		\
+			(params)->pflags |= BIT(pflag);		\
 		else						\
-			(params)->pflags &= ~(pflag);		\
+			(params)->pflags &= ~(BIT(pflag));	\
 	} while (0)
 
-#define MLX5E_GET_PFLAG(params, pflag) (!!((params)->pflags & (pflag)))
+#define MLX5E_GET_PFLAG(params, pflag) (!!((params)->pflags & (BIT(pflag))))
 
 #ifdef CONFIG_MLX5_CORE_EN_DCB
 #define MLX5E_MAX_BW_ALLOC 100 /* Max percentage of BW allocation */

commit 756c41603a189e4ae9dca77c3c3d80a926d8bedf
Author: Aya Levin <ayal@mellanox.com>
Date:   Tue Oct 23 16:03:33 2018 +0300

    net/mlx5e: ethtool, Support user configuration for RX hash fields
    
    Enable user configuration of RX hash fields that are used for traffic
    spreading into RX queues. User can change built-in RSS (Receive Side
    Scaling) profiles on the following traffic types: UDP4, UDP6, TCP4 and
    TCP6.  This configuration effects both outer and inner headers.  Added
    support for ethtool commands: ETHTOOL_SRXFH and ETHTOOL_GRXFH.
    
    Command example respectively:
    $ethtool -N eth1 rx-flow-hash tcp4 sdfn
    $ethtool -n eth1 rx-flow-hash tcpp4
    IP SA
    IP DA
    L4 bytes 0 & 1 [TCP/UDP src port]
    L4 bytes 2 & 3 [TCP/UDP dst port]
    
    Signed-off-by: Aya Levin <ayal@mellanox.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 330c476c1df2..c6686256a92a 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -650,6 +650,7 @@ enum {
 
 struct mlx5e_rss_params {
 	u32	indirection_rqt[MLX5E_INDIR_RQT_SIZE];
+	u32	rx_hash_fields[MLX5E_NUM_INDIR_TIRS];
 	u8	toeplitz_hash_key[40];
 	u8	hfunc;
 };

commit bbeb53b8b2c92fd4e5b51e8fd9ab1c4000e4d350
Author: Aya Levin <ayal@mellanox.com>
Date:   Tue Nov 6 21:05:29 2018 +0200

    net/mlx5e: Move RSS params to a dedicated struct
    
    Remove RSS params from params struct under channels, and introduce
    a new struct with RSS configuration params under priv struct. There is
    no functional change here.
    
    Signed-off-by: Aya Levin <ayal@mellanox.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 8539ea9a67ca..330c476c1df2 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -244,9 +244,6 @@ struct mlx5e_params {
 	bool lro_en;
 	u32 lro_wqe_sz;
 	u8  tx_min_inline_mode;
-	u8  rss_hfunc;
-	u8  toeplitz_hash_key[40];
-	u32 indirection_rqt[MLX5E_INDIR_RQT_SIZE];
 	bool vlan_strip_disable;
 	bool scatter_fcs_en;
 	bool rx_dim_enabled;
@@ -651,6 +648,12 @@ enum {
 	MLX5E_NIC_PRIO
 };
 
+struct mlx5e_rss_params {
+	u32	indirection_rqt[MLX5E_INDIR_RQT_SIZE];
+	u8	toeplitz_hash_key[40];
+	u8	hfunc;
+};
+
 struct mlx5e_priv {
 	/* priv data path fields - start */
 	struct mlx5e_txqsq *txq2sq[MLX5E_MAX_NUM_CHANNELS * MLX5E_MAX_NUM_TC];
@@ -671,6 +674,7 @@ struct mlx5e_priv {
 	struct mlx5e_tir           indir_tir[MLX5E_NUM_INDIR_TIRS];
 	struct mlx5e_tir           inner_indir_tir[MLX5E_NUM_INDIR_TIRS];
 	struct mlx5e_tir           direct_tir[MLX5E_MAX_NUM_CHANNELS];
+	struct mlx5e_rss_params    rss_params;
 	u32                        tx_rates[MLX5E_MAX_NUM_SQS];
 
 	struct mlx5e_flow_steering fs;
@@ -796,7 +800,7 @@ struct mlx5e_redirect_rqt_param {
 
 int mlx5e_redirect_rqt(struct mlx5e_priv *priv, u32 rqtn, int sz,
 		       struct mlx5e_redirect_rqt_param rrp);
-void mlx5e_build_indir_tir_ctx_hash(struct mlx5e_params *params,
+void mlx5e_build_indir_tir_ctx_hash(struct mlx5e_rss_params *rss_params,
 				    const struct mlx5e_tirc_config *ttconfig,
 				    void *tirc, bool inner);
 void mlx5e_modify_tirs_hash(struct mlx5e_priv *priv, void *in, int inlen);
@@ -982,11 +986,13 @@ int mlx5e_attach_netdev(struct mlx5e_priv *priv);
 void mlx5e_detach_netdev(struct mlx5e_priv *priv);
 void mlx5e_destroy_netdev(struct mlx5e_priv *priv);
 void mlx5e_build_nic_params(struct mlx5_core_dev *mdev,
+			    struct mlx5e_rss_params *rss_params,
 			    struct mlx5e_params *params,
 			    u16 max_channels, u16 mtu);
 void mlx5e_build_rq_params(struct mlx5_core_dev *mdev,
 			   struct mlx5e_params *params);
-void mlx5e_build_rss_params(struct mlx5e_params *params);
+void mlx5e_build_rss_params(struct mlx5e_rss_params *rss_params,
+			    u16 num_channels);
 u8 mlx5e_params_calculate_tx_min_inline(struct mlx5_core_dev *mdev);
 void mlx5e_rx_dim_work(struct work_struct *work);
 void mlx5e_tx_dim_work(struct work_struct *work);

commit d930ac796fdb3eed62b0053297a728ced8fd58b1
Author: Aya Levin <ayal@mellanox.com>
Date:   Sun Oct 28 16:22:57 2018 +0200

    net/mlx5e: Refactor TIR configuration function
    
    Refactor mlx5e_build_indir_tir_ctx_hash for better code re-use. TIR
    stands for Transport Interface Receive, which is responsible for all
    transport related operations on the receive side. Added a
    static array with TIR default configuration values. This separates
    configuration values from command setting, which is needed for
    downstream patch.
    
    Signed-off-by: Aya Levin <ayal@mellanox.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 86d60222b3cb..8539ea9a67ca 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -797,9 +797,10 @@ struct mlx5e_redirect_rqt_param {
 int mlx5e_redirect_rqt(struct mlx5e_priv *priv, u32 rqtn, int sz,
 		       struct mlx5e_redirect_rqt_param rrp);
 void mlx5e_build_indir_tir_ctx_hash(struct mlx5e_params *params,
-				    enum mlx5e_traffic_types tt,
+				    const struct mlx5e_tirc_config *ttconfig,
 				    void *tirc, bool inner);
 void mlx5e_modify_tirs_hash(struct mlx5e_priv *priv, void *in, int inlen);
+struct mlx5e_tirc_config mlx5e_tirc_get_default_config(enum mlx5e_traffic_types tt);
 
 int mlx5e_open_locked(struct net_device *netdev);
 int mlx5e_close_locked(struct net_device *netdev);

commit 080d1b17fbc51789287cd8b2044a8cc9ec600e81
Author: Aya Levin <ayal@mellanox.com>
Date:   Tue Oct 23 10:02:08 2018 +0300

    net/mlx5e: Move modify tirs hash functionality
    
    Move modify tirs hash functionality (mlx5e_modify_tirs_hash) from
    en_ethtool.c to en_main.c. This allows future use of this fuctionality
    from en_fs_ethtool.c, while keeping current convention: en_ethtool.c
    doesn't have an API.  There is no functional change here.
    
    Signed-off-by: Aya Levin <ayal@mellanox.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 2af43465ba2c..86d60222b3cb 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -799,6 +799,7 @@ int mlx5e_redirect_rqt(struct mlx5e_priv *priv, u32 rqtn, int sz,
 void mlx5e_build_indir_tir_ctx_hash(struct mlx5e_params *params,
 				    enum mlx5e_traffic_types tt,
 				    void *tirc, bool inner);
+void mlx5e_modify_tirs_hash(struct mlx5e_priv *priv, void *in, int inlen);
 
 int mlx5e_open_locked(struct net_device *netdev);
 int mlx5e_close_locked(struct net_device *netdev);

commit 305438310938e9163f9366aa261336f93d2622f6
Author: Gal Pressman <pressmangal@gmail.com>
Date:   Wed Nov 7 20:31:36 2018 +0200

    net/mlx5e: Cleanup unused defines
    
    Remove couple of defines that are no longer used.
    
    Signed-off-by: Gal Pressman <pressmangal@gmail.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 118324802926..2af43465ba2c 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -147,9 +147,6 @@ struct page_pool;
 	       MLX5_UMR_MTT_ALIGNMENT))
 #define MLX5E_UMR_WQEBBS \
 	(DIV_ROUND_UP(MLX5E_UMR_WQE_INLINE_SZ, MLX5_SEND_WQE_BB))
-#define MLX5E_ICOSQ_MAX_WQEBBS MLX5E_UMR_WQEBBS
-
-#define MLX5E_NUM_MAIN_GROUPS 9
 
 #define MLX5E_MSG_LEVEL			NETIF_MSG_LINK
 

commit 7cffaddd39b438fab49863ea4602d6294a9d657f
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Mon Nov 26 14:38:58 2018 -0800

    net/mlx5e: Use the new mlx5 core notifier API
    
    Remove the deprecated mlx5_interface->event mlx5e callback and use new
    mlx5 notifier API to subscribe for mlx5 events, handle port change event
    as received from FW rather than handling the mlx5 core processed port
    change software version event.
    
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index aea74856c702..13d8a74d3db5 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -631,7 +631,6 @@ struct mlx5e_channel_stats {
 } ____cacheline_aligned_in_smp;
 
 enum {
-	MLX5E_STATE_ASYNC_EVENTS_ENABLED,
 	MLX5E_STATE_OPENED,
 	MLX5E_STATE_DESTROYING,
 };
@@ -690,6 +689,8 @@ struct mlx5e_priv {
 	struct hwtstamp_config     tstamp;
 	u16                        q_counter;
 	u16                        drop_rq_q_counter;
+	struct notifier_block      events_nb;
+
 #ifdef CONFIG_MLX5_CORE_EN_DCB
 	struct mlx5e_dcbx          dcbx;
 #endif

commit f2f3df5501391bc784c8462dc97d989c2194fb74
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Mon Nov 19 10:52:38 2018 -0800

    net/mlx5: EQ, Privatize eq_table and friends
    
    Move unnecessary EQ table structures and declaration from the
    public include/linux/mlx5/driver.h into the private area of mlx5_core
    and into eq.c/eq.h.
    
    Introduce new mlx5 EQ APIs:
    
    mlx5_comp_vectors_count(dev);
    mlx5_comp_irq_get_affinity_mask(dev, vector);
    
    And use them from mlx5_ib or mlx5e netdevice instead of direct access to
    mlx5_core internal structures.
    
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Reviewed-by: Leon Romanovsky <leonro@mellanox.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Leon Romanovsky <leonro@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index d7fbd5b6ac95..aea74856c702 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -178,8 +178,7 @@ static inline int mlx5e_get_max_num_channels(struct mlx5_core_dev *mdev)
 {
 	return is_kdump_kernel() ?
 		MLX5E_MIN_NUM_CHANNELS :
-		min_t(int, mdev->priv.eq_table.num_comp_vectors,
-		      MLX5E_MAX_NUM_CHANNELS);
+		min_t(int, mlx5_comp_vectors_count(mdev), MLX5E_MAX_NUM_CHANNELS);
 }
 
 /* Use this function to get max num channels after netdev was created */

commit 0073c8f72736b423aade8a817587a5f3e4df4ad8
Author: Moshe Shemesh <moshe@mellanox.com>
Date:   Thu Oct 11 07:31:10 2018 +0300

    net/mlx5e: RX, verify received packet size in Linear Striding RQ
    
    In case of striding RQ, we use  MPWRQ (Multi Packet WQE RQ), which means
    that WQE (RX descriptor) can be used for many packets and so the WQE is
    much bigger than MTU.  In virtualization setups where the port mtu can
    be larger than the vf mtu, if received packet is bigger than MTU, it
    won't be dropped by HW on too small receive WQE. If we use linear SKB in
    striding RQ, since each stride has room for mtu size payload and skb
    info, an oversized packet can lead to crash for crossing allocated page
    boundary upon the call to build_skb. So driver needs to check packet
    size and drop it.
    
    Introduce new SW rx counter, rx_oversize_pkts_sw_drop, which counts the
    number of packets dropped by the driver for being too large.
    
    As a new field is added to the RQ struct, re-open the channels whenever
    this field is being used in datapath (i.e., in the case of linear
    Striding RQ).
    
    Fixes: 619a8f2a42f1 ("net/mlx5e: Use linear SKB in Striding RQ")
    Signed-off-by: Moshe Shemesh <moshe@mellanox.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index d7fbd5b6ac95..118324802926 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -569,6 +569,7 @@ struct mlx5e_rq {
 
 	unsigned long          state;
 	int                    ix;
+	unsigned int           hw_mtu;
 
 	struct net_dim         dim; /* Dynamic Interrupt Moderation */
 

commit 779d986d60de51aabc3bec385f92b832089c5c01
Author: Feras Daoud <ferasda@mellanox.com>
Date:   Thu Sep 6 14:56:56 2018 +0300

    net/mlx5e: Do not ignore netdevice TX/RX queues number
    
    The current design of mlx5e driver ignores the netdevice TX/RX queues
    number for netdevices that RDMA IPoIB ULP creates. Instead, the queue
    number is initialized to the maximum number that mlx5 thinks best for
    performance. As a result, ULP drivers that choose to create a netdevice
    with queue number that is less than the maximum channels mlx5 creates,
    will get a memory corruption.
    
    This fix changes the mlx5e netdev logic to respect ULP netdevices TX/RX
    queue number and use it when creating resources instead of the maximum
    channel number.
    
    Fixes: cd565b4b51e5 ("IB/IPoIB: Support acceleration options callbacks")
    Signed-off-by: Feras Daoud <ferasda@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 0fbdd7696ec4..d7fbd5b6ac95 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -173,6 +173,7 @@ static inline u16 mlx5_min_rx_wqes(int wq_type, u32 wq_size)
 	}
 }
 
+/* Use this function to get max num channels (rxqs/txqs) only to create netdev */
 static inline int mlx5e_get_max_num_channels(struct mlx5_core_dev *mdev)
 {
 	return is_kdump_kernel() ?
@@ -181,6 +182,13 @@ static inline int mlx5e_get_max_num_channels(struct mlx5_core_dev *mdev)
 		      MLX5E_MAX_NUM_CHANNELS);
 }
 
+/* Use this function to get max num channels after netdev was created */
+static inline int mlx5e_get_netdev_max_channels(struct net_device *netdev)
+{
+	return min_t(unsigned int, netdev->num_rx_queues,
+		     netdev->num_tx_queues);
+}
+
 struct mlx5e_tx_wqe {
 	struct mlx5_wqe_ctrl_seg ctrl;
 	struct mlx5_wqe_eth_seg  eth;
@@ -710,7 +718,6 @@ struct mlx5e_profile {
 	void	(*disable)(struct mlx5e_priv *priv);
 	void	(*update_stats)(struct mlx5e_priv *priv);
 	void	(*update_carrier)(struct mlx5e_priv *priv);
-	int	(*max_nch)(struct mlx5_core_dev *mdev);
 	struct {
 		mlx5e_fp_handle_rx_cqe handle_rx_cqe;
 		mlx5e_fp_handle_rx_cqe handle_rx_cqe_mpwqe;
@@ -970,7 +977,7 @@ int mlx5e_netdev_init(struct net_device *netdev,
 void mlx5e_netdev_cleanup(struct net_device *netdev, struct mlx5e_priv *priv);
 struct net_device*
 mlx5e_create_netdev(struct mlx5_core_dev *mdev, const struct mlx5e_profile *profile,
-		    void *ppriv);
+		    int nch, void *ppriv);
 int mlx5e_attach_netdev(struct mlx5e_priv *priv);
 void mlx5e_detach_netdev(struct mlx5e_priv *priv);
 void mlx5e_destroy_netdev(struct mlx5e_priv *priv);

commit cdeef2b1528b6faa7a7e60de7a56c48bc6838ae6
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Tue Sep 11 23:45:33 2018 -0700

    net/mlx5e: Use non-delayed work for update stats
    
    Convert mlx5e update stats work to a normal work structure, since it is
    never used delayed.
    
    Add a helper function to queue update stats work on demand which checks
    for some conditions and reduce code duplication to have a better
    abstraction.
    
    Fixes: ed56c5193ad8 ("net/mlx5e: Update NIC HW stats on demand only")
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index f6dd0254d103..0fbdd7696ec4 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -673,7 +673,7 @@ struct mlx5e_priv {
 	struct work_struct         update_carrier_work;
 	struct work_struct         set_rx_mode_work;
 	struct work_struct         tx_timeout_work;
-	struct delayed_work        update_stats_work;
+	struct work_struct         update_stats_work;
 
 	struct mlx5_core_dev      *mdev;
 	struct net_device         *netdev;
@@ -927,6 +927,7 @@ void mlx5e_cleanup_nic_tx(struct mlx5e_priv *priv);
 int mlx5e_close(struct net_device *netdev);
 int mlx5e_open(struct net_device *netdev);
 
+void mlx5e_queue_update_stats(struct mlx5e_priv *priv);
 int mlx5e_bits_invert(unsigned long a, int size);
 
 typedef int (*change_hw_mtu_cb)(struct mlx5e_priv *priv);

commit 519a0bf5b20c370fb05ee05ea6c00e1377c13229
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Wed Sep 12 15:02:05 2018 -0700

    net/mlx5e: Initialize all netdev common structures in one place
    
    Move all mlx5e generic structures initializations to mlx5e_netdev_init.
    The common structure new initializer function will be used to initialize
    mlx5 context for netlink created netdevs such as IPoIB mlx5 accelerated
    child netdevs.
    
    Fixes: cd565b4b51e5 ("IB/IPoIB: Support acceleration options callbacks")
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: Feras Daoud <ferasda@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 59d07a6fe7c5..f6dd0254d103 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -961,7 +961,11 @@ int mlx5e_ethtool_flash_device(struct mlx5e_priv *priv,
 			       struct ethtool_flash *flash);
 
 /* mlx5e generic netdev management API */
-int mlx5e_netdev_init(struct net_device *netdev, struct mlx5e_priv *priv);
+int mlx5e_netdev_init(struct net_device *netdev,
+		      struct mlx5e_priv *priv,
+		      struct mlx5_core_dev *mdev,
+		      const struct mlx5e_profile *profile,
+		      void *ppriv);
 void mlx5e_netdev_cleanup(struct net_device *netdev, struct mlx5e_priv *priv);
 struct net_device*
 mlx5e_create_netdev(struct mlx5_core_dev *mdev, const struct mlx5e_profile *profile,

commit 303211b44ce3faa4456f78ed725d80e0212aa842
Author: Feras Daoud <ferasda@mellanox.com>
Date:   Tue Oct 9 13:06:02 2018 +0300

    net/mlx5e: Always initialize update stats delayed work
    
    mlx5e_detach_netdev cancels update_stats work which was not initialized
    in ipoib netdevice profile, as a result, the following assert occurs:
    
    ODEBUG: assert_init not available (active state 0) object type:
    timer_list hint:(null)
    
    This change moves the update stats work to be initialized for all
    mlx5e netdevices.
    
    Fixes: cd565b4b51e5 ("IB/IPoIB: Support acceleration options callbacks")
    Signed-off-by: Feras Daoud <ferasda@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 4b2737c613fe..59d07a6fe7c5 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -926,7 +926,6 @@ int mlx5e_create_tises(struct mlx5e_priv *priv);
 void mlx5e_cleanup_nic_tx(struct mlx5e_priv *priv);
 int mlx5e_close(struct net_device *netdev);
 int mlx5e_open(struct net_device *netdev);
-void mlx5e_update_stats_work(struct work_struct *work);
 
 int mlx5e_bits_invert(unsigned long a, int size);
 

commit 182570b26223deea0eaf09ba384c79a29d0a328f
Author: Feras Daoud <ferasda@mellanox.com>
Date:   Tue Oct 2 09:54:59 2018 +0300

    net/mlx5e: Gather common netdev init/cleanup functionality in one place
    
    Introduce a helper init/cleanup function that initializes mlx5e generic
    netdev private structure, and use them from all profiles init/cleanup
    callbacks.
    
    This patch will also be helpful to initialize/cleanup netdevs that are
    not created by mlx5 driver, e.g: accelerated ipoib child netdevs.
    
    Fixes: 26e59d8077a3 ("net/mlx5e: Implement mlx5e interface attach/detach callbacks")
    Signed-off-by: Feras Daoud <ferasda@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 1d743bd5d212..4b2737c613fe 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -698,7 +698,7 @@ struct mlx5e_priv {
 };
 
 struct mlx5e_profile {
-	void	(*init)(struct mlx5_core_dev *mdev,
+	int	(*init)(struct mlx5_core_dev *mdev,
 			struct net_device *netdev,
 			const struct mlx5e_profile *profile, void *ppriv);
 	void	(*cleanup)(struct mlx5e_priv *priv);
@@ -962,6 +962,8 @@ int mlx5e_ethtool_flash_device(struct mlx5e_priv *priv,
 			       struct ethtool_flash *flash);
 
 /* mlx5e generic netdev management API */
+int mlx5e_netdev_init(struct net_device *netdev, struct mlx5e_priv *priv);
+void mlx5e_netdev_cleanup(struct net_device *netdev, struct mlx5e_priv *priv);
 struct net_device*
 mlx5e_create_netdev(struct mlx5_core_dev *mdev, const struct mlx5e_profile *profile,
 		    void *ppriv);

commit 6f41617bf23a17d9cb7cc6ca8161534f05f80293
Merge: 7bdaae270cb5 cec4de302c5f
Author: David S. Miller <davem@davemloft.net>
Date:   Wed Oct 3 21:00:17 2018 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Minor conflict in net/core/rtnetlink.c, David Ahern's bug fix in 'net'
    overlapped the renaming of a netlink attribute in net-next.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit b856df28f9230a47669efbdd57896084caadb2b3
Author: Or Gerlitz <ogerlitz@mellanox.com>
Date:   Sun Jul 1 08:58:38 2018 +0000

    net/mlx5e: Allow reporting of checksum unnecessary
    
    Currently we practically never report checksum unnecessary, because
    for all IP packets we take the checksum complete path.
    
    Enable non-default runs with reprorting checksum unnecessary, using
    an ethtool private flag. This can be useful for performance evals
    and other explorations.
    
    Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index b3bd79833517..ef7a44eb9adb 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -209,6 +209,7 @@ enum mlx5e_priv_flag {
 	MLX5E_PFLAG_TX_CQE_BASED_MODER = (1 << 1),
 	MLX5E_PFLAG_RX_CQE_COMPRESS = (1 << 2),
 	MLX5E_PFLAG_RX_STRIDING_RQ = (1 << 3),
+	MLX5E_PFLAG_RX_NO_CSUM_COMPLETE = (1 << 4),
 };
 
 #define MLX5E_SET_PFLAG(params, pflag, enable)			\
@@ -290,6 +291,7 @@ struct mlx5e_dcbx_dp {
 enum {
 	MLX5E_RQ_STATE_ENABLED,
 	MLX5E_RQ_STATE_AM,
+	MLX5E_RQ_STATE_NO_CSUM_COMPLETE,
 };
 
 struct mlx5e_cq {

commit a5355de878f4d5cc3eaf6f447cc33c103fab74dd
Author: Or Gerlitz <ogerlitz@mellanox.com>
Date:   Sun Aug 26 12:53:51 2018 +0300

    net/mlx5e: Expose ethtool rss key size / indirection table functions
    
    Towards enabling RSS for the vport representors, expose the functions for
    querying the rss hash key size and indirection table size via ethtool.
    
    Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
    Reviewed-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 98390a5b106a..b3bd79833517 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -951,6 +951,8 @@ int mlx5e_ethtool_get_coalesce(struct mlx5e_priv *priv,
 			       struct ethtool_coalesce *coal);
 int mlx5e_ethtool_set_coalesce(struct mlx5e_priv *priv,
 			       struct ethtool_coalesce *coal);
+u32 mlx5e_ethtool_get_rxfh_key_size(struct mlx5e_priv *priv);
+u32 mlx5e_ethtool_get_rxfh_indir_size(struct mlx5e_priv *priv);
 int mlx5e_ethtool_get_ts_info(struct mlx5e_priv *priv,
 			      struct ethtool_ts_info *info);
 int mlx5e_ethtool_flash_device(struct mlx5e_priv *priv,

commit 3edc0159c01406a21a8c850b268d1ac8fb2b4c66
Author: Gavi Teitz <gavi@mellanox.com>
Date:   Sun Aug 19 15:01:13 2018 +0300

    net/mlx5e: Expose function for building RSS params
    
    Towards enabling RSS for the vport representors, extract the
    procedure for building a device's RSS params, and expose the
    function.
    
    Signed-off-by: Gavi Teitz <gavi@mellanox.com>
    Reviewed-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 275af3bd63b3..98390a5b106a 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -968,6 +968,7 @@ void mlx5e_build_nic_params(struct mlx5_core_dev *mdev,
 			    u16 max_channels, u16 mtu);
 void mlx5e_build_rq_params(struct mlx5_core_dev *mdev,
 			   struct mlx5e_params *params);
+void mlx5e_build_rss_params(struct mlx5e_params *params);
 u8 mlx5e_params_calculate_tx_min_inline(struct mlx5_core_dev *mdev);
 void mlx5e_rx_dim_work(struct work_struct *work);
 void mlx5e_tx_dim_work(struct work_struct *work);

commit 46dc933cee82c8fd18f5fb8dfa10aceb7a29d5ab
Author: Or Gerlitz <ogerlitz@mellanox.com>
Date:   Tue Aug 28 20:53:55 2018 +0300

    net/mlx5e: Provide explicit directive if to create inner indirect tirs
    
    Change the driver functions that deal with creating indirect tirs
    to get a flag telling if inner ttc is desired.
    
    A pre-step for enabling rss on the vport representors, where
    inner ttc is not needed.
    
    Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
    Reviewed-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index b298456da8e7..275af3bd63b3 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -906,8 +906,8 @@ void mlx5e_close_drop_rq(struct mlx5e_rq *drop_rq);
 
 int mlx5e_create_indirect_rqt(struct mlx5e_priv *priv);
 
-int mlx5e_create_indirect_tirs(struct mlx5e_priv *priv);
-void mlx5e_destroy_indirect_tirs(struct mlx5e_priv *priv);
+int mlx5e_create_indirect_tirs(struct mlx5e_priv *priv, bool inner_ttc);
+void mlx5e_destroy_indirect_tirs(struct mlx5e_priv *priv, bool inner_ttc);
 
 int mlx5e_create_direct_rqts(struct mlx5e_priv *priv);
 void mlx5e_destroy_direct_rqts(struct mlx5e_priv *priv);

commit 749359f4aae624ed8fe7aed80e6b011b6e3db2ef
Author: Gavi Teitz <gavi@mellanox.com>
Date:   Thu Aug 16 14:25:24 2018 +0300

    net/mlx5e: Change VF representors' RQ type
    
    The representors' RQ size was not large enough for them to achieve
    high enough performance, and therefore needed to be enlarged, while
    suffering a minimum hit to its memory usage. To achieve this the
    representors RQ size was increased, and its type was changed to be a
    striding RQ if it is supported.
    
    Towards that goal the following changes were made:
    
    * Extracted the sequence for setting the standard netdev's RQ parmas
      into a function
    
    * Replaced the sequence for setting the representor's RQ params with
      the standard sequence
    
    The impact of this change can be seen in the following measurements
    taken on a setup of a VM over a VF, connected to OVS via the VF
    representor, to an external host:
    
    Before current change:
                         TCP Throughput [Gb/s]
    VM to external host         ~  7.2
    
    With the current change (measured with a striding RQ):
                         TCP Throughput [Gb/s]
    VM to external host         ~ 23.5
    
    Each representor now consumes 2 [MB] of memory for its packet
    buffers.
    
    Signed-off-by: Gavi Teitz <gavi@mellanox.com>
    Reviewed-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 01a967e717e7..b298456da8e7 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -966,6 +966,8 @@ void mlx5e_destroy_netdev(struct mlx5e_priv *priv);
 void mlx5e_build_nic_params(struct mlx5_core_dev *mdev,
 			    struct mlx5e_params *params,
 			    u16 max_channels, u16 mtu);
+void mlx5e_build_rq_params(struct mlx5_core_dev *mdev,
+			   struct mlx5e_params *params);
 u8 mlx5e_params_calculate_tx_min_inline(struct mlx5_core_dev *mdev);
 void mlx5e_rx_dim_work(struct work_struct *work);
 void mlx5e_tx_dim_work(struct work_struct *work);

commit 4d8fcf216c90bc25e34ae2200aa8985ee3158898
Author: Alaa Hleihel <alaa@mellanox.com>
Date:   Wed Sep 5 11:43:23 2018 +0300

    net/mlx5e: Avoid unbounded peer devices when unpairing TC hairpin rules
    
    If the peer device was already unbound, then do not attempt to modify
    it's resources, otherwise we will crash on dereferencing non-existing
    device.
    
    Fixes: 5c65c564c962 ("net/mlx5e: Support offloading TC NIC hairpin flows")
    Signed-off-by: Alaa Hleihel <alaa@mellanox.com>
    Reviewed-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index db2cfcd21d43..0f189f873859 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -54,6 +54,7 @@
 #include "en_stats.h"
 #include "en/fs.h"
 
+extern const struct net_device_ops mlx5e_netdev_ops;
 struct page_pool;
 
 #define MLX5E_METADATA_ETHER_TYPE (0x8CE4)

commit 1462e48db0597a3dec8e73d64f3a001936ec398e
Author: Roi Dayan <roid@mellanox.com>
Date:   Sun Aug 5 06:58:05 2018 +0300

    net/mlx5e: Move Q counters allocation and drop RQ to init_rx
    
    Not all profiles query the HW Q counters in update_stats() callback.
    HW Q couners are limited per device and in case of representors all
    their Q counters are allocated on the parent PF device.
    Avoid reundant allocation of HW Q counters by moving the allocation
    to init_rx profile callback.
    
    Signed-off-by: Roi Dayan <roid@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index de0f7702c86a..01a967e717e7 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -898,6 +898,12 @@ void mlx5e_destroy_mdev_resources(struct mlx5_core_dev *mdev);
 int mlx5e_refresh_tirs(struct mlx5e_priv *priv, bool enable_uc_lb);
 
 /* common netdev helpers */
+void mlx5e_create_q_counters(struct mlx5e_priv *priv);
+void mlx5e_destroy_q_counters(struct mlx5e_priv *priv);
+int mlx5e_open_drop_rq(struct mlx5e_priv *priv,
+		       struct mlx5e_rq *drop_rq);
+void mlx5e_close_drop_rq(struct mlx5e_rq *drop_rq);
+
 int mlx5e_create_indirect_rqt(struct mlx5e_priv *priv);
 
 int mlx5e_create_indirect_tirs(struct mlx5e_priv *priv);

commit d24082050f2f626582902f7c65b91d699ec60613
Author: Kamal Heib <kamalheib1@gmail.com>
Date:   Sun Jul 15 22:06:28 2018 +0300

    net/mlx5e: Move mlx5e_priv_flags into en_ethtool.c
    
    Move the definition of mlx5e_priv_flags into en_ethtool.c because it's
    only used there.
    
    Fixes: 4e59e2888139 ("net/mlx5e: Introduce net device priv flags infrastructure")
    Signed-off-by: Kamal Heib <kamalheib1@gmail.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index db2cfcd21d43..de0f7702c86a 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -204,13 +204,6 @@ struct mlx5e_umr_wqe {
 
 extern const char mlx5e_self_tests[][ETH_GSTRING_LEN];
 
-static const char mlx5e_priv_flags[][ETH_GSTRING_LEN] = {
-	"rx_cqe_moder",
-	"tx_cqe_moder",
-	"rx_cqe_compress",
-	"rx_striding_rq",
-};
-
 enum mlx5e_priv_flag {
 	MLX5E_PFLAG_RX_CQE_BASED_MODER = (1 << 0),
 	MLX5E_PFLAG_TX_CQE_BASED_MODER = (1 << 1),

commit 44f68ae0cad6afbd72d3de1c7b0e149c1655b626
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Thu Jul 12 03:09:19 2018 -0700

    net/mlx5e: Move flow steering declarations into en/fs.h
    
    Move flow steering declarations and definitions into the dedicated
    en/fs.h header file
    
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Reviewed-by: Moshe Shemesh <moshe@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 8743bbe1baa2..db2cfcd21d43 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -627,112 +627,12 @@ struct mlx5e_channel_stats {
 	struct mlx5e_xdpsq_stats xdpsq;
 } ____cacheline_aligned_in_smp;
 
-enum mlx5e_traffic_types {
-	MLX5E_TT_IPV4_TCP,
-	MLX5E_TT_IPV6_TCP,
-	MLX5E_TT_IPV4_UDP,
-	MLX5E_TT_IPV6_UDP,
-	MLX5E_TT_IPV4_IPSEC_AH,
-	MLX5E_TT_IPV6_IPSEC_AH,
-	MLX5E_TT_IPV4_IPSEC_ESP,
-	MLX5E_TT_IPV6_IPSEC_ESP,
-	MLX5E_TT_IPV4,
-	MLX5E_TT_IPV6,
-	MLX5E_TT_ANY,
-	MLX5E_NUM_TT,
-	MLX5E_NUM_INDIR_TIRS = MLX5E_TT_ANY,
-};
-
-enum mlx5e_tunnel_types {
-	MLX5E_TT_IPV4_GRE,
-	MLX5E_TT_IPV6_GRE,
-	MLX5E_NUM_TUNNEL_TT,
-};
-
 enum {
 	MLX5E_STATE_ASYNC_EVENTS_ENABLED,
 	MLX5E_STATE_OPENED,
 	MLX5E_STATE_DESTROYING,
 };
 
-struct mlx5e_l2_rule {
-	u8  addr[ETH_ALEN + 2];
-	struct mlx5_flow_handle *rule;
-};
-
-#define MLX5E_L2_ADDR_HASH_SIZE BIT(BITS_PER_BYTE)
-
-struct mlx5e_tc_table {
-	struct mlx5_flow_table		*t;
-
-	struct rhashtable               ht;
-
-	DECLARE_HASHTABLE(mod_hdr_tbl, 8);
-	DECLARE_HASHTABLE(hairpin_tbl, 8);
-};
-
-struct mlx5e_vlan_table {
-	struct mlx5e_flow_table		ft;
-	DECLARE_BITMAP(active_cvlans, VLAN_N_VID);
-	DECLARE_BITMAP(active_svlans, VLAN_N_VID);
-	struct mlx5_flow_handle	*active_cvlans_rule[VLAN_N_VID];
-	struct mlx5_flow_handle	*active_svlans_rule[VLAN_N_VID];
-	struct mlx5_flow_handle	*untagged_rule;
-	struct mlx5_flow_handle	*any_cvlan_rule;
-	struct mlx5_flow_handle	*any_svlan_rule;
-	bool			cvlan_filter_disabled;
-};
-
-struct mlx5e_l2_table {
-	struct mlx5e_flow_table    ft;
-	struct hlist_head          netdev_uc[MLX5E_L2_ADDR_HASH_SIZE];
-	struct hlist_head          netdev_mc[MLX5E_L2_ADDR_HASH_SIZE];
-	struct mlx5e_l2_rule	   broadcast;
-	struct mlx5e_l2_rule	   allmulti;
-	struct mlx5e_l2_rule	   promisc;
-	bool                       broadcast_enabled;
-	bool                       allmulti_enabled;
-	bool                       promisc_enabled;
-};
-
-/* L3/L4 traffic type classifier */
-struct mlx5e_ttc_table {
-	struct mlx5e_flow_table  ft;
-	struct mlx5_flow_handle	 *rules[MLX5E_NUM_TT];
-	struct mlx5_flow_handle  *tunnel_rules[MLX5E_NUM_TUNNEL_TT];
-};
-
-/* NIC prio FTS */
-enum {
-	MLX5E_VLAN_FT_LEVEL = 0,
-	MLX5E_L2_FT_LEVEL,
-	MLX5E_TTC_FT_LEVEL,
-	MLX5E_INNER_TTC_FT_LEVEL,
-#ifdef CONFIG_MLX5_EN_ARFS
-	MLX5E_ARFS_FT_LEVEL
-#endif
-};
-
-enum {
-	MLX5E_TC_FT_LEVEL = 0,
-	MLX5E_TC_TTC_FT_LEVEL,
-};
-
-struct mlx5e_flow_steering {
-	struct mlx5_flow_namespace      *ns;
-#ifdef CONFIG_MLX5_EN_RXNFC
-	struct mlx5e_ethtool_steering   ethtool;
-#endif
-	struct mlx5e_tc_table           tc;
-	struct mlx5e_vlan_table         vlan;
-	struct mlx5e_l2_table           l2;
-	struct mlx5e_ttc_table          ttc;
-	struct mlx5e_ttc_table          inner_ttc;
-#ifdef CONFIG_MLX5_EN_ARFS
-	struct mlx5e_arfs_tables        arfs;
-#endif
-};
-
 struct mlx5e_rqt {
 	u32              rqtn;
 	bool		 enabled;
@@ -866,10 +766,7 @@ mlx5e_skb_from_cqe_nonlinear(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe,
 
 void mlx5e_update_stats(struct mlx5e_priv *priv);
 
-int mlx5e_create_flow_steering(struct mlx5e_priv *priv);
-void mlx5e_destroy_flow_steering(struct mlx5e_priv *priv);
 void mlx5e_init_l2_addr(struct mlx5e_priv *priv);
-void mlx5e_destroy_flow_table(struct mlx5e_flow_table *ft);
 int mlx5e_self_test_num(struct mlx5e_priv *priv);
 void mlx5e_self_test(struct net_device *ndev, struct ethtool_test *etest,
 		     u64 *buf);
@@ -883,8 +780,6 @@ int mlx5e_vlan_rx_add_vid(struct net_device *dev, __always_unused __be16 proto,
 			  u16 vid);
 int mlx5e_vlan_rx_kill_vid(struct net_device *dev, __always_unused __be16 proto,
 			   u16 vid);
-void mlx5e_enable_cvlan_filter(struct mlx5e_priv *priv);
-void mlx5e_disable_cvlan_filter(struct mlx5e_priv *priv);
 void mlx5e_timestamp_init(struct mlx5e_priv *priv);
 
 struct mlx5e_redirect_rqt_param {
@@ -1021,27 +916,6 @@ int mlx5e_create_direct_tirs(struct mlx5e_priv *priv);
 void mlx5e_destroy_direct_tirs(struct mlx5e_priv *priv);
 void mlx5e_destroy_rqt(struct mlx5e_priv *priv, struct mlx5e_rqt *rqt);
 
-struct ttc_params {
-	struct mlx5_flow_table_attr ft_attr;
-	u32 any_tt_tirn;
-	u32 indir_tirn[MLX5E_NUM_INDIR_TIRS];
-	struct mlx5e_ttc_table *inner_ttc;
-};
-
-void mlx5e_set_ttc_basic_params(struct mlx5e_priv *priv, struct ttc_params *ttc_params);
-void mlx5e_set_ttc_ft_params(struct ttc_params *ttc_params);
-void mlx5e_set_inner_ttc_ft_params(struct ttc_params *ttc_params);
-
-int mlx5e_create_ttc_table(struct mlx5e_priv *priv, struct ttc_params *params,
-			   struct mlx5e_ttc_table *ttc);
-void mlx5e_destroy_ttc_table(struct mlx5e_priv *priv,
-			     struct mlx5e_ttc_table *ttc);
-
-int mlx5e_create_inner_ttc_table(struct mlx5e_priv *priv, struct ttc_params *params,
-				 struct mlx5e_ttc_table *ttc);
-void mlx5e_destroy_inner_ttc_table(struct mlx5e_priv *priv,
-				   struct mlx5e_ttc_table *ttc);
-
 int mlx5e_create_tis(struct mlx5_core_dev *mdev, int tc,
 		     u32 underlay_qpn, u32 *tisn);
 void mlx5e_destroy_tis(struct mlx5_core_dev *mdev, u32 tisn);

commit ec080045977073c5e69a7fddee3a8aef9bf62881
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Thu Jul 12 03:01:26 2018 -0700

    net/mlx5e: Add CONFIG_MLX5_EN_ARFS for accelerated flow steering support
    
    Add new mlx5 Kconfig flag to allow selecting accelerated flow steering
    support, and compile out en_arfs.c if not selected.
    
    Move arfs declarations and definitions to en/fs.h header file.
    
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Reviewed-by: Moshe Shemesh <moshe@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 19728f9f25e7..8743bbe1baa2 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -660,12 +660,6 @@ struct mlx5e_l2_rule {
 	struct mlx5_flow_handle *rule;
 };
 
-struct mlx5e_flow_table {
-	int num_groups;
-	struct mlx5_flow_table *t;
-	struct mlx5_flow_group **g;
-};
-
 #define MLX5E_L2_ADDR_HASH_SIZE BIT(BITS_PER_BYTE)
 
 struct mlx5e_tc_table {
@@ -708,38 +702,15 @@ struct mlx5e_ttc_table {
 	struct mlx5_flow_handle  *tunnel_rules[MLX5E_NUM_TUNNEL_TT];
 };
 
-#define ARFS_HASH_SHIFT BITS_PER_BYTE
-#define ARFS_HASH_SIZE BIT(BITS_PER_BYTE)
-struct arfs_table {
-	struct mlx5e_flow_table  ft;
-	struct mlx5_flow_handle	 *default_rule;
-	struct hlist_head	 rules_hash[ARFS_HASH_SIZE];
-};
-
-enum  arfs_type {
-	ARFS_IPV4_TCP,
-	ARFS_IPV6_TCP,
-	ARFS_IPV4_UDP,
-	ARFS_IPV6_UDP,
-	ARFS_NUM_TYPES,
-};
-
-struct mlx5e_arfs_tables {
-	struct arfs_table arfs_tables[ARFS_NUM_TYPES];
-	/* Protect aRFS rules list */
-	spinlock_t                     arfs_lock;
-	struct list_head               rules;
-	int                            last_filter_id;
-	struct workqueue_struct        *wq;
-};
-
 /* NIC prio FTS */
 enum {
 	MLX5E_VLAN_FT_LEVEL = 0,
 	MLX5E_L2_FT_LEVEL,
 	MLX5E_TTC_FT_LEVEL,
 	MLX5E_INNER_TTC_FT_LEVEL,
+#ifdef CONFIG_MLX5_EN_ARFS
 	MLX5E_ARFS_FT_LEVEL
+#endif
 };
 
 enum {
@@ -757,7 +728,9 @@ struct mlx5e_flow_steering {
 	struct mlx5e_l2_table           l2;
 	struct mlx5e_ttc_table          ttc;
 	struct mlx5e_ttc_table          inner_ttc;
+#ifdef CONFIG_MLX5_EN_ARFS
 	struct mlx5e_arfs_tables        arfs;
+#endif
 };
 
 struct mlx5e_rqt {
@@ -1028,32 +1001,6 @@ void mlx5e_dcbnl_init_app(struct mlx5e_priv *priv);
 void mlx5e_dcbnl_delete_app(struct mlx5e_priv *priv);
 #endif
 
-#ifndef CONFIG_RFS_ACCEL
-static inline int mlx5e_arfs_create_tables(struct mlx5e_priv *priv)
-{
-	return 0;
-}
-
-static inline void mlx5e_arfs_destroy_tables(struct mlx5e_priv *priv) {}
-
-static inline int mlx5e_arfs_enable(struct mlx5e_priv *priv)
-{
-	return -EOPNOTSUPP;
-}
-
-static inline int mlx5e_arfs_disable(struct mlx5e_priv *priv)
-{
-	return -EOPNOTSUPP;
-}
-#else
-int mlx5e_arfs_create_tables(struct mlx5e_priv *priv);
-void mlx5e_arfs_destroy_tables(struct mlx5e_priv *priv);
-int mlx5e_arfs_enable(struct mlx5e_priv *priv);
-int mlx5e_arfs_disable(struct mlx5e_priv *priv);
-int mlx5e_rx_flow_steer(struct net_device *dev, const struct sk_buff *skb,
-			u16 rxq_index, u32 flow_id);
-#endif
-
 int mlx5e_create_tir(struct mlx5_core_dev *mdev,
 		     struct mlx5e_tir *tir, u32 *in, int inlen);
 void mlx5e_destroy_tir(struct mlx5_core_dev *mdev,

commit fe6d86b3c3165e6c55d6b0049a13d3b65371163a
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Wed Jul 11 12:02:42 2018 -0700

    net/mlx5e: Add CONFIG_MLX5_EN_RXNFC for ethtool rx nfc
    
    Add new mlx5 Kconfig flag to allow selecting ethtool rx nfc support,
    and compile out en_fs_ehtool.c if not selected.
    
    Add en/fs.h header file to host all steering declarations and
    definitions.
    
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Reviewed-by: Moshe Shemesh <moshe@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 31a29b73f558..19728f9f25e7 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -52,6 +52,7 @@
 #include "wq.h"
 #include "mlx5_core.h"
 #include "en_stats.h"
+#include "en/fs.h"
 
 struct page_pool;
 
@@ -746,24 +747,11 @@ enum {
 	MLX5E_TC_TTC_FT_LEVEL,
 };
 
-struct mlx5e_ethtool_table {
-	struct mlx5_flow_table *ft;
-	int                    num_rules;
-};
-
-#define ETHTOOL_NUM_L3_L4_FTS 7
-#define ETHTOOL_NUM_L2_FTS 4
-
-struct mlx5e_ethtool_steering {
-	struct mlx5e_ethtool_table      l3_l4_ft[ETHTOOL_NUM_L3_L4_FTS];
-	struct mlx5e_ethtool_table      l2_ft[ETHTOOL_NUM_L2_FTS];
-	struct list_head                rules;
-	int                             tot_num_rules;
-};
-
 struct mlx5e_flow_steering {
 	struct mlx5_flow_namespace      *ns;
+#ifdef CONFIG_MLX5_EN_RXNFC
 	struct mlx5e_ethtool_steering   ethtool;
+#endif
 	struct mlx5e_tc_table           tc;
 	struct mlx5e_vlan_table         vlan;
 	struct mlx5e_l2_table           l2;
@@ -912,11 +900,6 @@ void mlx5e_destroy_flow_table(struct mlx5e_flow_table *ft);
 int mlx5e_self_test_num(struct mlx5e_priv *priv);
 void mlx5e_self_test(struct net_device *ndev, struct ethtool_test *etest,
 		     u64 *buf);
-void mlx5e_ethtool_init_steering(struct mlx5e_priv *priv);
-void mlx5e_ethtool_cleanup_steering(struct mlx5e_priv *priv);
-int mlx5e_set_rxnfc(struct net_device *dev, struct ethtool_rxnfc *cmd);
-int mlx5e_get_rxnfc(struct net_device *dev,
-		    struct ethtool_rxnfc *info, u32 *rule_locs);
 void mlx5e_set_rx_mode_work(struct work_struct *work);
 
 int mlx5e_hwstamp_set(struct mlx5e_priv *priv, struct ifreq *ifr);

commit cff2b1e3e83286e569466663b35bf0ae23afcbcf
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Tue Jul 10 17:04:49 2018 -0700

    net/mlx5e: Ethtool steering, move ethtool callbacks
    
    Move ethool rxnfc callback into en_fs_etthool file where they belong.
    This will allow us to make many ethtool fs related helper functions
    static.
    
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index dddd29a3be97..31a29b73f558 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -912,16 +912,11 @@ void mlx5e_destroy_flow_table(struct mlx5e_flow_table *ft);
 int mlx5e_self_test_num(struct mlx5e_priv *priv);
 void mlx5e_self_test(struct net_device *ndev, struct ethtool_test *etest,
 		     u64 *buf);
-int mlx5e_ethtool_get_flow(struct mlx5e_priv *priv, struct ethtool_rxnfc *info,
-			   int location);
-int mlx5e_ethtool_get_all_flows(struct mlx5e_priv *priv,
-				struct ethtool_rxnfc *info, u32 *rule_locs);
-int mlx5e_ethtool_flow_replace(struct mlx5e_priv *priv,
-			       struct ethtool_rx_flow_spec *fs);
-int mlx5e_ethtool_flow_remove(struct mlx5e_priv *priv,
-			      int location);
 void mlx5e_ethtool_init_steering(struct mlx5e_priv *priv);
 void mlx5e_ethtool_cleanup_steering(struct mlx5e_priv *priv);
+int mlx5e_set_rxnfc(struct net_device *dev, struct ethtool_rxnfc *cmd);
+int mlx5e_get_rxnfc(struct net_device *dev,
+		    struct ethtool_rxnfc *info, u32 *rule_locs);
 void mlx5e_set_rx_mode_work(struct work_struct *work);
 
 int mlx5e_hwstamp_set(struct mlx5e_priv *priv, struct ifreq *ifr);

commit a736e074680745faa5dc6be8dd3c58ad4850aab9
Merge: 192e91d244e3 112cbae26d18
Author: David S. Miller <davem@davemloft.net>
Date:   Thu Aug 9 11:52:36 2018 -0700

    Merge ra.kernel.org:/pub/scm/linux/kernel/git/davem/net
    
    Overlapping changes in RXRPC, changing to ktime_get_seconds() whilst
    adding some tracepoints.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit f280c6a1e548cd3223a56bb480454ffb96050e87
Author: Huy Nguyen <huyn@mellanox.com>
Date:   Wed Aug 8 15:48:08 2018 -0700

    net/mlx5e: Cleanup of dcbnl related fields
    
    Remove unused netdev_registered_init/remove in en.h
    Return ENOSUPPORT if the check MLX5_DSCP_SUPPORTED fails.
    Remove extra white space
    
    Fixes: 2a5e7a1344f4 ("net/mlx5e: Add dcbnl dscp to priority support")
    Signed-off-by: Huy Nguyen <huyn@mellanox.com>
    Cc: Yuval Shaia <yuval.shaia@oracle.com>
    Reviewed-by: Parav Pandit <parav@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index eb9eb7aa953a..405236cf0b04 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -858,8 +858,6 @@ struct mlx5e_profile {
 		mlx5e_fp_handle_rx_cqe handle_rx_cqe;
 		mlx5e_fp_handle_rx_cqe handle_rx_cqe_mpwqe;
 	} rx_handlers;
-	void	(*netdev_registered_init)(struct mlx5e_priv *priv);
-	void    (*netdev_registered_remove)(struct mlx5e_priv *priv);
 	int	max_tc;
 };
 

commit 358aa5ce288aa1085f0f3ef9f315119563fa6541
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Wed May 9 13:28:00 2018 -0700

    net/mlx5e: Vxlan, move vxlan logic to core driver
    
    Move vxlan logic and objects to mlx5 core dirver.
    Since it going to be used from different mlx5 interfaces.
    e.g. mlx5e PF NIC netdev and mlx5e E-Switch representors.
    
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Reviewed-by: Or Gerlitz <ogerlitz@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 1bd4536b9061..c7ed3d20fd54 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -52,7 +52,6 @@
 #include "wq.h"
 #include "mlx5_core.h"
 #include "en_stats.h"
-#include "vxlan.h"
 
 struct page_pool;
 
@@ -812,7 +811,6 @@ struct mlx5e_priv {
 	u32                        tx_rates[MLX5E_MAX_NUM_SQS];
 
 	struct mlx5e_flow_steering fs;
-	struct mlx5_vxlan          *vxlan;
 
 	struct workqueue_struct    *wq;
 	struct work_struct         update_carrier_work;

commit a3c785d73cf280a023684dc6c7bbeff25b8b8163
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Tue May 8 02:51:23 2018 -0700

    net/mlx5e: Vxlan, rename from mlx5e to mlx5
    
    Rename vxlan functions from mlx5e_vxlan_* to mlx5_vxlan_*.
    Rename mlx5e_vxlan_db to mlx5_vxlan and move it from en.h to vxlan.c
    since it is not related to mlx5e anymore.
    
    Allocate mlx5_vxlan structure dynamically in order to make it easier to
    move later to core driver and to make it private in vxlan.c.
    
    This is in preparation to move vxlan API to mlx5 core.
    
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Reviewed-by: Or Gerlitz <ogerlitz@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 6878925c3abf..1bd4536b9061 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -52,6 +52,7 @@
 #include "wq.h"
 #include "mlx5_core.h"
 #include "en_stats.h"
+#include "vxlan.h"
 
 struct page_pool;
 
@@ -654,13 +655,6 @@ enum {
 	MLX5E_STATE_DESTROYING,
 };
 
-struct mlx5e_vxlan_db {
-	spinlock_t			lock; /* protect vxlan table */
-	/* max_num_ports is usuallly 4, 16 buckets is more than enough */
-	DECLARE_HASHTABLE(htable, 4);
-	int				num_ports;
-};
-
 struct mlx5e_l2_rule {
 	u8  addr[ETH_ALEN + 2];
 	struct mlx5_flow_handle *rule;
@@ -818,7 +812,7 @@ struct mlx5e_priv {
 	u32                        tx_rates[MLX5E_MAX_NUM_SQS];
 
 	struct mlx5e_flow_steering fs;
-	struct mlx5e_vxlan_db      vxlan;
+	struct mlx5_vxlan          *vxlan;
 
 	struct workqueue_struct    *wq;
 	struct work_struct         update_carrier_work;

commit d30d8cde19726f177be7615f2e12700d994f6d7f
Author: Gal Pressman <galp@mellanox.com>
Date:   Tue Dec 26 18:27:08 2017 +0200

    net/mlx5e: Vxlan, replace ports radix-tree with hash table
    
    The VXLAN database is accessed in the data path for each VXLAN TX skb in
    order to check whether the UDP port is being offloaded or not.
    The number of elements in the database is relatively small, we can
    simplify the radix-tree to a hash table and speedup the lookup process.
    
    Measuring mlx5e_vxlan_lookup_port execution time:
    
                      Radix Tree   Hash Table
     --------------- ------------ ------------
      Single Stream   161 ns       79  ns (51% improvement)
      Multi Stream    259 ns       136 ns (47% improvement)
    
    Measuring UDP stream packet rate, single fully utilized TX core:
    Radix Tree: 498,300 PPS
    Hash Table: 555,468 PPS (11% improvement)
    
    Signed-off-by: Gal Pressman <galp@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index c4d4db8722f5..6878925c3abf 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -656,7 +656,8 @@ enum {
 
 struct mlx5e_vxlan_db {
 	spinlock_t			lock; /* protect vxlan table */
-	struct radix_tree_root		tree;
+	/* max_num_ports is usuallly 4, 16 buckets is more than enough */
+	DECLARE_HASHTABLE(htable, 4);
 	int				num_ports;
 };
 

commit 22a65aa8b1a84ca429c0fc8415dee5681ab36eb3
Author: Gal Pressman <galp@mellanox.com>
Date:   Mon Dec 25 18:40:52 2017 +0200

    net/mlx5e: Vxlan, check maximum number of UDP ports
    
    The NIC has a limited number of offloaded VXLAN UDP ports (usually 4).
    Instead of letting the firmware fail when trying to add more ports than
    it can handle, let the driver check it on its own.
    
    Signed-off-by: Gal Pressman <galp@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index c41cfc2a4b70..c4d4db8722f5 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -657,6 +657,7 @@ enum {
 struct mlx5e_vxlan_db {
 	spinlock_t			lock; /* protect vxlan table */
 	struct radix_tree_root		tree;
+	int				num_ports;
 };
 
 struct mlx5e_l2_rule {

commit 9a3956da1c0a59b5853594d9a498289d9773df63
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Tue May 22 17:06:38 2018 +0300

    net/mlx5e: TX, Move DB fields in TXQ-SQ struct
    
    Pointers in DB are static, move them to read-only area so they
    do not share a cacheline with fields modified in datapath.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 3f21cafe6be3..c41cfc2a4b70 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -365,16 +365,14 @@ struct mlx5e_txqsq {
 
 	struct mlx5e_cq            cq;
 
-	/* write@xmit, read@completion */
-	struct {
-		struct mlx5e_sq_dma       *dma_fifo;
-		struct mlx5e_tx_wqe_info  *wqe_info;
-	} db;
-
 	/* read only */
 	struct mlx5_wq_cyc         wq;
 	u32                        dma_fifo_mask;
 	struct mlx5e_sq_stats     *stats;
+	struct {
+		struct mlx5e_sq_dma       *dma_fifo;
+		struct mlx5e_tx_wqe_info  *wqe_info;
+	} db;
 	void __iomem              *uar_map;
 	struct netdev_queue       *txq;
 	u32                        sqn;

commit 58b99ee3e3ebecfaccc5641a4014d92a818494a5
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Tue May 22 16:48:48 2018 +0300

    net/mlx5e: Add support for XDP_REDIRECT in device-out side
    
    Add implementation for the ndo_xdp_xmit callback.
    
    Dedicate a new set of XDP-SQ instances to satisfy the XDP_REDIRECT
    requests.  These instances are totally separated from the existing
    XDP-SQ objects that satisfy local XDP_TX actions.
    
    Performance tests:
    
    xdp_redirect_map from ConnectX-5 to ConnectX-5.
    CPU: Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz
    Packet-rate of 64B packets.
    
    Single queue: 7 Mpps.
    Multi queue: 55 Mpps.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Eugenia Emantayev <eugenia@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 16e219a8f240..3f21cafe6be3 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -344,6 +344,7 @@ enum {
 	MLX5E_SQ_STATE_IPSEC,
 	MLX5E_SQ_STATE_AM,
 	MLX5E_SQ_STATE_TLS,
+	MLX5E_SQ_STATE_REDIRECT,
 };
 
 struct mlx5e_sq_wqe_info {
@@ -598,6 +599,9 @@ struct mlx5e_channel {
 	__be32                     mkey_be;
 	u8                         num_tc;
 
+	/* XDP_REDIRECT */
+	struct mlx5e_xdpsq         xdpsq;
+
 	/* data path - accessed per napi poll */
 	struct irq_desc *irq_desc;
 	struct mlx5e_ch_stats     *stats;
@@ -621,6 +625,7 @@ struct mlx5e_channel_stats {
 	struct mlx5e_sq_stats sq[MLX5E_MAX_NUM_TC];
 	struct mlx5e_rq_stats rq;
 	struct mlx5e_xdpsq_stats rq_xdpsq;
+	struct mlx5e_xdpsq_stats xdpsq;
 } ____cacheline_aligned_in_smp;
 
 enum mlx5e_traffic_types {

commit dac0d15fff6f825e93aafa6ba8bf24fd77ecb2ae
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Tue May 22 16:43:54 2018 +0300

    net/mlx5e: Re-order fields of struct mlx5e_xdpsq
    
    In the downstream patch that adds support to XDP_REDIRECT-out,
    the XDP xmit frame function doesn't share the same run context as
    the NAPI that polls the XDP-SQ completion queue.
    
    Hence, need to re-order the XDP-SQ fields to avoid cacheline
    false-sharing.
    
    Take redirect_flush and doorbell out of DB, into separated
    cachelines.
    
    Add a cacheline breaker within the stats struct.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index d9e24fbe6a28..16e219a8f240 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -409,22 +409,22 @@ struct mlx5e_xdp_info {
 struct mlx5e_xdpsq {
 	/* data path */
 
-	/* dirtied @rx completion */
+	/* dirtied @completion */
 	u16                        cc;
-	u16                        pc;
+	bool                       redirect_flush;
 
-	struct mlx5e_cq            cq;
+	/* dirtied @xmit */
+	u16                        pc ____cacheline_aligned_in_smp;
+	bool                       doorbell;
 
-	/* write@xmit, read@completion */
-	struct {
-		struct mlx5e_xdp_info     *xdpi;
-		bool                       doorbell;
-		bool                       redirect_flush;
-	} db;
+	struct mlx5e_cq            cq;
 
 	/* read only */
 	struct mlx5_wq_cyc         wq;
 	struct mlx5e_xdpsq_stats  *stats;
+	struct {
+		struct mlx5e_xdp_info     *xdpi;
+	} db;
 	void __iomem              *uar_map;
 	u32                        sqn;
 	struct device             *pdev;

commit 890388ad6f6b3740265173fa5296ece5d945e977
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Tue May 22 16:29:31 2018 +0300

    net/mlx5e: Refactor XDP counters
    
    Separate the XDP counters into two sets:
    (1) One set reside in the RQ stats, and they monitor XDP stats
    in the RQ side.
    (2) Another set is per XDP-SQ, and they monitor XDP stats that
    are related to XDP transmit flow.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Eugenia Emantayev <eugenia@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 118d66207079..d9e24fbe6a28 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -424,6 +424,7 @@ struct mlx5e_xdpsq {
 
 	/* read only */
 	struct mlx5_wq_cyc         wq;
+	struct mlx5e_xdpsq_stats  *stats;
 	void __iomem              *uar_map;
 	u32                        sqn;
 	struct device             *pdev;
@@ -619,6 +620,7 @@ struct mlx5e_channel_stats {
 	struct mlx5e_ch_stats ch;
 	struct mlx5e_sq_stats sq[MLX5E_MAX_NUM_TC];
 	struct mlx5e_rq_stats rq;
+	struct mlx5e_xdpsq_stats rq_xdpsq;
 } ____cacheline_aligned_in_smp;
 
 enum mlx5e_traffic_types {

commit c94e4f117e473dec11c7b9395b4d88cae2ba27c9
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Sun Jul 15 10:34:39 2018 +0300

    net/mlx5e: Make XDP xmit functions more generic
    
    Convert the XDP xmit functions to use the generic xdp_frame API
    in XDP_TX flow.
    Same functions will be used later in this series to transmit
    the XDP redirect-out packets as well.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Eugenia Emantayev <eugenia@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 2f1058da0907..118d66207079 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -395,6 +395,17 @@ struct mlx5e_txqsq {
 	} recover;
 } ____cacheline_aligned_in_smp;
 
+struct mlx5e_dma_info {
+	struct page     *page;
+	dma_addr_t      addr;
+};
+
+struct mlx5e_xdp_info {
+	struct xdp_frame      *xdpf;
+	dma_addr_t            dma_addr;
+	struct mlx5e_dma_info di;
+};
+
 struct mlx5e_xdpsq {
 	/* data path */
 
@@ -406,7 +417,7 @@ struct mlx5e_xdpsq {
 
 	/* write@xmit, read@completion */
 	struct {
-		struct mlx5e_dma_info     *di;
+		struct mlx5e_xdp_info     *xdpi;
 		bool                       doorbell;
 		bool                       redirect_flush;
 	} db;
@@ -419,6 +430,7 @@ struct mlx5e_xdpsq {
 	__be32                     mkey_be;
 	u8                         min_inline_mode;
 	unsigned long              state;
+	unsigned int               hw_mtu;
 
 	/* control path */
 	struct mlx5_wq_ctrl        wq_ctrl;
@@ -455,11 +467,6 @@ mlx5e_wqc_has_room_for(struct mlx5_wq_cyc *wq, u16 cc, u16 pc, u16 n)
 	return (mlx5_wq_cyc_ctr2ix(wq, cc - pc) >= n) || (cc == pc);
 }
 
-struct mlx5e_dma_info {
-	struct page	*page;
-	dma_addr_t	addr;
-};
-
 struct mlx5e_wqe_frag_info {
 	struct mlx5e_dma_info *di;
 	u32 offset;
@@ -562,7 +569,6 @@ struct mlx5e_rq {
 
 	/* XDP */
 	struct bpf_prog       *xdp_prog;
-	unsigned int           hw_mtu;
 	struct mlx5e_xdpsq     xdpsq;
 	DECLARE_BITMAP(flags, 8);
 	struct page_pool      *page_pool;

commit 159d21313423b5ffe301834273cba79e915c65ee
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Sun Jul 15 10:28:44 2018 +0300

    net/mlx5e: Move XDP related code into new XDP files
    
    Take XDP code out of the general EN header and RX file into
    new XDP files.
    
    Currently, XDP-SQ resides only within an RQ and used from a
    single flow (XDP_TX) triggered upon RX completions.
    In a downstream patch, additional type of XDP-SQ instances will be
    presented and used for the XDP_REDIRECT flow, totally unrelated to
    the RX context.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 7840f6f44379..2f1058da0907 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -72,8 +72,6 @@ struct page_pool;
 #define MLX5_RX_HEADROOM NET_SKB_PAD
 #define MLX5_SKB_FRAG_SZ(len)	(SKB_DATA_ALIGN(len) +	\
 				 SKB_DATA_ALIGN(sizeof(struct skb_shared_info)))
-#define MLX5E_XDP_MAX_MTU ((int)(PAGE_SIZE - \
-				 MLX5_SKB_FRAG_SZ(XDP_PACKET_HEADROOM)))
 
 #define MLX5_MPWRQ_MIN_LOG_STRIDE_SZ(mdev) \
 	(6 + MLX5_CAP_GEN(mdev, cache_line_128byte)) /* HW restriction */
@@ -149,10 +147,6 @@ struct page_pool;
 	(DIV_ROUND_UP(MLX5E_UMR_WQE_INLINE_SZ, MLX5_SEND_WQE_BB))
 #define MLX5E_ICOSQ_MAX_WQEBBS MLX5E_UMR_WQEBBS
 
-#define MLX5E_XDP_MIN_INLINE (ETH_HLEN + VLAN_HLEN)
-#define MLX5E_XDP_TX_DS_COUNT \
-	((sizeof(struct mlx5e_tx_wqe) / MLX5_SEND_WQE_DS) + 1 /* SG DS */)
-
 #define MLX5E_NUM_MAIN_GROUPS 9
 
 #define MLX5E_MSG_LEVEL			NETIF_MSG_LINK
@@ -878,14 +872,13 @@ void mlx5e_cq_error_event(struct mlx5_core_cq *mcq, enum mlx5_event event);
 int mlx5e_napi_poll(struct napi_struct *napi, int budget);
 bool mlx5e_poll_tx_cq(struct mlx5e_cq *cq, int napi_budget);
 int mlx5e_poll_rx_cq(struct mlx5e_cq *cq, int budget);
-bool mlx5e_poll_xdpsq_cq(struct mlx5e_cq *cq);
 void mlx5e_free_txqsq_descs(struct mlx5e_txqsq *sq);
-void mlx5e_free_xdpsq_descs(struct mlx5e_xdpsq *sq);
 
 bool mlx5e_check_fragmented_striding_rq_cap(struct mlx5_core_dev *mdev);
 bool mlx5e_striding_rq_possible(struct mlx5_core_dev *mdev,
 				struct mlx5e_params *params);
 
+void mlx5e_page_dma_unmap(struct mlx5e_rq *rq, struct mlx5e_dma_info *dma_info);
 void mlx5e_page_release(struct mlx5e_rq *rq, struct mlx5e_dma_info *dma_info,
 			bool recycle);
 void mlx5e_handle_rx_cqe(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe);

commit a26a5bdf3ee2da419ba2c2baa54f467103dc8cc5
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Sun Dec 31 15:50:13 2017 +0200

    net/mlx5e: Restrict the combination of large MTU and XDP
    
    Add checks in control path upon an MTU change or an XDP program set,
    to prevent reaching cases where large MTU and XDP are set simultaneously.
    
    This is to make sure we allow XDP only with the linear RX memory scheme,
    i.e. a received packet is not scattered to different pages.
    Change mlx5e_rx_get_linear_frag_sz() accordingly, so that we make sure
    the XDP configuration can really be set, instead of assuming that it is.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index dc9aa070eafa..7840f6f44379 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -72,6 +72,8 @@ struct page_pool;
 #define MLX5_RX_HEADROOM NET_SKB_PAD
 #define MLX5_SKB_FRAG_SZ(len)	(SKB_DATA_ALIGN(len) +	\
 				 SKB_DATA_ALIGN(sizeof(struct skb_shared_info)))
+#define MLX5E_XDP_MAX_MTU ((int)(PAGE_SIZE - \
+				 MLX5_SKB_FRAG_SZ(XDP_PACKET_HEADROOM)))
 
 #define MLX5_MPWRQ_MIN_LOG_STRIDE_SZ(mdev) \
 	(6 + MLX5_CAP_GEN(mdev, cache_line_128byte)) /* HW restriction */

commit afab995e06ee1fb76de195a4fba9d03267f8dbe3
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Tue Jun 12 10:08:43 2018 +0300

    net/mlx5e: Replace call to MPWQE free with dealloc in interface down flow
    
    No need to expose the MPWQE free function to control path.
    The dealloc function already exposed, use it.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Reviewed-by: Eran Ben Elisha <eranbe@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index e1b237ccdf56..dc9aa070eafa 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -892,7 +892,6 @@ bool mlx5e_post_rx_wqes(struct mlx5e_rq *rq);
 bool mlx5e_post_rx_mpwqes(struct mlx5e_rq *rq);
 void mlx5e_dealloc_rx_wqe(struct mlx5e_rq *rq, u16 ix);
 void mlx5e_dealloc_rx_mpwqe(struct mlx5e_rq *rq, u16 ix);
-void mlx5e_free_rx_mpwqe(struct mlx5e_rq *rq, struct mlx5e_mpw_info *wi);
 struct sk_buff *
 mlx5e_skb_from_cqe_mpwrq_linear(struct mlx5e_rq *rq, struct mlx5e_mpw_info *wi,
 				u16 cqe_bcnt, u32 head_offset, u32 page_idx);

commit 4f49dec9075aa0277b8c9c657ec31e6361f88724
Author: Alexander Duyck <alexander.h.duyck@intel.com>
Date:   Mon Jul 9 12:19:59 2018 -0400

    net: allow ndo_select_queue to pass netdev
    
    This patch makes it so that instead of passing a void pointer as the
    accel_priv we instead pass a net_device pointer as sb_dev. Making this
    change allows us to pass the subordinate device through to the fallback
    function eventually so that we can keep the actual code in the
    ndo_select_queue call as focused on possible on the exception cases.
    
    Signed-off-by: Alexander Duyck <alexander.h.duyck@intel.com>
    Tested-by: Andrew Bowers <andrewx.bowers@intel.com>
    Signed-off-by: Jeff Kirsher <jeffrey.t.kirsher@intel.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index e2b7586ed7a0..e1b237ccdf56 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -865,7 +865,8 @@ struct mlx5e_profile {
 void mlx5e_build_ptys2ethtool_map(void);
 
 u16 mlx5e_select_queue(struct net_device *dev, struct sk_buff *skb,
-		       void *accel_priv, select_queue_fallback_t fallback);
+		       struct net_device *sb_dev,
+		       select_queue_fallback_t fallback);
 netdev_tx_t mlx5e_xmit(struct sk_buff *skb, struct net_device *dev);
 netdev_tx_t mlx5e_sq_xmit(struct mlx5e_txqsq *sq, struct sk_buff *skb,
 			  struct mlx5e_tx_wqe *wqe, u16 pi);

commit ed56c5193ad89d1097cdbdc87abeb062e03a06eb
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Wed May 23 18:26:09 2018 -0700

    net/mlx5e: Update NIC HW stats on demand only
    
    Disable periodic stats update background thread and update stats in
    background on demand when ndo_get_stats is called.
    
    Having a background thread running in the driver all the time is bad for
    power consumption and normally a user space daemon will query the stats
    once every specific interval, so ideally the background thread and its
    interval can be done in user space..
    
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Reviewed-by: Eran Ben Elisha <eranbe@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index eb9eb7aa953a..e2b7586ed7a0 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -137,7 +137,6 @@ struct page_pool;
 #define MLX5E_MAX_NUM_CHANNELS         (MLX5E_INDIR_RQT_SIZE >> 1)
 #define MLX5E_MAX_NUM_SQS              (MLX5E_MAX_NUM_CHANNELS * MLX5E_MAX_NUM_TC)
 #define MLX5E_TX_CQ_POLL_BUDGET        128
-#define MLX5E_UPDATE_STATS_INTERVAL    200 /* msecs */
 #define MLX5E_SQ_RECOVER_MIN_INTERVAL  500 /* msecs */
 
 #define MLX5E_UMR_WQE_INLINE_SZ \

commit 069d11465a802a2b58ce530312b7bb6e1e0b61a9
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Wed May 2 18:23:58 2018 +0300

    net/mlx5e: RX, Enhance legacy Receive Queue memory scheme
    
    Enhance the memory scheme of the legacy RQ, such that
    only order-0 pages are used.
    
    Whenever possible, prefer using a linear SKB, and build it
    wrapping the WQE buffer.
    
    Otherwise (for example, jumbo frames on x86), use non-linear SKB,
    with as many frags as needed. In this case, multiple WQE
    scatter entries are used, up to a maximum of 4 frags and 10KB of MTU.
    
    This implied to remove support of HW LRO in legacy RQ, as it would
    require large number of page allocations and scatter entries per WQE
    on archs with PAGE_SIZE = 4KB, yielding bad performance.
    
    In earlier patches, we guaranteed that all completions are in-order,
    and that we use a cyclic WQ.
    This creates an oppurtunity for a performance optimization:
    The mapping between a "struct mlx5e_dma_info", and the
    WQEs (struct mlx5e_wqe_frag_info) pointing to it, is constant
    across different cycles of a WQ. This allows initializing
    the mapping in the time of RQ creation, and not handle it
    in datapath.
    
    A struct mlx5e_dma_info that is shared between different WQEs
    is allocated by the first WQE, and freed by the last one.
    This implies an important requirement: WQEs that share the same
    struct mlx5e_dma_info must be posted within the same NAPI.
    Otherwise, upon completion, struct mlx5e_wqe_frag_info would mistakenly
    point to the new struct mlx5e_dma_info, not the one that was posted
    (and the HW wrote to).
    This bulking requirement is actually good also for performance reasons,
    hence we extend the bulk beyong the minimal requirement above.
    
    With this memory scheme, the RQs memory footprint is reduce by a
    factor of 2 on x86, and by a factor of 32 on PowerPC.
    Same factors apply for the number of pages in a GRO session.
    
    Performance tests:
    ConnectX-4, single core, single RX ring, default MTU.
    
    x86:
    CPU: Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz
    
    Packet rate (early drop in TC): no degradation
    TCP streams: ~5% improvement
    
    PowerPC:
    CPU: POWER8 (raw), altivec supported
    
    Packet rate (early drop in TC): 20% gain
    TCP streams: 25% gain
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index af521dd52993..eb9eb7aa953a 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -101,11 +101,15 @@ struct page_pool;
 	(MLX5E_PARAMS_MAXIMUM_LOG_RQ_SIZE_MPW + \
 	 (MLX5_MPWRQ_LOG_WQE_SZ - MLX5E_ORDER2_MAX_PACKET_MTU))
 
+#define MLX5E_MIN_SKB_FRAG_SZ		(MLX5_SKB_FRAG_SZ(MLX5_RX_HEADROOM))
+#define MLX5E_LOG_MAX_RX_WQE_BULK	\
+	(ilog2(PAGE_SIZE / roundup_pow_of_two(MLX5E_MIN_SKB_FRAG_SZ)))
+
 #define MLX5E_PARAMS_MINIMUM_LOG_SQ_SIZE                0x6
 #define MLX5E_PARAMS_DEFAULT_LOG_SQ_SIZE                0xa
 #define MLX5E_PARAMS_MAXIMUM_LOG_SQ_SIZE                0xd
 
-#define MLX5E_PARAMS_MINIMUM_LOG_RQ_SIZE                0x1
+#define MLX5E_PARAMS_MINIMUM_LOG_RQ_SIZE (1 + MLX5E_LOG_MAX_RX_WQE_BULK)
 #define MLX5E_PARAMS_DEFAULT_LOG_RQ_SIZE                0xa
 #define MLX5E_PARAMS_MAXIMUM_LOG_RQ_SIZE min_t(u8, 0xd,	\
 					       MLX5E_LOG_MAX_RQ_NUM_PACKETS_MPW)
@@ -462,8 +466,9 @@ struct mlx5e_dma_info {
 };
 
 struct mlx5e_wqe_frag_info {
-	struct mlx5e_dma_info di;
+	struct mlx5e_dma_info *di;
 	u32 offset;
+	bool last_in_page;
 };
 
 struct mlx5e_umr_dma_info {
@@ -476,6 +481,8 @@ struct mlx5e_mpw_info {
 	DECLARE_BITMAP(xdp_xmit_bitmap, MLX5_MPWRQ_PAGES_PER_WQE);
 };
 
+#define MLX5E_MAX_RX_FRAGS 4
+
 /* a single cache unit is capable to serve one napi call (for non-striding rq)
  * or a MPWQE (for striding rq).
  */
@@ -493,6 +500,9 @@ typedef void (*mlx5e_fp_handle_rx_cqe)(struct mlx5e_rq*, struct mlx5_cqe64*);
 typedef struct sk_buff *
 (*mlx5e_fp_skb_from_cqe_mpwrq)(struct mlx5e_rq *rq, struct mlx5e_mpw_info *wi,
 			       u16 cqe_bcnt, u32 head_offset, u32 page_idx);
+typedef struct sk_buff *
+(*mlx5e_fp_skb_from_cqe)(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe,
+			 struct mlx5e_wqe_frag_info *wi, u32 cqe_bcnt);
 typedef bool (*mlx5e_fp_post_rx_wqes)(struct mlx5e_rq *rq);
 typedef void (*mlx5e_fp_dealloc_wqe)(struct mlx5e_rq*, u16);
 
@@ -500,16 +510,27 @@ enum mlx5e_rq_flag {
 	MLX5E_RQ_FLAG_XDP_XMIT = BIT(0),
 };
 
+struct mlx5e_rq_frag_info {
+	int frag_size;
+	int frag_stride;
+};
+
+struct mlx5e_rq_frags_info {
+	struct mlx5e_rq_frag_info arr[MLX5E_MAX_RX_FRAGS];
+	u8 num_frags;
+	u8 log_num_frags;
+	u8 wqe_bulk;
+};
+
 struct mlx5e_rq {
 	/* data path */
 	union {
 		struct {
-			struct mlx5_wq_cyc     wq;
-			struct mlx5e_wqe_frag_info *frag_info;
-			u32 frag_sz;	/* max possible skb frag_sz */
-			union {
-				bool page_reuse;
-			};
+			struct mlx5_wq_cyc          wq;
+			struct mlx5e_wqe_frag_info *frags;
+			struct mlx5e_dma_info      *di;
+			struct mlx5e_rq_frags_info  info;
+			mlx5e_fp_skb_from_cqe       skb_from_cqe;
 		} wqe;
 		struct {
 			struct mlx5_wq_ll      wq;
@@ -523,7 +544,6 @@ struct mlx5e_rq {
 	};
 	struct {
 		u16            headroom;
-		u8             page_order;
 		u8             map_dir;   /* dma map direction */
 	} buff;
 
@@ -879,6 +899,12 @@ mlx5e_skb_from_cqe_mpwrq_linear(struct mlx5e_rq *rq, struct mlx5e_mpw_info *wi,
 struct sk_buff *
 mlx5e_skb_from_cqe_mpwrq_nonlinear(struct mlx5e_rq *rq, struct mlx5e_mpw_info *wi,
 				   u16 cqe_bcnt, u32 head_offset, u32 page_idx);
+struct sk_buff *
+mlx5e_skb_from_cqe_linear(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe,
+			  struct mlx5e_wqe_frag_info *wi, u32 cqe_bcnt);
+struct sk_buff *
+mlx5e_skb_from_cqe_nonlinear(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe,
+			     struct mlx5e_wqe_frag_info *wi, u32 cqe_bcnt);
 
 void mlx5e_update_stats(struct mlx5e_priv *priv);
 

commit 99cbfa93a6122b1e9011d3f4e94b58e10d2f5cd0
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Mon Apr 2 17:31:31 2018 +0300

    net/mlx5e: RX, Use cyclic WQ in legacy RQ
    
    Now that LRO is not supported for Legacy RQ, there is no source of
    out-of-order completions in the WQ, and we can use a cyclic one.
    This has multiple advantages:
    - reduces the WQE size (smaller PCI transactions).
    - lower overhead in datapath (no handling of 'next' pointers).
    - no reserved WQE for the WQ head (was need in linked-list).
    - allows using a constant map between frag and dma_info struct, in downstream patch.
    
    Performance tests:
    ConnectX-4, single core, single RX ring.
    Major gain in packet rate of single ring XDP drop.
    Bottleneck is shifted form HW (at 16Mpps) to SW (at 20Mpps).
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index f2f2dcf6b23c..af521dd52993 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -186,9 +186,13 @@ struct mlx5e_tx_wqe {
 	struct mlx5_wqe_data_seg data[0];
 };
 
-struct mlx5e_rx_wqe {
+struct mlx5e_rx_wqe_ll {
 	struct mlx5_wqe_srq_next_seg  next;
-	struct mlx5_wqe_data_seg      data;
+	struct mlx5_wqe_data_seg      data[0];
+};
+
+struct mlx5e_rx_wqe_cyc {
+	struct mlx5_wqe_data_seg      data[0];
 };
 
 struct mlx5e_umr_wqe {
@@ -500,7 +504,7 @@ struct mlx5e_rq {
 	/* data path */
 	union {
 		struct {
-			struct mlx5_wq_ll      wq;
+			struct mlx5_wq_cyc     wq;
 			struct mlx5e_wqe_frag_info *frag_info;
 			u32 frag_sz;	/* max possible skb frag_sz */
 			union {

commit 422d4c401edd186722d7530ffdaf11a6bb542cab
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Mon Apr 2 17:23:14 2018 +0300

    net/mlx5e: RX, Split WQ objects for different RQ types
    
    Replace the common RQ WQ object with two separate ones for the
    different RQ types.
    This is in preparation for switching to using a cyclic WQ type
    in Legacy RQ.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 9b4ed83783e4..f2f2dcf6b23c 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -498,10 +498,9 @@ enum mlx5e_rq_flag {
 
 struct mlx5e_rq {
 	/* data path */
-	struct mlx5_wq_ll      wq;
-
 	union {
 		struct {
+			struct mlx5_wq_ll      wq;
 			struct mlx5e_wqe_frag_info *frag_info;
 			u32 frag_sz;	/* max possible skb frag_sz */
 			union {
@@ -509,6 +508,7 @@ struct mlx5e_rq {
 			};
 		} wqe;
 		struct {
+			struct mlx5_wq_ll      wq;
 			struct mlx5e_umr_wqe   umr_wqe;
 			struct mlx5e_mpw_info *info;
 			mlx5e_fp_skb_from_cqe_mpwrq skb_from_cqe_mpwrq;

commit 75aa889fb9112874171dcb55b97302293dcd581e
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Mon Apr 2 14:30:34 2018 +0300

    net/mlx5e: RX, Generalise name of non-linear SKB head size
    
    Make name more generic by dropping MPWRQ from it, as it will be
    used also in Legacy RQ in a downstream patch.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index a23fa47fbbcc..9b4ed83783e4 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -112,7 +112,7 @@ struct page_pool;
 
 #define MLX5E_PARAMS_MINIMUM_LOG_RQ_SIZE_MPW            0x2
 
-#define MLX5_MPWRQ_SMALL_PACKET_THRESHOLD	(256)
+#define MLX5E_RX_MAX_HEAD (256)
 
 #define MLX5E_PARAMS_DEFAULT_LRO_WQE_SZ                 (64 * 1024)
 #define MLX5E_DEFAULT_LRO_TIMEOUT                       32

commit 250a42b6a764a7954a7a7a137bc71883f05657cb
Author: Adi Nissim <adin@mellanox.com>
Date:   Sun Apr 1 16:54:27 2018 +0300

    net/mlx5e: Support configurable MTU for vport representors
    
    The representor MTU was hard coded to 1500 bytes.
    Allow setting arbitrary MTU values up to the max supported by the FW.
    
    Signed-off-by: Adi Nissim <adin@mellanox.com>
    Reviewed-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 372cdf8a496c..a23fa47fbbcc 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -1102,6 +1102,10 @@ void mlx5e_update_stats_work(struct work_struct *work);
 
 int mlx5e_bits_invert(unsigned long a, int size);
 
+typedef int (*change_hw_mtu_cb)(struct mlx5e_priv *priv);
+int mlx5e_change_mtu(struct net_device *netdev, int new_mtu,
+		     change_hw_mtu_cb set_mtu_cb);
+
 /* ethtool helpers */
 void mlx5e_ethtool_get_drvinfo(struct mlx5e_priv *priv,
 			       struct ethtool_drvinfo *drvinfo);

commit 6c63efe4cfabf230a8ed4b1d880249875ffdac13
Author: Eran Ben Elisha <eranbe@mellanox.com>
Date:   Tue May 29 11:06:31 2018 +0300

    net/mlx5e: Remove redundant active_channels indication
    
    Now, when all channels stats are saved regardless of the channel's state
    {open, closed}, we can safely remove this indication and the stats spin
    lock which protects it.
    
    Fixes: 76c3810bade3 ("net/mlx5e: Avoid reset netdev stats on configuration changes")
    Signed-off-by: Eran Ben Elisha <eranbe@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 1c04df043e07..372cdf8a496c 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -777,8 +777,6 @@ struct mlx5e_priv {
 	struct mutex               state_lock; /* Protects Interface state */
 	struct mlx5e_rq            drop_rq;
 
-	rwlock_t                   stats_lock; /* Protects channels SW stats updates */
-	bool                       channels_active;
 	struct mlx5e_channels      channels;
 	u32                        tisn[MLX5E_MAX_NUM_TC];
 	struct mlx5e_rqt           indir_rqt;

commit 05909babce5328f468f7ac3a1033431c895f97a5
Author: Eran Ben Elisha <eranbe@mellanox.com>
Date:   Thu Apr 12 16:03:37 2018 +0300

    net/mlx5e: Avoid reset netdev stats on configuration changes
    
    Move all RQ, SQ and channel counters from the channel objects into the
    priv structure.  With this change, counters will not be reset upon
    channel configuration changes.
    
    Channel's statistics for SQs which are associated with TCs higher than
    zero will be presented in ethtool -S, only for SQs which were opened at
    least once since the module was loaded (regardless of their open/close
    current status).  This is done in order to decrease the total amount of
    statistics presented and calculated for the common out of box use (no
    QoS).
    
    mlx5e_channel_stats is a compound of CH,RQ,SQs stats in order to
    create locality for the NAPI when handling TX and RX of the same
    channel.
    
    Align the new statistics struct per ring to avoid several channels
    update to the same cache line at the same time.
    Packet rate was tested, no degradation sensed.
    
    Signed-off-by: Eran Ben Elisha <eranbe@mellanox.com>
    CC: Qing Huang <qing.huang@oracle.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index c3c79f2835d2..1c04df043e07 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -358,7 +358,6 @@ struct mlx5e_txqsq {
 	/* dirtied @xmit */
 	u16                        pc ____cacheline_aligned_in_smp;
 	u32                        dma_fifo_pc;
-	struct mlx5e_sq_stats      stats;
 
 	struct mlx5e_cq            cq;
 
@@ -371,6 +370,7 @@ struct mlx5e_txqsq {
 	/* read only */
 	struct mlx5_wq_cyc         wq;
 	u32                        dma_fifo_mask;
+	struct mlx5e_sq_stats     *stats;
 	void __iomem              *uar_map;
 	struct netdev_queue       *txq;
 	u32                        sqn;
@@ -526,7 +526,7 @@ struct mlx5e_rq {
 	struct mlx5e_channel  *channel;
 	struct device         *pdev;
 	struct net_device     *netdev;
-	struct mlx5e_rq_stats  stats;
+	struct mlx5e_rq_stats *stats;
 	struct mlx5e_cq        cq;
 	struct mlx5e_page_cache page_cache;
 	struct hwtstamp_config *tstamp;
@@ -574,7 +574,7 @@ struct mlx5e_channel {
 
 	/* data path - accessed per napi poll */
 	struct irq_desc *irq_desc;
-	struct mlx5e_ch_stats      stats;
+	struct mlx5e_ch_stats     *stats;
 
 	/* control */
 	struct mlx5e_priv         *priv;
@@ -590,6 +590,12 @@ struct mlx5e_channels {
 	struct mlx5e_params    params;
 };
 
+struct mlx5e_channel_stats {
+	struct mlx5e_ch_stats ch;
+	struct mlx5e_sq_stats sq[MLX5E_MAX_NUM_TC];
+	struct mlx5e_rq_stats rq;
+} ____cacheline_aligned_in_smp;
+
 enum mlx5e_traffic_types {
 	MLX5E_TT_IPV4_TCP,
 	MLX5E_TT_IPV6_TCP,
@@ -793,6 +799,8 @@ struct mlx5e_priv {
 	struct mlx5_core_dev      *mdev;
 	struct net_device         *netdev;
 	struct mlx5e_stats         stats;
+	struct mlx5e_channel_stats channel_stats[MLX5E_MAX_NUM_CHANNELS];
+	u8                         max_opened_tc;
 	struct hwtstamp_config     tstamp;
 	u16                        q_counter;
 	u16                        drop_rq_q_counter;

commit 868a01a27d80a59e719f0c369d1b26b923fc7674
Author: Shalom Lagziel <shaloml@mellanox.com>
Date:   Mon Feb 12 17:43:07 2018 +0200

    net/mlx5e: Introducing new statistics rwlock
    
    Introduce a new read/write lock that will protect statistics gathering from
    netdev channels configuration changes.
    e.g. when channels are being replaced (increase/decrease number of rings)
    prevent statistic gathering (ndo_get_stats64) to read the statistics of
    in-active channels (channels that are being closed).
    
    Plus update channels software statistics on the fly when calling
    ndo_get_stats64, and remove it from stats periodic work.
    
    Fixes: 9218b44dcc05 ("net/mlx5e: Statistics handling refactoring")
    Signed-off-by: Shalom Lagziel <shaloml@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 9396db54973f..c3c79f2835d2 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -771,6 +771,8 @@ struct mlx5e_priv {
 	struct mutex               state_lock; /* Protects Interface state */
 	struct mlx5e_rq            drop_rq;
 
+	rwlock_t                   stats_lock; /* Protects channels SW stats updates */
+	bool                       channels_active;
 	struct mlx5e_channels      channels;
 	u32                        tisn[MLX5E_MAX_NUM_TC];
 	struct mlx5e_rqt           indir_rqt;

commit 3a2f70331226c140e5aa27ee6bbe2a5c618acb4c
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Wed Apr 4 12:54:23 2018 +0300

    net/mlx5: Use order-0 allocations for all WQ types
    
    Complete the transition of all WQ types to use fragmented
    order-0 coherent memory instead of high-order allocations.
    
    CQ-WQ already uses order-0.
    Here we do the same for cyclic and linked-list WQs.
    
    This allows the driver to load cleanly on systems with a highly
    fragmented coherent memory.
    
    Performance tests:
    ConnectX-5 100Gbps, CPU: Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz
    Packet rate of 64B packets, single transmit ring, size 8K.
    
    No degradation is sensed.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 3c0f0a0343fd..9396db54973f 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -314,7 +314,7 @@ struct mlx5e_cq {
 
 	/* control */
 	struct mlx5_core_dev      *mdev;
-	struct mlx5_frag_wq_ctrl   wq_ctrl;
+	struct mlx5_wq_ctrl        wq_ctrl;
 } ____cacheline_aligned_in_smp;
 
 struct mlx5e_tx_wqe_info {

commit 043dc78ecf07f3fc5b87270518d7f322aea2f748
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Wed Mar 21 16:31:08 2018 +0200

    net/mlx5e: TX, Use actual WQE size for SQ edge fill
    
    We fill SQ edge with NOPs to avoid WQEs wrap.
    Here, instead of doing that in advance for the maximum possible
    WQE size, we do it on-demand using the actual WQE size.
    We re-order some parts in mlx5e_sq_xmit to finish the calculation
    of WQE size (ds_cnt) before doing any writes to the WQE buffer.
    
    When SQ work queue is fragmented (introduced in an downstream patch),
    dealing with WQE wraps becomes more frequent. This change would drastically
    reduce the overhead in this case.
    
    Performance tests:
    ConnectX-5 100Gbps, CPU: Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz
    Packet rate of 64B packets, single transmit ring, size 8K.
    
    Before: 14.9 Mpps
    After:  15.8 Mpps
    
    Improvement of 6%.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 061c4e90692e..3c0f0a0343fd 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -183,6 +183,7 @@ static inline int mlx5e_get_max_num_channels(struct mlx5_core_dev *mdev)
 struct mlx5e_tx_wqe {
 	struct mlx5_wqe_ctrl_seg ctrl;
 	struct mlx5_wqe_eth_seg  eth;
+	struct mlx5_wqe_data_seg data[0];
 };
 
 struct mlx5e_rx_wqe {
@@ -374,7 +375,6 @@ struct mlx5e_txqsq {
 	struct netdev_queue       *txq;
 	u32                        sqn;
 	u8                         min_inline_mode;
-	u16                        edge;
 	struct device             *pdev;
 	__be32                     mkey_be;
 	unsigned long              state;
@@ -439,7 +439,6 @@ struct mlx5e_icosq {
 	struct mlx5_wq_cyc         wq;
 	void __iomem              *uar_map;
 	u32                        sqn;
-	u16                        edge;
 	unsigned long              state;
 
 	/* control path */

commit ddf385e31f574c1c47215b6b1cf53343d7d204d6
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Wed May 2 18:30:56 2018 +0300

    net/mlx5e: Use WQ API functions instead of direct fields access
    
    Use the WQ API to get the WQ size, and to map a counter
    into a WQ entry index.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index c5c7a6d687ff..061c4e90692e 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -450,7 +450,7 @@ struct mlx5e_icosq {
 static inline bool
 mlx5e_wqc_has_room_for(struct mlx5_wq_cyc *wq, u16 cc, u16 pc, u16 n)
 {
-	return (((wq->sz_m1 & (cc - pc)) >= n) || (cc == pc));
+	return (mlx5_wq_cyc_ctr2ix(wq, cc - pc) >= n) || (cc == pc);
 }
 
 struct mlx5e_dma_info {
@@ -956,10 +956,9 @@ static inline void mlx5e_sq_fetch_wqe(struct mlx5e_txqsq *sq,
 				      struct mlx5e_tx_wqe **wqe,
 				      u16 *pi)
 {
-	struct mlx5_wq_cyc *wq;
+	struct mlx5_wq_cyc *wq = &sq->wq;
 
-	wq = &sq->wq;
-	*pi = sq->pc & wq->sz_m1;
+	*pi  = mlx5_wq_cyc_ctr2ix(wq, sq->pc);
 	*wqe = mlx5_wq_cyc_get_wqe(wq, *pi);
 	memset(*wqe, 0, sizeof(**wqe));
 }
@@ -967,7 +966,7 @@ static inline void mlx5e_sq_fetch_wqe(struct mlx5e_txqsq *sq,
 static inline
 struct mlx5e_tx_wqe *mlx5e_post_nop(struct mlx5_wq_cyc *wq, u32 sqn, u16 *pc)
 {
-	u16                         pi   = *pc & wq->sz_m1;
+	u16                         pi   = mlx5_wq_cyc_ctr2ix(wq, *pc);
 	struct mlx5e_tx_wqe        *wqe  = mlx5_wq_cyc_get_wqe(wq, pi);
 	struct mlx5_wqe_ctrl_seg   *cseg = &wqe->ctrl;
 

commit ecdf2dadee8e8c5015771b802a9851ff332d3fc4
Author: Huy Nguyen <huyn@mellanox.com>
Date:   Wed Mar 21 21:39:17 2018 -0500

    net/mlx5e: Receive buffer support for DCBX
    
    Add dcbnl's set/get buffer configuration callback that allows user to
    set/get buffer size configuration and priority to buffer mapping.
    
    By default, firmware controls receive buffer configuration and priority
    of buffer mapping based on the changes in pfc settings. When set buffer
    call back is triggered, the buffer configuration changes to manual mode.
    
    The manual mode means mlx5 driver will adjust the buffer configuration
    accordingly based on the changes in pfc settings.
    
    ConnectX buffer stride is 128 Bytes. If the buffer size is not multiple
    of 128, the buffer size will be rounded down to the nearest multiple of
    128.
    
    Signed-off-by: Huy Nguyen <huyn@mellanox.com>
    Reviewed-by: Parav Pandit <parav@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 9ab7158a7ce7..c5c7a6d687ff 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -278,6 +278,7 @@ struct mlx5e_dcbx {
 	u8                         cap;
 
 	/* Buffer configuration */
+	bool                       manual_buffer;
 	u32                        cable_len;
 	u32                        xoff;
 };

commit 0696d60853d58f1b9431bc689e4e4526845b790a
Author: Huy Nguyen <huyn@mellanox.com>
Date:   Wed Mar 21 21:10:22 2018 -0500

    net/mlx5e: Receive buffer configuration
    
    Add APIs for buffer configuration based on the changes in
    pfc configuration, cable len, buffer size configuration,
    and priority to buffer mapping.
    
    Note that the xoff fomula is as below
      xoff = ((301+2.16 * len [m]) * speed [Gbps] + 2.72 MTU [B]
      xoff_threshold = buffer_size - xoff
      xon_threshold = xoff_threshold - MTU
    
    Signed-off-by: Huy Nguyen <huyn@mellanox.com>
    Reviewed-by: Parav Pandit <parav@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index d13a86a1d702..9ab7158a7ce7 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -65,6 +65,7 @@ struct page_pool;
 #define MLX5E_HW2SW_MTU(params, hwmtu) ((hwmtu) - ((params)->hard_mtu))
 #define MLX5E_SW2HW_MTU(params, swmtu) ((swmtu) + ((params)->hard_mtu))
 
+#define MLX5E_MAX_PRIORITY      8
 #define MLX5E_MAX_DSCP          64
 #define MLX5E_MAX_NUM_TC	8
 
@@ -275,6 +276,10 @@ struct mlx5e_dcbx {
 	/* The only setting that cannot be read from FW */
 	u8                         tc_tsa[IEEE_8021QAZ_MAX_TCS];
 	u8                         cap;
+
+	/* Buffer configuration */
+	u32                        cable_len;
+	u32                        xoff;
 };
 
 struct mlx5e_dcbx_dp {

commit 2c81bfd5ae5659df44b38ec71c404b4b261a9515
Author: Huy Nguyen <huyn@mellanox.com>
Date:   Thu Feb 22 13:22:56 2018 -0600

    net/mlx5e: Move port speed code from en_ethtool.c to en/port.c
    
    Move four below functions from en_ethtool.c to en/port.c. These
    functions are used by both en_ethtool.c and en_main.c. Future code
    can use these functions without ethtool link mode dependency.
      u32 mlx5e_port_ptys2speed(u32 eth_proto_oper);
      int mlx5e_port_linkspeed(struct mlx5_core_dev *mdev, u32 *speed);
      int mlx5e_port_max_linkspeed(struct mlx5_core_dev *mdev, u32 *speed);
      u32 mlx5e_port_speed2linkmodes(u32 speed);
    
    Delete the speed field from table mlx5e_build_ptys2ethtool_map. This
    table only keeps the mapping between the mlx5e link mode and
    ethtool link mode. Add new table mlx5e_link_speed for translation
    from mlx5e link mode to actual speed.
    
    Signed-off-by: Huy Nguyen <huyn@mellanox.com>
    Reviewed-by: Parav Pandit <parav@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index bc91a7335c93..d13a86a1d702 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -932,8 +932,6 @@ void mlx5e_deactivate_priv_channels(struct mlx5e_priv *priv);
 
 void mlx5e_build_default_indir_rqt(u32 *indirection_rqt, int len,
 				   int num_channels);
-int mlx5e_get_max_linkspeed(struct mlx5_core_dev *mdev, u32 *speed);
-
 void mlx5e_set_tx_cq_mode_params(struct mlx5e_params *params,
 				 u8 cq_period_mode);
 void mlx5e_set_rx_cq_mode_params(struct mlx5e_params *params,

commit 05866c82360aef6005de1920d585ae604309e732
Author: Or Gerlitz <ogerlitz@mellanox.com>
Date:   Tue Apr 10 14:27:43 2018 +0300

    net/mlx5e: Prepare for shared table to keep TC eswitch flows
    
    This is a refactoring step to be able and store the hash table which
    keeps track of offloaded TC flows in a different location for NIC
    vs e-switch rules.
    
    Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: Jiri Pirko <jiri@mellanox.com>
    Reviewed-by: Paul Blakey <paulb@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 51a1d36a56c5..bc91a7335c93 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -634,7 +634,6 @@ struct mlx5e_flow_table {
 struct mlx5e_tc_table {
 	struct mlx5_flow_table		*t;
 
-	struct rhashtable_params        ht_params;
 	struct rhashtable               ht;
 
 	DECLARE_HASHTABLE(mod_hdr_tbl, 8);

commit 60bd4af814fec164c42bdd2efd7984b85d6b1e1e
Author: Or Gerlitz <ogerlitz@mellanox.com>
Date:   Wed Apr 18 13:45:11 2018 +0300

    net/mlx5e: Add ingress/egress indication for offloaded TC flows
    
    When an e-switch TC rule is offloaded through the egdev (egress
    device) mechanism, we treat this as egress, all other cases (NIC
    and e-switch) are considred ingress.
    
    This is preparation step that will allow us to  identify "wrong"
    stat/del offload calls made by the TC core on egdev based flows and
    ignore them.
    
    Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: Jiri Pirko <jiri@mellanox.com>
    Reviewed-by: Paul Blakey <paulb@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 7c930088e96e..51a1d36a56c5 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -1118,9 +1118,6 @@ int mlx5e_ethtool_get_ts_info(struct mlx5e_priv *priv,
 int mlx5e_ethtool_flash_device(struct mlx5e_priv *priv,
 			       struct ethtool_flash *flash);
 
-int mlx5e_setup_tc_block_cb(enum tc_setup_type type, void *type_data,
-			    void *cb_priv);
-
 /* mlx5e generic netdev management API */
 struct net_device*
 mlx5e_create_netdev(struct mlx5_core_dev *mdev, const struct mlx5e_profile *profile,

commit 0e5c04f6b52b248cfdb7e791fd5702f12270df7b
Author: Gal Pressman <galp@mellanox.com>
Date:   Tue Jan 23 12:23:26 2018 +0200

    net/mlx5e: Remove MLX5E_TEST_BIT macro
    
    MLX5E_TEST_BIT macro is the same as the already existent test_bit,
    remove it and replace all usages.
    
    Signed-off-by: Gal Pressman <galp@mellanox.com>
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 9cc07da09b70..7c930088e96e 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -288,8 +288,6 @@ enum {
 	MLX5E_RQ_STATE_AM,
 };
 
-#define MLX5E_TEST_BIT(state, nr) (state & BIT(nr))
-
 struct mlx5e_cq {
 	/* data path - accessed per cqe */
 	struct mlx5_cqwq           wq;

commit 43585a41bd894925abe5015edbce475beb3d8c10
Author: Ilya Lesokhin <ilyal@mellanox.com>
Date:   Mon Apr 30 10:16:21 2018 +0300

    net/mlx5e: TLS, Add error statistics
    
    Add statistics for rare TLS related errors.
    Since the errors are rare we have a counter per netdev
    rather then per SQ.
    
    Signed-off-by: Ilya Lesokhin <ilyal@mellanox.com>
    Signed-off-by: Boris Pismenny <borisp@mellanox.com>
    Acked-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index ca5b8523bc95..9cc07da09b70 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -801,6 +801,9 @@ struct mlx5e_priv {
 #ifdef CONFIG_MLX5_EN_IPSEC
 	struct mlx5e_ipsec        *ipsec;
 #endif
+#ifdef CONFIG_MLX5_EN_TLS
+	struct mlx5e_tls          *tls;
+#endif
 };
 
 struct mlx5e_profile {

commit bf23974104fa7aa2f3cbd2f2295f866a71875efd
Author: Ilya Lesokhin <ilyal@mellanox.com>
Date:   Mon Apr 30 10:16:20 2018 +0300

    net/mlx5e: TLS, Add Innova TLS TX offload data path
    
    Implement the TLS tx offload data path according to the
    requirements of the TLS generic NIC offload infrastructure.
    
    Special metadata ethertype is used to pass information to
    the hardware.
    
    Signed-off-by: Ilya Lesokhin <ilyal@mellanox.com>
    Signed-off-by: Boris Pismenny <borisp@mellanox.com>
    Acked-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 66ecb1c81002..ca5b8523bc95 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -335,6 +335,7 @@ enum {
 	MLX5E_SQ_STATE_RECOVERING,
 	MLX5E_SQ_STATE_IPSEC,
 	MLX5E_SQ_STATE_AM,
+	MLX5E_SQ_STATE_TLS,
 };
 
 struct mlx5e_sq_wqe_info {
@@ -830,6 +831,8 @@ void mlx5e_build_ptys2ethtool_map(void);
 u16 mlx5e_select_queue(struct net_device *dev, struct sk_buff *skb,
 		       void *accel_priv, select_queue_fallback_t fallback);
 netdev_tx_t mlx5e_xmit(struct sk_buff *skb, struct net_device *dev);
+netdev_tx_t mlx5e_sq_xmit(struct mlx5e_txqsq *sq, struct sk_buff *skb,
+			  struct mlx5e_tx_wqe *wqe, u16 pi);
 
 void mlx5e_completion_event(struct mlx5_core_cq *mcq);
 void mlx5e_cq_error_event(struct mlx5_core_cq *mcq, enum mlx5_event event);
@@ -945,6 +948,18 @@ static inline bool mlx5e_tunnel_inner_ft_supported(struct mlx5_core_dev *mdev)
 		MLX5_CAP_FLOWTABLE_NIC_RX(mdev, ft_field_support.inner_ip_version));
 }
 
+static inline void mlx5e_sq_fetch_wqe(struct mlx5e_txqsq *sq,
+				      struct mlx5e_tx_wqe **wqe,
+				      u16 *pi)
+{
+	struct mlx5_wq_cyc *wq;
+
+	wq = &sq->wq;
+	*pi = sq->pc & wq->sz_m1;
+	*wqe = mlx5_wq_cyc_get_wqe(wq, *pi);
+	memset(*wqe, 0, sizeof(**wqe));
+}
+
 static inline
 struct mlx5e_tx_wqe *mlx5e_post_nop(struct mlx5_wq_cyc *wq, u32 sqn, u16 *pc)
 {

commit bb9094161b2320e431a5d8a7b9c3dc632bc92ae6
Author: Ilya Lesokhin <ilyal@mellanox.com>
Date:   Mon Apr 30 10:16:17 2018 +0300

    net/mlx5e: Move defines out of ipsec code
    
    The defines are not IPSEC specific.
    
    Signed-off-by: Ilya Lesokhin <ilyal@mellanox.com>
    Signed-off-by: Boris Pismenny <borisp@mellanox.com>
    Acked-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 56c748ca2a09..66ecb1c81002 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -55,6 +55,9 @@
 
 struct page_pool;
 
+#define MLX5E_METADATA_ETHER_TYPE (0x8CE4)
+#define MLX5E_METADATA_ETHER_LEN 8
+
 #define MLX5_SET_CFG(p, f, v) MLX5_SET(create_flow_group_in, p, f, v)
 
 #define MLX5E_ETH_HARD_MTU (ETH_HLEN + VLAN_HLEN + ETH_FCS_LEN)

commit cbce4f44479856cd4621e3dce531cfb078357b1f
Author: Tal Gilboa <talgi@mellanox.com>
Date:   Tue Apr 24 13:36:03 2018 +0300

    net/mlx5e: Enable adaptive-TX moderation
    
    Add support for adaptive TX moderation. This greatly reduces TX interrupt
    rate and increases bandwidth, mostly for TCP bandwidth over ARM
    architecture (below). There is a slight single stream TCP with very large
    message sizes degradation (x86). In this case if there's any moderation on
    transmitted packets the bandwidth would reduce due to hitting TCP output limit.
    Since this is a synthetic case, this is still worth doing.
    
    Performance improvement (ConnectX-4Lx 40GbE, ARM)
    TCP 64B bandwidth with 1-50 streams increased 6-35%.
    TCP 64B bandwidth with 100-500 streams increased 20-70%.
    
    Performance improvement (ConnectX-5 100GbE, x86)
    Bandwidth: increased up to 40% (1024B with 10s of streams).
    Interrupt rate: reduced up to 50% (1024B with 1000s of streams).
    
    Performance degradation (ConnectX-5 100GbE, x86)
    Bandwidth: up to 10% decrease single stream TCP (1MB message size from
    51Gb/s to 47Gb/s).
    
    Signed-off-by: Tal Gilboa <talgi@mellanox.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>
    Acked-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 3317a4da87cb..56c748ca2a09 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -241,6 +241,7 @@ struct mlx5e_params {
 	bool vlan_strip_disable;
 	bool scatter_fcs_en;
 	bool rx_dim_enabled;
+	bool tx_dim_enabled;
 	u32 lro_timeout;
 	u32 pflags;
 	struct bpf_prog *xdp_prog;
@@ -330,6 +331,7 @@ enum {
 	MLX5E_SQ_STATE_ENABLED,
 	MLX5E_SQ_STATE_RECOVERING,
 	MLX5E_SQ_STATE_IPSEC,
+	MLX5E_SQ_STATE_AM,
 };
 
 struct mlx5e_sq_wqe_info {
@@ -342,6 +344,7 @@ struct mlx5e_txqsq {
 	/* dirtied @completion */
 	u16                        cc;
 	u32                        dma_fifo_cc;
+	struct net_dim             dim; /* Adaptive Moderation */
 
 	/* dirtied @xmit */
 	u16                        pc ____cacheline_aligned_in_smp;
@@ -1111,4 +1114,5 @@ void mlx5e_build_nic_params(struct mlx5_core_dev *mdev,
 			    u16 max_channels, u16 mtu);
 u8 mlx5e_params_calculate_tx_min_inline(struct mlx5_core_dev *mdev);
 void mlx5e_rx_dim_work(struct work_struct *work);
+void mlx5e_tx_dim_work(struct work_struct *work);
 #endif /* __MLX5_EN_H__ */

commit 60bbf7eeef10dc647430646d7fe5e3d8d132dbec
Author: Jesper Dangaard Brouer <brouer@redhat.com>
Date:   Tue Apr 17 16:46:27 2018 +0200

    mlx5: use page_pool for xdp_return_frame call
    
    This patch shows how it is possible to have both the driver local page
    cache, which uses elevated refcnt for "catching"/avoiding SKB
    put_page returns the page through the page allocator.  And at the
    same time, have pages getting returned to the page_pool from
    ndp_xdp_xmit DMA completion.
    
    The performance improvement for XDP_REDIRECT in this patch is really
    good.  Especially considering that (currently) the xdp_return_frame
    API and page_pool_put_page() does per frame operations of both
    rhashtable ID-lookup and locked return into (page_pool) ptr_ring.
    (It is the plan to remove these per frame operation in a followup
    patchset).
    
    The benchmark performed was RX on mlx5 and XDP_REDIRECT out ixgbe,
    with xdp_redirect_map (using devmap) . And the target/maximum
    capability of ixgbe is 13Mpps (on this HW setup).
    
    Before this patch for mlx5, XDP redirected frames were returned via
    the page allocator.  The single flow performance was 6Mpps, and if I
    started two flows the collective performance drop to 4Mpps, because we
    hit the page allocator lock (further negative scaling occurs).
    
    Two test scenarios need to be covered, for xdp_return_frame API, which
    is DMA-TX completion running on same-CPU or cross-CPU free/return.
    Results were same-CPU=10Mpps, and cross-CPU=12Mpps.  This is very
    close to our 13Mpps max target.
    
    The reason max target isn't reached in cross-CPU test, is likely due
    to RX-ring DMA unmap/map overhead (which doesn't occur in ixgbe to
    ixgbe testing).  It is also planned to remove this unnecessary DMA
    unmap in a later patchset
    
    V2: Adjustments requested by Tariq
     - Changed page_pool_create return codes not return NULL, only
       ERR_PTR, as this simplifies err handling in drivers.
     - Save a branch in mlx5e_page_release
     - Correct page_pool size calc for MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ
    
    V5: Updated patch desc
    
    V8: Adjust for b0cedc844c00 ("net/mlx5e: Remove rq_headroom field from params")
    V9:
     - Adjust for 121e89275471 ("net/mlx5e: Refactor RQ XDP_TX indication")
     - Adjust for 73281b78a37a ("net/mlx5e: Derive Striding RQ size from MTU")
     - Correct handling if page_pool_create fail for MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ
    
    V10: Req from Tariq
     - Change pool_size calc for MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ
    
    Signed-off-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>
    Acked-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 1a05d1072c5e..3317a4da87cb 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -53,6 +53,8 @@
 #include "mlx5_core.h"
 #include "en_stats.h"
 
+struct page_pool;
+
 #define MLX5_SET_CFG(p, f, v) MLX5_SET(create_flow_group_in, p, f, v)
 
 #define MLX5E_ETH_HARD_MTU (ETH_HLEN + VLAN_HLEN + ETH_FCS_LEN)
@@ -534,6 +536,7 @@ struct mlx5e_rq {
 	unsigned int           hw_mtu;
 	struct mlx5e_xdpsq     xdpsq;
 	DECLARE_BITMAP(flags, 8);
+	struct page_pool      *page_pool;
 
 	/* control */
 	struct mlx5_wq_ctrl    wq_ctrl;

commit 5168d73201898c581d0503434ee9743cfc8f964b
Author: Jesper Dangaard Brouer <brouer@redhat.com>
Date:   Tue Apr 17 16:45:21 2018 +0200

    mlx5: basic XDP_REDIRECT forward support
    
    This implements basic XDP redirect support in mlx5 driver.
    
    Notice that the ndo_xdp_xmit() is NOT implemented, because that API
    need some changes that this patchset is working towards.
    
    The main purpose of this patch is have different drivers doing
    XDP_REDIRECT to show how different memory models behave in a cross
    driver world.
    
    Update(pre-RFCv2 Tariq): Need to DMA unmap page before xdp_do_redirect,
    as the return API does not exist yet to to keep this mapped.
    
    Update(pre-RFCv3 Saeed): Don't mix XDP_TX and XDP_REDIRECT flushing,
    introduce xdpsq.db.redirect_flush boolian.
    
    V9: Adjust for commit 121e89275471 ("net/mlx5e: Refactor RQ XDP_TX indication")
    
    Signed-off-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>
    Acked-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 30cad07be2b5..1a05d1072c5e 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -392,6 +392,7 @@ struct mlx5e_xdpsq {
 	struct {
 		struct mlx5e_dma_info     *di;
 		bool                       doorbell;
+		bool                       redirect_flush;
 	} db;
 
 	/* read only */

commit b8a98a4cf3221d8140969e3f5bde09206a6cb623
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Wed Dec 20 11:56:35 2017 +0200

    net/mlx5e: Keep single pre-initialized UMR WQE per RQ
    
    All UMR WQEs of an RQ share many common fields. We use
    pre-initialized structures to save calculations in datapath.
    One field (xlt_offset) was the only reason we saved a pre-initialized
    copy per WQE index.
    Here we remove its initialization (move its calculation to datapath),
    and reduce the number of copies to one-per-RQ.
    
    A very small datapath calculation is added, it occurs once per a MPWQE
    (i.e. once every 256KB), but reduces memory consumption and gives
    better cache utilization.
    
    Performance testing:
    Tested packet rate, no degradation sensed.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 5853de4e4fc7..30cad07be2b5 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -84,6 +84,7 @@
 
 #define MLX5_MTT_OCTW(npages) (ALIGN(npages, 8) / 2)
 #define MLX5E_REQUIRED_WQE_MTTS		(ALIGN(MLX5_MPWRQ_PAGES_PER_WQE, 8))
+#define MLX5E_LOG_ALIGNED_MPWQE_PPW	(ilog2(MLX5E_REQUIRED_WQE_MTTS))
 #define MLX5E_REQUIRED_MTTS(wqes)	(wqes * MLX5E_REQUIRED_WQE_MTTS)
 #define MLX5E_MAX_RQ_NUM_MTTS	\
 	((1 << 16) * 2) /* So that MLX5_MTT_OCTW(num_mtts) fits into u16 */
@@ -450,7 +451,6 @@ struct mlx5e_wqe_frag_info {
 
 struct mlx5e_umr_dma_info {
 	struct mlx5e_dma_info  dma_info[MLX5_MPWRQ_PAGES_PER_WQE];
-	struct mlx5e_umr_wqe   wqe;
 };
 
 struct mlx5e_mpw_info {
@@ -496,6 +496,7 @@ struct mlx5e_rq {
 			};
 		} wqe;
 		struct {
+			struct mlx5e_umr_wqe   umr_wqe;
 			struct mlx5e_mpw_info *info;
 			mlx5e_fp_skb_from_cqe_mpwrq skb_from_cqe_mpwrq;
 			u16                    num_strides;
@@ -978,11 +979,6 @@ static inline void mlx5e_cq_arm(struct mlx5e_cq *cq)
 	mlx5_cq_arm(mcq, MLX5_CQ_DB_REQ_NOT, mcq->uar->map, cq->wq.cc);
 }
 
-static inline u32 mlx5e_get_wqe_mtt_offset(struct mlx5e_rq *rq, u16 wqe_ix)
-{
-	return wqe_ix * ALIGN(MLX5_MPWRQ_PAGES_PER_WQE, 8);
-}
-
 extern const struct ethtool_ops mlx5e_ethtool_ops;
 #ifdef CONFIG_MLX5_CORE_EN_DCB
 extern const struct dcbnl_rtnl_ops mlx5e_dcbnl_ops;

commit 9f9e9cd50eac6ad09cb053509f2e764bddc05f18
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Tue Nov 28 11:03:37 2017 +0200

    net/mlx5e: Remove page_ref bulking in Striding RQ
    
    When many packets reside on the same page, the bulking of
    page_ref modifications reduces the total number of atomic
    operations executed.
    
    Besides the necessary 2 operations on page alloc/free, we
    have the following extra ops per page:
    - one on WQE allocation (bump refcnt to maximum possible),
    - zero ops for SKBs,
    - one on WQE free,
    a constant of two operations in total, no matter how many
    packets/SKBs actually populate the page.
    
    Without this bulking, we have:
    - no ops on WQE allocation or free,
    - one op per SKB,
    
    Comparing the two methods when PAGE_SIZE is 4K:
    - As mentioned above, bulking method always executes 2 operations,
      not more, but not less.
    - In the default MTU configuration (1500, stride size is 2K),
      the non-bulking method execute 2 ops as well.
    - For larger MTUs with stride size of 4K, non-bulking method
      executes only a single op.
    - For XDP (stride size of 4K, no SKBs), non-bulking method
      executes no ops at all!
    
    Hence, to optimize the flows with linear SKB and XDP over Striding RQ,
    we here remove the page_ref bulking method.
    
    Performance testing:
    ConnectX-5, Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz.
    
    Single core packet rate (64 bytes).
    
    Early drop in TC: no degradation.
    
    XDP_DROP:
    before: 14,270,188 pps
    after:  20,503,603 pps, 43% improvement.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 7997d7c159db..5853de4e4fc7 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -456,7 +456,6 @@ struct mlx5e_umr_dma_info {
 struct mlx5e_mpw_info {
 	struct mlx5e_umr_dma_info umr;
 	u16 consumed_strides;
-	u16 skbs_frags[MLX5_MPWRQ_PAGES_PER_WQE];
 	DECLARE_BITMAP(xdp_xmit_bitmap, MLX5_MPWRQ_PAGES_PER_WQE);
 };
 

commit 22f4539881944a8acc9c6f5afa7aa7f028898002
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Wed Feb 7 14:46:36 2018 +0200

    net/mlx5e: Support XDP over Striding RQ
    
    Add XDP support over Striding RQ.
    Now that linear SKB is supported over Striding RQ,
    we can support XDP by setting stride size to PAGE_SIZE
    and headroom to XDP_PACKET_HEADROOM.
    
    Upon a MPWQE free, do not release pages that are being
    XDP xmit, they will be released upon completions.
    
    Striding RQ is capable of a higher packet-rate than
    conventional RQ.
    A performance gain is expected for all cases that had
    a HW packet-rate bottleneck. This is the case whenever
    using many flows that distribute to many cores.
    
    Performance testing:
    ConnectX-5, 24 rings, default MTU.
    CQE compression ON (to reduce completions BW in PCI).
    
    XDP_DROP packet rate:
    --------------------------------------------------
    | pkt size | XDP rate   | 100GbE linerate | pct% |
    --------------------------------------------------
    |   64byte | 126.2 Mpps |      148.0 Mpps |  85% |
    |  128byte |  80.0 Mpps |       84.8 Mpps |  94% |
    |  256byte |  42.7 Mpps |       42.7 Mpps | 100% |
    |  512byte |  23.4 Mpps |       23.4 Mpps | 100% |
    --------------------------------------------------
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index a6ca54393bb6..7997d7c159db 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -457,6 +457,7 @@ struct mlx5e_mpw_info {
 	struct mlx5e_umr_dma_info umr;
 	u16 consumed_strides;
 	u16 skbs_frags[MLX5_MPWRQ_PAGES_PER_WQE];
+	DECLARE_BITMAP(xdp_xmit_bitmap, MLX5_MPWRQ_PAGES_PER_WQE);
 };
 
 /* a single cache unit is capable to serve one napi call (for non-striding rq)

commit 121e89275471dccbd5b252e40ad6a823fa0527f7
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Tue Dec 12 15:46:49 2017 +0200

    net/mlx5e: Refactor RQ XDP_TX indication
    
    Make the xdp_xmit indication available for Striding RQ
    by taking it out of the type-specific union.
    This refactor is a preparation for a downstream patch that
    adds XDP support over Striding RQ.
    In addition, use a bitmap instead of a boolean for possible
    future flags.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index d26dd4bc89f4..a6ca54393bb6 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -479,6 +479,10 @@ typedef struct sk_buff *
 typedef bool (*mlx5e_fp_post_rx_wqes)(struct mlx5e_rq *rq);
 typedef void (*mlx5e_fp_dealloc_wqe)(struct mlx5e_rq*, u16);
 
+enum mlx5e_rq_flag {
+	MLX5E_RQ_FLAG_XDP_XMIT = BIT(0),
+};
+
 struct mlx5e_rq {
 	/* data path */
 	struct mlx5_wq_ll      wq;
@@ -489,7 +493,6 @@ struct mlx5e_rq {
 			u32 frag_sz;	/* max possible skb frag_sz */
 			union {
 				bool page_reuse;
-				bool xdp_xmit;
 			};
 		} wqe;
 		struct {
@@ -528,6 +531,7 @@ struct mlx5e_rq {
 	struct bpf_prog       *xdp_prog;
 	unsigned int           hw_mtu;
 	struct mlx5e_xdpsq     xdpsq;
+	DECLARE_BITMAP(flags, 8);
 
 	/* control */
 	struct mlx5_wq_ctrl    wq_ctrl;

commit 619a8f2a42f1031cdbd74435b6a9191eb4913139
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Wed Feb 7 14:41:25 2018 +0200

    net/mlx5e: Use linear SKB in Striding RQ
    
    Current Striding RQ HW feature utilizes the RX buffers so that
    there is no wasted room between the strides. This maximises
    the memory utilization.
    This prevents the use of build_skb() (which requires headroom
    and tailroom), and demands to memcpy the packets headers into
    the skb linear part.
    
    In this patch, whenever a set of conditions holds, we apply
    an RQ configuration that allows combining the use of linear SKB
    on top of a Striding RQ.
    
    To use build_skb() with Striding RQ, the following must hold:
    1. packet does not cross a page boundary.
    2. there is enough headroom and tailroom surrounding the packet.
    
    We can satisfy 1 and 2 by configuring:
            stride size = MTU + headroom + tailoom.
    
    This is possible only when:
    a. (MTU - headroom - tailoom) does not exceed PAGE_SIZE.
    b. HW LRO is turned off.
    
    Using linear SKB has many advantages:
    - Saves a memcpy of the headers.
    - No page-boundary checks in datapath.
    - No filler CQEs.
    - Significantly smaller CQ.
    - SKB data continuously resides in linear part, and not split to
      small amount (linear part) and large amount (fragment).
      This saves datapath cycles in driver and improves utilization
      of SKB fragments in GRO.
    - The fragments of a resulting GRO SKB follow the IP forwarding
      assumption of equal-size fragments.
    
    Some implementation details:
    HW writes the packets to the beginning of a stride,
    i.e. does not keep headroom. To overcome this we make sure we can
    extend backwards and use the last bytes of stride i-1.
    Extra care is needed for stride 0 as it has no preceding stride.
    We make sure headroom bytes are available by shifting the buffer
    pointer passed to HW by headroom bytes.
    
    This configuration now becomes default, whenever capable.
    Of course, this implies turning LRO off.
    
    Performance testing:
    ConnectX-5, single core, single RX ring, default MTU.
    
    UDP packet rate, early drop in TC layer:
    
    --------------------------------------------
    | pkt size | before    | after     | ratio |
    --------------------------------------------
    | 1500byte | 4.65 Mpps | 5.96 Mpps | 1.28x |
    |  500byte | 5.23 Mpps | 5.97 Mpps | 1.14x |
    |   64byte | 5.94 Mpps | 5.96 Mpps | 1.00x |
    --------------------------------------------
    
    TCP streams: ~20% gain
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index c1d3a29388bd..d26dd4bc89f4 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -473,6 +473,9 @@ struct mlx5e_page_cache {
 
 struct mlx5e_rq;
 typedef void (*mlx5e_fp_handle_rx_cqe)(struct mlx5e_rq*, struct mlx5_cqe64*);
+typedef struct sk_buff *
+(*mlx5e_fp_skb_from_cqe_mpwrq)(struct mlx5e_rq *rq, struct mlx5e_mpw_info *wi,
+			       u16 cqe_bcnt, u32 head_offset, u32 page_idx);
 typedef bool (*mlx5e_fp_post_rx_wqes)(struct mlx5e_rq *rq);
 typedef void (*mlx5e_fp_dealloc_wqe)(struct mlx5e_rq*, u16);
 
@@ -491,6 +494,7 @@ struct mlx5e_rq {
 		} wqe;
 		struct {
 			struct mlx5e_mpw_info *info;
+			mlx5e_fp_skb_from_cqe_mpwrq skb_from_cqe_mpwrq;
 			u16                    num_strides;
 			u8                     log_stride_sz;
 			bool                   umr_in_progress;
@@ -834,6 +838,12 @@ bool mlx5e_post_rx_mpwqes(struct mlx5e_rq *rq);
 void mlx5e_dealloc_rx_wqe(struct mlx5e_rq *rq, u16 ix);
 void mlx5e_dealloc_rx_mpwqe(struct mlx5e_rq *rq, u16 ix);
 void mlx5e_free_rx_mpwqe(struct mlx5e_rq *rq, struct mlx5e_mpw_info *wi);
+struct sk_buff *
+mlx5e_skb_from_cqe_mpwrq_linear(struct mlx5e_rq *rq, struct mlx5e_mpw_info *wi,
+				u16 cqe_bcnt, u32 head_offset, u32 page_idx);
+struct sk_buff *
+mlx5e_skb_from_cqe_mpwrq_nonlinear(struct mlx5e_rq *rq, struct mlx5e_mpw_info *wi,
+				   u16 cqe_bcnt, u32 head_offset, u32 page_idx);
 
 void mlx5e_update_stats(struct mlx5e_priv *priv);
 

commit ea3886cab76f1026a5db988fa6fad997e98f3a32
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Mon Jul 10 12:52:36 2017 +0300

    net/mlx5e: Use inline MTTs in UMR WQEs
    
    When modifying the page mapping of a HW memory region
    (via a UMR post), post the new values inlined in WQE,
    instead of using a data pointer.
    
    This is a micro-optimization, inline UMR WQEs of different
    rings scale better in HW.
    
    In addition, this obsoletes a few control flows and helps
    delete ~50 LOC.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 1f89e2194b61..c1d3a29388bd 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -105,7 +105,6 @@
 
 #define MLX5E_PARAMS_MINIMUM_LOG_RQ_SIZE_MPW            0x2
 
-#define MLX5_UMR_ALIGN				(2048)
 #define MLX5_MPWRQ_SMALL_PACKET_THRESHOLD	(256)
 
 #define MLX5E_PARAMS_DEFAULT_LRO_WQE_SZ                 (64 * 1024)
@@ -130,8 +129,13 @@
 #define MLX5E_UPDATE_STATS_INTERVAL    200 /* msecs */
 #define MLX5E_SQ_RECOVER_MIN_INTERVAL  500 /* msecs */
 
-#define MLX5E_ICOSQ_MAX_WQEBBS \
-	(DIV_ROUND_UP(sizeof(struct mlx5e_umr_wqe), MLX5_SEND_WQE_BB))
+#define MLX5E_UMR_WQE_INLINE_SZ \
+	(sizeof(struct mlx5e_umr_wqe) + \
+	 ALIGN(MLX5_MPWRQ_PAGES_PER_WQE * sizeof(struct mlx5_mtt), \
+	       MLX5_UMR_MTT_ALIGNMENT))
+#define MLX5E_UMR_WQEBBS \
+	(DIV_ROUND_UP(MLX5E_UMR_WQE_INLINE_SZ, MLX5_SEND_WQE_BB))
+#define MLX5E_ICOSQ_MAX_WQEBBS MLX5E_UMR_WQEBBS
 
 #define MLX5E_XDP_MIN_INLINE (ETH_HLEN + VLAN_HLEN)
 #define MLX5E_XDP_TX_DS_COUNT \
@@ -183,7 +187,7 @@ struct mlx5e_umr_wqe {
 	struct mlx5_wqe_ctrl_seg       ctrl;
 	struct mlx5_wqe_umr_ctrl_seg   uctrl;
 	struct mlx5_mkey_seg           mkc;
-	struct mlx5_wqe_data_seg       data;
+	struct mlx5_mtt                inline_mtts[0];
 };
 
 extern const char mlx5e_self_tests[][ETH_GSTRING_LEN];
@@ -421,7 +425,6 @@ struct mlx5e_icosq {
 	void __iomem              *uar_map;
 	u32                        sqn;
 	u16                        edge;
-	__be32                     mkey_be;
 	unsigned long              state;
 
 	/* control path */
@@ -446,8 +449,6 @@ struct mlx5e_wqe_frag_info {
 };
 
 struct mlx5e_umr_dma_info {
-	__be64                *mtt;
-	dma_addr_t             mtt_addr;
 	struct mlx5e_dma_info  dma_info[MLX5_MPWRQ_PAGES_PER_WQE];
 	struct mlx5e_umr_wqe   wqe;
 };
@@ -490,7 +491,6 @@ struct mlx5e_rq {
 		} wqe;
 		struct {
 			struct mlx5e_mpw_info *info;
-			void                  *mtt_no_align;
 			u16                    num_strides;
 			u8                     log_stride_sz;
 			bool                   umr_in_progress;

commit 73281b78a37a1a3f392fd5b6116d04e597484529
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Sun Feb 11 15:21:33 2018 +0200

    net/mlx5e: Derive Striding RQ size from MTU
    
    In Striding RQ, each WQE serves multiple packets
    (hence called Multi-Packet WQE, MPWQE).
    The size of a MPWQE is constant (currently 256KB).
    
    Upon a ringparam set operation, we calculate the number of
    MPWQEs per RQ. For this, first it is needed to determine the
    number of packets that can reside within a single MPWQE.
    In this patch we use the actual MTU size instead of ETH_DATA_LEN
    for this calculation.
    
    This implies that a change in MTU might require a change
    in Striding RQ ring size.
    
    In addition, this obsoletes some WQEs-to-packets translation
    functions and helps delete ~60 LOC.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 823876bfd6ab..1f89e2194b61 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -63,18 +63,6 @@
 #define MLX5E_MAX_DSCP          64
 #define MLX5E_MAX_NUM_TC	8
 
-#define MLX5E_PARAMS_MINIMUM_LOG_SQ_SIZE                0x6
-#define MLX5E_PARAMS_DEFAULT_LOG_SQ_SIZE                0xa
-#define MLX5E_PARAMS_MAXIMUM_LOG_SQ_SIZE                0xd
-
-#define MLX5E_PARAMS_MINIMUM_LOG_RQ_SIZE                0x1
-#define MLX5E_PARAMS_DEFAULT_LOG_RQ_SIZE                0xa
-#define MLX5E_PARAMS_MAXIMUM_LOG_RQ_SIZE                0xd
-
-#define MLX5E_PARAMS_MINIMUM_LOG_RQ_SIZE_MPW            0x2
-#define MLX5E_PARAMS_DEFAULT_LOG_RQ_SIZE_MPW            0x3
-#define MLX5E_PARAMS_MAXIMUM_LOG_RQ_SIZE_MPW            0x6
-
 #define MLX5_RX_HEADROOM NET_SKB_PAD
 #define MLX5_SKB_FRAG_SZ(len)	(SKB_DATA_ALIGN(len) +	\
 				 SKB_DATA_ALIGN(sizeof(struct skb_shared_info)))
@@ -95,9 +83,27 @@
 #define MLX5_MPWRQ_PAGES_PER_WQE		BIT(MLX5_MPWRQ_WQE_PAGE_ORDER)
 
 #define MLX5_MTT_OCTW(npages) (ALIGN(npages, 8) / 2)
-#define MLX5E_REQUIRED_MTTS(wqes)		\
-	(wqes * ALIGN(MLX5_MPWRQ_PAGES_PER_WQE, 8))
-#define MLX5E_VALID_NUM_MTTS(num_mtts) (MLX5_MTT_OCTW(num_mtts) - 1 <= U16_MAX)
+#define MLX5E_REQUIRED_WQE_MTTS		(ALIGN(MLX5_MPWRQ_PAGES_PER_WQE, 8))
+#define MLX5E_REQUIRED_MTTS(wqes)	(wqes * MLX5E_REQUIRED_WQE_MTTS)
+#define MLX5E_MAX_RQ_NUM_MTTS	\
+	((1 << 16) * 2) /* So that MLX5_MTT_OCTW(num_mtts) fits into u16 */
+#define MLX5E_ORDER2_MAX_PACKET_MTU (order_base_2(10 * 1024))
+#define MLX5E_PARAMS_MAXIMUM_LOG_RQ_SIZE_MPW	\
+		(ilog2(MLX5E_MAX_RQ_NUM_MTTS / MLX5E_REQUIRED_WQE_MTTS))
+#define MLX5E_LOG_MAX_RQ_NUM_PACKETS_MPW \
+	(MLX5E_PARAMS_MAXIMUM_LOG_RQ_SIZE_MPW + \
+	 (MLX5_MPWRQ_LOG_WQE_SZ - MLX5E_ORDER2_MAX_PACKET_MTU))
+
+#define MLX5E_PARAMS_MINIMUM_LOG_SQ_SIZE                0x6
+#define MLX5E_PARAMS_DEFAULT_LOG_SQ_SIZE                0xa
+#define MLX5E_PARAMS_MAXIMUM_LOG_SQ_SIZE                0xd
+
+#define MLX5E_PARAMS_MINIMUM_LOG_RQ_SIZE                0x1
+#define MLX5E_PARAMS_DEFAULT_LOG_RQ_SIZE                0xa
+#define MLX5E_PARAMS_MAXIMUM_LOG_RQ_SIZE min_t(u8, 0xd,	\
+					       MLX5E_LOG_MAX_RQ_NUM_PACKETS_MPW)
+
+#define MLX5E_PARAMS_MINIMUM_LOG_RQ_SIZE_MPW            0x2
 
 #define MLX5_UMR_ALIGN				(2048)
 #define MLX5_MPWRQ_SMALL_PACKET_THRESHOLD	(256)
@@ -155,26 +161,6 @@ static inline u16 mlx5_min_rx_wqes(int wq_type, u32 wq_size)
 	}
 }
 
-static inline int mlx5_min_log_rq_size(int wq_type)
-{
-	switch (wq_type) {
-	case MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ:
-		return MLX5E_PARAMS_MINIMUM_LOG_RQ_SIZE_MPW;
-	default:
-		return MLX5E_PARAMS_MINIMUM_LOG_RQ_SIZE;
-	}
-}
-
-static inline int mlx5_max_log_rq_size(int wq_type)
-{
-	switch (wq_type) {
-	case MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ:
-		return MLX5E_PARAMS_MAXIMUM_LOG_RQ_SIZE_MPW;
-	default:
-		return MLX5E_PARAMS_MAXIMUM_LOG_RQ_SIZE;
-	}
-}
-
 static inline int mlx5e_get_max_num_channels(struct mlx5_core_dev *mdev)
 {
 	return is_kdump_kernel() ?
@@ -233,7 +219,7 @@ enum mlx5e_priv_flag {
 struct mlx5e_params {
 	u8  log_sq_size;
 	u8  rq_wq_type;
-	u8  log_rq_size;
+	u8  log_rq_mtu_frames;
 	u16 num_channels;
 	u8  num_tc;
 	bool rx_cqe_compress_def;
@@ -849,11 +835,6 @@ void mlx5e_dealloc_rx_wqe(struct mlx5e_rq *rq, u16 ix);
 void mlx5e_dealloc_rx_mpwqe(struct mlx5e_rq *rq, u16 ix);
 void mlx5e_free_rx_mpwqe(struct mlx5e_rq *rq, struct mlx5e_mpw_info *wi);
 
-u8 mlx5e_mpwqe_get_log_stride_size(struct mlx5_core_dev *mdev,
-				   struct mlx5e_params *params);
-u8 mlx5e_mpwqe_get_log_num_strides(struct mlx5_core_dev *mdev,
-				   struct mlx5e_params *params);
-
 void mlx5e_update_stats(struct mlx5e_priv *priv);
 
 int mlx5e_create_flow_steering(struct mlx5e_priv *priv);

commit 472a1e44b3495df01c83e048667ef93dd2ea1ca0
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Mon Mar 12 14:24:41 2018 +0200

    net/mlx5e: Save MTU in channels params
    
    Knowing the MTU is required for RQ creation flow.
    By our design, channels creation flow is totally isolated
    from priv/netdev, and can be completed with access to
    channels params and mdev.
    Adding the MTU to the channels params helps preserving that.
    In addition, we save it in RQ to make its access faster in
    datapath checks.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 353ac6daa3dc..823876bfd6ab 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -57,8 +57,8 @@
 
 #define MLX5E_ETH_HARD_MTU (ETH_HLEN + VLAN_HLEN + ETH_FCS_LEN)
 
-#define MLX5E_HW2SW_MTU(priv, hwmtu) ((hwmtu) - ((priv)->hard_mtu))
-#define MLX5E_SW2HW_MTU(priv, swmtu) ((swmtu) + ((priv)->hard_mtu))
+#define MLX5E_HW2SW_MTU(params, hwmtu) ((hwmtu) - ((params)->hard_mtu))
+#define MLX5E_SW2HW_MTU(params, swmtu) ((swmtu) + ((params)->hard_mtu))
 
 #define MLX5E_MAX_DSCP          64
 #define MLX5E_MAX_NUM_TC	8
@@ -251,6 +251,8 @@ struct mlx5e_params {
 	u32 lro_timeout;
 	u32 pflags;
 	struct bpf_prog *xdp_prog;
+	unsigned int sw_mtu;
+	int hard_mtu;
 };
 
 #ifdef CONFIG_MLX5_CORE_EN_DCB
@@ -534,6 +536,7 @@ struct mlx5e_rq {
 
 	/* XDP */
 	struct bpf_prog       *xdp_prog;
+	unsigned int           hw_mtu;
 	struct mlx5e_xdpsq     xdpsq;
 
 	/* control */
@@ -767,7 +770,6 @@ struct mlx5e_priv {
 	struct mlx5e_tir           inner_indir_tir[MLX5E_NUM_INDIR_TIRS];
 	struct mlx5e_tir           direct_tir[MLX5E_MAX_NUM_CHANNELS];
 	u32                        tx_rates[MLX5E_MAX_NUM_SQS];
-	int                        hard_mtu;
 
 	struct mlx5e_flow_steering fs;
 	struct mlx5e_vxlan_db      vxlan;
@@ -1111,7 +1113,7 @@ void mlx5e_detach_netdev(struct mlx5e_priv *priv);
 void mlx5e_destroy_netdev(struct mlx5e_priv *priv);
 void mlx5e_build_nic_params(struct mlx5_core_dev *mdev,
 			    struct mlx5e_params *params,
-			    u16 max_channels);
+			    u16 max_channels, u16 mtu);
 u8 mlx5e_params_calculate_tx_min_inline(struct mlx5_core_dev *mdev);
 void mlx5e_rx_dim_work(struct work_struct *work);
 #endif /* __MLX5_EN_H__ */

commit db75373c91b0cfb6a68ad6ae88721e4e21ae6261
Author: Eran Ben Elisha <eranbe@mellanox.com>
Date:   Tue Dec 26 16:02:24 2017 +0200

    net/mlx5e: Recover Send Queue (SQ) from error state
    
    An error TX completion (CQE) which arrived on a specific SQ indicates
    that this SQ got moved by the hardware to error state, which means all
    pending and incoming TX requests are dropped or will be dropped and no
    further "Good" CQEs will be generated for that SQ.
    
    Before this patch TX completions (CQEs) were not monitored and were
    handled as a regular CQE. This caused the SQ to stay in an error state,
    making it useless for xmiting new packets.
    
    Mitigation plan:
    In case of an error completion, schedule a recovery work which would do
    the following:
    - Mark the TXQ as DRV_XOFF to disable new packets to arrive from the
      stack
    - NAPI to flush all pending SQ WQEs (via flush_in_error_en bit) to
      release SW and HW resources(SKB, DMA, etc) and have the SQ and CQ
      consumer/producer indices synced.
    - Modify the SQ state ERR -> RST -> RDY (restart the SQ).
    - Reactivate the SQ and reset SQ cc and pc
    
    If we identify two consecutive requests for SQ recover in less than
    500 msecs, drop the recover request to avoid CPU overload, as this
    scenario most likely happened due to a severe repeated bug.
    
    In addition, add SQ recover SW counter to monitor successful recoveries.
    
    Signed-off-by: Eran Ben Elisha <eranbe@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 6898f5e26006..353ac6daa3dc 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -122,6 +122,7 @@
 #define MLX5E_MAX_NUM_SQS              (MLX5E_MAX_NUM_CHANNELS * MLX5E_MAX_NUM_TC)
 #define MLX5E_TX_CQ_POLL_BUDGET        128
 #define MLX5E_UPDATE_STATS_INTERVAL    200 /* msecs */
+#define MLX5E_SQ_RECOVER_MIN_INTERVAL  500 /* msecs */
 
 #define MLX5E_ICOSQ_MAX_WQEBBS \
 	(DIV_ROUND_UP(sizeof(struct mlx5e_umr_wqe), MLX5_SEND_WQE_BB))
@@ -332,6 +333,7 @@ struct mlx5e_sq_dma {
 
 enum {
 	MLX5E_SQ_STATE_ENABLED,
+	MLX5E_SQ_STATE_RECOVERING,
 	MLX5E_SQ_STATE_IPSEC,
 };
 
@@ -378,6 +380,10 @@ struct mlx5e_txqsq {
 	struct mlx5e_channel      *channel;
 	int                        txq_ix;
 	u32                        rate_limit;
+	struct mlx5e_txqsq_recover {
+		struct work_struct         recover_work;
+		u64                        last_recover;
+	} recover;
 } ____cacheline_aligned_in_smp;
 
 struct mlx5e_xdpsq {

commit c4554fbccaa3306f65954ed0f1dab7abce7889f8
Author: Gal Pressman <galp@mellanox.com>
Date:   Sun Jan 21 10:52:17 2018 +0200

    net/mlx5e: Remove unused max inline related code
    
    Commit 58d522912ac7 ("net/mlx5e: Support TX packet copy into WQE")
    introduced the max inline WQE as an ethtool tunable. One commit later,
    that functionality was made dependent on BlueFlame.
    
    Commit 6982ab609768 ("net/mlx5e: Xmit, no write combining") removed
    BlueFlame support, and with it the max inline WQE.
    This patch cleans up the leftovers from the removed feature.
    
    Signed-off-by: Gal Pressman <galp@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 13dd7a97ae04..6898f5e26006 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -240,7 +240,6 @@ struct mlx5e_params {
 	struct net_dim_cq_moder tx_cq_moderation;
 	bool lro_en;
 	u32 lro_wqe_sz;
-	u16 tx_max_inline;
 	u8  tx_min_inline_mode;
 	u8  rss_hfunc;
 	u8  toeplitz_hash_key[40];
@@ -366,7 +365,6 @@ struct mlx5e_txqsq {
 	void __iomem              *uar_map;
 	struct netdev_queue       *txq;
 	u32                        sqn;
-	u16                        max_inline;
 	u8                         min_inline_mode;
 	u16                        edge;
 	struct device             *pdev;
@@ -1017,7 +1015,6 @@ int mlx5e_rx_flow_steer(struct net_device *dev, const struct sk_buff *skb,
 			u16 rxq_index, u32 flow_id);
 #endif
 
-u16 mlx5e_get_max_inline_cap(struct mlx5_core_dev *mdev);
 int mlx5e_create_tir(struct mlx5_core_dev *mdev,
 		     struct mlx5e_tir *tir, u32 *in, int inlen);
 void mlx5e_destroy_tir(struct mlx5_core_dev *mdev,

commit 2ccb0a79018c9fafa913654163adc9dbac1280c5
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Wed Feb 7 14:51:45 2018 +0200

    net/mlx5e: Add ethtool priv-flag for Striding RQ
    
    Add a control private flag in ethtool to enable/disable
    Striding RQ feature.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 45d0c64e77e5..13dd7a97ae04 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -205,12 +205,14 @@ static const char mlx5e_priv_flags[][ETH_GSTRING_LEN] = {
 	"rx_cqe_moder",
 	"tx_cqe_moder",
 	"rx_cqe_compress",
+	"rx_striding_rq",
 };
 
 enum mlx5e_priv_flag {
 	MLX5E_PFLAG_RX_CQE_BASED_MODER = (1 << 0),
 	MLX5E_PFLAG_TX_CQE_BASED_MODER = (1 << 1),
 	MLX5E_PFLAG_RX_CQE_COMPRESS = (1 << 2),
+	MLX5E_PFLAG_RX_STRIDING_RQ = (1 << 3),
 };
 
 #define MLX5E_SET_PFLAG(params, pflag, enable)			\
@@ -827,6 +829,10 @@ bool mlx5e_poll_xdpsq_cq(struct mlx5e_cq *cq);
 void mlx5e_free_txqsq_descs(struct mlx5e_txqsq *sq);
 void mlx5e_free_xdpsq_descs(struct mlx5e_xdpsq *sq);
 
+bool mlx5e_check_fragmented_striding_rq_cap(struct mlx5_core_dev *mdev);
+bool mlx5e_striding_rq_possible(struct mlx5_core_dev *mdev,
+				struct mlx5e_params *params);
+
 void mlx5e_page_release(struct mlx5e_rq *rq, struct mlx5e_dma_info *dma_info,
 			bool recycle);
 void mlx5e_handle_rx_cqe(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe);
@@ -917,6 +923,7 @@ void mlx5e_set_tx_cq_mode_params(struct mlx5e_params *params,
 				 u8 cq_period_mode);
 void mlx5e_set_rx_cq_mode_params(struct mlx5e_params *params,
 				 u8 cq_period_mode);
+void mlx5e_set_rq_type(struct mlx5_core_dev *mdev, struct mlx5e_params *params);
 void mlx5e_init_rq_type_params(struct mlx5_core_dev *mdev,
 			       struct mlx5e_params *params);
 

commit 2a0f561bf81c13d36910c7312ac8c67f83320a07
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Sun Feb 18 11:37:06 2018 +0200

    net/mlx5e: Do not reset Receive Queue params on every type change
    
    Do not implicit a call to mlx5e_init_rq_type_params() upon every
    change in RQ type. It should be called only on channels creation.
    
    Fixes: 2fc4bfb7250d ("net/mlx5e: Dynamic RQ type infrastructure")
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index ff9aeda186a1..45d0c64e77e5 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -918,8 +918,7 @@ void mlx5e_set_tx_cq_mode_params(struct mlx5e_params *params,
 void mlx5e_set_rx_cq_mode_params(struct mlx5e_params *params,
 				 u8 cq_period_mode);
 void mlx5e_init_rq_type_params(struct mlx5_core_dev *mdev,
-			       struct mlx5e_params *params,
-			       u8 rq_type);
+			       struct mlx5e_params *params);
 
 static inline bool mlx5e_tunnel_inner_ft_supported(struct mlx5_core_dev *mdev)
 {

commit b0cedc844c00991d323c13b82d651d372dcbbb12
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Wed Feb 7 13:28:35 2018 +0200

    net/mlx5e: Remove rq_headroom field from params
    
    It can be derived from other params, calculate it
    via the dedicated function when needed.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index ba7f1ceb6dcd..ff9aeda186a1 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -230,7 +230,6 @@ enum mlx5e_priv_flag {
 struct mlx5e_params {
 	u8  log_sq_size;
 	u8  rq_wq_type;
-	u16 rq_headroom;
 	u8  log_rq_size;
 	u16 num_channels;
 	u8  num_tc;

commit f1e4fc9b4b02ec728b06ad09d6cdf5a9bb7ec372
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Wed Feb 7 13:21:30 2018 +0200

    net/mlx5e: Remove RQ MPWQE fields from params
    
    Introduce functions to calculate them when needed.
    They can be derived from other params.
    This will simplify transition between RQ configurations.
    
    In general, any parameter that is not explicitly set
    or controlled, but derived from other parameters,
    should not have a control-path field itself, but a
    getter function.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 85767f0869d8..ba7f1ceb6dcd 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -231,8 +231,6 @@ struct mlx5e_params {
 	u8  log_sq_size;
 	u8  rq_wq_type;
 	u16 rq_headroom;
-	u8  mpwqe_log_stride_sz;
-	u8  mpwqe_log_num_strides;
 	u8  log_rq_size;
 	u16 num_channels;
 	u8  num_tc;
@@ -840,6 +838,11 @@ void mlx5e_dealloc_rx_wqe(struct mlx5e_rq *rq, u16 ix);
 void mlx5e_dealloc_rx_mpwqe(struct mlx5e_rq *rq, u16 ix);
 void mlx5e_free_rx_mpwqe(struct mlx5e_rq *rq, struct mlx5e_mpw_info *wi);
 
+u8 mlx5e_mpwqe_get_log_stride_size(struct mlx5_core_dev *mdev,
+				   struct mlx5e_params *params);
+u8 mlx5e_mpwqe_get_log_num_strides(struct mlx5_core_dev *mdev,
+				   struct mlx5e_params *params);
+
 void mlx5e_update_stats(struct mlx5e_priv *priv);
 
 int mlx5e_create_flow_steering(struct mlx5e_priv *priv);

commit 73faf36c5ae6b300490f0a5f2835fa702adef0e1
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Tue Nov 21 17:43:50 2017 +0200

    net/mlx5e: Remove unused define MLX5_MPWRQ_STRIDES_PER_PAGE
    
    Clean it up as it's not in use.
    
    Fixes: d9d9f156f380 ("net/mlx5e: Expand WQE stride when CQE compression is enabled")
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 294bc9f175a5..85767f0869d8 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -93,8 +93,6 @@
 #define MLX5_MPWRQ_WQE_PAGE_ORDER  (MLX5_MPWRQ_LOG_WQE_SZ - PAGE_SHIFT > 0 ? \
 				    MLX5_MPWRQ_LOG_WQE_SZ - PAGE_SHIFT : 0)
 #define MLX5_MPWRQ_PAGES_PER_WQE		BIT(MLX5_MPWRQ_WQE_PAGE_ORDER)
-#define MLX5_MPWRQ_STRIDES_PER_PAGE		(MLX5_MPWRQ_NUM_STRIDES >> \
-						 MLX5_MPWRQ_WQE_PAGE_ORDER)
 
 #define MLX5_MTT_OCTW(npages) (ALIGN(npages, 8) / 2)
 #define MLX5E_REQUIRED_MTTS(wqes)		\

commit 707129dceaf4b8c4721425d75e855aed4fd7b832
Author: Gal Pressman <galp@mellanox.com>
Date:   Wed Jan 31 14:45:40 2018 +0200

    net/mlx5e: Make choose LRO timeout function static
    
    The function is used in en_main.c only, we can make it static and remove
    its declaration from en.h
    
    Signed-off-by: Gal Pressman <galp@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 48e0b2a747d9..294bc9f175a5 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -1062,7 +1062,6 @@ void mlx5e_cleanup_nic_tx(struct mlx5e_priv *priv);
 int mlx5e_close(struct net_device *netdev);
 int mlx5e_open(struct net_device *netdev);
 void mlx5e_update_stats_work(struct work_struct *work);
-u32 mlx5e_choose_lro_timeout(struct mlx5_core_dev *mdev, u32 wanted_timeout);
 
 int mlx5e_bits_invert(unsigned long a, int size);
 

commit 7cbaf9a3ea09ea36c7101e7b244f734f9c09c72b
Author: Moshe Shemesh <moshe@mellanox.com>
Date:   Thu Feb 8 15:09:57 2018 +0200

    net/mlx5e: Add interface down dropped packets statistics
    
    Added the following packets drop counter:
    Rx interface down dropped packets - counts packets which were received
    while the ETH interface was down.
    This counter will be shown on ethtool as a new counter called
    rx_if_down_packets.
    
    The implementation allocates a q_counter for drop rq which gets all the
    received traffic while the interface is down.
    
    Signed-off-by: Moshe Shemesh <moshe@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 4c9360b25532..48e0b2a747d9 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -781,7 +781,8 @@ struct mlx5e_priv {
 	struct net_device         *netdev;
 	struct mlx5e_stats         stats;
 	struct hwtstamp_config     tstamp;
-	u16 q_counter;
+	u16                        q_counter;
+	u16                        drop_rq_q_counter;
 #ifdef CONFIG_MLX5_CORE_EN_DCB
 	struct mlx5e_dcbx          dcbx;
 #endif

commit 193861773534a5711aa439d97eba3515310bb586
Author: Kamal Heib <kamalh@mellanox.com>
Date:   Tue Nov 28 23:52:13 2017 +0200

    net/mlx5e: Extend the stats group API to have update_stats()
    
    Extend the stats group API to have an update_stats() callback which
    will be used to fetch the hardware or software counters data.
    
    Signed-off-by: Kamal Heib <kamalh@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index a70ae3abcbbe..4c9360b25532 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -841,7 +841,7 @@ void mlx5e_dealloc_rx_wqe(struct mlx5e_rq *rq, u16 ix);
 void mlx5e_dealloc_rx_mpwqe(struct mlx5e_rq *rq, u16 ix);
 void mlx5e_free_rx_mpwqe(struct mlx5e_rq *rq, struct mlx5e_mpw_info *wi);
 
-void mlx5e_update_stats(struct mlx5e_priv *priv, bool full);
+void mlx5e_update_stats(struct mlx5e_priv *priv);
 
 int mlx5e_create_flow_steering(struct mlx5e_priv *priv);
 void mlx5e_destroy_flow_steering(struct mlx5e_priv *priv);

commit 57d689a8ca7b360f902328da95e8b441c24823ca
Author: Eran Ben Elisha <eranbe@mellanox.com>
Date:   Tue Dec 19 14:53:34 2017 +0200

    net/mlx5e: Add per-channel counters infrastructure, use it upon TX timeout
    
    Add per-channel counter ch#_eq_rearm to monitor how many lost interrupt
    recovery actions happened upon TX timeouts.
    
    Signed-off-by: Eran Ben Elisha <eranbe@mellanox.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index a3536e051052..a70ae3abcbbe 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -561,6 +561,7 @@ struct mlx5e_channel {
 
 	/* data path - accessed per napi poll */
 	struct irq_desc *irq_desc;
+	struct mlx5e_ch_stats      stats;
 
 	/* control */
 	struct mlx5e_priv         *priv;

commit 3f6d08d196b2d05e36c86d7ccbbaa3431a431bed
Author: Or Gerlitz <ogerlitz@mellanox.com>
Date:   Sun Nov 26 20:39:12 2017 +0200

    net/mlx5e: Add RSS support for hairpin
    
    Support RSS for hairpin traffic. We create multiple hairpin RQ/SQ pairs
    and RSS TTC table per hairpin instance and steer the related flows
    through that table so they are spread between the pairs.
    
    We open one pair per 50Gbs link speed, for all speeds <= 50Gbs, there
    is one pair and no RSS while for 100Gbs ports two RSSed pairs.
    
    Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 10620c3e0c61..a3536e051052 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -697,6 +697,11 @@ enum {
 	MLX5E_ARFS_FT_LEVEL
 };
 
+enum {
+	MLX5E_TC_FT_LEVEL = 0,
+	MLX5E_TC_TTC_FT_LEVEL,
+};
+
 struct mlx5e_ethtool_table {
 	struct mlx5_flow_table *ft;
 	int                    num_rules;
@@ -1057,6 +1062,8 @@ int mlx5e_open(struct net_device *netdev);
 void mlx5e_update_stats_work(struct work_struct *work);
 u32 mlx5e_choose_lro_timeout(struct mlx5_core_dev *mdev, u32 wanted_timeout);
 
+int mlx5e_bits_invert(unsigned long a, int size);
+
 /* ethtool helpers */
 void mlx5e_ethtool_get_drvinfo(struct mlx5e_priv *priv,
 			       struct ethtool_drvinfo *drvinfo);

commit 1ae1df3a119395048a7b2614315d7e88d88a78f3
Author: Or Gerlitz <ogerlitz@mellanox.com>
Date:   Wed Dec 6 21:05:01 2017 +0200

    net/mlx5e: Refactor RSS related objects and code
    
    In order to use RSS for hairpin, we refactor the code that deals with
    setup of the TTC steering tables. This is done using an interim ttc
    params object that has the flow table attributes, TIR numbers, etc.
    
    Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 7b988595ac5f..10620c3e0c61 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -44,6 +44,7 @@
 #include <linux/mlx5/port.h>
 #include <linux/mlx5/vport.h>
 #include <linux/mlx5/transobj.h>
+#include <linux/mlx5/fs.h>
 #include <linux/rhashtable.h>
 #include <net/switchdev.h>
 #include <net/xdp.h>
@@ -1024,11 +1025,26 @@ int mlx5e_create_direct_tirs(struct mlx5e_priv *priv);
 void mlx5e_destroy_direct_tirs(struct mlx5e_priv *priv);
 void mlx5e_destroy_rqt(struct mlx5e_priv *priv, struct mlx5e_rqt *rqt);
 
-int mlx5e_create_ttc_table(struct mlx5e_priv *priv);
-void mlx5e_destroy_ttc_table(struct mlx5e_priv *priv);
+struct ttc_params {
+	struct mlx5_flow_table_attr ft_attr;
+	u32 any_tt_tirn;
+	u32 indir_tirn[MLX5E_NUM_INDIR_TIRS];
+	struct mlx5e_ttc_table *inner_ttc;
+};
+
+void mlx5e_set_ttc_basic_params(struct mlx5e_priv *priv, struct ttc_params *ttc_params);
+void mlx5e_set_ttc_ft_params(struct ttc_params *ttc_params);
+void mlx5e_set_inner_ttc_ft_params(struct ttc_params *ttc_params);
+
+int mlx5e_create_ttc_table(struct mlx5e_priv *priv, struct ttc_params *params,
+			   struct mlx5e_ttc_table *ttc);
+void mlx5e_destroy_ttc_table(struct mlx5e_priv *priv,
+			     struct mlx5e_ttc_table *ttc);
 
-int mlx5e_create_inner_ttc_table(struct mlx5e_priv *priv);
-void mlx5e_destroy_inner_ttc_table(struct mlx5e_priv *priv);
+int mlx5e_create_inner_ttc_table(struct mlx5e_priv *priv, struct ttc_params *params,
+				 struct mlx5e_ttc_table *ttc);
+void mlx5e_destroy_inner_ttc_table(struct mlx5e_priv *priv,
+				   struct mlx5e_ttc_table *ttc);
 
 int mlx5e_create_tis(struct mlx5_core_dev *mdev, int tc,
 		     u32 underlay_qpn, u32 *tisn);

commit c02b3741eb99a1ec733e6134c53ba59e43e19e97
Merge: 7018d1b3f20f 8cbab92dff77
Author: David S. Miller <davem@davemloft.net>
Date:   Wed Jan 17 00:00:25 2018 -0500

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Overlapping changes all over.
    
    The mini-qdisc bits were a little bit tricky, however.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 237f258c42c905f71c694670fe4d9773d85c36ed
Author: Feras Daoud <ferasda@mellanox.com>
Date:   Mon Jan 8 10:01:04 2018 +0200

    net/mlx5e: Remove timestamp set from netdevice open flow
    
    To avoid configuration override, timestamp set call will
    be moved from the netdevice open flow to the init flow.
    By this, a close-open procedure will not override the timestamp
    configuration.
    In addition, the change will rename mlx5e_timestamp_set function
    to be mlx5e_timestamp_init.
    
    Fixes: ef9814deafd0 ("net/mlx5e: Add HW timestamping (TS) support")
    Signed-off-by: Feras Daoud <ferasda@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 543060c305a0..c2d89bfa1a70 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -895,7 +895,7 @@ int mlx5e_vlan_rx_kill_vid(struct net_device *dev, __always_unused __be16 proto,
 			   u16 vid);
 void mlx5e_enable_cvlan_filter(struct mlx5e_priv *priv);
 void mlx5e_disable_cvlan_filter(struct mlx5e_priv *priv);
-void mlx5e_timestamp_set(struct mlx5e_priv *priv);
+void mlx5e_timestamp_init(struct mlx5e_priv *priv);
 
 struct mlx5e_redirect_rqt_param {
 	bool is_rss;

commit 4c4dbb4a7363ce56c77cf8eb2090f67c1b0aa2cc
Author: Andy Gospodarek <gospo@broadcom.com>
Date:   Tue Jan 9 16:06:18 2018 -0500

    net/mlx5e: Move dynamic interrupt coalescing code to include/linux
    
    This move allows drivers to add private structure elements to track the
    number of packets, bytes, and interrupts events per ring.  A driver
    also defines a workqueue handler to act on this collected data once per
    poll and modify the coalescing parameters per ring.
    
    Signed-off-by: Andy Gospodarek <gospo@broadcom.com>
    Acked-by: Tal Gilboa <talgi@mellanox.com>
    Acked-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 349918c4ff73..d629da213511 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -47,10 +47,10 @@
 #include <linux/rhashtable.h>
 #include <net/switchdev.h>
 #include <net/xdp.h>
+#include <linux/net_dim.h>
 #include "wq.h"
 #include "mlx5_core.h"
 #include "en_stats.h"
-#include "net_dim.h"
 
 #define MLX5_SET_CFG(p, f, v) MLX5_SET(create_flow_group_in, p, f, v)
 

commit 9a31742531018c0b6ab1bccff97109c8020ad86e
Author: Andy Gospodarek <gospo@broadcom.com>
Date:   Tue Jan 9 16:06:17 2018 -0500

    net/mlx5e: Change Mellanox references in DIM code
    
    Change all appropriate mlx5_am* and MLX5_AM* references to net_dim and
    NET_DIM, respectively, in code that handles dynamic interrupt
    moderation.  Also change all references from 'am' to 'dim' when used as
    local variables and add generic profile references.
    
    Signed-off-by: Andy Gospodarek <gospo@broadcom.com>
    Acked-by: Tal Gilboa <talgi@mellanox.com>
    Acked-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index d13fa1e8a40b..349918c4ff73 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -238,8 +238,8 @@ struct mlx5e_params {
 	u16 num_channels;
 	u8  num_tc;
 	bool rx_cqe_compress_def;
-	struct mlx5e_cq_moder rx_cq_moderation;
-	struct mlx5e_cq_moder tx_cq_moderation;
+	struct net_dim_cq_moder rx_cq_moderation;
+	struct net_dim_cq_moder tx_cq_moderation;
 	bool lro_en;
 	u32 lro_wqe_sz;
 	u16 tx_max_inline;
@@ -249,7 +249,7 @@ struct mlx5e_params {
 	u32 indirection_rqt[MLX5E_INDIR_RQT_SIZE];
 	bool vlan_strip_disable;
 	bool scatter_fcs_en;
-	bool rx_am_enabled;
+	bool rx_dim_enabled;
 	u32 lro_timeout;
 	u32 pflags;
 	struct bpf_prog *xdp_prog;
@@ -528,7 +528,7 @@ struct mlx5e_rq {
 	unsigned long          state;
 	int                    ix;
 
-	struct mlx5e_rx_am     am; /* Adaptive Moderation */
+	struct net_dim         dim; /* Dynamic Interrupt Moderation */
 
 	/* XDP */
 	struct bpf_prog       *xdp_prog;
@@ -1080,4 +1080,5 @@ void mlx5e_build_nic_params(struct mlx5_core_dev *mdev,
 			    struct mlx5e_params *params,
 			    u16 max_channels);
 u8 mlx5e_params_calculate_tx_min_inline(struct mlx5_core_dev *mdev);
+void mlx5e_rx_dim_work(struct work_struct *work);
 #endif /* __MLX5_EN_H__ */

commit b9c872f2316d27e2a323ffb410d706d680508093
Author: Andy Gospodarek <gospo@broadcom.com>
Date:   Tue Jan 9 16:06:16 2018 -0500

    net/mlx5e: Move generic functions to new file
    
    These functions were identified as ones that could be made generic and
    used by multiple drivers.  Most of the contents of en_rx_am.c are moved
    to net_dim.c.
    
    Signed-off-by: Andy Gospodarek <gospo@broadcom.com>
    Acked-by: Tal Gilboa <talgi@mellanox.com>
    Acked-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 470dcf5b2cc8..d13fa1e8a40b 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -50,7 +50,7 @@
 #include "wq.h"
 #include "mlx5_core.h"
 #include "en_stats.h"
-#include "en_dim.h"
+#include "net_dim.h"
 
 #define MLX5_SET_CFG(p, f, v) MLX5_SET(create_flow_group_in, p, f, v)
 

commit f58ee099f32e3bfc19f1cf308c7cd89bb82467fe
Author: Andy Gospodarek <gospo@broadcom.com>
Date:   Tue Jan 9 16:06:13 2018 -0500

    net/mlx5e: Move interrupt moderation forward declarations
    
    Move these to newly created file to prepare to move these functions to a
    library.
    
    Signed-off-by: Andy Gospodarek <gospo@broadcom.com>
    Acked-by: Tal Gilboa <talgi@mellanox.com>
    Acked-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index f692a66e42ef..470dcf5b2cc8 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -834,10 +834,6 @@ void mlx5e_dealloc_rx_wqe(struct mlx5e_rq *rq, u16 ix);
 void mlx5e_dealloc_rx_mpwqe(struct mlx5e_rq *rq, u16 ix);
 void mlx5e_free_rx_mpwqe(struct mlx5e_rq *rq, struct mlx5e_mpw_info *wi);
 
-void mlx5e_rx_am(struct mlx5e_rq *rq);
-void mlx5e_rx_am_work(struct work_struct *work);
-struct mlx5e_cq_moder mlx5e_am_get_def_profile(u8 rx_cq_period_mode);
-
 void mlx5e_update_stats(struct mlx5e_priv *priv, bool full);
 
 int mlx5e_create_flow_steering(struct mlx5e_priv *priv);

commit 98dd1edffc6cd1a2ff3b18596f7c88f6c082fd2e
Author: Andy Gospodarek <gospo@broadcom.com>
Date:   Tue Jan 9 16:06:12 2018 -0500

    net/mlx5e: Move interrupt moderation structs to new file
    
    Create new header file to prepare to move code that handles irq
    moderation to a library that lives in a header file.
    
    Signed-off-by: Andy Gospodarek <gospo@broadcom.com>
    Acked-by: Tal Gilboa <talgi@mellanox.com>
    Acked-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 72bab8d3f4b0..f692a66e42ef 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -50,6 +50,7 @@
 #include "wq.h"
 #include "mlx5_core.h"
 #include "en_stats.h"
+#include "en_dim.h"
 
 #define MLX5_SET_CFG(p, f, v) MLX5_SET(create_flow_group_in, p, f, v)
 
@@ -227,12 +228,6 @@ enum mlx5e_priv_flag {
 #define MLX5E_MAX_BW_ALLOC 100 /* Max percentage of BW allocation */
 #endif
 
-struct mlx5e_cq_moder {
-	u16 usec;
-	u16 pkts;
-	u8 cq_period_mode;
-};
-
 struct mlx5e_params {
 	u8  log_sq_size;
 	u8  rq_wq_type;
@@ -473,32 +468,6 @@ struct mlx5e_mpw_info {
 	u16 skbs_frags[MLX5_MPWRQ_PAGES_PER_WQE];
 };
 
-struct mlx5e_rx_am_stats {
-	int ppms; /* packets per msec */
-	int bpms; /* bytes per msec */
-	int epms; /* events per msec */
-};
-
-struct mlx5e_rx_am_sample {
-	ktime_t	time;
-	u32	pkt_ctr;
-	u32	byte_ctr;
-	u16	event_ctr;
-};
-
-struct mlx5e_rx_am { /* Adaptive Moderation */
-	u8					state;
-	struct mlx5e_rx_am_stats		prev_stats;
-	struct mlx5e_rx_am_sample		start_sample;
-	struct work_struct			work;
-	u8					profile_ix;
-	u8					mode;
-	u8					tune_state;
-	u8					steps_right;
-	u8					steps_left;
-	u8					tired;
-};
-
 /* a single cache unit is capable to serve one napi call (for non-striding rq)
  * or a MPWQE (for striding rq).
  */

commit 5c65c564c96266110bec58f42c505bff6d5564f7
Author: Or Gerlitz <ogerlitz@mellanox.com>
Date:   Wed Nov 22 21:09:05 2017 +0200

    net/mlx5e: Support offloading TC NIC hairpin flows
    
    We refer to TC NIC rule that involves forwarding as "hairpin".
    
    All hairpin rules from the current NIC device (called "func" in
    the code) to a given NIC device ("peer") are steered into the
    same hairpin RQ/SQ pair.
    
    The hairpin pair is set on demand and removed when there are no
    TC rules that need it.
    
    Here's a TC rule that matches on icmp, does header re-write of the
    dst mac and hairpin from RX/enp1s2f1 to TX/enp1s2f2 (enp1s2f1/2 are
    two mlx5 devices):
    
    tc filter add dev enp1s2f1 protocol ip parent ffff: prio 2
        flower skip_sw ip_proto icmp
         action pedit ex munge eth dst set 10:22:33:44:55:66 pipe
         action mirred egress redirect dev enp1s2f2
    
    Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 5299310f2481..72bab8d3f4b0 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -659,6 +659,7 @@ struct mlx5e_tc_table {
 	struct rhashtable               ht;
 
 	DECLARE_HASHTABLE(mod_hdr_tbl, 8);
+	DECLARE_HASHTABLE(hairpin_tbl, 8);
 };
 
 struct mlx5e_vlan_table {

commit 0ddf543226acacfb9f521dafc6c817d5b04c7b1f
Author: Jesper Dangaard Brouer <brouer@redhat.com>
Date:   Wed Jan 3 11:25:18 2018 +0100

    xdp/mlx5: setup xdp_rxq_info
    
    The mlx5 driver have a special drop-RQ queue (one per interface) that
    simply drops all incoming traffic. It helps driver keep other HW
    objects (flow steering) alive upon down/up operations.  It is
    temporarily pointed by flow steering objects during the interface
    setup, and when interface is down. It lacks many fields that are set
    in a regular RQ (for example its state is never switched to
    MLX5_RQC_STATE_RDY). (Thanks to Tariq Toukan for explanation).
    
    The XDP RX-queue info for this drop-RQ marked as unused, which
    allow us to use the same takedown/free code path as other RX-queues.
    
    Driver hook points for xdp_rxq_info:
     * reg   : mlx5e_alloc_rq()
     * unused: mlx5e_alloc_drop_rq()
     * unreg : mlx5e_free_rq()
    
    Tested on actual hardware with samples/bpf program
    
    Cc: Saeed Mahameed <saeedm@mellanox.com>
    Cc: Matan Barak <matanb@mellanox.com>
    Cc: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 543060c305a0..5299310f2481 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -46,6 +46,7 @@
 #include <linux/mlx5/transobj.h>
 #include <linux/rhashtable.h>
 #include <net/switchdev.h>
+#include <net/xdp.h>
 #include "wq.h"
 #include "mlx5_core.h"
 #include "en_stats.h"
@@ -571,6 +572,9 @@ struct mlx5e_rq {
 	u32                    rqn;
 	struct mlx5_core_dev  *mdev;
 	struct mlx5_core_mkey  umr_mkey;
+
+	/* XDP read-mostly */
+	struct xdp_rxq_info    xdp_rxq;
 } ____cacheline_aligned_in_smp;
 
 struct mlx5e_channel {

commit 696a97cf9f5c551fca257e0d4aa07b5cbde6084a
Author: Eugenia Emantayev <eugenia@mellanox.com>
Date:   Tue Nov 14 09:44:55 2017 +0200

    net/mlx5e: Fix defaulting RX ring size when not needed
    
    Fixes the bug when turning on/off CQE compression mechanism
    resets the RX rings size to default value when it is not
    needed.
    
    Fixes: 2fc4bfb7250d ("net/mlx5e: Dynamic RQ type infrastructure")
    Signed-off-by: Eugenia Emantayev <eugenia@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 43f9054830e5..543060c305a0 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -82,6 +82,9 @@
 	max_t(u32, MLX5_MPWRQ_MIN_LOG_STRIDE_SZ(mdev), req)
 #define MLX5_MPWRQ_DEF_LOG_STRIDE_SZ(mdev)       MLX5_MPWRQ_LOG_STRIDE_SZ(mdev, 6)
 #define MLX5_MPWRQ_CQE_CMPRS_LOG_STRIDE_SZ(mdev) MLX5_MPWRQ_LOG_STRIDE_SZ(mdev, 8)
+#define MLX5E_MPWQE_STRIDE_SZ(mdev, cqe_cmprs) \
+	(cqe_cmprs ? MLX5_MPWRQ_CQE_CMPRS_LOG_STRIDE_SZ(mdev) : \
+	MLX5_MPWRQ_DEF_LOG_STRIDE_SZ(mdev))
 
 #define MLX5_MPWRQ_LOG_WQE_SZ			18
 #define MLX5_MPWRQ_WQE_PAGE_ORDER  (MLX5_MPWRQ_LOG_WQE_SZ - PAGE_SHIFT > 0 ? \
@@ -936,8 +939,9 @@ void mlx5e_set_tx_cq_mode_params(struct mlx5e_params *params,
 				 u8 cq_period_mode);
 void mlx5e_set_rx_cq_mode_params(struct mlx5e_params *params,
 				 u8 cq_period_mode);
-void mlx5e_set_rq_type_params(struct mlx5_core_dev *mdev,
-			      struct mlx5e_params *params, u8 rq_type);
+void mlx5e_init_rq_type_params(struct mlx5_core_dev *mdev,
+			       struct mlx5e_params *params,
+			       u8 rq_type);
 
 static inline bool mlx5e_tunnel_inner_ft_supported(struct mlx5_core_dev *mdev)
 {

commit 231243c82793428467524227ae02ca451e6a98e7
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Fri Nov 10 15:59:52 2017 +0900

    Revert "mlx5: move affinity hints assignments to generic code"
    
    Before the offending commit, mlx5 core did the IRQ affinity itself,
    and it seems that the new generic code have some drawbacks and one
    of them is the lack for user ability to modify irq affinity after
    the initial affinity values got assigned.
    
    The issue is still being discussed and a solution in the new generic code
    is required, until then we need to revert this patch.
    
    This fixes the following issue:
    echo <new affinity> > /proc/irq/<x>/smp_affinity
    fails with  -EIO
    
    This reverts commit a435393acafbf0ecff4deb3e3cb554b34f0d0664.
    Note: kept mlx5_get_vector_affinity in include/linux/mlx5/driver.h since
    it is used in mlx5_ib driver.
    
    Fixes: a435393acafb ("mlx5: move affinity hints assignments to generic code")
    Cc: Sagi Grimberg <sagi@grimberg.me>
    Cc: Thomas Gleixner <tglx@linutronix.de>
    Cc: Jes Sorensen <jsorensen@fb.com>
    Reported-by: Jes Sorensen <jsorensen@fb.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index c0872b3284cb..43f9054830e5 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -590,6 +590,7 @@ struct mlx5e_channel {
 	struct mlx5_core_dev      *mdev;
 	struct hwtstamp_config    *tstamp;
 	int                        ix;
+	int                        cpu;
 };
 
 struct mlx5e_channels {

commit fdae5f37a88caed9d2105f5a1ff609322f9e5416
Merge: 7c5556decd0a b39545684a90
Author: David S. Miller <davem@davemloft.net>
Date:   Sun Nov 12 09:17:05 2017 +0900

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net

commit d1c61e6d79ea0d4d53dc18bcd2db30ef2d99cfa7
Author: Eugenia Emantayev <eugenia@mellanox.com>
Date:   Thu Jan 12 17:11:45 2017 +0200

    net/mlx5e: Increase Striding RQ minimum size limit to 4 multi-packet WQEs
    
    This is to prevent the case of working with a single MPWQE
    (1 WQE is always reserved as RQ is linked-list).
    When the WQE is fully consumed, HW should still have available buffer
    in order not to drop packets.
    
    Fixes: 461017cb006a ("net/mlx5e: Support RX multi-packet WQE (Striding RQ)")
    Signed-off-by: Eugenia Emantayev <eugenia@mellanox.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>
    Cc: kernel-team@fb.com
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index cc13d3dbd366..13b5ef9d8703 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -67,7 +67,7 @@
 #define MLX5E_PARAMS_DEFAULT_LOG_RQ_SIZE                0xa
 #define MLX5E_PARAMS_MAXIMUM_LOG_RQ_SIZE                0xd
 
-#define MLX5E_PARAMS_MINIMUM_LOG_RQ_SIZE_MPW            0x1
+#define MLX5E_PARAMS_MINIMUM_LOG_RQ_SIZE_MPW            0x2
 #define MLX5E_PARAMS_DEFAULT_LOG_RQ_SIZE_MPW            0x3
 #define MLX5E_PARAMS_MAXIMUM_LOG_RQ_SIZE_MPW            0x6
 

commit 7d92d580334a18800aaf66aaf2e103271c48bafb
Author: Gal Pressman <galp@mellanox.com>
Date:   Sun Sep 10 10:36:43 2017 +0300

    net/mlx5e: Add 802.1ad VLAN filter steering rules
    
    When a user chooses to use 802.1ad VLAN the proper steering rules will
    be added to the VLAN flow table (matching the specific S-tag VID).
    Due to current hardware limitation, when using 802.1ad, we must disable
    C-tag VLAN stripping on the RQs.
    
    Signed-off-by: Gal Pressman <galp@mellanox.com>
    Reviewed-by: Maor Gottlieb <maorg@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index f952796e9e82..2a0739d07a08 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -656,7 +656,9 @@ struct mlx5e_tc_table {
 struct mlx5e_vlan_table {
 	struct mlx5e_flow_table		ft;
 	DECLARE_BITMAP(active_cvlans, VLAN_N_VID);
+	DECLARE_BITMAP(active_svlans, VLAN_N_VID);
 	struct mlx5_flow_handle	*active_cvlans_rule[VLAN_N_VID];
+	struct mlx5_flow_handle	*active_svlans_rule[VLAN_N_VID];
 	struct mlx5_flow_handle	*untagged_rule;
 	struct mlx5_flow_handle	*any_cvlan_rule;
 	struct mlx5_flow_handle	*any_svlan_rule;

commit 03eda9541f351fb289fe18c87cb111f8bfca9837
Author: Gal Pressman <galp@mellanox.com>
Date:   Mon Sep 18 13:09:18 2017 +0300

    net/mlx5e: Declare bitmap using kernel macro
    
    Replace explicit declaration of bitmap with DECLARE_BITMAP kernel macro.
    
    Signed-off-by: Gal Pressman <galp@mellanox.com>
    Reviewed-by: Maor Gottlieb <maorg@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index eba5db75214b..f952796e9e82 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -655,7 +655,7 @@ struct mlx5e_tc_table {
 
 struct mlx5e_vlan_table {
 	struct mlx5e_flow_table		ft;
-	unsigned long active_cvlans[BITS_TO_LONGS(VLAN_N_VID)];
+	DECLARE_BITMAP(active_cvlans, VLAN_N_VID);
 	struct mlx5_flow_handle	*active_cvlans_rule[VLAN_N_VID];
 	struct mlx5_flow_handle	*untagged_rule;
 	struct mlx5_flow_handle	*any_cvlan_rule;

commit 2b52a283907b7f6d439444bfdde7f2241ddde735
Author: Gal Pressman <galp@mellanox.com>
Date:   Sun Sep 10 17:51:10 2017 +0300

    net/mlx5e: Rename VLAN related variables and functions
    
    Rename VLAN related symbols to better reflect the fact that they
    are associated to C-tag VLAN.
    
    Signed-off-by: Gal Pressman <galp@mellanox.com>
    Reviewed-by: Maor Gottlieb <maorg@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 751f62cae969..eba5db75214b 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -655,12 +655,12 @@ struct mlx5e_tc_table {
 
 struct mlx5e_vlan_table {
 	struct mlx5e_flow_table		ft;
-	unsigned long active_vlans[BITS_TO_LONGS(VLAN_N_VID)];
-	struct mlx5_flow_handle	*active_vlans_rule[VLAN_N_VID];
+	unsigned long active_cvlans[BITS_TO_LONGS(VLAN_N_VID)];
+	struct mlx5_flow_handle	*active_cvlans_rule[VLAN_N_VID];
 	struct mlx5_flow_handle	*untagged_rule;
 	struct mlx5_flow_handle	*any_cvlan_rule;
 	struct mlx5_flow_handle	*any_svlan_rule;
-	bool			filter_disabled;
+	bool			cvlan_filter_disabled;
 };
 
 struct mlx5e_l2_table {
@@ -887,8 +887,8 @@ int mlx5e_vlan_rx_add_vid(struct net_device *dev, __always_unused __be16 proto,
 			  u16 vid);
 int mlx5e_vlan_rx_kill_vid(struct net_device *dev, __always_unused __be16 proto,
 			   u16 vid);
-void mlx5e_enable_vlan_filter(struct mlx5e_priv *priv);
-void mlx5e_disable_vlan_filter(struct mlx5e_priv *priv);
+void mlx5e_enable_cvlan_filter(struct mlx5e_priv *priv);
+void mlx5e_disable_cvlan_filter(struct mlx5e_priv *priv);
 void mlx5e_timestamp_set(struct mlx5e_priv *priv);
 
 struct mlx5e_redirect_rqt_param {

commit 0088cbbc4b66b287132a8a04b3e2509d44a6387c
Author: Tal Gilboa <talgi@mellanox.com>
Date:   Tue Sep 26 16:20:43 2017 +0300

    net/mlx5e: Enable CQE based moderation on TX CQ
    
    By using CQE based moderation on TX CQ we can reduce the number of TX
    interrupt rate. Besides the benefit of less interrupts, this also
    allows the kernel to better utilize TSO. Since TSO has some CPU overhead,
    it might not aggregate when CPU is under high stress. By reducing the
    interrupt rate and the CPU utilization, we can get better aggregation
    and better overall throughput.
    The feature is enabled by default and has a private flag in ethtool
    for control.
    
    Throughput, interrupt rate and TSO utilization improvements:
    (ConnectX-4Lx 40GbE, unidirectional, 1/16 TCP streams, 64B packets)
    ---------------------------------------------------------
    Metric   | Streams | CQE Based | EQE Based | improvement
    ---------------------------------------------------------
    BW       |    1    |  2.4Gb/s  | 2.15Gb/s  |  +11.6%
    IR       |    1    |  27Kips   | 50.6Kips  |  -46.7%
    TSO Util |    1    |  74.6%    | 71%       |  +5%
    BW       |    16   |  29Gb/s   | 25.85Gb/s |  +12.2%
    IR       |    16   |  482Kips  | 745Kips   |  -35.3%
    TSO Util |    16   |  69.1%    | 49%       |  +41.1%
    
    *BW = Bandwidth, IR = Interrupt rate, ips = interrupt per second.
    TSO Util = bytes in TSO sessions / all bytes transferred
    
    Signed-off-by: Tal Gilboa <talgi@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 95facdf62c77..751f62cae969 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -106,6 +106,7 @@
 #define MLX5E_PARAMS_DEFAULT_RX_CQ_MODERATION_USEC_FROM_CQE 0x3
 #define MLX5E_PARAMS_DEFAULT_RX_CQ_MODERATION_PKTS      0x20
 #define MLX5E_PARAMS_DEFAULT_TX_CQ_MODERATION_USEC      0x10
+#define MLX5E_PARAMS_DEFAULT_TX_CQ_MODERATION_USEC_FROM_CQE 0x10
 #define MLX5E_PARAMS_DEFAULT_TX_CQ_MODERATION_PKTS      0x20
 #define MLX5E_PARAMS_DEFAULT_MIN_RX_WQES                0x80
 #define MLX5E_PARAMS_DEFAULT_MIN_RX_WQES_MPW            0x2
@@ -198,12 +199,14 @@ extern const char mlx5e_self_tests[][ETH_GSTRING_LEN];
 
 static const char mlx5e_priv_flags[][ETH_GSTRING_LEN] = {
 	"rx_cqe_moder",
+	"tx_cqe_moder",
 	"rx_cqe_compress",
 };
 
 enum mlx5e_priv_flag {
 	MLX5E_PFLAG_RX_CQE_BASED_MODER = (1 << 0),
-	MLX5E_PFLAG_RX_CQE_COMPRESS = (1 << 1),
+	MLX5E_PFLAG_TX_CQE_BASED_MODER = (1 << 1),
+	MLX5E_PFLAG_RX_CQE_COMPRESS = (1 << 2),
 };
 
 #define MLX5E_SET_PFLAG(params, pflag, enable)			\
@@ -223,6 +226,7 @@ enum mlx5e_priv_flag {
 struct mlx5e_cq_moder {
 	u16 usec;
 	u16 pkts;
+	u8 cq_period_mode;
 };
 
 struct mlx5e_params {
@@ -234,7 +238,6 @@ struct mlx5e_params {
 	u8  log_rq_size;
 	u16 num_channels;
 	u8  num_tc;
-	u8  rx_cq_period_mode;
 	bool rx_cqe_compress_def;
 	struct mlx5e_cq_moder rx_cq_moderation;
 	struct mlx5e_cq_moder tx_cq_moderation;
@@ -926,6 +929,8 @@ void mlx5e_build_default_indir_rqt(u32 *indirection_rqt, int len,
 				   int num_channels);
 int mlx5e_get_max_linkspeed(struct mlx5_core_dev *mdev, u32 *speed);
 
+void mlx5e_set_tx_cq_mode_params(struct mlx5e_params *params,
+				 u8 cq_period_mode);
 void mlx5e_set_rx_cq_mode_params(struct mlx5e_params *params,
 				 u8 cq_period_mode);
 void mlx5e_set_rq_type_params(struct mlx5_core_dev *mdev,

commit 458821c72bd02fcd484b9e46526c55e4ab6f57a4
Author: Feras Daoud <ferasda@mellanox.com>
Date:   Thu Sep 7 16:05:10 2017 +0300

    net/mlx5e: IPoIB, Add inner TTC table to IPoIB flow steering
    
    For supported platforms, add inner TTC flow table to enhanced IPoIB
    flow steering.
    
    Signed-off-by: Feras Daoud <ferasda@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 8c872e2e1aa0..95facdf62c77 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -1045,6 +1045,9 @@ void mlx5e_destroy_rqt(struct mlx5e_priv *priv, struct mlx5e_rqt *rqt);
 int mlx5e_create_ttc_table(struct mlx5e_priv *priv);
 void mlx5e_destroy_ttc_table(struct mlx5e_priv *priv);
 
+int mlx5e_create_inner_ttc_table(struct mlx5e_priv *priv);
+void mlx5e_destroy_inner_ttc_table(struct mlx5e_priv *priv);
+
 int mlx5e_create_tis(struct mlx5_core_dev *mdev, int tc,
 		     u32 underlay_qpn, u32 *tisn);
 void mlx5e_destroy_tis(struct mlx5_core_dev *mdev, u32 tisn);

commit 79c48764e1da40341b0e8149417c00efc9849b43
Author: Gal Pressman <galp@mellanox.com>
Date:   Tue Jul 28 09:35:31 2015 +0300

    net/mlx5e: Add support for ethtool msglvl support
    
    Use ethtool -s <devname> msglvl <type> on/off to toggle debug messages.
    
    Signed-off-by: Gal Pressman <galp@mellanox.com>
    Signed-off-by: Inbar Karmy <inbark@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index fae7b62d173f..8c872e2e1aa0 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -127,6 +127,16 @@
 
 #define MLX5E_NUM_MAIN_GROUPS 9
 
+#define MLX5E_MSG_LEVEL			NETIF_MSG_LINK
+
+#define mlx5e_dbg(mlevel, priv, format, ...)                    \
+do {                                                            \
+	if (NETIF_MSG_##mlevel & (priv)->msglevel)              \
+		netdev_warn(priv->netdev, format,               \
+			    ##__VA_ARGS__);                     \
+} while (0)
+
+
 static inline u16 mlx5_min_rx_wqes(int wq_type, u32 wq_size)
 {
 	switch (wq_type) {
@@ -754,6 +764,7 @@ struct mlx5e_priv {
 #endif
 	/* priv data path fields - end */
 
+	u32                        msglevel;
 	unsigned long              state;
 	struct mutex               state_lock; /* Protects Interface state */
 	struct mlx5e_rq            drop_rq;

commit fbcb127e89ba8a4ccbec609a27f8d110474044c8
Author: Huy Nguyen <huyn@mellanox.com>
Date:   Tue Jul 18 16:34:51 2017 -0500

    net/mlx5e: Support DSCP trust state to Ethernet's IP packet on SQ
    
    If the port is in DSCP trust state, packets are placed in the right
    priority queue based on the dscp value. This is done by selecting
    the transmit queue based on the dscp of the skb.
    
    Until now select_queue honors priority only from the vlan header.
    However that is not sufficient in cases where port trust state is DSCP
    mode as packet might not even contain vlan header. Therefore if the port
    is in dscp trust state and vport's min inline mode is not NONE,
    copy the IP header to the eseg's inline header if the skb has it.
    This is done by changing the transmit queue sq's min inline mode to L3.
    Note that the min inline mode of sqs that belong to other features such
    as xdpsq, icosq are not modified.
    
    Signed-off-by: Huy Nguyen <huyn@mellanox.com>
    Reviewed-by: Parav Pandit <parav@mellanox.com>
    Reviewed-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index ab6f0c18850f..fae7b62d173f 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -1083,4 +1083,5 @@ void mlx5e_destroy_netdev(struct mlx5e_priv *priv);
 void mlx5e_build_nic_params(struct mlx5_core_dev *mdev,
 			    struct mlx5e_params *params,
 			    u16 max_channels);
+u8 mlx5e_params_calculate_tx_min_inline(struct mlx5_core_dev *mdev);
 #endif /* __MLX5_EN_H__ */

commit 2a5e7a1344f4dff71bb921ee0c9ecf7f5932e570
Author: Huy Nguyen <huyn@mellanox.com>
Date:   Tue Jul 18 16:23:36 2017 -0500

    net/mlx5e: Add dcbnl dscp to priority support
    
    This patch implements dcbnl hooks to set and delete DSCP to priority map
    as defined by the DCB subsystem. Device maintains internal trust state
    which needs to be set to DSCP state for performing DSCP to priority mapping.
    
    When the first dscp to priority APP entry is added by the user, the
    trust state is changed to dscp.
    
    When the last dscp to priority APP entry is deleted by the user, the
    trust state is changed to pcp.
    
    If user sends multiple dscp to priority APP entries on the same dscp,
    the last sent one will take effect. All the previous sent will be
    deleted.
    
    The dscp to priority APP entries are added and deleted in the net/dcb
    APP database using dcb_ieee_setapp/getapp.
    
    Signed-off-by: Huy Nguyen <huyn@mellanox.com>
    Reviewed-by: Parav Pandit <parav@mellanox.com>
    Reviewed-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index e613ce02216d..ab6f0c18850f 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -57,6 +57,7 @@
 #define MLX5E_HW2SW_MTU(priv, hwmtu) ((hwmtu) - ((priv)->hard_mtu))
 #define MLX5E_SW2HW_MTU(priv, swmtu) ((swmtu) + ((priv)->hard_mtu))
 
+#define MLX5E_MAX_DSCP          64
 #define MLX5E_MAX_NUM_TC	8
 
 #define MLX5E_PARAMS_MINIMUM_LOG_SQ_SIZE                0x6
@@ -260,11 +261,17 @@ enum {
 struct mlx5e_dcbx {
 	enum mlx5_dcbx_oper_mode   mode;
 	struct mlx5e_cee_config    cee_cfg; /* pending configuration */
+	u8                         dscp_app_cnt;
 
 	/* The only setting that cannot be read from FW */
 	u8                         tc_tsa[IEEE_8021QAZ_MAX_TCS];
 	u8                         cap;
 };
+
+struct mlx5e_dcbx_dp {
+	u8                         dscp2prio[MLX5E_MAX_DSCP];
+	u8                         trust_state;
+};
 #endif
 
 enum {
@@ -742,6 +749,9 @@ struct mlx5e_priv {
 	/* priv data path fields - start */
 	struct mlx5e_txqsq *txq2sq[MLX5E_MAX_NUM_CHANNELS * MLX5E_MAX_NUM_TC];
 	int channel_tc2txq[MLX5E_MAX_NUM_CHANNELS][MLX5E_MAX_NUM_TC];
+#ifdef CONFIG_MLX5_CORE_EN_DCB
+	struct mlx5e_dcbx_dp       dcbx_dp;
+#endif
 	/* priv data path fields - end */
 
 	unsigned long              state;
@@ -800,6 +810,8 @@ struct mlx5e_profile {
 		mlx5e_fp_handle_rx_cqe handle_rx_cqe;
 		mlx5e_fp_handle_rx_cqe handle_rx_cqe_mpwqe;
 	} rx_handlers;
+	void	(*netdev_registered_init)(struct mlx5e_priv *priv);
+	void    (*netdev_registered_remove)(struct mlx5e_priv *priv);
 	int	max_tc;
 };
 
@@ -968,6 +980,8 @@ extern const struct ethtool_ops mlx5e_ethtool_ops;
 extern const struct dcbnl_rtnl_ops mlx5e_dcbnl_ops;
 int mlx5e_dcbnl_ieee_setets_core(struct mlx5e_priv *priv, struct ieee_ets *ets);
 void mlx5e_dcbnl_initialize(struct mlx5e_priv *priv);
+void mlx5e_dcbnl_init_app(struct mlx5e_priv *priv);
+void mlx5e_dcbnl_delete_app(struct mlx5e_priv *priv);
 #endif
 
 #ifndef CONFIG_RFS_ACCEL
@@ -1069,5 +1083,4 @@ void mlx5e_destroy_netdev(struct mlx5e_priv *priv);
 void mlx5e_build_nic_params(struct mlx5_core_dev *mdev,
 			    struct mlx5e_params *params,
 			    u16 max_channels);
-
 #endif /* __MLX5_EN_H__ */

commit d6c862baaf9fb27a385a101c7e86b6112bfd8f07
Author: Jiri Pirko <jiri@mellanox.com>
Date:   Thu Oct 19 15:50:38 2017 +0200

    mlx5e: Convert ndo_setup_tc offloads to block callbacks
    
    Benefit from the newly introduced block callback infrastructure and
    convert ndo_setup_tc calls for flower offloads to block callbacks.
    
    Signed-off-by: Jiri Pirko <jiri@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index ca8845b641c0..e613ce02216d 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -1056,8 +1056,8 @@ int mlx5e_ethtool_get_ts_info(struct mlx5e_priv *priv,
 int mlx5e_ethtool_flash_device(struct mlx5e_priv *priv,
 			       struct ethtool_flash *flash);
 
-int mlx5e_setup_tc(struct net_device *dev, enum tc_setup_type type,
-		   void *type_data);
+int mlx5e_setup_tc_block_cb(enum tc_setup_type type, void *type_data,
+			    void *cb_priv);
 
 /* mlx5e generic netdev management API */
 struct net_device*

commit af28f6f26a67ef850a26c42c49a1d9ca4927e10c
Merge: e4655e4a7913 b5ae577741be
Author: David S. Miller <davem@davemloft.net>
Date:   Mon Oct 16 05:42:41 2017 +0100

    Merge tag 'mlx5-updates-2017-10-11' of git://git.kernel.org/pub/scm/linux/kernel/git/mellanox/linux
    
    Saeed Mahameed says:
    
    ====================
    mlx5-updates-2017-10-11: IPoIB Multi Pkey support
    
    This series provides the support for IPoIB Multi Pkey.
    InfiniBand Pkeys are the equivalent of Ethernet vlans.
    Currently IPoIB device driver supports only default Pkey and IPoIB Pkey child
    interfaces are not supported with IPoIB offloads mode, this series will add
    the support for that by allowing creating mlx5 multiple IPoIB netdevices with
    a non-default Pkey.
    
    mlx5 IPoIB Pkey child interface is smaller version of mlx5i IPoIB interfaces and shares
    most of its resources with the parent IPoIB interface, namely RX steering and ring
    queue resources.
    
    The only mlx5 resources a child Pkey interface will be creating are the TX rings,
    since they should be assigned to a specific Pkey.
    
    mlx5i Pkey netdev is implemented via new mlx5e netdev profile implemented in
    mlx5/core/ipoib/ipoib_vlan.c.
    
    The series starts with a refactoring of mlx5e PTP and mlx5 clock implementation
    to move the code to be part of mlx5 core rather than mlx5e netdevice, in order to
    make mlx5 clock and PTP registration part of the core to be shared with mlx5e
    master Ethernet netdev/IPoIB parent netdev and mlx5_ib in the near future.
    
    Add the support for attaching multiple underlay QPs for the different Pkeys
    in mlx5 core RX steering.
    
    Add Pkey index to rdma_netdev to add the ability to set PKEY index to lower
    IPoIB offload netdev.
    
    Use hash-table to map between DQPN (Destination QP number) to child netdev
    for the IPoIB parent netdev to forward RX packets to the corresponding
    child Pkey netdev, since the RX rings are shared.
    
    The reset of the series adds the ipoib child Pkey: mlx5e netdev profile,
    netdev nods implementation and minimal set of ethtool callbacks.
    ====================
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 7c39afb394c79e72c3795b4a42d55155b34ee073
Author: Feras Daoud <ferasda@mellanox.com>
Date:   Tue Aug 15 13:46:04 2017 +0300

    net/mlx5: PTP code migration to driver core section
    
    PTP code is moved to core section of mlx5 driver in order to share
    it between ethernet and infiniband. This movement involves the following
    changes:
    - Change mlx5e_ prefix to be mlx5_
    - Add clock structs to Core
    - Add clock object to mlx5_core_dev
    - Call Init/Uninit clock from core init/cleanup
    - Rename mlx5e_tstamp to be mlx5_clock
    
    Signed-off-by: Feras Daoud <ferasda@mellanox.com>
    Signed-off-by: Eitan Rabin <rabin@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index cc13d3dbd366..2059122eb089 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -267,28 +267,6 @@ struct mlx5e_dcbx {
 };
 #endif
 
-#define MAX_PIN_NUM	8
-struct mlx5e_pps {
-	u8                         pin_caps[MAX_PIN_NUM];
-	struct work_struct         out_work;
-	u64                        start[MAX_PIN_NUM];
-	u8                         enabled;
-};
-
-struct mlx5e_tstamp {
-	rwlock_t                   lock;
-	struct cyclecounter        cycles;
-	struct timecounter         clock;
-	struct hwtstamp_config     hwtstamp_config;
-	u32                        nominal_c_mult;
-	unsigned long              overflow_period;
-	struct delayed_work        overflow_work;
-	struct mlx5_core_dev      *mdev;
-	struct ptp_clock          *ptp;
-	struct ptp_clock_info      ptp_info;
-	struct mlx5e_pps           pps_info;
-};
-
 enum {
 	MLX5E_RQ_STATE_ENABLED,
 	MLX5E_RQ_STATE_AM,
@@ -375,9 +353,10 @@ struct mlx5e_txqsq {
 	u8                         min_inline_mode;
 	u16                        edge;
 	struct device             *pdev;
-	struct mlx5e_tstamp       *tstamp;
 	__be32                     mkey_be;
 	unsigned long              state;
+	struct hwtstamp_config    *tstamp;
+	struct mlx5_clock         *clock;
 
 	/* control path */
 	struct mlx5_wq_ctrl        wq_ctrl;
@@ -543,10 +522,11 @@ struct mlx5e_rq {
 	struct mlx5e_channel  *channel;
 	struct device         *pdev;
 	struct net_device     *netdev;
-	struct mlx5e_tstamp   *tstamp;
 	struct mlx5e_rq_stats  stats;
 	struct mlx5e_cq        cq;
 	struct mlx5e_page_cache page_cache;
+	struct hwtstamp_config *tstamp;
+	struct mlx5_clock      *clock;
 
 	mlx5e_fp_handle_rx_cqe handle_rx_cqe;
 	mlx5e_fp_post_rx_wqes  post_wqes;
@@ -588,7 +568,7 @@ struct mlx5e_channel {
 	/* control */
 	struct mlx5e_priv         *priv;
 	struct mlx5_core_dev      *mdev;
-	struct mlx5e_tstamp       *tstamp;
+	struct hwtstamp_config    *tstamp;
 	int                        ix;
 };
 
@@ -789,7 +769,7 @@ struct mlx5e_priv {
 	struct mlx5_core_dev      *mdev;
 	struct net_device         *netdev;
 	struct mlx5e_stats         stats;
-	struct mlx5e_tstamp        tstamp;
+	struct hwtstamp_config     tstamp;
 	u16 q_counter;
 #ifdef CONFIG_MLX5_CORE_EN_DCB
 	struct mlx5e_dcbx          dcbx;
@@ -873,12 +853,6 @@ void mlx5e_ethtool_init_steering(struct mlx5e_priv *priv);
 void mlx5e_ethtool_cleanup_steering(struct mlx5e_priv *priv);
 void mlx5e_set_rx_mode_work(struct work_struct *work);
 
-void mlx5e_fill_hwstamp(struct mlx5e_tstamp *clock, u64 timestamp,
-			struct skb_shared_hwtstamps *hwts);
-void mlx5e_timestamp_init(struct mlx5e_priv *priv);
-void mlx5e_timestamp_cleanup(struct mlx5e_priv *priv);
-void mlx5e_pps_event_handler(struct mlx5e_priv *priv,
-			     struct ptp_clock_event *event);
 int mlx5e_hwstamp_set(struct mlx5e_priv *priv, struct ifreq *ifr);
 int mlx5e_hwstamp_get(struct mlx5e_priv *priv, struct ifreq *ifr);
 int mlx5e_modify_rx_cqe_compression_locked(struct mlx5e_priv *priv, bool val);
@@ -889,6 +863,7 @@ int mlx5e_vlan_rx_kill_vid(struct net_device *dev, __always_unused __be16 proto,
 			   u16 vid);
 void mlx5e_enable_vlan_filter(struct mlx5e_priv *priv);
 void mlx5e_disable_vlan_filter(struct mlx5e_priv *priv);
+void mlx5e_timestamp_set(struct mlx5e_priv *priv);
 
 struct mlx5e_redirect_rqt_param {
 	bool is_rss;

commit 717503b9cf57c0bb7ea4d3a9f5699c9a04adf988
Author: Jiri Pirko <jiri@mellanox.com>
Date:   Wed Oct 11 09:41:09 2017 +0200

    net: sched: convert cls_flower->egress_dev users to tc_setup_cb_egdev infra
    
    The only user of cls_flower->egress_dev is mlx5. So do the conversion
    there alongside with the code originating the call in cls_flower
    function fl_hw_replace_filter to the newly introduced egress device
    callback infrastucture.
    
    Signed-off-by: Jiri Pirko <jiri@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index cc13d3dbd366..5ec6d3e8dc89 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -1081,6 +1081,9 @@ int mlx5e_ethtool_get_ts_info(struct mlx5e_priv *priv,
 int mlx5e_ethtool_flash_device(struct mlx5e_priv *priv,
 			       struct ethtool_flash *flash);
 
+int mlx5e_setup_tc(struct net_device *dev, enum tc_setup_type type,
+		   void *type_data);
+
 /* mlx5e generic netdev management API */
 struct net_device*
 mlx5e_create_netdev(struct mlx5_core_dev *mdev, const struct mlx5e_profile *profile,

commit aae3dbb4776e7916b6cd442d00159bea27a695c1
Merge: ec3604c7a5aa 66bed8465a80
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Wed Sep 6 14:45:08 2017 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next
    
    Pull networking updates from David Miller:
    
     1) Support ipv6 checksum offload in sunvnet driver, from Shannon
        Nelson.
    
     2) Move to RB-tree instead of custom AVL code in inetpeer, from Eric
        Dumazet.
    
     3) Allow generic XDP to work on virtual devices, from John Fastabend.
    
     4) Add bpf device maps and XDP_REDIRECT, which can be used to build
        arbitrary switching frameworks using XDP. From John Fastabend.
    
     5) Remove UFO offloads from the tree, gave us little other than bugs.
    
     6) Remove the IPSEC flow cache, from Florian Westphal.
    
     7) Support ipv6 route offload in mlxsw driver.
    
     8) Support VF representors in bnxt_en, from Sathya Perla.
    
     9) Add support for forward error correction modes to ethtool, from
        Vidya Sagar Ravipati.
    
    10) Add time filter for packet scheduler action dumping, from Jamal Hadi
        Salim.
    
    11) Extend the zerocopy sendmsg() used by virtio and tap to regular
        sockets via MSG_ZEROCOPY. From Willem de Bruijn.
    
    12) Significantly rework value tracking in the BPF verifier, from Edward
        Cree.
    
    13) Add new jump instructions to eBPF, from Daniel Borkmann.
    
    14) Rework rtnetlink plumbing so that operations can be run without
        taking the RTNL semaphore. From Florian Westphal.
    
    15) Support XDP in tap driver, from Jason Wang.
    
    16) Add 32-bit eBPF JIT for ARM, from Shubham Bansal.
    
    17) Add Huawei hinic ethernet driver.
    
    18) Allow to report MD5 keys in TCP inet_diag dumps, from Ivan
        Delalande.
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next: (1780 commits)
      i40e: point wb_desc at the nvm_wb_desc during i40e_read_nvm_aq
      i40e: avoid NVM acquire deadlock during NVM update
      drivers: net: xgene: Remove return statement from void function
      drivers: net: xgene: Configure tx/rx delay for ACPI
      drivers: net: xgene: Read tx/rx delay for ACPI
      rocker: fix kcalloc parameter order
      rds: Fix non-atomic operation on shared flag variable
      net: sched: don't use GFP_KERNEL under spin lock
      vhost_net: correctly check tx avail during rx busy polling
      net: mdio-mux: add mdio_mux parameter to mdio_mux_init()
      rxrpc: Make service connection lookup always check for retry
      net: stmmac: Delete dead code for MDIO registration
      gianfar: Fix Tx flow control deactivation
      cxgb4: Ignore MPS_TX_INT_CAUSE[Bubble] for T6
      cxgb4: Fix pause frame count in t4_get_port_stats
      cxgb4: fix memory leak
      tun: rename generic_xdp to skb_xdp
      tun: reserve extra headroom only when XDP is set
      net: dsa: bcm_sf2: Configure IMP port TC2QOS mapping
      net: dsa: bcm_sf2: Advertise number of egress queues
      ...

commit aa9d4648c2fbb455df7750ade1b73dd9ad9b3690
Merge: 906dde0f355b 8eb19e8e7c86
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sun Sep 3 17:49:17 2017 -0700

    Merge tag 'for-linus-ioctl' of git://git.kernel.org/pub/scm/linux/kernel/git/dledford/rdma
    
    Pull rdma updates from Doug Ledford:
     "This is a big pull request.
    
      Of note is that I'm sending you the new ioctl API for the rdma
      subsystem. We put it up on linux-api@, but didn't get much response.
      The API is complex, but it solves two different problems in one go:
    
       1) The bi-directional nature of the RDMA file write calls, which
          created the security hole we had to handle (and for which the fix
          is now causing problems for systems in production, we were a bit
          over zealous in the fix and the ability to open a device, then
          fork, then create new queue pairs on the device and use them is
          broken).
    
       2) The bloat caused by different vendors implementing extensions to
          the base verbs API. Each vendor's hardware is slightly different,
          and the hardware might be suitable for one extension but not
          another.
    
          By the time we add generic extensions for all the different ways
          that the different hardware can offload things, the API becomes
          bloated. Things like our completion structs have started to exceed
          a cache line in size because of all the elements needed to support
          this. That in turn shows up heavily in the performance graphs with
          a noticable drop in performance on 100Gigabit links as our
          completion structs go from occupying one cache line to 1+.
    
          This API makes things like the completion structs modular in a
          very similar way to netlink so that your structs can only include
          the items needed for the offloads/features you are actually using
          on a given queue pair. In that way we support everything, but only
          use what we need, and our structs stay smaller.
    
      The ioctl API is better explained by the posting on linux-api@ than I
      can explain it here, so I'll just leave it at that.
    
      The rest of the pull request is typical stuff.
    
      Updates for 4.14 kernel merge window
    
       - Lots of hfi1 driver updates (mixed with a few qib and core updates
         as well)
    
       - rxe updates
    
       - various mlx updates
    
       - Set default roce type to RoCEv2
    
       - Several larger fixes for bnxt_re that were too big for -rc
    
       - Several larger fixes for qedr that, likewise, were too big for -rc
    
       - Misc core changes
    
       - Make the hns_roce driver compilable on arches other than aarch64 so
         we can more easily debug build issues related to it
    
       - Add rdma-netlink infrastructure updates
    
       - Add automatic IRQ affinity infrastructure
    
       - Add 32bit lid support
    
       - Lots of misc fixes across the subsystem from random people
    
       - Autoloading of RDMA netlink modules
    
       - PCI pool cleanups from Romain Perier
    
       - mlx5 driver feature additions and fixes
    
       - Hardware tag matchine feature
    
       - Fix sleeping in atomic when resolving roce ah
    
       - Add experimental ioctl interface as posted to linux-api@"
    
    * tag 'for-linus-ioctl' of git://git.kernel.org/pub/scm/linux/kernel/git/dledford/rdma: (328 commits)
      IB/core: Expose ioctl interface through experimental Kconfig
      IB/core: Assign root to all drivers
      IB/core: Add completion queue (cq) object actions
      IB/core: Add legacy driver's user-data
      IB/core: Export ioctl enum types to user-space
      IB/core: Explicitly destroy an object while keeping uobject
      IB/core: Add macros for declaring methods and attributes
      IB/core: Add uverbs merge trees functionality
      IB/core: Add DEVICE object and root tree structure
      IB/core: Declare an object instead of declaring only type attributes
      IB/core: Add new ioctl interface
      RDMA/vmw_pvrdma: Fix a signedness
      RDMA/vmw_pvrdma: Report network header type in WC
      IB/core: Add might_sleep() annotation to ib_init_ah_from_wc()
      IB/cm: Fix sleeping in atomic when RoCE is used
      IB/core: Add support to finalize objects in one transaction
      IB/core: Add a generic way to execute an operation on a uobject
      Documentation: Hardware tag matching
      IB/mlx5: Support IB_SRQT_TM
      net/mlx5: Add XRQ support
      ...

commit d4b6c48800dda97f5a0824305d7c8175a127d414
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Wed Jun 7 13:55:34 2017 +0300

    net/mlx5e: Distribute RSS table among all RX rings
    
    In default, uniformly distribute the RSS indirection table entries
    among all RX rings, rather than restricting this only to the rings
    on the close NUMA node. irqbalancer would anyway dynamically override
    the default affinities set to the RX rings.
    This gives better multi-stream performance and CPU util.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 1388a1e2f835..8b7d83bcc11a 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -925,8 +925,7 @@ void mlx5e_switch_priv_channels(struct mlx5e_priv *priv,
 void mlx5e_activate_priv_channels(struct mlx5e_priv *priv);
 void mlx5e_deactivate_priv_channels(struct mlx5e_priv *priv);
 
-void mlx5e_build_default_indir_rqt(struct mlx5_core_dev *mdev,
-				   u32 *indirection_rqt, int len,
+void mlx5e_build_default_indir_rqt(u32 *indirection_rqt, int len,
 				   int num_channels);
 int mlx5e_get_max_linkspeed(struct mlx5_core_dev *mdev, u32 *speed);
 

commit a8c2eb15797a0f0bf17734d365da7bdc3e263155
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Sun Jul 2 13:17:42 2017 +0300

    net/mlx5e: Stop NAPI when irq balancer changes affinity
    
    NAPI context keeps rescheduling on same CPU as long as it's busy.
    This doesn't give the oppurtunity for changes in irq affinities
    to take effect.
    Fix that by calling napi_complete_done() upon a change in affinity.
    This would stop the NAPI and reschedule it on the new CPU.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 7c046ae8b18e..1388a1e2f835 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -582,6 +582,9 @@ struct mlx5e_channel {
 	__be32                     mkey_be;
 	u8                         num_tc;
 
+	/* data path - accessed per napi poll */
+	struct irq_desc *irq_desc;
+
 	/* control */
 	struct mlx5e_priv         *priv;
 	struct mlx5_core_dev      *mdev;

commit 7b33aaeaae1acfec01cbb0cf41fa8c07aea55609
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Sun Jul 2 12:56:12 2017 +0300

    net/mlx5e: Use kernel's mechanism to avoid missing NAPIs
    
    We used a channel state bit MLX5E_CHANNEL_NAPI_SCHED to make
    sure no NAPI is missed when a channel's napi_schedule() is called
    for completion events of the different channel's resources/rings
    while NAPI is currently running.
    Now, as similar mechanism is implemented in kernel,
    ("39e6c8208d7b net: solve a NAPI race"),
    we obsolete our own implementation and rely on the return value
    of napi_complete_done().
    
    This patch removes a redundant overhead of atomic bit operations.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Cc: Eric Dumazet <edumazet@google.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 9c0c07ffe557..7c046ae8b18e 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -570,10 +570,6 @@ struct mlx5e_rq {
 	struct mlx5_core_mkey  umr_mkey;
 } ____cacheline_aligned_in_smp;
 
-enum channel_flags {
-	MLX5E_CHANNEL_NAPI_SCHED = 1,
-};
-
 struct mlx5e_channel {
 	/* data path */
 	struct mlx5e_rq            rq;
@@ -585,7 +581,6 @@ struct mlx5e_channel {
 	struct net_device         *netdev;
 	__be32                     mkey_be;
 	u8                         num_tc;
-	unsigned long              flags;
 
 	/* control */
 	struct mlx5e_priv         *priv;

commit 29c2849e0d204bdc892842e2aae744d2219f07f2
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Sun Jul 2 17:33:59 2017 +0300

    net/mlx5e: Slightly increase RX page-cache size
    
    In XDP_TX flow, we now get back quicker to each page in page-cache,
    and on some occasions refcount does not get back to 1 on time, causing
    some costly page allocations.
    Slightly increase the size of RX page-cache to significantly decrease
    the chances for this to happen.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index e55d9439bc12..9c0c07ffe557 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -501,7 +501,7 @@ struct mlx5e_rx_am { /* Adaptive Moderation */
  */
 #define MLX5E_CACHE_UNIT	(MLX5_MPWRQ_PAGES_PER_WQE > NAPI_POLL_WEIGHT ? \
 				 MLX5_MPWRQ_PAGES_PER_WQE : NAPI_POLL_WEIGHT)
-#define MLX5E_CACHE_SIZE	(2 * roundup_pow_of_two(MLX5E_CACHE_UNIT))
+#define MLX5E_CACHE_SIZE	(4 * roundup_pow_of_two(MLX5E_CACHE_UNIT))
 struct mlx5e_page_cache {
 	u32 head;
 	u32 tail;

commit 3b56f7b2af86033f421d0f0fd10cf5c3f74872e0
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Mon Jul 17 14:31:39 2017 +0300

    net/mlx5e: Remove unnecessary fields in ICO SQ
    
    As of current design, in each NAPI, only a single UMR WQE
    completion could be available in the completion queue of the
    the internal control operations (ICO) send queue, in addition
    to nop operations that require no actions upon completion.
    This renders the consume index obsolete, as the wqe_counter
    field in CQE is sufficient.
    
    This helps removing a memory barrier, and obsoletes the need
    for tracking the num_wqebbs to update the consumer counter.
    
    In addition, remove other unused fields in icosq struct:
    pdev, dma_fifo_pc, and prev_cc.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 8d29a6eb9406..e55d9439bc12 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -343,7 +343,6 @@ enum {
 
 struct mlx5e_sq_wqe_info {
 	u8  opcode;
-	u8  num_wqebbs;
 };
 
 struct mlx5e_txqsq {
@@ -419,13 +418,8 @@ struct mlx5e_xdpsq {
 struct mlx5e_icosq {
 	/* data path */
 
-	/* dirtied @completion */
-	u16                        cc;
-
 	/* dirtied @xmit */
 	u16                        pc ____cacheline_aligned_in_smp;
-	u32                        dma_fifo_pc;
-	u16                        prev_cc;
 
 	struct mlx5e_cq            cq;
 
@@ -439,7 +433,6 @@ struct mlx5e_icosq {
 	void __iomem              *uar_map;
 	u32                        sqn;
 	u16                        edge;
-	struct device             *pdev;
 	__be32                     mkey_be;
 	unsigned long              state;
 

commit 7cc6d77bb56de3a428ed7cde91a09fffbdbef794
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Mon Jul 17 12:27:26 2017 +0300

    net/mlx5e: Type-specific optimizations for RX post WQEs function
    
    Separate the RX post WQEs function of the different RQ types.
    This enables RQ type-specific optimizations in data-path.
    
    Poll the ICOSQ completion queue only for Striding RQ,
    and only when a UMR post completion could be possibly available.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index bce2080eb86a..8d29a6eb9406 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -517,7 +517,7 @@ struct mlx5e_page_cache {
 
 struct mlx5e_rq;
 typedef void (*mlx5e_fp_handle_rx_cqe)(struct mlx5e_rq*, struct mlx5_cqe64*);
-typedef int (*mlx5e_fp_alloc_wqe)(struct mlx5e_rq*, struct mlx5e_rx_wqe*, u16);
+typedef bool (*mlx5e_fp_post_rx_wqes)(struct mlx5e_rq *rq);
 typedef void (*mlx5e_fp_dealloc_wqe)(struct mlx5e_rq*, u16);
 
 struct mlx5e_rq {
@@ -547,6 +547,7 @@ struct mlx5e_rq {
 		u8             map_dir;   /* dma map direction */
 	} buff;
 
+	struct mlx5e_channel  *channel;
 	struct device         *pdev;
 	struct net_device     *netdev;
 	struct mlx5e_tstamp   *tstamp;
@@ -555,7 +556,7 @@ struct mlx5e_rq {
 	struct mlx5e_page_cache page_cache;
 
 	mlx5e_fp_handle_rx_cqe handle_rx_cqe;
-	mlx5e_fp_alloc_wqe     alloc_wqe;
+	mlx5e_fp_post_rx_wqes  post_wqes;
 	mlx5e_fp_dealloc_wqe   dealloc_wqe;
 
 	unsigned long          state;
@@ -572,7 +573,6 @@ struct mlx5e_rq {
 	__be32                 mkey_be;
 	u8                     wq_type;
 	u32                    rqn;
-	struct mlx5e_channel  *channel;
 	struct mlx5_core_dev  *mdev;
 	struct mlx5_core_mkey  umr_mkey;
 } ____cacheline_aligned_in_smp;
@@ -853,11 +853,9 @@ void mlx5e_page_release(struct mlx5e_rq *rq, struct mlx5e_dma_info *dma_info,
 void mlx5e_handle_rx_cqe(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe);
 void mlx5e_handle_rx_cqe_mpwrq(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe);
 bool mlx5e_post_rx_wqes(struct mlx5e_rq *rq);
-int mlx5e_alloc_rx_wqe(struct mlx5e_rq *rq, struct mlx5e_rx_wqe *wqe, u16 ix);
-int mlx5e_alloc_rx_mpwqe(struct mlx5e_rq *rq, struct mlx5e_rx_wqe *wqe,	u16 ix);
+bool mlx5e_post_rx_mpwqes(struct mlx5e_rq *rq);
 void mlx5e_dealloc_rx_wqe(struct mlx5e_rq *rq, u16 ix);
 void mlx5e_dealloc_rx_mpwqe(struct mlx5e_rq *rq, u16 ix);
-void mlx5e_post_rx_mpwqe(struct mlx5e_rq *rq);
 void mlx5e_free_rx_mpwqe(struct mlx5e_rq *rq, struct mlx5e_mpw_info *wi);
 
 void mlx5e_rx_am(struct mlx5e_rq *rq);

commit a071cb9f25bef32ec309ca9d123022c52175341b
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Mon Jul 3 10:18:19 2017 +0300

    net/mlx5e: Non-atomic RQ state indicator for UMR WQE in progress
    
    The indication for a UMR WQE in progress is needed only within
    the NAPI context, and hence no races possible and no need for
    the use of atomic operations.
    The only place the flag is read outside of NAPI context is
    in closure flow, after RQ is disabled flag is no more accessed
    in NAPI.
    Use a boolean instead of a bit in ring state, so that its
    non-atomic set operations do not race with the atomic sets of
    the other bits.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 0c4f1f30085a..bce2080eb86a 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -291,7 +291,6 @@ struct mlx5e_tstamp {
 
 enum {
 	MLX5E_RQ_STATE_ENABLED,
-	MLX5E_RQ_STATE_UMR_WQE_IN_PROGRESS,
 	MLX5E_RQ_STATE_AM,
 };
 
@@ -539,6 +538,7 @@ struct mlx5e_rq {
 			void                  *mtt_no_align;
 			u16                    num_strides;
 			u8                     log_stride_sz;
+			bool                   umr_in_progress;
 		} mpwqe;
 	};
 	struct {

commit a1eaba4c5c29b6b5196b2237ce3a8d7d97622b2f
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Mon Jul 3 11:27:20 2017 +0300

    net/mlx5e: Non-atomic indicator for ring enabled state
    
    Rings enabled state change occurs in control path only, and is always
    followed by a napi_sychronize(), so that following NAPIs read the
    new value. This read does not need to be atomic.
    
    The RQ auto-moderation bit is not set/cleared in data-path.
    No need for atomic read, a regular read operation is sufficient.
    In RQ creation time as well, there's no multiple threads trying
    to access it yet, hence a regular read can be used.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index ce8b4f648757..0c4f1f30085a 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -295,6 +295,8 @@ enum {
 	MLX5E_RQ_STATE_AM,
 };
 
+#define MLX5E_TEST_BIT(state, nr) (state & BIT(nr))
+
 struct mlx5e_cq {
 	/* data path - accessed per cqe */
 	struct mlx5_cqwq           wq;

commit b681c481f11bd2839d759f493b00008a6c690e61
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Mon Jul 3 11:51:17 2017 +0300

    net/mlx5e: Remove unnecessary wqe_sz field from RQ buffer
    
    Field is used only locally within the RQ create function.
    The use of a local variable is sufficient.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 44bd8df905ca..ce8b4f648757 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -540,7 +540,6 @@ struct mlx5e_rq {
 		} mpwqe;
 	};
 	struct {
-		u32            wqe_sz;    /* wqe data buffer size */
 		u16            headroom;
 		u8             page_order;
 		u8             map_dir;   /* dma map direction */

commit 89e89f7a9fded02b81f4ee21e30c64892116cf34
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Sun Jul 2 19:02:05 2017 +0300

    net/mlx5e: Replace multiplication by stride size with a shift
    
    In RX data-path, use shift operations instead of a regular multiplication
    by stride size, as it is a power of two.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index d964db286c95..44bd8df905ca 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -535,8 +535,8 @@ struct mlx5e_rq {
 		struct {
 			struct mlx5e_mpw_info *info;
 			void                  *mtt_no_align;
-			u16                    stride_sz;
 			u16                    num_strides;
+			u8                     log_stride_sz;
 		} mpwqe;
 	};
 	struct {

commit b45d8b50b8264cac7b2f1245ca04a2f009038ac7
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Mon Feb 13 18:41:30 2017 +0200

    net/mlx5e: Reorganize struct mlx5e_rq
    
    Bring fast-path fields together, and combine RX WQE mutual
    exclusive fields into a union.
    
    Page-reuse and XDP are mutually exclusive and cannot be used at
    the same time.
    Use a union to combine their footprints.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 6c2abeccfa5a..d964db286c95 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -527,20 +527,24 @@ struct mlx5e_rq {
 		struct {
 			struct mlx5e_wqe_frag_info *frag_info;
 			u32 frag_sz;	/* max possible skb frag_sz */
-			bool page_reuse;
-			bool xdp_xmit;
+			union {
+				bool page_reuse;
+				bool xdp_xmit;
+			};
 		} wqe;
 		struct {
 			struct mlx5e_mpw_info *info;
 			void                  *mtt_no_align;
+			u16                    stride_sz;
+			u16                    num_strides;
 		} mpwqe;
 	};
 	struct {
-		u8             page_order;
 		u32            wqe_sz;    /* wqe data buffer size */
+		u16            headroom;
+		u8             page_order;
 		u8             map_dir;   /* dma map direction */
 	} buff;
-	__be32                 mkey_be;
 
 	struct device         *pdev;
 	struct net_device     *netdev;
@@ -555,7 +559,6 @@ struct mlx5e_rq {
 
 	unsigned long          state;
 	int                    ix;
-	u16                    rx_headroom;
 
 	struct mlx5e_rx_am     am; /* Adaptive Moderation */
 
@@ -565,9 +568,8 @@ struct mlx5e_rq {
 
 	/* control */
 	struct mlx5_wq_ctrl    wq_ctrl;
+	__be32                 mkey_be;
 	u8                     wq_type;
-	u32                    mpwqe_stride_sz;
-	u32                    mpwqe_num_strides;
 	u32                    rqn;
 	struct mlx5e_channel  *channel;
 	struct mlx5_core_dev  *mdev;

commit 6026e043d09012c6269f9a96a808d52d9c498224
Merge: 4cc5b44b29a9 138e4ad67afd
Author: David S. Miller <davem@davemloft.net>
Date:   Fri Sep 1 17:42:05 2017 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Three cases of simple overlapping changes.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 7b3722fa9ef647eb1ae6a60a5d46f7c67ab09a33
Author: Gal Pressman <galp@mellanox.com>
Date:   Sun Aug 13 16:22:38 2017 +0300

    net/mlx5e: Support RSS for GRE tunneled packets
    
    Introduce a new flow table and indirect TIRs which are used to hash the
    inner packet headers of GRE tunneled packets.
    
    When a GRE tunneled packet is received, the TTC flow table will match
    the new IPv4/6->GRE rules which will forward it to the inner TTC table.
    The inner TTC is similar to its counterpart outer TTC table, but
    matching the inner packet headers instead of the outer ones (and does
    not include the new IPv4/6->GRE rules).
    The new rules will not add steering hops since they are added to an
    already existing flow group which will be matched regardless of this
    patch. Non GRE traffic will not be affected.
    
    The inner flow table will forward the packet to inner indirect TIRs
    which hash the inner packet and thus result in RSS for the tunneled
    packets.
    
    Testing 8 TCP streams bandwidth over GRE:
    System: Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz
    NIC: Mellanox Technologies MT28800 Family [ConnectX-5 Ex]
    Before: 21.3 Gbps (Single RQ)
    Now   : 90.5 Gbps (RSS spread on 8 RQs)
    
    Signed-off-by: Gal Pressman <galp@mellanox.com>
    Reviewed-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 0039b4725405..a31912415264 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -620,6 +620,12 @@ enum mlx5e_traffic_types {
 	MLX5E_NUM_INDIR_TIRS = MLX5E_TT_ANY,
 };
 
+enum mlx5e_tunnel_types {
+	MLX5E_TT_IPV4_GRE,
+	MLX5E_TT_IPV6_GRE,
+	MLX5E_NUM_TUNNEL_TT,
+};
+
 enum {
 	MLX5E_STATE_ASYNC_EVENTS_ENABLED,
 	MLX5E_STATE_OPENED,
@@ -679,6 +685,7 @@ struct mlx5e_l2_table {
 struct mlx5e_ttc_table {
 	struct mlx5e_flow_table  ft;
 	struct mlx5_flow_handle	 *rules[MLX5E_NUM_TT];
+	struct mlx5_flow_handle  *tunnel_rules[MLX5E_NUM_TUNNEL_TT];
 };
 
 #define ARFS_HASH_SHIFT BITS_PER_BYTE
@@ -711,6 +718,7 @@ enum {
 	MLX5E_VLAN_FT_LEVEL = 0,
 	MLX5E_L2_FT_LEVEL,
 	MLX5E_TTC_FT_LEVEL,
+	MLX5E_INNER_TTC_FT_LEVEL,
 	MLX5E_ARFS_FT_LEVEL
 };
 
@@ -736,6 +744,7 @@ struct mlx5e_flow_steering {
 	struct mlx5e_vlan_table         vlan;
 	struct mlx5e_l2_table           l2;
 	struct mlx5e_ttc_table          ttc;
+	struct mlx5e_ttc_table          inner_ttc;
 	struct mlx5e_arfs_tables        arfs;
 };
 
@@ -769,6 +778,7 @@ struct mlx5e_priv {
 	u32                        tisn[MLX5E_MAX_NUM_TC];
 	struct mlx5e_rqt           indir_rqt;
 	struct mlx5e_tir           indir_tir[MLX5E_NUM_INDIR_TIRS];
+	struct mlx5e_tir           inner_indir_tir[MLX5E_NUM_INDIR_TIRS];
 	struct mlx5e_tir           direct_tir[MLX5E_MAX_NUM_CHANNELS];
 	u32                        tx_rates[MLX5E_MAX_NUM_SQS];
 	int                        hard_mtu;
@@ -903,7 +913,7 @@ int mlx5e_redirect_rqt(struct mlx5e_priv *priv, u32 rqtn, int sz,
 		       struct mlx5e_redirect_rqt_param rrp);
 void mlx5e_build_indir_tir_ctx_hash(struct mlx5e_params *params,
 				    enum mlx5e_traffic_types tt,
-				    void *tirc);
+				    void *tirc, bool inner);
 
 int mlx5e_open_locked(struct net_device *netdev);
 int mlx5e_close_locked(struct net_device *netdev);
@@ -932,6 +942,12 @@ void mlx5e_set_rx_cq_mode_params(struct mlx5e_params *params,
 void mlx5e_set_rq_type_params(struct mlx5_core_dev *mdev,
 			      struct mlx5e_params *params, u8 rq_type);
 
+static inline bool mlx5e_tunnel_inner_ft_supported(struct mlx5_core_dev *mdev)
+{
+	return (MLX5_CAP_ETH(mdev, tunnel_stateless_gre) &&
+		MLX5_CAP_FLOWTABLE_NIC_RX(mdev, ft_field_support.inner_ip_version));
+}
+
 static inline
 struct mlx5e_tx_wqe *mlx5e_post_nop(struct mlx5_wq_cyc *wq, u32 sqn, u16 *pc)
 {

commit 9e10bf1d349787f373484d835efe2dbb5f9c5614
Author: Huy Nguyen <huyn@mellanox.com>
Date:   Mon Jul 10 14:00:23 2017 -0500

    net/mlx5e: Fix DCB_CAP_ATTR_DCBX capability for DCBNL getcap.
    
    Current code doesn't report DCB_CAP_DCBX_HOST capability when query
    through getcap. User space lldptool expects capability to have HOST mode
    set when it wants to configure DCBX CEE mode. In absence of HOST mode
    capability, lldptool fails to switch to CEE mode.
    
    This fix returns DCB_CAP_DCBX_HOST capability when port's DCBX
    controlled mode is under software control.
    
    Fixes: 3a6a931dfb8e ("net/mlx5e: Support DCBX CEE API")
    Signed-off-by: Huy Nguyen <huyn@mellanox.com>
    Reviewed-by: Parav Pandit <parav@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 0039b4725405..2f26fb34d741 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -263,6 +263,7 @@ struct mlx5e_dcbx {
 
 	/* The only setting that cannot be read from FW */
 	u8                         tc_tsa[IEEE_8021QAZ_MAX_TCS];
+	u8                         cap;
 };
 #endif
 

commit a435393acafbf0ecff4deb3e3cb554b34f0d0664
Author: Sagi Grimberg <sagi@grimberg.me>
Date:   Thu Jul 13 11:09:40 2017 +0300

    mlx5: move affinity hints assignments to generic code
    
    generic api takes care of spreading affinity similar to
    what mlx5 open coded (and even handles better asymmetric
    configurations). Ask the generic API to spread affinity
    for us, and feed him pre_vectors that do not participate
    in affinity settings (which is an improvement to what we
    had before).
    
    The affinity assignments should match what mlx5 tried to
    do earlier but now we do not set affinity to async, cmd
    and pages dedicated vectors.
    
    Also, remove mlx5e_get_cpu and introduce mlx5e_get_node
    (used for allocation purposes) and mlx5_get_vector_affinity
    (for indirection table construction) as they provide the needed
    information. Luckily, we have generic helpers to get cpumask
    and node given a irq vector. mlx5_get_vector_affinity will
    be used by mlx5_ib in a subsequent patch.
    
    Reviewed-by: Christoph Hellwig <hch@lst.de>
    Acked-by: Leon Romanovsky <leonro@mellanox.com>
    Signed-off-by: Sagi Grimberg <sagi@grimberg.me>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index e1b7ddfecd01..909123243a85 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -587,7 +587,6 @@ struct mlx5e_channel {
 	struct mlx5_core_dev      *mdev;
 	struct mlx5e_tstamp       *tstamp;
 	int                        ix;
-	int                        cpu;
 };
 
 struct mlx5e_channels {

commit cf5033089b078303b102b65e3ccbbfa3ce0f4367
Author: Eugenia Emantayev <eugenia@mellanox.com>
Date:   Sun May 28 14:27:02 2017 +0300

    net/mlx5e: Add missing support for PTP_CLK_REQ_PPS request
    
    Add the missing option to enable the PTP_CLK_PPS function.
    In this case pin should be configured as 1PPS IN first and
    then it will be connected to PPS mechanism.
    Events will be reported as PTP_CLOCK_PPSUSR events to relevant sysfs.
    
    Fixes: ee7f12205abc ('net/mlx5e: Implement 1PPS support')
    Signed-off-by: Eugenia Emantayev <eugenia@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index d42826f0f0d6..0039b4725405 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -271,6 +271,7 @@ struct mlx5e_pps {
 	u8                         pin_caps[MAX_PIN_NUM];
 	struct work_struct         out_work;
 	u64                        start[MAX_PIN_NUM];
+	u8                         enabled;
 };
 
 struct mlx5e_tstamp {

commit 4272f9b88db9223216cdf87314f570f6d81295b4
Author: Eugenia Emantayev <eugenia@mellanox.com>
Date:   Sun May 28 14:06:01 2017 +0300

    net/mlx5e: Change 1PPS out scheme
    
    In order to fix the drift in 1PPS out need to adjust the next pulse.
    On each 1PPS out falling edge driver gets the event, then the event
    handler adjusts the next pulse starting time.
    
    Fixes: ee7f12205abc ('net/mlx5e: Implement 1PPS support')
    Signed-off-by: Eugenia Emantayev <eugenia@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index e1b7ddfecd01..d42826f0f0d6 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -266,6 +266,13 @@ struct mlx5e_dcbx {
 };
 #endif
 
+#define MAX_PIN_NUM	8
+struct mlx5e_pps {
+	u8                         pin_caps[MAX_PIN_NUM];
+	struct work_struct         out_work;
+	u64                        start[MAX_PIN_NUM];
+};
+
 struct mlx5e_tstamp {
 	rwlock_t                   lock;
 	struct cyclecounter        cycles;
@@ -277,7 +284,7 @@ struct mlx5e_tstamp {
 	struct mlx5_core_dev      *mdev;
 	struct ptp_clock          *ptp;
 	struct ptp_clock_info      ptp_info;
-	u8                        *pps_pin_caps;
+	struct mlx5e_pps           pps_info;
 };
 
 enum {

commit 2ac9cfe78223bb88be8cff3b59e0e13551b4e29c
Author: Ilan Tayari <ilant@mellanox.com>
Date:   Tue Apr 18 16:08:23 2017 +0300

    net/mlx5e: IPSec, Add Innova IPSec offload TX data path
    
    In the TX data path, prepend a special metadata ethertype which
    instructs the hardware to perform cryptography.
    
    In addition, fill Software-Parser segment in TX descriptor so
    that the hardware may parse the ESP protocol, and perform TX
    checksum offload on the inner payload.
    
    Support GSO, by providing the inverse of gso_size in the metadata.
    This allows the FPGA to update the ESP header (seqno and seqiv) on the
    resulting packets, by calculating the packet number within the GSO
    back from the TCP sequence number.
    
    Note that for GSO SKBs, the stack does not include an ESP trailer,
    unlike the non-GSO case.
    
    Signed-off-by: Ilan Tayari <ilant@mellanox.com>
    Signed-off-by: Yossi Kuperman <yossiku@mellanox.com>
    Signed-off-by: Yevgeny Kliteynik <kliteyn@mellanox.com>
    Signed-off-by: Boris Pismenny <borisp@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 535ffd78a34e..e1b7ddfecd01 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -328,6 +328,7 @@ struct mlx5e_sq_dma {
 
 enum {
 	MLX5E_SQ_STATE_ENABLED,
+	MLX5E_SQ_STATE_IPSEC,
 };
 
 struct mlx5e_sq_wqe_info {

commit 547eede070eb981f1442e494f08f4567dcf1d1c7
Author: Ilan Tayari <ilant@mellanox.com>
Date:   Tue Apr 18 16:04:28 2017 +0300

    net/mlx5e: IPSec, Innova IPSec offload infrastructure
    
    Add Innova IPSec ESP crypto offload configuration paths.
    Detect Innova IPSec device and set the NETIF_F_HW_ESP flag.
    Configure Security Associations using the API introduced in a previous
    patch.
    
    Add Software-parser hardware descriptor layout
    Software-Parser (swp) is a hardware feature in ConnectX which allows the
    host software to specify protocol header offsets in the TX path, thus
    overriding the hardware parser.
    This is useful for protocols that the ASIC may not be able to parse on
    its own.
    
    Note that due to inline metadata, XDP is not supported in Innova IPSec.
    
    Signed-off-by: Ilan Tayari <ilant@mellanox.com>
    Signed-off-by: Yossi Kuperman <yossiku@mellanox.com>
    Signed-off-by: Yevgeny Kliteynik <kliteyn@mellanox.com>
    Signed-off-by: Boris Pismenny <borisp@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index f93f44d1d1cf..535ffd78a34e 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -784,6 +784,9 @@ struct mlx5e_priv {
 
 	const struct mlx5e_profile *profile;
 	void                      *ppriv;
+#ifdef CONFIG_MLX5_EN_IPSEC
+	struct mlx5e_ipsec        *ipsec;
+#endif
 };
 
 struct mlx5e_profile {

commit 4b67379376b3674c069477aa48fe8923f735247e
Author: Ilan Tayari <ilant@mellanox.com>
Date:   Mon Jun 19 12:53:25 2017 +0300

    net/mlx5: Make get_cqe routine not ethernet-specific
    
    Move mlx5e_get_cqe routine to wq.h and rename it to
    mlx5_cqwq_get_cqe.
    
    This allows it to be used by other CQ users outside of the
    ethernet driver code.
    
    A later patch in this patchset will make use of it from
    FPGA code for the FPGA high-speed connection.
    
    Signed-off-by: Ilan Tayari <ilant@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index eef0a50e2388..f93f44d1d1cf 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -833,7 +833,6 @@ void mlx5e_dealloc_rx_wqe(struct mlx5e_rq *rq, u16 ix);
 void mlx5e_dealloc_rx_mpwqe(struct mlx5e_rq *rq, u16 ix);
 void mlx5e_post_rx_mpwqe(struct mlx5e_rq *rq);
 void mlx5e_free_rx_mpwqe(struct mlx5e_rq *rq, struct mlx5e_mpw_info *wi);
-struct mlx5_cqe64 *mlx5e_get_cqe(struct mlx5e_cq *cq);
 
 void mlx5e_rx_am(struct mlx5e_rq *rq);
 void mlx5e_rx_am_work(struct work_struct *work);

commit 3ffaabecd1a1a014a484f293c311b8ecb0545541
Author: Or Gerlitz <ogerlitz@mellanox.com>
Date:   Tue Apr 18 17:48:46 2017 +0300

    net/mlx5e: Support the flash device ethtool callback
    
    This callback further invokes the mlxfw module to flash the new
    firmware file to the device.
    
    As the firmware flash process takes about 20 seconds and ethtool
    takes the rtnl lock during the flash_device callback, we release
    the rtnl lock at the beginning of the flash process and take it
    again before leaving the callback.
    
    This way, rtnl is not held during the process. To make sure the
    device does not get deleted while being flashed, we take a
    reference to it before releasing rtnl lock.
    
    Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: Yotam Gigi <yotamg@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 709f500ef16f..eef0a50e2388 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -1060,6 +1060,8 @@ int mlx5e_ethtool_set_coalesce(struct mlx5e_priv *priv,
 			       struct ethtool_coalesce *coal);
 int mlx5e_ethtool_get_ts_info(struct mlx5e_priv *priv,
 			      struct ethtool_ts_info *info);
+int mlx5e_ethtool_flash_device(struct mlx5e_priv *priv,
+			       struct ethtool_flash *flash);
 
 /* mlx5e generic netdev management API */
 struct net_device*

commit accd58833237d4ad835f7f176303ac2b582704e3
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Sun Jan 29 17:42:26 2017 +0200

    net/mlx5e: Introduce RX Page-Reuse
    
    Introduce a Page-Reuse mechanism in non-Striding RQ RX datapath.
    
    A WQE (RX descriptor) buffer is a page, that in most cases was fully
    wasted on a packet that is much smaller, requiring a new page for
    the next round.
    
    In this patch, we implement a page-reuse mechanism, that resembles a
    `SW Striding RQ`.
    We allow the WQE to reuse its allocated page as much as it could,
    until the page is fully consumed.  In each round, the WQE is capable
    of receiving packet of maximal size (MTU). Yet, upon the reception of
    a packet, the WQE knows the actual packet size, and consumes the exact
    amount of memory needed to build a linear SKB. Then, it updates the
    buffer pointer within the page accordingly, for the next round.
    
    Feature is mutually exclusive with XDP (packet-per-page)
    and LRO (session size is a power of two, needs unused page).
    
    Performance tests:
    iperf tcp tests show huge gain:
    
    --------------------------------------------
    num streams | BW before | BW after | ratio |
              1 |      22.2 |     30.9 | 1.39x |
              8 |      64.2 |     93.6 | 1.46x |
             64 |      56.7 |     91.4 | 1.61x |
    --------------------------------------------
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 00ff2a1651e7..709f500ef16f 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -448,6 +448,11 @@ struct mlx5e_dma_info {
 	dma_addr_t	addr;
 };
 
+struct mlx5e_wqe_frag_info {
+	struct mlx5e_dma_info di;
+	u32 offset;
+};
+
 struct mlx5e_umr_dma_info {
 	__be64                *mtt;
 	dma_addr_t             mtt_addr;
@@ -509,7 +514,12 @@ struct mlx5e_rq {
 	struct mlx5_wq_ll      wq;
 
 	union {
-		struct mlx5e_dma_info *dma_info;
+		struct {
+			struct mlx5e_wqe_frag_info *frag_info;
+			u32 frag_sz;	/* max possible skb frag_sz */
+			bool page_reuse;
+			bool xdp_xmit;
+		} wqe;
 		struct {
 			struct mlx5e_mpw_info *info;
 			void                  *mtt_no_align;

commit bce2b2bf66825a56f66229980a30884704cba3d2
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Tue Jan 31 16:48:59 2017 +0200

    net/mlx5e: Enhance RX SKB headroom logic
    
    In the RX memory scheme of non Striding RQ, we use linear SKBs.
    Keeping NET_IP_ALIGN in headroom can improve performance on some archs.
    In addition, take this headroom into account when calculating the
    LRO WQE size.
    
    These are not needed in Striding RQ as they're done implicitly
    within the non-linear SKB allocation.
    
    Fixes: 1bfecfca565c ("net/mlx5e: Build RX SKB on demand")
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 6af1fbe62082..00ff2a1651e7 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -217,6 +217,7 @@ struct mlx5e_cq_moder {
 struct mlx5e_params {
 	u8  log_sq_size;
 	u8  rq_wq_type;
+	u16 rq_headroom;
 	u8  mpwqe_log_stride_sz;
 	u8  mpwqe_log_num_strides;
 	u8  log_rq_size;

commit 78aedd327982ddd3cbb6ffc58f6fe0179cb6238b
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Wed Jan 18 14:28:53 2017 +0200

    net/mlx5e: Build SKB with exact frag_size
    
    Build the SKB over the receive packet instead of the
    whole page. Getting the SKB's linear data and shared_info
    closer improves locality.
    In addition, this opens up the possibility to make use of
    other parts of the page in the downstream page-reuse patch.
    
    Fixes: 1bfecfca565c ("net/mlx5e: Build RX SKB on demand")
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 5d9ace493d85..6af1fbe62082 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -72,6 +72,8 @@
 #define MLX5E_PARAMS_MAXIMUM_LOG_RQ_SIZE_MPW            0x6
 
 #define MLX5_RX_HEADROOM NET_SKB_PAD
+#define MLX5_SKB_FRAG_SZ(len)	(SKB_DATA_ALIGN(len) +	\
+				 SKB_DATA_ALIGN(sizeof(struct skb_shared_info)))
 
 #define MLX5_MPWRQ_MIN_LOG_STRIDE_SZ(mdev) \
 	(6 + MLX5_CAP_GEN(mdev, cache_line_128byte)) /* HW restriction */

commit 1170fbd8ff43ff7808ae4a698054762b8bfae340
Author: Feras Daoud <ferasda@mellanox.com>
Date:   Thu Jun 1 14:56:17 2017 +0300

    net/mlx5e: IPoIB, Add ioctl support to IPoIB device driver
    
    Add ioctl support to IPoIB device driver. For now, this
    ioctl will support timestamp get and set.
    
    Signed-off-by: Feras Daoud <ferasda@mellanox.com>
    Signed-off-by: Eitan Rabin <rabin@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 626683f0f487..5d9ace493d85 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -853,8 +853,8 @@ void mlx5e_timestamp_init(struct mlx5e_priv *priv);
 void mlx5e_timestamp_cleanup(struct mlx5e_priv *priv);
 void mlx5e_pps_event_handler(struct mlx5e_priv *priv,
 			     struct ptp_clock_event *event);
-int mlx5e_hwstamp_set(struct net_device *dev, struct ifreq *ifr);
-int mlx5e_hwstamp_get(struct net_device *dev, struct ifreq *ifr);
+int mlx5e_hwstamp_set(struct mlx5e_priv *priv, struct ifreq *ifr);
+int mlx5e_hwstamp_get(struct mlx5e_priv *priv, struct ifreq *ifr);
 int mlx5e_modify_rx_cqe_compression_locked(struct mlx5e_priv *priv, bool val);
 
 int mlx5e_vlan_rx_add_vid(struct net_device *dev, __always_unused __be16 proto,

commit 3844b07ee4c96d0cf8886611c21bb3a367b759e1
Author: Feras Daoud <ferasda@mellanox.com>
Date:   Thu Jun 1 14:43:43 2017 +0300

    net/mlx5e: IPoIB, Add PTP support to IPoIB device driver
    
    Enable PTP for IPoIB rdma_netdev and add the ability
    to get the time stamping parameters using ethtool.
    
    Signed-off-by: Feras Daoud <ferasda@mellanox.com>
    Signed-off-by: Eitan Rabin <rabin@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 318ef7f0292f..626683f0f487 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -1045,6 +1045,8 @@ int mlx5e_ethtool_get_coalesce(struct mlx5e_priv *priv,
 			       struct ethtool_coalesce *coal);
 int mlx5e_ethtool_set_coalesce(struct mlx5e_priv *priv,
 			       struct ethtool_coalesce *coal);
+int mlx5e_ethtool_get_ts_info(struct mlx5e_priv *priv,
+			      struct ethtool_ts_info *info);
 
 /* mlx5e generic netdev management API */
 struct net_device*

commit c139dbfddd2c7848550ed06345060aa87701e818
Author: Erez Shitrit <erezsh@mellanox.com>
Date:   Thu May 18 17:03:21 2017 +0300

    net/mlx5e: Use hard_mtu as part of the mlx5e_priv struct
    
    The mtu extra space that kept for the HW is specific for each link type,
    and it is different in mlx5e and mlx5i modules.
    Now it is kept in the priv structures, set by the mlx5e/mlx5i driver
    accordingly.
    
    Signed-off-by: Erez Shitrit <erezsh@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index a223a8e15ece..318ef7f0292f 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -52,8 +52,10 @@
 
 #define MLX5_SET_CFG(p, f, v) MLX5_SET(create_flow_group_in, p, f, v)
 
-#define MLX5E_HW2SW_MTU(hwmtu) ((hwmtu) - (ETH_HLEN + VLAN_HLEN + ETH_FCS_LEN))
-#define MLX5E_SW2HW_MTU(swmtu) ((swmtu) + (ETH_HLEN + VLAN_HLEN + ETH_FCS_LEN))
+#define MLX5E_ETH_HARD_MTU (ETH_HLEN + VLAN_HLEN + ETH_FCS_LEN)
+
+#define MLX5E_HW2SW_MTU(priv, hwmtu) ((hwmtu) - ((priv)->hard_mtu))
+#define MLX5E_SW2HW_MTU(priv, swmtu) ((swmtu) + ((priv)->hard_mtu))
 
 #define MLX5E_MAX_NUM_TC	8
 
@@ -747,6 +749,7 @@ struct mlx5e_priv {
 	struct mlx5e_tir           indir_tir[MLX5E_NUM_INDIR_TIRS];
 	struct mlx5e_tir           direct_tir[MLX5E_MAX_NUM_CHANNELS];
 	u32                        tx_rates[MLX5E_MAX_NUM_SQS];
+	int                        hard_mtu;
 
 	struct mlx5e_flow_steering fs;
 	struct mlx5e_vxlan_db      vxlan;

commit 7ca42c8094a4e265b8007a6776380f57cf093624
Author: Erez Shitrit <erezsh@mellanox.com>
Date:   Thu May 18 14:32:11 2017 +0300

    net/mlx5e: Add new profile function update_carrier
    
    Updating the carrier involves specific HW setting, each profile should
    use its own function for that.
    
    Both IPoIB and VF representor don't need carrier update function, since
    VF representor has only a logical link to VF and IPoIB manages its own
    link via ib_core upper layer.
    
    Signed-off-by: Erez Shitrit <erezsh@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 49c5fe9fdff0..a223a8e15ece 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -782,6 +782,7 @@ struct mlx5e_profile {
 	void	(*enable)(struct mlx5e_priv *priv);
 	void	(*disable)(struct mlx5e_priv *priv);
 	void	(*update_stats)(struct mlx5e_priv *priv);
+	void	(*update_carrier)(struct mlx5e_priv *priv);
 	int	(*max_nch)(struct mlx5_core_dev *mdev);
 	struct {
 		mlx5e_fp_handle_rx_cqe handle_rx_cqe;

commit 076b0936e5fb8dd5513c1472a2c1d487b64d1580
Author: Erez Shitrit <erezsh@mellanox.com>
Date:   Mon May 15 13:32:28 2017 +0300

    net/mlx5e: IPoIB, Add ethtool support
    
    Add support for the following:
            "ethtool -S" (statistics).
            "ethtool -i" (driver info).
            "ethtool -g/G" (rings parameters).
            "ethtool -l/L" (channels parameters).
            "ethtool -c/C" (coalesce options).
    
    Signed-off-by: Erez Shitrit <erezsh@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 8094e78292de..49c5fe9fdff0 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -1021,6 +1021,27 @@ int mlx5e_open(struct net_device *netdev);
 void mlx5e_update_stats_work(struct work_struct *work);
 u32 mlx5e_choose_lro_timeout(struct mlx5_core_dev *mdev, u32 wanted_timeout);
 
+/* ethtool helpers */
+void mlx5e_ethtool_get_drvinfo(struct mlx5e_priv *priv,
+			       struct ethtool_drvinfo *drvinfo);
+void mlx5e_ethtool_get_strings(struct mlx5e_priv *priv,
+			       uint32_t stringset, uint8_t *data);
+int mlx5e_ethtool_get_sset_count(struct mlx5e_priv *priv, int sset);
+void mlx5e_ethtool_get_ethtool_stats(struct mlx5e_priv *priv,
+				     struct ethtool_stats *stats, u64 *data);
+void mlx5e_ethtool_get_ringparam(struct mlx5e_priv *priv,
+				 struct ethtool_ringparam *param);
+int mlx5e_ethtool_set_ringparam(struct mlx5e_priv *priv,
+				struct ethtool_ringparam *param);
+void mlx5e_ethtool_get_channels(struct mlx5e_priv *priv,
+				struct ethtool_channels *ch);
+int mlx5e_ethtool_set_channels(struct mlx5e_priv *priv,
+			       struct ethtool_channels *ch);
+int mlx5e_ethtool_get_coalesce(struct mlx5e_priv *priv,
+			       struct ethtool_coalesce *coal);
+int mlx5e_ethtool_set_coalesce(struct mlx5e_priv *priv,
+			       struct ethtool_coalesce *coal);
+
 /* mlx5e generic netdev management API */
 struct net_device*
 mlx5e_create_netdev(struct mlx5_core_dev *mdev, const struct mlx5e_profile *profile,

commit 3834a5e62617603673474ada9831aa4bda955e03
Author: Gal Pressman <galp@mellanox.com>
Date:   Wed May 10 15:10:33 2017 +0300

    net/mlx5e: Optimize update stats work
    
    Unlike ethtool stats, get_stats ndo provides information cached by
    update stats work that is running in the background without updating
    them explicitly.
    We cannot update all counters inside the ndo because some
    updates require firmware commands that cannot be performed under a
    spinlock.
    
    update_stats work does not need to update ALL counters, since only
    some of them are needed by ndo_get_stats.
    This patch will allow for a minimal run of update_stats using an extra
    parameter which will update necessary counters only and cut 13
    firmware commands in each iteration of the work.
    
    Work duration previous to this patch: ~4200us.
    Work duration after this patch: ~700us (17% of the original time).
    
    Signed-off-by: Gal Pressman <galp@mellanox.com>
    Reviewed-by: Eran Ben Elisha <eranbe@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Cc: kernel-team@fb.com

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index a0516b0a5273..8094e78292de 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -822,7 +822,7 @@ void mlx5e_rx_am(struct mlx5e_rq *rq);
 void mlx5e_rx_am_work(struct work_struct *work);
 struct mlx5e_cq_moder mlx5e_am_get_def_profile(u8 rx_cq_period_mode);
 
-void mlx5e_update_stats(struct mlx5e_priv *priv);
+void mlx5e_update_stats(struct mlx5e_priv *priv, bool full);
 
 int mlx5e_create_flow_steering(struct mlx5e_priv *priv);
 void mlx5e_destroy_flow_steering(struct mlx5e_priv *priv);

commit 0ddead90b223faae475f3296a50bf574b7f7c69a
Merge: f7aec129a356 a090bd4ff838
Author: David S. Miller <davem@davemloft.net>
Date:   Thu Jun 15 11:31:37 2017 -0400

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    The conflicts were two cases of overlapping changes in
    batman-adv and the qed driver.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 53acd76ce571e3b71f9205f2d49ab285a9f1aad8
Author: Tal Gilboa <talgi@mellanox.com>
Date:   Mon May 29 17:02:55 2017 +0300

    net/mlx5e: Fix wrong indications in DIM due to counter wraparound
    
    DIM (Dynamically-tuned Interrupt Moderation) is a mechanism designed for
    changing the channel interrupt moderation values in order to reduce CPU
    overhead for all traffic types.
    Each iteration of the algorithm, DIM calculates the difference in
    throughput, packet rate and interrupt rate from last iteration in order
    to make a decision. DIM relies on counters for each metric. When these
    counters get to their type's max value they wraparound. In this case
    the delta between 'end' and 'start' samples is negative and when
    translated to unsigned integers - very high. This results in a false
    indication to the algorithm and might result in a wrong decision.
    
    The fix calculates the 'distance' between 'end' and 'start' samples in a
    cyclic way around the relevant type's max value. It can also be viewed as
    an absolute value around the type's max value instead of around 0.
    
    Testing show higher stability in DIM profile selection and no wraparound
    issues.
    
    Fixes: cb3c7fd4f839 ("net/mlx5e: Support adaptive RX coalescing")
    Signed-off-by: Tal Gilboa <talgi@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 429da21a583d..944fc1742464 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -463,10 +463,10 @@ struct mlx5e_rx_am_stats {
 };
 
 struct mlx5e_rx_am_sample {
-	ktime_t		time;
-	unsigned int	pkt_ctr;
-	unsigned int    byte_ctr;
-	u16		event_ctr;
+	ktime_t	time;
+	u32	pkt_ctr;
+	u32	byte_ctr;
+	u16	event_ctr;
 };
 
 struct mlx5e_rx_am { /* Adaptive Moderation */

commit c3164d2fc48fd4fa0477ab658b644559c3fe9073
Author: Tal Gilboa <talgi@mellanox.com>
Date:   Mon May 15 14:13:16 2017 +0300

    net/mlx5e: Added BW check for DIM decision mechanism
    
    DIM (Dynamically-tuned Interrupt Moderation) is a mechanism designed for
    changing the channel interrupt moderation values in order to reduce CPU
    overhead for all traffic types.
    Until now only interrupt and packet rate were sampled.
    We found a scenario on which we get a false indication since a change in
    DIM caused more aggregation and reduced packet rate while increasing BW.
    
    We now regard a change as succesfull iff:
    current_BW > (prev_BW + threshold) or
    current_BW ~= prev_BW and current_PR > (prev_PR + threshold) or
    current_BW ~= prev_BW and current_PR ~= prev_PR and
        current_IR < (prev_IR - threshold)
    Where BW = Bandwidth, PR = Packet rate and IR = Interrupt rate
    
    Improvements (ConnectX-4Lx 25GbE, single RX queue, LRO off)
        --------------------------------------------------
        packet size | before[Mb/s] | after[Mb/s] | gain  |
        2B          | 343.4        | 359.4       |  4.5% |
        16B         | 2739.7       | 2814.8      |  2.7% |
        64B         | 9739         | 10185.3     |  4.5% |
    
    Fixes: cb3c7fd4f839 ("net/mlx5e: Support adaptive RX coalescing")
    Signed-off-by: Tal Gilboa <talgi@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 2fd044b23875..429da21a583d 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -458,12 +458,14 @@ struct mlx5e_mpw_info {
 
 struct mlx5e_rx_am_stats {
 	int ppms; /* packets per msec */
+	int bpms; /* bytes per msec */
 	int epms; /* events per msec */
 };
 
 struct mlx5e_rx_am_sample {
 	ktime_t		time;
 	unsigned int	pkt_ctr;
+	unsigned int    byte_ctr;
 	u16		event_ctr;
 };
 

commit 11c9c548ceb31f431facbc51c7081b4957223c47
Author: Or Gerlitz <ogerlitz@mellanox.com>
Date:   Thu May 4 21:46:11 2017 +0300

    net/mlx5e: Add cache for HW modify header IDs
    
    Packets belonging to flows which are different by matching may still need
    to go through the same header re-write. Add a cache for header re-write IDs
    keyed by the binary chain of modify header actions.
    
    The caching is supported for both eswitch and NIC use-cases, where the
    actual conversion of the code to use caching comes in next patches, one
    per use-case.
    
    Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
    Reviewed-by: Paul Blakey <paulb@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 2fd044b23875..f4b95dbd1c7f 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -623,6 +623,8 @@ struct mlx5e_tc_table {
 
 	struct rhashtable_params        ht_params;
 	struct rhashtable               ht;
+
+	DECLARE_HASHTABLE(mod_hdr_tbl, 8);
 };
 
 struct mlx5e_vlan_table {

commit 508541146af18e43072e41a31aa62fac2b01aac1
Author: Yishai Hadas <yishaih@mellanox.com>
Date:   Tue Apr 25 10:39:57 2017 +0300

    net/mlx5: Use underlay QPN from the root name space
    
    Root flow table is dynamically changed by the underlying flow steering
    layer, and IPoIB/ULPs have no idea what will be the root flow table in
    the future, hence we need a dynamic infrastructure to move Underlay QPs
    with the root flow table.
    
    Fixes: b3ba51498bdd ("net/mlx5: Refactor create flow table method to accept underlay QP")
    Signed-off-by: Erez Shitrit <erezsh@mellanox.com>
    Signed-off-by: Maor Gottlieb <maorg@mellanox.com>
    Signed-off-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 0099a3e397bc..2fd044b23875 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -1003,7 +1003,7 @@ int mlx5e_create_direct_tirs(struct mlx5e_priv *priv);
 void mlx5e_destroy_direct_tirs(struct mlx5e_priv *priv);
 void mlx5e_destroy_rqt(struct mlx5e_priv *priv, struct mlx5e_rqt *rqt);
 
-int mlx5e_create_ttc_table(struct mlx5e_priv *priv, u32 underlay_qpn);
+int mlx5e_create_ttc_table(struct mlx5e_priv *priv);
 void mlx5e_destroy_ttc_table(struct mlx5e_priv *priv);
 
 int mlx5e_create_tis(struct mlx5_core_dev *mdev, int tc,

commit 1d447a39142e0eaf8526fcb37699e6b8cce5eaa8
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Mon Apr 24 12:36:42 2017 +0300

    net/mlx5e: Extendable vport representor netdev private data
    
    Make representor netdev private data extendable by adding new struct
    "mlx5e_rep_priv" and use it as the rep netdev private data struct
    instead of directly pointing to mlx5_eswitch_rep.
    
    Added new en_rep.h header file to contain all representor related
    definitions and prototypes, and moved all representor specific logic
    into en_rep.c.
    
    Needed for downstream patches to extend representor functionality to
    support neighbour update.
    
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Reviewed-by: Or Gerlitz <ogerlitz@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 632a04b0ecaf..0099a3e397bc 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -991,20 +991,6 @@ int mlx5e_create_mdev_resources(struct mlx5_core_dev *mdev);
 void mlx5e_destroy_mdev_resources(struct mlx5_core_dev *mdev);
 int mlx5e_refresh_tirs(struct mlx5e_priv *priv, bool enable_uc_lb);
 
-struct mlx5_eswitch_rep;
-int mlx5e_vport_rep_load(struct mlx5_eswitch *esw,
-			 struct mlx5_eswitch_rep *rep);
-void mlx5e_vport_rep_unload(struct mlx5_eswitch *esw,
-			    struct mlx5_eswitch_rep *rep);
-int mlx5e_nic_rep_load(struct mlx5_eswitch *esw, struct mlx5_eswitch_rep *rep);
-void mlx5e_nic_rep_unload(struct mlx5_eswitch *esw,
-			  struct mlx5_eswitch_rep *rep);
-int mlx5e_add_sqs_fwd_rules(struct mlx5e_priv *priv);
-void mlx5e_remove_sqs_fwd_rules(struct mlx5e_priv *priv);
-int mlx5e_attr_get(struct net_device *dev, struct switchdev_attr *attr);
-void mlx5e_handle_rx_cqe_rep(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe);
-void mlx5e_update_hw_rep_counters(struct mlx5e_priv *priv);
-
 /* common netdev helpers */
 int mlx5e_create_indirect_rqt(struct mlx5e_priv *priv);
 
@@ -1031,12 +1017,6 @@ int mlx5e_open(struct net_device *netdev);
 void mlx5e_update_stats_work(struct work_struct *work);
 u32 mlx5e_choose_lro_timeout(struct mlx5_core_dev *mdev, u32 wanted_timeout);
 
-int mlx5e_get_offload_stats(int attr_id, const struct net_device *dev,
-			    void *sp);
-bool mlx5e_has_offload_stats(const struct net_device *dev, int attr_id);
-
-bool mlx5e_is_uplink_rep(struct mlx5e_priv *priv);
-
 /* mlx5e generic netdev management API */
 struct net_device*
 mlx5e_create_netdev(struct mlx5_core_dev *mdev, const struct mlx5e_profile *profile,

commit b1513c35317c106a1588f3ab32f6888f0e2afd71
Merge: 78a57b482aa5 f83246089ca0
Author: David S. Miller <davem@davemloft.net>
Date:   Wed Apr 26 22:39:08 2017 -0400

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit cbad8cddb6ed7ef3a5f0a9a70f1711d4d7fb9a8f
Author: Eugenia Emantayev <eugenia@mellanox.com>
Date:   Wed Mar 22 11:44:14 2017 +0200

    net/mlx5e: Fix small packet threshold
    
    RX packet headers are meant to be contained in SKB linear part,
    and chose a threshold of 128.
    It turns out this is not enough, i.e. for IPv6 packet over VxLAN.
    In this case, UDP/IPv4 needs 42 bytes, GENEVE header is 8 bytes,
    and 86 bytes for TCP/IPv6. In total 136 bytes that is more than
    current 128 bytes. In this case expand header flow is reached.
    The warning in skb_try_coalesce() caused by a wrong truesize
    was already fixed here:
    commit 158f323b9868 ("net: adjust skb->truesize in pskb_expand_head()").
    Still, we prefer to totally avoid the expand header flow for performance reasons.
    Tested regular TCP_STREAM with iperf for 1 and 8 streams, no degradation was found.
    
    Fixes: 461017cb006a ("net/mlx5e: Support RX multi-packet WQE (Striding RQ)")
    Signed-off-by: Eugenia Emantayev <eugenia@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index dc52053128bc..3d9490cd2db1 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -90,7 +90,7 @@
 #define MLX5E_VALID_NUM_MTTS(num_mtts) (MLX5_MTT_OCTW(num_mtts) - 1 <= U16_MAX)
 
 #define MLX5_UMR_ALIGN				(2048)
-#define MLX5_MPWRQ_SMALL_PACKET_THRESHOLD	(128)
+#define MLX5_MPWRQ_SMALL_PACKET_THRESHOLD	(256)
 
 #define MLX5E_PARAMS_DEFAULT_LRO_WQE_SZ                 (64 * 1024)
 #define MLX5E_DEFAULT_LRO_TIMEOUT                       32

commit 20fd0c193ff6384717a6d500aa0e737a81c6a1fa
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Thu Apr 13 06:37:03 2017 +0300

    net/mlx5e: RX handlers per netdev profile
    
    In order to have different RX handler per profile, fix and refactor the
    current code to take the rx handler directly from the netdevice profile
    rather than computing it on runtime as it was done with the switchdev
    mode representor rx handler.
    
    This will also remove the current wrong assumption in mlx5e_alloc_rq
    code that mlx5e_priv->ppriv is of the type vport_rep.
    
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Reviewed-by: Erez Shitrit <erezsh@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 25185f8c3562..0881325fba04 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -779,6 +779,10 @@ struct mlx5e_profile {
 	void	(*disable)(struct mlx5e_priv *priv);
 	void	(*update_stats)(struct mlx5e_priv *priv);
 	int	(*max_nch)(struct mlx5_core_dev *mdev);
+	struct {
+		mlx5e_fp_handle_rx_cqe handle_rx_cqe;
+		mlx5e_fp_handle_rx_cqe handle_rx_cqe_mpwqe;
+	} rx_handlers;
 	int	max_tc;
 };
 
@@ -1032,7 +1036,6 @@ int mlx5e_get_offload_stats(int attr_id, const struct net_device *dev,
 bool mlx5e_has_offload_stats(const struct net_device *dev, int attr_id);
 
 bool mlx5e_is_uplink_rep(struct mlx5e_priv *priv);
-bool mlx5e_is_vf_vport_rep(struct mlx5e_priv *priv);
 
 /* mlx5e generic netdev management API */
 struct net_device*

commit 77bdf8950b3cd17c780b4e5a2803806a9f573f51
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Thu Apr 13 06:37:01 2017 +0300

    net/mlx5e: Xmit flow break down
    
    Break current mlx5e xmit flow into smaller blocks (helper functions)
    in order to reuse them for IPoIB SKB transmission.
    
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Reviewed-by: Erez Shitrit <erezsh@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 23b92ec54e12..25185f8c3562 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -304,6 +304,7 @@ struct mlx5e_cq {
 } ____cacheline_aligned_in_smp;
 
 struct mlx5e_tx_wqe_info {
+	struct sk_buff *skb;
 	u32 num_bytes;
 	u8  num_wqebbs;
 	u8  num_dma;
@@ -345,7 +346,6 @@ struct mlx5e_txqsq {
 
 	/* write@xmit, read@completion */
 	struct {
-		struct sk_buff           **skb;
 		struct mlx5e_sq_dma       *dma_fifo;
 		struct mlx5e_tx_wqe_info  *wqe_info;
 	} db;

commit 603f4a45214d602f27749ad8b3bb6bd767dac2d5
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Thu Apr 13 06:36:59 2017 +0300

    net/mlx5e: IPoIB, Basic netdev ndos open/close
    
    Implement open/close of IPoIB netdevice ndos using mlx5e's
    channels API to manage data path resources (RQs/SQs/CQs).
    
    Set IPoIB netdev address on dev_init ndo.
    
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Reviewed-by: Erez Shitrit <erezsh@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 5345d875b695..23b92ec54e12 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -883,6 +883,8 @@ typedef int (*mlx5e_fp_hw_modify)(struct mlx5e_priv *priv);
 void mlx5e_switch_priv_channels(struct mlx5e_priv *priv,
 				struct mlx5e_channels *new_chs,
 				mlx5e_fp_hw_modify hw_modify);
+void mlx5e_activate_priv_channels(struct mlx5e_priv *priv);
+void mlx5e_deactivate_priv_channels(struct mlx5e_priv *priv);
 
 void mlx5e_build_default_indir_rqt(struct mlx5_core_dev *mdev,
 				   u32 *indirection_rqt, int len,

commit 5426a0b2746a629eed8608397a385f34081f3f66
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Thu Apr 13 06:36:58 2017 +0300

    net/mlx5e: IPoIB, TX TIS creation
    
    Modify mlx5e tis creation function to accept underlay qp number, which
    will be needed by IPoIB.
    
    Implement mlx5i (IPoIB) tx init/cleanup netdevice profile flows to
    create one TIS with the IPoIB underlay qp, for IPoIB TX SQs.
    
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Reviewed-by: Erez Shitrit <erezsh@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index c813eab5d764..5345d875b695 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -1014,6 +1014,10 @@ void mlx5e_destroy_rqt(struct mlx5e_priv *priv, struct mlx5e_rqt *rqt);
 int mlx5e_create_ttc_table(struct mlx5e_priv *priv, u32 underlay_qpn);
 void mlx5e_destroy_ttc_table(struct mlx5e_priv *priv);
 
+int mlx5e_create_tis(struct mlx5_core_dev *mdev, int tc,
+		     u32 underlay_qpn, u32 *tisn);
+void mlx5e_destroy_tis(struct mlx5_core_dev *mdev, u32 tisn);
+
 int mlx5e_create_tises(struct mlx5e_priv *priv);
 void mlx5e_cleanup_nic_tx(struct mlx5e_priv *priv);
 int mlx5e_close(struct net_device *netdev);

commit bc81b9d3267c4d23d829e9af88b21eb46d9d388b
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Thu Apr 13 06:36:57 2017 +0300

    net/mlx5e: IPoIB, RSS flow steering tables
    
    Like the mlx5e ethernet mode, on IPoIB mode we need to create RX steering
    tables, but IPoIB do not require MAC and VLAN steering tables so the
    only tables we create in here are:
    1. TTC Table (Traffic Type Classifier table for RSS steering)
    2. ARFS Table (for accelerated RFS support)
    
    Creation of those tables is identical to mlx5e ethernet mode, hence the
    use of mlx5e_create_ttc_table and mlx5e_arfs_create_tables.
    
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Reviewed-by: Erez Shitrit <erezsh@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index e5518536d56f..c813eab5d764 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -999,6 +999,7 @@ int mlx5e_attr_get(struct net_device *dev, struct switchdev_attr *attr);
 void mlx5e_handle_rx_cqe_rep(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe);
 void mlx5e_update_hw_rep_counters(struct mlx5e_priv *priv);
 
+/* common netdev helpers */
 int mlx5e_create_indirect_rqt(struct mlx5e_priv *priv);
 
 int mlx5e_create_indirect_tirs(struct mlx5e_priv *priv);
@@ -1010,6 +1011,9 @@ int mlx5e_create_direct_tirs(struct mlx5e_priv *priv);
 void mlx5e_destroy_direct_tirs(struct mlx5e_priv *priv);
 void mlx5e_destroy_rqt(struct mlx5e_priv *priv, struct mlx5e_rqt *rqt);
 
+int mlx5e_create_ttc_table(struct mlx5e_priv *priv, u32 underlay_qpn);
+void mlx5e_destroy_ttc_table(struct mlx5e_priv *priv);
+
 int mlx5e_create_tises(struct mlx5e_priv *priv);
 void mlx5e_cleanup_nic_tx(struct mlx5e_priv *priv);
 int mlx5e_close(struct net_device *netdev);

commit 8f493ffd88eac1b45685f0f21005b65352320b8a
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Thu Apr 13 06:36:56 2017 +0300

    net/mlx5e: IPoIB, RX steering RSS RQTs and TIRs
    
    Implement IPoIB RX RSS (RQTs and TIRs) HW objects creation,
    All we do here is simply reuse the mlx5e implementation to create
    direct and indirect (RSS) steering HW objects.
    
    For that we just expose
    mlx5e_{create,destroy}_{direct,indirect}_{rqt,tir} functions into en.h
    and call them from ipoib.c in init/cleanup_rx IPoIB netdevice profile
    callbacks.
    
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Reviewed-by: Erez Shitrit <erezsh@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 02aa3cc59dc3..e5518536d56f 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -999,10 +999,17 @@ int mlx5e_attr_get(struct net_device *dev, struct switchdev_attr *attr);
 void mlx5e_handle_rx_cqe_rep(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe);
 void mlx5e_update_hw_rep_counters(struct mlx5e_priv *priv);
 
+int mlx5e_create_indirect_rqt(struct mlx5e_priv *priv);
+
+int mlx5e_create_indirect_tirs(struct mlx5e_priv *priv);
+void mlx5e_destroy_indirect_tirs(struct mlx5e_priv *priv);
+
 int mlx5e_create_direct_rqts(struct mlx5e_priv *priv);
-void mlx5e_destroy_rqt(struct mlx5e_priv *priv, struct mlx5e_rqt *rqt);
+void mlx5e_destroy_direct_rqts(struct mlx5e_priv *priv);
 int mlx5e_create_direct_tirs(struct mlx5e_priv *priv);
 void mlx5e_destroy_direct_tirs(struct mlx5e_priv *priv);
+void mlx5e_destroy_rqt(struct mlx5e_priv *priv, struct mlx5e_rqt *rqt);
+
 int mlx5e_create_tises(struct mlx5e_priv *priv);
 void mlx5e_cleanup_nic_tx(struct mlx5e_priv *priv);
 int mlx5e_close(struct net_device *netdev);
@@ -1024,5 +1031,8 @@ mlx5e_create_netdev(struct mlx5_core_dev *mdev, const struct mlx5e_profile *prof
 int mlx5e_attach_netdev(struct mlx5e_priv *priv);
 void mlx5e_detach_netdev(struct mlx5e_priv *priv);
 void mlx5e_destroy_netdev(struct mlx5e_priv *priv);
+void mlx5e_build_nic_params(struct mlx5_core_dev *mdev,
+			    struct mlx5e_params *params,
+			    u16 max_channels);
 
 #endif /* __MLX5_EN_H__ */

commit 48935bbb7ae8bd4157a84fe12da48cd8d337c40a
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Thu Apr 13 06:36:55 2017 +0300

    net/mlx5e: IPoIB, Add netdevice profile skeleton
    
    Create mlx5e IPoIB netdevice profile skeleton in the new ipoib.c
    file with empty implementation.
    
    Downstream patches will provide the full mlx5 rdma netdevice acceleration
    support for IPoIB into this new file, by using the mlx5e netdevice
    profile and new mlx5_channels APIs and infrastructures.
    Same as already done in mlx5e NIC netdevice and switchdev mode VF
    representors.
    
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Reviewed-by: Erez Shitrit <erezsh@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index ced31906b8fd..02aa3cc59dc3 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -37,6 +37,7 @@
 #include <linux/timecounter.h>
 #include <linux/net_tstamp.h>
 #include <linux/ptp_clock_kernel.h>
+#include <linux/crash_dump.h>
 #include <linux/mlx5/driver.h>
 #include <linux/mlx5/qp.h>
 #include <linux/mlx5/cq.h>
@@ -153,6 +154,14 @@ static inline int mlx5_max_log_rq_size(int wq_type)
 	}
 }
 
+static inline int mlx5e_get_max_num_channels(struct mlx5_core_dev *mdev)
+{
+	return is_kdump_kernel() ?
+		MLX5E_MIN_NUM_CHANNELS :
+		min_t(int, mdev->priv.eq_table.num_comp_vectors,
+		      MLX5E_MAX_NUM_CHANNELS);
+}
+
 struct mlx5e_tx_wqe {
 	struct mlx5_wqe_ctrl_seg ctrl;
 	struct mlx5_wqe_eth_seg  eth;

commit 2c3b5beec46ab0d77c94828eb15170b333ae769a
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Thu Apr 13 06:36:54 2017 +0300

    net/mlx5e: More generic netdev management API
    
    In preparation for mlx5e RDMA net_device support, here we generalize
    mlx5e_attach/detach in a way that those functions will be agnostic
    to link type.  For that we move ethernet specific NIC net device logic out
    of those functions into {nic,rep}_{enable/disable} mlx5e NIC and
    representor profiles callbacks.
    
    Also some of the logic was moved only to NIC profile since it is not right
    to have this logic for representor net device (e.g. set port MTU).
    
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Reviewed-by: Erez Shitrit <erezsh@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index b7feecfbb5a5..ced31906b8fd 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -999,12 +999,6 @@ void mlx5e_cleanup_nic_tx(struct mlx5e_priv *priv);
 int mlx5e_close(struct net_device *netdev);
 int mlx5e_open(struct net_device *netdev);
 void mlx5e_update_stats_work(struct work_struct *work);
-struct net_device *mlx5e_create_netdev(struct mlx5_core_dev *mdev,
-				       const struct mlx5e_profile *profile,
-				       void *ppriv);
-void mlx5e_destroy_netdev(struct mlx5_core_dev *mdev, struct mlx5e_priv *priv);
-int mlx5e_attach_netdev(struct mlx5_core_dev *mdev, struct net_device *netdev);
-void mlx5e_detach_netdev(struct mlx5_core_dev *mdev, struct net_device *netdev);
 u32 mlx5e_choose_lro_timeout(struct mlx5_core_dev *mdev, u32 wanted_timeout);
 
 int mlx5e_get_offload_stats(int attr_id, const struct net_device *dev,
@@ -1013,4 +1007,13 @@ bool mlx5e_has_offload_stats(const struct net_device *dev, int attr_id);
 
 bool mlx5e_is_uplink_rep(struct mlx5e_priv *priv);
 bool mlx5e_is_vf_vport_rep(struct mlx5e_priv *priv);
+
+/* mlx5e generic netdev management API */
+struct net_device*
+mlx5e_create_netdev(struct mlx5_core_dev *mdev, const struct mlx5e_profile *profile,
+		    void *ppriv);
+int mlx5e_attach_netdev(struct mlx5e_priv *priv);
+void mlx5e_detach_netdev(struct mlx5e_priv *priv);
+void mlx5e_destroy_netdev(struct mlx5e_priv *priv);
+
 #endif /* __MLX5_EN_H__ */

commit f6d96a2092bf448be417575b4725523072f4fc19
Author: Guy Ergas <guye@mellanox.com>
Date:   Mon Feb 20 17:31:20 2017 +0200

    net/mlx5e: Make mlx5e_modify_rqs_vsd a static function
    
    Make mlx5e_modify_rqs_vsd a static function and remove from en.h in
    order to reduce redundant exposure of functions.
    
    Signed-off-by: Guy Ergas <guye@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index a58031da7fad..b7feecfbb5a5 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -843,8 +843,6 @@ int mlx5e_vlan_rx_kill_vid(struct net_device *dev, __always_unused __be16 proto,
 void mlx5e_enable_vlan_filter(struct mlx5e_priv *priv);
 void mlx5e_disable_vlan_filter(struct mlx5e_priv *priv);
 
-int mlx5e_modify_channels_vsd(struct mlx5e_channels *chs, bool vsd);
-
 struct mlx5e_redirect_rqt_param {
 	bool is_rss;
 	union {

commit 102722fc6832a16850c05595b98c9232549d99f3
Author: Guy Ergas <guye@mellanox.com>
Date:   Mon Feb 20 16:18:17 2017 +0200

    net/mlx5e: Add support for RXFCS feature flag
    
    Add support for rx-fcs flag from ethtool.
    In case this flag is set, update all RQs to scatter the FCS data into
    the packet.
    
    Signed-off-by: Guy Ergas <guye@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 150fb52a0737..a58031da7fad 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -221,6 +221,7 @@ struct mlx5e_params {
 	u8  toeplitz_hash_key[40];
 	u32 indirection_rqt[MLX5E_INDIR_RQT_SIZE];
 	bool vlan_strip_disable;
+	bool scatter_fcs_en;
 	bool rx_am_enabled;
 	u32 lro_timeout;
 	u32 pflags;

commit 2e20a151205be8e7efa9644cdb942381e7bec787
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Mon Feb 13 01:19:14 2017 +0200

    net/mlx5e: Fail safe mtu and lro setting
    
    Use the new fail-safe channels switch mechanism to set new
    netdev mtu and lro settings.
    
    MTU and lro settings demand some HW configuration changes after new
    channels are created and ready for action. In order to unify switch
    channels routine for LRO and MTU changes, and maybe future configuration
    features, we now pass to it a modify HW function pointer to be
    invoked directly after old channels are de-activated and before new
    channels are activated.
    
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 8b93d8d02116..150fb52a0737 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -867,8 +867,14 @@ int mlx5e_close_locked(struct net_device *netdev);
 int mlx5e_open_channels(struct mlx5e_priv *priv,
 			struct mlx5e_channels *chs);
 void mlx5e_close_channels(struct mlx5e_channels *chs);
+
+/* Function pointer to be used to modify WH settings while
+ * switching channels
+ */
+typedef int (*mlx5e_fp_hw_modify)(struct mlx5e_priv *priv);
 void mlx5e_switch_priv_channels(struct mlx5e_priv *priv,
-				struct mlx5e_channels *new_chs);
+				struct mlx5e_channels *new_chs,
+				mlx5e_fp_hw_modify hw_modify);
 
 void mlx5e_build_default_indir_rqt(struct mlx5_core_dev *mdev,
 				   u32 *indirection_rqt, int len,

commit be7e87f92b5802df9302af7856990172091a385f
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Mon Feb 13 00:42:54 2017 +0200

    net/mlx5e: Fail safe cqe compressing/moderation mode setting
    
    Use the new fail-safe channels switch mechanism to set new
    CQE compressing and CQE moderation mode settings.
    
    We also move RX CQE compression modify function out of en_rx file  to
    a more appropriate place.
    
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 2f259dfbf844..8b93d8d02116 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -833,7 +833,7 @@ void mlx5e_pps_event_handler(struct mlx5e_priv *priv,
 			     struct ptp_clock_event *event);
 int mlx5e_hwstamp_set(struct net_device *dev, struct ifreq *ifr);
 int mlx5e_hwstamp_get(struct net_device *dev, struct ifreq *ifr);
-void mlx5e_modify_rx_cqe_compression_locked(struct mlx5e_priv *priv, bool val);
+int mlx5e_modify_rx_cqe_compression_locked(struct mlx5e_priv *priv, bool val);
 
 int mlx5e_vlan_rx_add_vid(struct net_device *dev, __always_unused __be16 proto,
 			  u16 vid);

commit 55c2503dae1ac8aed14d261dc02f967b4d6b1f88
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Tue Dec 27 14:57:03 2016 +0200

    net/mlx5e: Introduce switch channels
    
    A fail safe helper functions that allows switching to new channels on the
    fly,  In simple words:
    
    make_new_config(new_params)
    {
        new_channels = open_channels(new_params);
        if (!new_channels)
             return "Failed, but current channels are still active :)"
    
        switch_channels(new_channels);
    
        return "SUCCESS";
    }
    
    Demonstrate mlx5e_switch_priv_channels usage in set channels ethtool
    callback and make it fail-safe using the new switch channels mechanism.
    
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 44c454b34754..2f259dfbf844 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -863,6 +863,13 @@ void mlx5e_build_indir_tir_ctx_hash(struct mlx5e_params *params,
 
 int mlx5e_open_locked(struct net_device *netdev);
 int mlx5e_close_locked(struct net_device *netdev);
+
+int mlx5e_open_channels(struct mlx5e_priv *priv,
+			struct mlx5e_channels *chs);
+void mlx5e_close_channels(struct mlx5e_channels *chs);
+void mlx5e_switch_priv_channels(struct mlx5e_priv *priv,
+				struct mlx5e_channels *new_chs);
+
 void mlx5e_build_default_indir_rqt(struct mlx5_core_dev *mdev,
 				   u32 *indirection_rqt, int len,
 				   int num_channels);

commit a43b25daef78610ccef025d59f82ce8e0c42ab6c
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Tue Mar 14 19:43:52 2017 +0200

    net/mlx5e: CQ and RQ don't need priv pointer
    
    Remove mlx5e_priv pointer from CQ and RQ structs,
    it was needed only to access mdev pointer from priv pointer.
    
    Instead we now pass mdev where needed.
    
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 007f91f54fda..44c454b34754 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -280,7 +280,6 @@ struct mlx5e_cq {
 	struct napi_struct        *napi;
 	struct mlx5_core_cq        mcq;
 	struct mlx5e_channel      *channel;
-	struct mlx5e_priv         *priv;
 
 	/* cqe decompression */
 	struct mlx5_cqe64          title;
@@ -290,6 +289,7 @@ struct mlx5e_cq {
 	u16                        decmprs_wqe_counter;
 
 	/* control */
+	struct mlx5_core_dev      *mdev;
 	struct mlx5_frag_wq_ctrl   wq_ctrl;
 } ____cacheline_aligned_in_smp;
 
@@ -533,7 +533,7 @@ struct mlx5e_rq {
 	u32                    mpwqe_num_strides;
 	u32                    rqn;
 	struct mlx5e_channel  *channel;
-	struct mlx5e_priv     *priv;
+	struct mlx5_core_dev  *mdev;
 	struct mlx5_core_mkey  umr_mkey;
 } ____cacheline_aligned_in_smp;
 
@@ -556,6 +556,8 @@ struct mlx5e_channel {
 
 	/* control */
 	struct mlx5e_priv         *priv;
+	struct mlx5_core_dev      *mdev;
+	struct mlx5e_tstamp       *tstamp;
 	int                        ix;
 	int                        cpu;
 };
@@ -715,22 +717,6 @@ enum {
 	MLX5E_NIC_PRIO
 };
 
-struct mlx5e_profile {
-	void	(*init)(struct mlx5_core_dev *mdev,
-			struct net_device *netdev,
-			const struct mlx5e_profile *profile, void *ppriv);
-	void	(*cleanup)(struct mlx5e_priv *priv);
-	int	(*init_rx)(struct mlx5e_priv *priv);
-	void	(*cleanup_rx)(struct mlx5e_priv *priv);
-	int	(*init_tx)(struct mlx5e_priv *priv);
-	void	(*cleanup_tx)(struct mlx5e_priv *priv);
-	void	(*enable)(struct mlx5e_priv *priv);
-	void	(*disable)(struct mlx5e_priv *priv);
-	void	(*update_stats)(struct mlx5e_priv *priv);
-	int	(*max_nch)(struct mlx5_core_dev *mdev);
-	int	max_tc;
-};
-
 struct mlx5e_priv {
 	/* priv data path fields - start */
 	struct mlx5e_txqsq *txq2sq[MLX5E_MAX_NUM_CHANNELS * MLX5E_MAX_NUM_TC];
@@ -770,6 +756,22 @@ struct mlx5e_priv {
 	void                      *ppriv;
 };
 
+struct mlx5e_profile {
+	void	(*init)(struct mlx5_core_dev *mdev,
+			struct net_device *netdev,
+			const struct mlx5e_profile *profile, void *ppriv);
+	void	(*cleanup)(struct mlx5e_priv *priv);
+	int	(*init_rx)(struct mlx5e_priv *priv);
+	void	(*cleanup_rx)(struct mlx5e_priv *priv);
+	int	(*init_tx)(struct mlx5e_priv *priv);
+	void	(*cleanup_tx)(struct mlx5e_priv *priv);
+	void	(*enable)(struct mlx5e_priv *priv);
+	void	(*disable)(struct mlx5e_priv *priv);
+	void	(*update_stats)(struct mlx5e_priv *priv);
+	int	(*max_nch)(struct mlx5_core_dev *mdev);
+	int	max_tc;
+};
+
 void mlx5e_build_ptys2ethtool_map(void);
 
 u16 mlx5e_select_queue(struct net_device *dev, struct sk_buff *skb,

commit 6a9764efb255f49a91e229799c38d5c1c9361987
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Wed Dec 21 17:24:35 2016 +0200

    net/mlx5e: Isolate open_channels from priv->params
    
    In order to have a clean separation between channels resources creation
    flows and current active mlx5e netdev parameters, make sure each
    resource creation function do not access priv->params, and only works
    with on a new fresh set of parameters.
    
    For this we add "new" mlx5e_params field to mlx5e_channels structure
    and use it down the road to mlx5e_open_{cq,rq,sq} and so on.
    
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index f1895ebe7fe5..007f91f54fda 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -182,15 +182,15 @@ enum mlx5e_priv_flag {
 	MLX5E_PFLAG_RX_CQE_COMPRESS = (1 << 1),
 };
 
-#define MLX5E_SET_PFLAG(priv, pflag, enable)			\
+#define MLX5E_SET_PFLAG(params, pflag, enable)			\
 	do {							\
 		if (enable)					\
-			(priv)->params.pflags |= (pflag);	\
+			(params)->pflags |= (pflag);		\
 		else						\
-			(priv)->params.pflags &= ~(pflag);	\
+			(params)->pflags &= ~(pflag);		\
 	} while (0)
 
-#define MLX5E_GET_PFLAG(priv, pflag) (!!((priv)->params.pflags & (pflag)))
+#define MLX5E_GET_PFLAG(params, pflag) (!!((params)->pflags & (pflag)))
 
 #ifdef CONFIG_MLX5_CORE_EN_DCB
 #define MLX5E_MAX_BW_ALLOC 100 /* Max percentage of BW allocation */
@@ -213,7 +213,6 @@ struct mlx5e_params {
 	bool rx_cqe_compress_def;
 	struct mlx5e_cq_moder rx_cq_moderation;
 	struct mlx5e_cq_moder tx_cq_moderation;
-	u16 min_rx_wqes;
 	bool lro_en;
 	u32 lro_wqe_sz;
 	u16 tx_max_inline;
@@ -225,6 +224,7 @@ struct mlx5e_params {
 	bool rx_am_enabled;
 	u32 lro_timeout;
 	u32 pflags;
+	struct bpf_prog *xdp_prog;
 };
 
 #ifdef CONFIG_MLX5_CORE_EN_DCB
@@ -357,7 +357,6 @@ struct mlx5e_txqsq {
 	/* control path */
 	struct mlx5_wq_ctrl        wq_ctrl;
 	struct mlx5e_channel      *channel;
-	int                        tc;
 	int                        txq_ix;
 	u32                        rate_limit;
 } ____cacheline_aligned_in_smp;
@@ -564,6 +563,7 @@ struct mlx5e_channel {
 struct mlx5e_channels {
 	struct mlx5e_channel **c;
 	unsigned int           num;
+	struct mlx5e_params    params;
 };
 
 enum mlx5e_traffic_types {
@@ -735,7 +735,6 @@ struct mlx5e_priv {
 	/* priv data path fields - start */
 	struct mlx5e_txqsq *txq2sq[MLX5E_MAX_NUM_CHANNELS * MLX5E_MAX_NUM_TC];
 	int channel_tc2txq[MLX5E_MAX_NUM_CHANNELS][MLX5E_MAX_NUM_TC];
-	struct bpf_prog *xdp_prog;
 	/* priv data path fields - end */
 
 	unsigned long              state;
@@ -752,7 +751,6 @@ struct mlx5e_priv {
 	struct mlx5e_flow_steering fs;
 	struct mlx5e_vxlan_db      vxlan;
 
-	struct mlx5e_params        params;
 	struct workqueue_struct    *wq;
 	struct work_struct         update_carrier_work;
 	struct work_struct         set_rx_mode_work;
@@ -857,8 +855,9 @@ struct mlx5e_redirect_rqt_param {
 
 int mlx5e_redirect_rqt(struct mlx5e_priv *priv, u32 rqtn, int sz,
 		       struct mlx5e_redirect_rqt_param rrp);
-void mlx5e_build_indir_tir_ctx_hash(struct mlx5e_priv *priv, void *tirc,
-				    enum mlx5e_traffic_types tt);
+void mlx5e_build_indir_tir_ctx_hash(struct mlx5e_params *params,
+				    enum mlx5e_traffic_types tt,
+				    void *tirc);
 
 int mlx5e_open_locked(struct net_device *netdev);
 int mlx5e_close_locked(struct net_device *netdev);
@@ -869,7 +868,8 @@ int mlx5e_get_max_linkspeed(struct mlx5_core_dev *mdev, u32 *speed);
 
 void mlx5e_set_rx_cq_mode_params(struct mlx5e_params *params,
 				 u8 cq_period_mode);
-void mlx5e_set_rq_type_params(struct mlx5e_priv *priv, u8 rq_type);
+void mlx5e_set_rq_type_params(struct mlx5_core_dev *mdev,
+			      struct mlx5e_params *params, u8 rq_type);
 
 static inline
 struct mlx5e_tx_wqe *mlx5e_post_nop(struct mlx5_wq_cyc *wq, u32 sqn, u16 *pc)

commit acc6c5953af1949fc17c09cacd4842f149b4569d
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Tue Dec 20 22:48:19 2016 +0200

    net/mlx5e: Split open/close channels to stages
    
    As a foundation for safe config flow, a simple clear API such as
    (Open then Activate) where the "Open" handles the heavy unsafe
    creation operation and the "activate" will be fast and fail safe,
    to enable the newly created channels.
    
    For this we split the RQs/TXQ SQs and channels open/close flows to
    open => activate, deactivate => close.
    
    This will simplify the ability to have fail safe configuration changes
    in downstream patches as follows:
    
    make_new_config(new_params)
    {
         old_channels = current_active_channels;
         new_channels = create_channels(new_params);
         if (!new_channels)
                  return "Failed, but current channels still active :)"
         deactivate_channels(old_channels); /* Can't fail */
         activate_channels(new_channels); /* Can't fail */
         close_channels(old_channels);
         current_active_channels = new_channels;
    
         return "SUCCESS";
    }
    
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 5f7cc58d900c..f1895ebe7fe5 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -358,6 +358,7 @@ struct mlx5e_txqsq {
 	struct mlx5_wq_ctrl        wq_ctrl;
 	struct mlx5e_channel      *channel;
 	int                        tc;
+	int                        txq_ix;
 	u32                        rate_limit;
 } ____cacheline_aligned_in_smp;
 
@@ -732,8 +733,8 @@ struct mlx5e_profile {
 
 struct mlx5e_priv {
 	/* priv data path fields - start */
-	struct mlx5e_txqsq         **txq_to_sq_map;
-	int channeltc_to_txq_map[MLX5E_MAX_NUM_CHANNELS][MLX5E_MAX_NUM_TC];
+	struct mlx5e_txqsq *txq2sq[MLX5E_MAX_NUM_CHANNELS * MLX5E_MAX_NUM_TC];
+	int channel_tc2txq[MLX5E_MAX_NUM_CHANNELS][MLX5E_MAX_NUM_TC];
 	struct bpf_prog *xdp_prog;
 	/* priv data path fields - end */
 

commit b676f653896aaf6b32b496709bd9ebbc771d7ed7
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Tue Dec 20 17:30:20 2016 +0200

    net/mlx5e: Refactor refresh TIRs
    
    Rename mlx5e_refresh_tirs_self_loopback to mlx5e_refresh_tirs,
    as it will be used in downstream (Safe config flow) patches, and make it
    fail safe on mlx5e_open.
    
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 50dfc4c6c8e4..5f7cc58d900c 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -959,8 +959,7 @@ void mlx5e_destroy_tir(struct mlx5_core_dev *mdev,
 		       struct mlx5e_tir *tir);
 int mlx5e_create_mdev_resources(struct mlx5_core_dev *mdev);
 void mlx5e_destroy_mdev_resources(struct mlx5_core_dev *mdev);
-int mlx5e_refresh_tirs_self_loopback(struct mlx5_core_dev *mdev,
-				     bool enable_uc_lb);
+int mlx5e_refresh_tirs(struct mlx5e_priv *priv, bool enable_uc_lb);
 
 struct mlx5_eswitch_rep;
 int mlx5e_vport_rep_load(struct mlx5_eswitch *esw,

commit a5f97fee743cd7ee9932036583dbe05298ff2648
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Mon Dec 19 23:20:17 2016 +0200

    net/mlx5e: Redirect RQT refactoring
    
    RQ Tables are always created once (on netdev creation) pointing to drop RQ
    and at that stage, RQ tables (indirection tables) are always directed to
    drop RQ.
    
    We don't need to use mlx5e_fill_{direct,indir}_rqt_rqns to fill the drop
    RQ in create RQT procedure.
    
    Instead of having separate flows to redirect direct and indirect RQ Tables
    to the current active channels Receive Queues (RQs), we unify the two
    flows by introducing mlx5e_redirect_rqt function and redirect_rqt_param
    struct. Combined, they provide one generic logic to fill the RQ table RQ
    numbers regardless of the RQ table purpose (direct/indirect).
    
    Demonstrated the usage with mlx5e_redirect_rqts_to_channels which will
    be called on mlx5e_open and with mlx5e_redirect_rqts_to_drop which will
    be called on mlx5e_close.
    
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index b00c6688ddcf..50dfc4c6c8e4 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -843,7 +843,19 @@ void mlx5e_disable_vlan_filter(struct mlx5e_priv *priv);
 
 int mlx5e_modify_channels_vsd(struct mlx5e_channels *chs, bool vsd);
 
-int mlx5e_redirect_rqt(struct mlx5e_priv *priv, u32 rqtn, int sz, int ix);
+struct mlx5e_redirect_rqt_param {
+	bool is_rss;
+	union {
+		u32 rqn; /* Direct RQN (Non-RSS) */
+		struct {
+			u8 hfunc;
+			struct mlx5e_channels *channels;
+		} rss; /* RSS data */
+	};
+};
+
+int mlx5e_redirect_rqt(struct mlx5e_priv *priv, u32 rqtn, int sz,
+		       struct mlx5e_redirect_rqt_param rrp);
 void mlx5e_build_indir_tir_ctx_hash(struct mlx5e_priv *priv, void *tirc,
 				    enum mlx5e_traffic_types tt);
 

commit ff9c852f91d14d11a35514dda495999cfdb41a7a
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Mon Feb 6 13:14:34 2017 +0200

    net/mlx5e: Introduce mlx5e_channels
    
    Have a dedicated "channels" handler that will serve as channels
    (RQs/SQs/etc..) holder to help with separating channels/parameters
    operations, for the downstream fail-safe configuration flow, where we will
    create a new instance of mlx5e_channels with the new requested parameters
    and switch to the new channels on the fly.
    
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index bace9233dc1f..b00c6688ddcf 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -560,6 +560,11 @@ struct mlx5e_channel {
 	int                        cpu;
 };
 
+struct mlx5e_channels {
+	struct mlx5e_channel **c;
+	unsigned int           num;
+};
+
 enum mlx5e_traffic_types {
 	MLX5E_TT_IPV4_TCP,
 	MLX5E_TT_IPV6_TCP,
@@ -736,7 +741,7 @@ struct mlx5e_priv {
 	struct mutex               state_lock; /* Protects Interface state */
 	struct mlx5e_rq            drop_rq;
 
-	struct mlx5e_channel     **channel;
+	struct mlx5e_channels      channels;
 	u32                        tisn[MLX5E_MAX_NUM_TC];
 	struct mlx5e_rqt           indir_rqt;
 	struct mlx5e_tir           indir_tir[MLX5E_NUM_INDIR_TIRS];
@@ -836,7 +841,7 @@ int mlx5e_vlan_rx_kill_vid(struct net_device *dev, __always_unused __be16 proto,
 void mlx5e_enable_vlan_filter(struct mlx5e_priv *priv);
 void mlx5e_disable_vlan_filter(struct mlx5e_priv *priv);
 
-int mlx5e_modify_rqs_vsd(struct mlx5e_priv *priv, bool vsd);
+int mlx5e_modify_channels_vsd(struct mlx5e_channels *chs, bool vsd);
 
 int mlx5e_redirect_rqt(struct mlx5e_priv *priv, u32 rqtn, int sz, int ix);
 void mlx5e_build_indir_tir_ctx_hash(struct mlx5e_priv *priv, void *tirc,

commit 3139104861d9a8fed13f813237cc998c8272eafc
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Sat Mar 25 00:52:14 2017 +0300

    net/mlx5e: Different SQ types
    
    Different SQ types (tx, xdp, ico) are growing apart, we separate them
    and remove unwanted parts in each one of them, to simplify data path and
    utilize data cache.
    
    Remove DB union from SQ structures since it is not needed anymore as we
    now have different SQ data type for each SQ.
    
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 50f895fa5f31..bace9233dc1f 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -319,13 +319,7 @@ struct mlx5e_sq_wqe_info {
 	u8  num_wqebbs;
 };
 
-enum mlx5e_sq_type {
-	MLX5E_SQ_TXQ,
-	MLX5E_SQ_ICO,
-	MLX5E_SQ_XDP
-};
-
-struct mlx5e_sq {
+struct mlx5e_txqsq {
 	/* data path */
 
 	/* dirtied @completion */
@@ -339,18 +333,11 @@ struct mlx5e_sq {
 
 	struct mlx5e_cq            cq;
 
-	/* pointers to per tx element info: write@xmit, read@completion */
-	union {
-		struct {
-			struct sk_buff           **skb;
-			struct mlx5e_sq_dma       *dma_fifo;
-			struct mlx5e_tx_wqe_info  *wqe_info;
-		} txq;
-		struct mlx5e_sq_wqe_info *ico_wqe;
-		struct {
-			struct mlx5e_dma_info     *di;
-			bool                       doorbell;
-		} xdp;
+	/* write@xmit, read@completion */
+	struct {
+		struct sk_buff           **skb;
+		struct mlx5e_sq_dma       *dma_fifo;
+		struct mlx5e_tx_wqe_info  *wqe_info;
 	} db;
 
 	/* read only */
@@ -372,7 +359,67 @@ struct mlx5e_sq {
 	struct mlx5e_channel      *channel;
 	int                        tc;
 	u32                        rate_limit;
-	u8                         type;
+} ____cacheline_aligned_in_smp;
+
+struct mlx5e_xdpsq {
+	/* data path */
+
+	/* dirtied @rx completion */
+	u16                        cc;
+	u16                        pc;
+
+	struct mlx5e_cq            cq;
+
+	/* write@xmit, read@completion */
+	struct {
+		struct mlx5e_dma_info     *di;
+		bool                       doorbell;
+	} db;
+
+	/* read only */
+	struct mlx5_wq_cyc         wq;
+	void __iomem              *uar_map;
+	u32                        sqn;
+	struct device             *pdev;
+	__be32                     mkey_be;
+	u8                         min_inline_mode;
+	unsigned long              state;
+
+	/* control path */
+	struct mlx5_wq_ctrl        wq_ctrl;
+	struct mlx5e_channel      *channel;
+} ____cacheline_aligned_in_smp;
+
+struct mlx5e_icosq {
+	/* data path */
+
+	/* dirtied @completion */
+	u16                        cc;
+
+	/* dirtied @xmit */
+	u16                        pc ____cacheline_aligned_in_smp;
+	u32                        dma_fifo_pc;
+	u16                        prev_cc;
+
+	struct mlx5e_cq            cq;
+
+	/* write@xmit, read@completion */
+	struct {
+		struct mlx5e_sq_wqe_info *ico_wqe;
+	} db;
+
+	/* read only */
+	struct mlx5_wq_cyc         wq;
+	void __iomem              *uar_map;
+	u32                        sqn;
+	u16                        edge;
+	struct device             *pdev;
+	__be32                     mkey_be;
+	unsigned long              state;
+
+	/* control path */
+	struct mlx5_wq_ctrl        wq_ctrl;
+	struct mlx5e_channel      *channel;
 } ____cacheline_aligned_in_smp;
 
 static inline bool
@@ -477,7 +524,7 @@ struct mlx5e_rq {
 
 	/* XDP */
 	struct bpf_prog       *xdp_prog;
-	struct mlx5e_sq        xdpsq;
+	struct mlx5e_xdpsq     xdpsq;
 
 	/* control */
 	struct mlx5_wq_ctrl    wq_ctrl;
@@ -497,8 +544,8 @@ enum channel_flags {
 struct mlx5e_channel {
 	/* data path */
 	struct mlx5e_rq            rq;
-	struct mlx5e_sq            sq[MLX5E_MAX_NUM_TC];
-	struct mlx5e_sq            icosq;   /* internal control operations */
+	struct mlx5e_txqsq         sq[MLX5E_MAX_NUM_TC];
+	struct mlx5e_icosq         icosq;   /* internal control operations */
 	bool                       xdp;
 	struct napi_struct         napi;
 	struct device             *pdev;
@@ -680,7 +727,7 @@ struct mlx5e_profile {
 
 struct mlx5e_priv {
 	/* priv data path fields - start */
-	struct mlx5e_sq            **txq_to_sq_map;
+	struct mlx5e_txqsq         **txq_to_sq_map;
 	int channeltc_to_txq_map[MLX5E_MAX_NUM_CHANNELS][MLX5E_MAX_NUM_TC];
 	struct bpf_prog *xdp_prog;
 	/* priv data path fields - end */
@@ -731,8 +778,8 @@ int mlx5e_napi_poll(struct napi_struct *napi, int budget);
 bool mlx5e_poll_tx_cq(struct mlx5e_cq *cq, int napi_budget);
 int mlx5e_poll_rx_cq(struct mlx5e_cq *cq, int budget);
 bool mlx5e_poll_xdpsq_cq(struct mlx5e_cq *cq);
-void mlx5e_free_xdpsq_descs(struct mlx5e_sq *sq);
-void mlx5e_free_sq_descs(struct mlx5e_sq *sq);
+void mlx5e_free_txqsq_descs(struct mlx5e_txqsq *sq);
+void mlx5e_free_xdpsq_descs(struct mlx5e_xdpsq *sq);
 
 void mlx5e_page_release(struct mlx5e_rq *rq, struct mlx5e_dma_info *dma_info,
 			bool recycle);

commit 864b2d715300d9082747fb5de2bb277359c75bff
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Sat Mar 25 00:52:11 2017 +0300

    net/mlx5e: Generalize tx helper functions for different SQ types
    
    In the next patches we will introduce different SQ types, for that we here
    generalize some TX helper functions to work with more basic SQ parameters,
    in order to re-use them for the different SQ types.
    
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index f02d2cb8d148..50f895fa5f31 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -375,10 +375,10 @@ struct mlx5e_sq {
 	u8                         type;
 } ____cacheline_aligned_in_smp;
 
-static inline bool mlx5e_sq_has_room_for(struct mlx5e_sq *sq, u16 n)
+static inline bool
+mlx5e_wqc_has_room_for(struct mlx5_wq_cyc *wq, u16 cc, u16 pc, u16 n)
 {
-	return (((sq->wq.sz_m1 & (sq->cc - sq->pc)) >= n) ||
-		(sq->cc  == sq->pc));
+	return (((wq->sz_m1 & (cc - pc)) >= n) || (cc == pc));
 }
 
 struct mlx5e_dma_info {
@@ -721,7 +721,6 @@ struct mlx5e_priv {
 
 void mlx5e_build_ptys2ethtool_map(void);
 
-void mlx5e_send_nop(struct mlx5e_sq *sq, bool notify_hw);
 u16 mlx5e_select_queue(struct net_device *dev, struct sk_buff *skb,
 		       void *accel_priv, select_queue_fallback_t fallback);
 netdev_tx_t mlx5e_xmit(struct sk_buff *skb, struct net_device *dev);
@@ -807,20 +806,40 @@ void mlx5e_set_rx_cq_mode_params(struct mlx5e_params *params,
 				 u8 cq_period_mode);
 void mlx5e_set_rq_type_params(struct mlx5e_priv *priv, u8 rq_type);
 
-static inline void
-mlx5e_tx_notify_hw(struct mlx5e_sq *sq, struct mlx5_wqe_ctrl_seg *ctrl)
+static inline
+struct mlx5e_tx_wqe *mlx5e_post_nop(struct mlx5_wq_cyc *wq, u32 sqn, u16 *pc)
 {
+	u16                         pi   = *pc & wq->sz_m1;
+	struct mlx5e_tx_wqe        *wqe  = mlx5_wq_cyc_get_wqe(wq, pi);
+	struct mlx5_wqe_ctrl_seg   *cseg = &wqe->ctrl;
+
+	memset(cseg, 0, sizeof(*cseg));
+
+	cseg->opmod_idx_opcode = cpu_to_be32((*pc << 8) | MLX5_OPCODE_NOP);
+	cseg->qpn_ds           = cpu_to_be32((sqn << 8) | 0x01);
+
+	(*pc)++;
+
+	return wqe;
+}
+
+static inline
+void mlx5e_notify_hw(struct mlx5_wq_cyc *wq, u16 pc,
+		     void __iomem *uar_map,
+		     struct mlx5_wqe_ctrl_seg *ctrl)
+{
+	ctrl->fm_ce_se = MLX5_WQE_CTRL_CQ_UPDATE;
 	/* ensure wqe is visible to device before updating doorbell record */
 	dma_wmb();
 
-	*sq->wq.db = cpu_to_be32(sq->pc);
+	*wq->db = cpu_to_be32(pc);
 
 	/* ensure doorbell record is visible to device before ringing the
 	 * doorbell
 	 */
 	wmb();
 
-	mlx5_write64((__be32 *)ctrl, sq->uar_map, NULL);
+	mlx5_write64((__be32 *)ctrl, uar_map, NULL);
 }
 
 static inline void mlx5e_cq_arm(struct mlx5e_cq *cq)

commit 2239185ccd888a9934525e5b2d2f55890ab826f6
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Sat Mar 25 00:52:10 2017 +0300

    net/mlx5e: Optimize XDP frame xmit
    
    XDP SQ has a fixed size WQE (MLX5E_XDP_TX_WQEBBS = 1) and only posts
    one kind of WQE (MLX5_OPCODE_SEND),
    
    Also we initialize SQ descriptors static fields once on open_xdpsq,
    rather than every time on critical path.
    
    Optimize the code in light of those facts and add a prefetch of the TX
    descriptor first thing in the xdp xmit function.
    
    Performance improvement:
    System: Intel(R) Xeon(R) CPU E5-2620 v3 @ 2.40GHz
    
    Test case              Before     Now        improvement
    ---------------------------------------------------------------
    XDP TX   (1 core)      13Mpps    13.7Mpps       5%
    
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 5e4ae94c9f6a..f02d2cb8d148 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -116,12 +116,8 @@
 	(DIV_ROUND_UP(sizeof(struct mlx5e_umr_wqe), MLX5_SEND_WQE_BB))
 
 #define MLX5E_XDP_MIN_INLINE (ETH_HLEN + VLAN_HLEN)
-#define MLX5E_XDP_IHS_DS_COUNT \
-	DIV_ROUND_UP(MLX5E_XDP_MIN_INLINE - 2, MLX5_SEND_WQE_DS)
 #define MLX5E_XDP_TX_DS_COUNT \
 	((sizeof(struct mlx5e_tx_wqe) / MLX5_SEND_WQE_DS) + 1 /* SG DS */)
-#define MLX5E_XDP_TX_WQEBBS \
-	DIV_ROUND_UP(MLX5E_XDP_TX_DS_COUNT, MLX5_SEND_WQEBB_NUM_DS)
 
 #define MLX5E_NUM_MAIN_GROUPS 9
 
@@ -352,7 +348,6 @@ struct mlx5e_sq {
 		} txq;
 		struct mlx5e_sq_wqe_info *ico_wqe;
 		struct {
-			struct mlx5e_sq_wqe_info  *wqe_info;
 			struct mlx5e_dma_info     *di;
 			bool                       doorbell;
 		} xdp;

commit 31871f87bb302efb26978f397bc2705138f0b73a
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Sat Mar 25 00:52:08 2017 +0300

    net/mlx5e: Move XDP SQ instance into RQ
    
    To save many rq->channel->sq dereferences in fast-path.
    And rename it to xdpsq.
    
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 8d789a25a1c0..5e4ae94c9f6a 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -479,7 +479,10 @@ struct mlx5e_rq {
 	u16                    rx_headroom;
 
 	struct mlx5e_rx_am     am; /* Adaptive Moderation */
+
+	/* XDP */
 	struct bpf_prog       *xdp_prog;
+	struct mlx5e_sq        xdpsq;
 
 	/* control */
 	struct mlx5_wq_ctrl    wq_ctrl;
@@ -499,7 +502,6 @@ enum channel_flags {
 struct mlx5e_channel {
 	/* data path */
 	struct mlx5e_rq            rq;
-	struct mlx5e_sq            xdp_sq;
 	struct mlx5e_sq            sq[MLX5E_MAX_NUM_TC];
 	struct mlx5e_sq            icosq;   /* internal control operations */
 	bool                       xdp;

commit eba2db2bd2c81140e56f06889781f8ba953af6da
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Sat Mar 25 00:52:07 2017 +0300

    net/mlx5e: Move mlx5e_rq struct declaration
    
    Move struct mlx5e_rq and friends to appear after mlx5e_sq declaration in
    en.h.
    
    We will need this for next patch to move the mlx5e_sq instance into
    mlx5e_rq struct for XDP SQs.
    
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index fce0eca0701c..8d789a25a1c0 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -297,19 +297,113 @@ struct mlx5e_cq {
 	struct mlx5_frag_wq_ctrl   wq_ctrl;
 } ____cacheline_aligned_in_smp;
 
-struct mlx5e_rq;
-typedef void (*mlx5e_fp_handle_rx_cqe)(struct mlx5e_rq *rq,
-				       struct mlx5_cqe64 *cqe);
-typedef int (*mlx5e_fp_alloc_wqe)(struct mlx5e_rq *rq, struct mlx5e_rx_wqe *wqe,
-				  u16 ix);
+struct mlx5e_tx_wqe_info {
+	u32 num_bytes;
+	u8  num_wqebbs;
+	u8  num_dma;
+};
+
+enum mlx5e_dma_map_type {
+	MLX5E_DMA_MAP_SINGLE,
+	MLX5E_DMA_MAP_PAGE
+};
+
+struct mlx5e_sq_dma {
+	dma_addr_t              addr;
+	u32                     size;
+	enum mlx5e_dma_map_type type;
+};
+
+enum {
+	MLX5E_SQ_STATE_ENABLED,
+};
+
+struct mlx5e_sq_wqe_info {
+	u8  opcode;
+	u8  num_wqebbs;
+};
 
-typedef void (*mlx5e_fp_dealloc_wqe)(struct mlx5e_rq *rq, u16 ix);
+enum mlx5e_sq_type {
+	MLX5E_SQ_TXQ,
+	MLX5E_SQ_ICO,
+	MLX5E_SQ_XDP
+};
+
+struct mlx5e_sq {
+	/* data path */
+
+	/* dirtied @completion */
+	u16                        cc;
+	u32                        dma_fifo_cc;
+
+	/* dirtied @xmit */
+	u16                        pc ____cacheline_aligned_in_smp;
+	u32                        dma_fifo_pc;
+	struct mlx5e_sq_stats      stats;
+
+	struct mlx5e_cq            cq;
+
+	/* pointers to per tx element info: write@xmit, read@completion */
+	union {
+		struct {
+			struct sk_buff           **skb;
+			struct mlx5e_sq_dma       *dma_fifo;
+			struct mlx5e_tx_wqe_info  *wqe_info;
+		} txq;
+		struct mlx5e_sq_wqe_info *ico_wqe;
+		struct {
+			struct mlx5e_sq_wqe_info  *wqe_info;
+			struct mlx5e_dma_info     *di;
+			bool                       doorbell;
+		} xdp;
+	} db;
+
+	/* read only */
+	struct mlx5_wq_cyc         wq;
+	u32                        dma_fifo_mask;
+	void __iomem              *uar_map;
+	struct netdev_queue       *txq;
+	u32                        sqn;
+	u16                        max_inline;
+	u8                         min_inline_mode;
+	u16                        edge;
+	struct device             *pdev;
+	struct mlx5e_tstamp       *tstamp;
+	__be32                     mkey_be;
+	unsigned long              state;
+
+	/* control path */
+	struct mlx5_wq_ctrl        wq_ctrl;
+	struct mlx5e_channel      *channel;
+	int                        tc;
+	u32                        rate_limit;
+	u8                         type;
+} ____cacheline_aligned_in_smp;
+
+static inline bool mlx5e_sq_has_room_for(struct mlx5e_sq *sq, u16 n)
+{
+	return (((sq->wq.sz_m1 & (sq->cc - sq->pc)) >= n) ||
+		(sq->cc  == sq->pc));
+}
 
 struct mlx5e_dma_info {
 	struct page	*page;
 	dma_addr_t	addr;
 };
 
+struct mlx5e_umr_dma_info {
+	__be64                *mtt;
+	dma_addr_t             mtt_addr;
+	struct mlx5e_dma_info  dma_info[MLX5_MPWRQ_PAGES_PER_WQE];
+	struct mlx5e_umr_wqe   wqe;
+};
+
+struct mlx5e_mpw_info {
+	struct mlx5e_umr_dma_info umr;
+	u16 consumed_strides;
+	u16 skbs_frags[MLX5_MPWRQ_PAGES_PER_WQE];
+};
+
 struct mlx5e_rx_am_stats {
 	int ppms; /* packets per msec */
 	int epms; /* events per msec */
@@ -346,6 +440,11 @@ struct mlx5e_page_cache {
 	struct mlx5e_dma_info page_cache[MLX5E_CACHE_SIZE];
 };
 
+struct mlx5e_rq;
+typedef void (*mlx5e_fp_handle_rx_cqe)(struct mlx5e_rq*, struct mlx5_cqe64*);
+typedef int (*mlx5e_fp_alloc_wqe)(struct mlx5e_rq*, struct mlx5e_rx_wqe*, u16);
+typedef void (*mlx5e_fp_dealloc_wqe)(struct mlx5e_rq*, u16);
+
 struct mlx5e_rq {
 	/* data path */
 	struct mlx5_wq_ll      wq;
@@ -393,108 +492,6 @@ struct mlx5e_rq {
 	struct mlx5_core_mkey  umr_mkey;
 } ____cacheline_aligned_in_smp;
 
-struct mlx5e_umr_dma_info {
-	__be64                *mtt;
-	dma_addr_t             mtt_addr;
-	struct mlx5e_dma_info  dma_info[MLX5_MPWRQ_PAGES_PER_WQE];
-	struct mlx5e_umr_wqe   wqe;
-};
-
-struct mlx5e_mpw_info {
-	struct mlx5e_umr_dma_info umr;
-	u16 consumed_strides;
-	u16 skbs_frags[MLX5_MPWRQ_PAGES_PER_WQE];
-};
-
-struct mlx5e_tx_wqe_info {
-	u32 num_bytes;
-	u8  num_wqebbs;
-	u8  num_dma;
-};
-
-enum mlx5e_dma_map_type {
-	MLX5E_DMA_MAP_SINGLE,
-	MLX5E_DMA_MAP_PAGE
-};
-
-struct mlx5e_sq_dma {
-	dma_addr_t              addr;
-	u32                     size;
-	enum mlx5e_dma_map_type type;
-};
-
-enum {
-	MLX5E_SQ_STATE_ENABLED,
-};
-
-struct mlx5e_sq_wqe_info {
-	u8  opcode;
-	u8  num_wqebbs;
-};
-
-enum mlx5e_sq_type {
-	MLX5E_SQ_TXQ,
-	MLX5E_SQ_ICO,
-	MLX5E_SQ_XDP
-};
-
-struct mlx5e_sq {
-	/* data path */
-
-	/* dirtied @completion */
-	u16                        cc;
-	u32                        dma_fifo_cc;
-
-	/* dirtied @xmit */
-	u16                        pc ____cacheline_aligned_in_smp;
-	u32                        dma_fifo_pc;
-	struct mlx5e_sq_stats      stats;
-
-	struct mlx5e_cq            cq;
-
-	/* pointers to per tx element info: write@xmit, read@completion */
-	union {
-		struct {
-			struct sk_buff           **skb;
-			struct mlx5e_sq_dma       *dma_fifo;
-			struct mlx5e_tx_wqe_info  *wqe_info;
-		} txq;
-		struct mlx5e_sq_wqe_info *ico_wqe;
-		struct {
-			struct mlx5e_sq_wqe_info  *wqe_info;
-			struct mlx5e_dma_info     *di;
-			bool                       doorbell;
-		} xdp;
-	} db;
-
-	/* read only */
-	struct mlx5_wq_cyc         wq;
-	u32                        dma_fifo_mask;
-	void __iomem              *uar_map;
-	struct netdev_queue       *txq;
-	u32                        sqn;
-	u16                        max_inline;
-	u8                         min_inline_mode;
-	u16                        edge;
-	struct device             *pdev;
-	struct mlx5e_tstamp       *tstamp;
-	__be32                     mkey_be;
-	unsigned long              state;
-
-	/* control path */
-	struct mlx5_wq_ctrl        wq_ctrl;
-	struct mlx5e_channel      *channel;
-	int                        tc;
-	u32                        rate_limit;
-	u8                         type;
-} ____cacheline_aligned_in_smp;
-
-static inline bool mlx5e_sq_has_room_for(struct mlx5e_sq *sq, u16 n)
-{
-	return (((sq->wq.sz_m1 & (sq->cc - sq->pc)) >= n) ||
-		(sq->cc  == sq->pc));
-}
-
 enum channel_flags {
 	MLX5E_CHANNEL_NAPI_SCHED = 1,
 };

commit 1c4bf940455cd91a9fa100f09bd304f141fb4429
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Sat Mar 25 00:52:06 2017 +0300

    net/mlx5e: Move XDP completion functions to rx file
    
    XDP code belongs to RX path, move mlx5e_poll_xdp_tx_cq and
    mlx5e_free_xdp_tx_descs to en_rx.c.
    
    Rename them to mlx5e_poll_xdpsq_cq and mlx5e_free_xdpsq_descs.
    
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 22e4bad03f05..fce0eca0701c 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -737,6 +737,8 @@ void mlx5e_cq_error_event(struct mlx5_core_cq *mcq, enum mlx5_event event);
 int mlx5e_napi_poll(struct napi_struct *napi, int budget);
 bool mlx5e_poll_tx_cq(struct mlx5e_cq *cq, int napi_budget);
 int mlx5e_poll_rx_cq(struct mlx5e_cq *cq, int budget);
+bool mlx5e_poll_xdpsq_cq(struct mlx5e_cq *cq);
+void mlx5e_free_xdpsq_descs(struct mlx5e_sq *sq);
 void mlx5e_free_sq_descs(struct mlx5e_sq *sq);
 
 void mlx5e_page_release(struct mlx5e_rq *rq, struct mlx5e_dma_info *dma_info,

commit aff2615763f206f897146e0ee1ddae8e22055ae3
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Sat Mar 25 00:52:05 2017 +0300

    net/mlx5e: Single bfreg (UAR) for all mlx5e SQs and netdevs
    
    One is sufficient since Blue Flame is not supported anymore.
    This will also come in handy for switchdev mode to save resources, since
    VF representors will use same single UAR as well for their own SQs.
    
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 85261e9ccf4a..22e4bad03f05 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -483,7 +483,6 @@ struct mlx5e_sq {
 
 	/* control path */
 	struct mlx5_wq_ctrl        wq_ctrl;
-	struct mlx5_sq_bfreg	   bfreg;
 	struct mlx5e_channel      *channel;
 	int                        tc;
 	u32                        rate_limit;

commit 6982ab609768f4e7c84eab053314effa0bc500fc
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Sat Mar 25 00:52:04 2017 +0300

    net/mlx5e: Xmit, no write combining
    
    mlx5e netdev Blue Flame (write combining) support demands a lot of
    overhead for a little latency gain for some special cases, this overhead
    is hurting the common case.
    
    Here we remove xmit Blue Flame support by creating all bfregs with no
    write combining for all SQs, and we remove a lot of BF logic and
    conditions from xmit data path.
    
    Simplify mlx5e_tx_notify_hw (doorbell function) by removing BF related
    code and by removing one memory barrier needed for WC mapped SQ doorbell
    buffers, which no longer exist.
    
    Performance improvement:
    System: Intel(R) Xeon(R) CPU E5-2620 v3 @ 2.40GHz
    
    Test case                   Before      Now      improvement
    ---------------------------------------------------------------
    TX packets (24 threads)     50Mpps      54Mpps    8%
    
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index dc52053128bc..85261e9ccf4a 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -111,7 +111,6 @@
 #define MLX5E_MAX_NUM_SQS              (MLX5E_MAX_NUM_CHANNELS * MLX5E_MAX_NUM_TC)
 #define MLX5E_TX_CQ_POLL_BUDGET        128
 #define MLX5E_UPDATE_STATS_INTERVAL    200 /* msecs */
-#define MLX5E_SQ_BF_BUDGET             16
 
 #define MLX5E_ICOSQ_MAX_WQEBBS \
 	(DIV_ROUND_UP(sizeof(struct mlx5e_umr_wqe), MLX5_SEND_WQE_BB))
@@ -426,7 +425,6 @@ struct mlx5e_sq_dma {
 
 enum {
 	MLX5E_SQ_STATE_ENABLED,
-	MLX5E_SQ_STATE_BF_ENABLE,
 };
 
 struct mlx5e_sq_wqe_info {
@@ -450,9 +448,6 @@ struct mlx5e_sq {
 	/* dirtied @xmit */
 	u16                        pc ____cacheline_aligned_in_smp;
 	u32                        dma_fifo_pc;
-	u16                        bf_offset;
-	u16                        prev_cc;
-	u8                         bf_budget;
 	struct mlx5e_sq_stats      stats;
 
 	struct mlx5e_cq            cq;
@@ -478,7 +473,6 @@ struct mlx5e_sq {
 	void __iomem              *uar_map;
 	struct netdev_queue       *txq;
 	u32                        sqn;
-	u16                        bf_buf_size;
 	u16                        max_inline;
 	u8                         min_inline_mode;
 	u16                        edge;
@@ -818,11 +812,9 @@ void mlx5e_set_rx_cq_mode_params(struct mlx5e_params *params,
 				 u8 cq_period_mode);
 void mlx5e_set_rq_type_params(struct mlx5e_priv *priv, u8 rq_type);
 
-static inline void mlx5e_tx_notify_hw(struct mlx5e_sq *sq,
-				      struct mlx5_wqe_ctrl_seg *ctrl, int bf_sz)
+static inline void
+mlx5e_tx_notify_hw(struct mlx5e_sq *sq, struct mlx5_wqe_ctrl_seg *ctrl)
 {
-	u16 ofst = sq->bf_offset;
-
 	/* ensure wqe is visible to device before updating doorbell record */
 	dma_wmb();
 
@@ -832,14 +824,8 @@ static inline void mlx5e_tx_notify_hw(struct mlx5e_sq *sq,
 	 * doorbell
 	 */
 	wmb();
-	if (bf_sz)
-		__iowrite64_copy(sq->uar_map + ofst, ctrl, bf_sz);
-	else
-		mlx5_write64((__be32 *)ctrl, sq->uar_map + ofst, NULL);
-	/* flush the write-combining mapped buffer */
-	wmb();
 
-	sq->bf_offset ^= sq->bf_buf_size;
+	mlx5_write64((__be32 *)ctrl, sq->uar_map, NULL);
 }
 
 static inline void mlx5e_cq_arm(struct mlx5e_cq *cq)

commit 1ad9a00ae0efc2e9337148d6c382fad3d27bf99a
Author: Paul Blakey <paulb@mellanox.com>
Date:   Tue Mar 21 15:59:16 2017 +0200

    net/mlx5e: Avoid supporting udp tunnel port ndo for VF reps
    
    This was added to allow the TC offloading code to identify offloading
    encap/decap vxlan rules.
    
    The VF reps are effectively related to the same mlx5 PCI device as the
    PF. Since the kernel invokes the (say) delete ndo for each netdev, the
    FW erred on multiple vxlan dst port deletes when the port was deleted
    from the system.
    
    We fix that by keeping the registration to be carried out only by the
    PF. Since the PF serves as the uplink device, the VF reps will look
    up a port there and realize if they are ok to offload that.
    
    Tested:
     <SETUP VFS>
     <SETUP switchdev mode to have representors>
     ip link add vxlan1 type vxlan id 44 dev ens5f0 dstport 9999
     ip link set vxlan1 up
     ip link del dev vxlan1
    
    Fixes: 4a25730eb202 ('net/mlx5e: Add ndo_udp_tunnel_add to VF representors')
    Signed-off-by: Paul Blakey <paulb@mellanox.com>
    Reviewed-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index f6a6ded204f6..dc52053128bc 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -928,10 +928,6 @@ void mlx5e_destroy_netdev(struct mlx5_core_dev *mdev, struct mlx5e_priv *priv);
 int mlx5e_attach_netdev(struct mlx5_core_dev *mdev, struct net_device *netdev);
 void mlx5e_detach_netdev(struct mlx5_core_dev *mdev, struct net_device *netdev);
 u32 mlx5e_choose_lro_timeout(struct mlx5_core_dev *mdev, u32 wanted_timeout);
-void mlx5e_add_vxlan_port(struct net_device *netdev,
-			  struct udp_tunnel_info *ti);
-void mlx5e_del_vxlan_port(struct net_device *netdev,
-			  struct udp_tunnel_info *ti);
 
 int mlx5e_get_offload_stats(int attr_id, const struct net_device *dev,
 			    void *sp);

commit 6dc4b54e77282caf17f0ff72aa32dd296037fbc0
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Wed Feb 22 17:20:15 2017 +0200

    net/mlx5e: Update MPWQE stride size when modifying CQE compress state
    
    When the admin enables/disables cqe compression, updating
    mpwqe stride size is required:
        CQE compress ON  ==> stride size = 256B
        CQE compress OFF ==> stride size = 64B
    
    This is already done on driver load via mlx5e_set_rq_type_params, all we
    need is just to call it on arbitrary admin changes of cqe compression
    state via priv flags or when changing timestamping state
    (as it is mutually exclusive with cqe compression).
    
    This bug introduces no functional damage, it only makes cqe compression
    occur less often, since in ConnectX4-LX CQE compression is performed
    only on packets smaller than stride size.
    
    Tested:
     ethtool --set-priv-flags ethxx rx_cqe_compress on
     pktgen with  64 < pkt size < 256 and netperf TCP_STREAM (IPv4/IPv6)
     verify `ethtool -S ethxx | grep compress` are advancing more often
     (rapidly)
    
    Fixes: 7219ab34f184 ("net/mlx5e: CQE compression")
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>
    Cc: kernel-team@fb.com
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 95ca03c0d9f5..f6a6ded204f6 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -816,6 +816,7 @@ int mlx5e_get_max_linkspeed(struct mlx5_core_dev *mdev, u32 *speed);
 
 void mlx5e_set_rx_cq_mode_params(struct mlx5e_params *params,
 				 u8 cq_period_mode);
+void mlx5e_set_rq_type_params(struct mlx5e_priv *priv, u8 rq_type);
 
 static inline void mlx5e_tx_notify_hw(struct mlx5e_sq *sq,
 				      struct mlx5_wqe_ctrl_seg *ctrl, int bf_sz)

commit 8ca967ab67671f07ac7daef4f854559bc66799a3
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Mon Jan 9 21:57:49 2017 +0200

    net/mlx5e: Bring back bfreg uar map dedicated pointer
    
    4K Uar series modified the mlx5e driver to use the new bfreg API,
    and mistakenly removed the sq->uar_map iomem data path dedicated
    pointer, which was meant to be read from xmit path for cache locality
    utilization.
    
    Fix that by returning that pointer to the SQ struct.
    
    Fixes: 7309cb4ad71e ("IB/mlx5: Support 4k UAR for libmlx5")
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 8be4b12b5545..95ca03c0d9f5 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -475,6 +475,7 @@ struct mlx5e_sq {
 	/* read only */
 	struct mlx5_wq_cyc         wq;
 	u32                        dma_fifo_mask;
+	void __iomem              *uar_map;
 	struct netdev_queue       *txq;
 	u32                        sqn;
 	u16                        bf_buf_size;
@@ -831,9 +832,9 @@ static inline void mlx5e_tx_notify_hw(struct mlx5e_sq *sq,
 	 */
 	wmb();
 	if (bf_sz)
-		__iowrite64_copy(sq->bfreg.map + ofst, ctrl, bf_sz);
+		__iowrite64_copy(sq->uar_map + ofst, ctrl, bf_sz);
 	else
-		mlx5_write64((__be32 *)ctrl, sq->bfreg.map + ofst, NULL);
+		mlx5_write64((__be32 *)ctrl, sq->uar_map + ofst, NULL);
 	/* flush the write-combining mapped buffer */
 	wmb();
 

commit b70149dd7dc98661fbbfe7281f127caa408028fe
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Tue Dec 6 14:04:05 2016 +0200

    net/mlx5e: XDP Tx, no inline copy on ConnectX-5
    
    ConnectX-5 and later HW generations will report min inline mode ==
    MLX5_INLINE_MODE_NONE, which means driver is not required to copy packet
    headers to inline fields of TX WQE.
    
    Avoid copy to inline segment in XDP TX routine when HW inline mode doesn't
    require it.
    
    This will improve CPU utilization and boost XDP TX performance.
    
    Tested with xdp2 single flow:
    CPU: Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz
    HCA: Mellanox Technologies MT28800 Family [ConnectX-5 Ex]
    
    Before: 7.4Mpps
    After:  7.8Mpps
    Improvement: 5%
    
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 9b23d3329847..8be4b12b5545 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -120,8 +120,7 @@
 #define MLX5E_XDP_IHS_DS_COUNT \
 	DIV_ROUND_UP(MLX5E_XDP_MIN_INLINE - 2, MLX5_SEND_WQE_DS)
 #define MLX5E_XDP_TX_DS_COUNT \
-	(MLX5E_XDP_IHS_DS_COUNT + \
-	 (sizeof(struct mlx5e_tx_wqe) / MLX5_SEND_WQE_DS) + 1 /* SG DS */)
+	((sizeof(struct mlx5e_tx_wqe) / MLX5_SEND_WQE_DS) + 1 /* SG DS */)
 #define MLX5E_XDP_TX_WQEBBS \
 	DIV_ROUND_UP(MLX5E_XDP_TX_DS_COUNT, MLX5_SEND_WQEBB_NUM_DS)
 

commit f32f5bd2eb7e91a5090c06bbe1ed14bffbbda5d3
Author: Daniel Jurgens <danielj@mellanox.com>
Date:   Thu Nov 19 17:12:26 2015 +0200

    net/mlx5: Configure cache line size for start and end padding
    
    There is a hardware feature that will pad the start or end of a DMA to
    be cache line aligned to avoid RMWs on the last cache line. The default
    cache line size setting for this feature is 64B. This change configures
    the hardware to use 128B alignment on systems with 128B cache lines.
    
    In addition we lower bound MPWRQ stride by HCA cacheline in mlx5e,
    MPWRQ stride should be at least the HCA cacheline, the current default
    is 64B and in case HCA_CAP.cach_line_128byte capability is set, MPWRQ RX
    stride will automatically be aligned to 128B.
    
    Signed-off-by: Daniel Jurgens <danielj@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 9b52c58cd528..9b23d3329847 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -70,8 +70,13 @@
 
 #define MLX5_RX_HEADROOM NET_SKB_PAD
 
-#define MLX5_MPWRQ_LOG_STRIDE_SIZE		6  /* >= 6, HW restriction */
-#define MLX5_MPWRQ_LOG_STRIDE_SIZE_CQE_COMPRESS	8  /* >= 6, HW restriction */
+#define MLX5_MPWRQ_MIN_LOG_STRIDE_SZ(mdev) \
+	(6 + MLX5_CAP_GEN(mdev, cache_line_128byte)) /* HW restriction */
+#define MLX5_MPWRQ_LOG_STRIDE_SZ(mdev, req) \
+	max_t(u32, MLX5_MPWRQ_MIN_LOG_STRIDE_SZ(mdev), req)
+#define MLX5_MPWRQ_DEF_LOG_STRIDE_SZ(mdev)       MLX5_MPWRQ_LOG_STRIDE_SZ(mdev, 6)
+#define MLX5_MPWRQ_CQE_CMPRS_LOG_STRIDE_SZ(mdev) MLX5_MPWRQ_LOG_STRIDE_SZ(mdev, 8)
+
 #define MLX5_MPWRQ_LOG_WQE_SZ			18
 #define MLX5_MPWRQ_WQE_PAGE_ORDER  (MLX5_MPWRQ_LOG_WQE_SZ - PAGE_SHIFT > 0 ? \
 				    MLX5_MPWRQ_LOG_WQE_SZ - PAGE_SHIFT : 0)

commit e2160156bf2a7d5018e99a9993fbcdda0abac09b
Merge: 62e13097c46c 6d04dfc89660
Author: David S. Miller <davem@davemloft.net>
Date:   Thu Feb 2 16:54:00 2017 -0500

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    All merge conflicts were simple overlapping changes.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit a100ff3eef193d2d79daf98dcd97a54776ffeb78
Author: Gal Pressman <galp@mellanox.com>
Date:   Thu Jan 12 16:25:46 2017 +0200

    net/mlx5e: Fix update of hash function/key via ethtool
    
    Modifying TIR hash should change selected fields bitmask in addition to
    the function and key.
    
    Formerly, Only on ethool mlx5e_set_rxfh "ethtoo -X" we would not set this
    field resulting in zeroing of its value, which means no packet fields are
    used for RX RSS hash calculation thus causing all traffic to arrive in
    RQ[0].
    
    On driver load out of the box we don't have this issue, since the TIR
    hash is fully created from scratch.
    
    Tested:
    ethtool -X ethX hkey  <new key>
    ethtool -X ethX hfunc <new func>
    ethtool -X ethX equal <new indirection table>
    
    All cases are verified with TCP Multi-Stream traffic over IPv4 & IPv6.
    
    Fixes: bdfc028de1b3 ("net/mlx5e: Fix ethtool RX hash func configuration change")
    Signed-off-by: Gal Pressman <galp@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 1619147a63e8..d5ecb8f53fd4 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -791,7 +791,8 @@ void mlx5e_disable_vlan_filter(struct mlx5e_priv *priv);
 int mlx5e_modify_rqs_vsd(struct mlx5e_priv *priv, bool vsd);
 
 int mlx5e_redirect_rqt(struct mlx5e_priv *priv, u32 rqtn, int sz, int ix);
-void mlx5e_build_tir_ctx_hash(void *tirc, struct mlx5e_priv *priv);
+void mlx5e_build_indir_tir_ctx_hash(struct mlx5e_priv *priv, void *tirc,
+				    enum mlx5e_traffic_types tt);
 
 int mlx5e_open_locked(struct net_device *netdev);
 int mlx5e_close_locked(struct net_device *netdev);

commit 9eb7892351a3a3b403d879b41c4e6efb2c96516f
Author: Or Gerlitz <ogerlitz@mellanox.com>
Date:   Wed Jan 11 19:35:41 2017 +0200

    net/mlx5: Change ENOTSUPP to EOPNOTSUPP
    
    As ENOTSUPP is specific to NFS, change the return error value to
    EOPNOTSUPP in various places in the mlx5 driver.
    
    Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
    Suggested-by: Yotam Gigi <yotamg@mellanox.com>
    Reviewed-by: Matan Barak <matanb@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 951dbd58594d..1619147a63e8 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -863,12 +863,12 @@ static inline void mlx5e_arfs_destroy_tables(struct mlx5e_priv *priv) {}
 
 static inline int mlx5e_arfs_enable(struct mlx5e_priv *priv)
 {
-	return -ENOTSUPP;
+	return -EOPNOTSUPP;
 }
 
 static inline int mlx5e_arfs_disable(struct mlx5e_priv *priv)
 {
-	return -ENOTSUPP;
+	return -EOPNOTSUPP;
 }
 #else
 int mlx5e_arfs_create_tables(struct mlx5e_priv *priv);

commit 5eb0249b4352c813f0a3c31b967f6cb4b9869a50
Author: Shaker Daibes <shakerd@mellanox.com>
Date:   Sat Dec 10 18:45:55 2016 +0200

    net/mlx5e: CQE compression control code reuse
    
    This patch is intended for code reuse of mlx5e_modify_rx_cqe_compression
    function.
    
    Signed-off-by: Shaker Daibes <shakerd@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 8d660a018bf8..46f728de9e76 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -787,7 +787,7 @@ void mlx5e_pps_event_handler(struct mlx5e_priv *priv,
 			     struct ptp_clock_event *event);
 int mlx5e_hwstamp_set(struct net_device *dev, struct ifreq *ifr);
 int mlx5e_hwstamp_get(struct net_device *dev, struct ifreq *ifr);
-void mlx5e_modify_rx_cqe_compression(struct mlx5e_priv *priv, bool val);
+void mlx5e_modify_rx_cqe_compression_locked(struct mlx5e_priv *priv, bool val);
 
 int mlx5e_vlan_rx_add_vid(struct net_device *dev, __always_unused __be16 proto,
 			  u16 vid);

commit b4e029da29ce8244879b607fbf0514d1087dc3f5
Author: Kamal Heib <kamalh@mellanox.com>
Date:   Tue Nov 22 11:03:32 2016 +0200

    net/mlx5e: Reduce memory consumption on kdump kernel
    
    Reduce memory consumption on kdump kernel by decreasing the number of
    channels to 1 and the size of RQs and SQs to the minimal values.
    
    Signed-off-by: Kamal Heib <kamalh@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index d603a30ad639..8d660a018bf8 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -101,6 +101,7 @@
 
 #define MLX5E_LOG_INDIR_RQT_SIZE       0x7
 #define MLX5E_INDIR_RQT_SIZE           BIT(MLX5E_LOG_INDIR_RQT_SIZE)
+#define MLX5E_MIN_NUM_CHANNELS         0x1
 #define MLX5E_MAX_NUM_CHANNELS         (MLX5E_INDIR_RQT_SIZE >> 1)
 #define MLX5E_MAX_NUM_SQS              (MLX5E_MAX_NUM_CHANNELS * MLX5E_MAX_NUM_TC)
 #define MLX5E_TX_CQ_POLL_BUDGET        128
@@ -847,12 +848,6 @@ static inline u32 mlx5e_get_wqe_mtt_offset(struct mlx5e_rq *rq, u16 wqe_ix)
 	return wqe_ix * ALIGN(MLX5_MPWRQ_PAGES_PER_WQE, 8);
 }
 
-static inline int mlx5e_get_max_num_channels(struct mlx5_core_dev *mdev)
-{
-	return min_t(int, mdev->priv.eq_table.num_comp_vectors,
-		     MLX5E_MAX_NUM_CHANNELS);
-}
-
 extern const struct ethtool_ops mlx5e_ethtool_ops;
 #ifdef CONFIG_MLX5_CORE_EN_DCB
 extern const struct dcbnl_rtnl_ops mlx5e_dcbnl_ops;

commit 8a271746a264c1bd05fd98da1d96d97b79799970
Author: Mohamad Haj Yahia <mohamad@mellanox.com>
Date:   Sun Oct 9 17:05:31 2016 +0300

    net/mlx5e: Receive s-tagged packets in promiscuous mode
    
    Today when the driver enter to promiscuous mode or vlan
    filter is disabled, we add flow rule to receive any c-taggd
    packets, therefore s-tagged packets are dropped.
    In order to receive s-tagged packets as well we need to add
    flow rule to receive any s-tagged packet.
    
    Fixes: 7cb21b794baa ('net/mlx5e: Rename en_flow_table.c to en_fs.c')
    Signed-off-by: Mohamad Haj Yahia <mohamad@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 22c3d26c6d3d..d603a30ad639 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -572,8 +572,9 @@ struct mlx5e_vlan_table {
 	unsigned long active_vlans[BITS_TO_LONGS(VLAN_N_VID)];
 	struct mlx5_flow_handle	*active_vlans_rule[VLAN_N_VID];
 	struct mlx5_flow_handle	*untagged_rule;
-	struct mlx5_flow_handle	*any_vlan_rule;
-	bool		filter_disabled;
+	struct mlx5_flow_handle	*any_cvlan_rule;
+	struct mlx5_flow_handle	*any_svlan_rule;
+	bool			filter_disabled;
 };
 
 struct mlx5e_l2_table {

commit ee7f12205abc21c2e83b266636e27b1cb2fd4e21
Author: Eugenia Emantayev <eugenia@mellanox.com>
Date:   Mon Aug 22 14:57:41 2016 +0300

    net/mlx5e: Implement 1PPS support
    
    This patch enables the 1PPS IN and 1PPS OUT support according
    to the advertised HCA capability. Single pin may be configured
    to one of the above mutual exclusive functions via standard
    Linux tools and APIs. For example, testptp open source application.
    
    Signed-off-by: Eugenia Emantayev <eugenia@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 0d9dd860a295..22c3d26c6d3d 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -262,6 +262,7 @@ struct mlx5e_tstamp {
 	struct mlx5_core_dev      *mdev;
 	struct ptp_clock          *ptp;
 	struct ptp_clock_info      ptp_info;
+	u8                        *pps_pin_caps;
 };
 
 enum {
@@ -780,6 +781,8 @@ void mlx5e_fill_hwstamp(struct mlx5e_tstamp *clock, u64 timestamp,
 			struct skb_shared_hwtstamps *hwts);
 void mlx5e_timestamp_init(struct mlx5e_priv *priv);
 void mlx5e_timestamp_cleanup(struct mlx5e_priv *priv);
+void mlx5e_pps_event_handler(struct mlx5e_priv *priv,
+			     struct ptp_clock_event *event);
 int mlx5e_hwstamp_set(struct net_device *dev, struct ifreq *ifr);
 int mlx5e_hwstamp_get(struct net_device *dev, struct ifreq *ifr);
 void mlx5e_modify_rx_cqe_compression(struct mlx5e_priv *priv, bool val);

commit d8bec2b29a82981acb229215e3970d4d50a7e6eb
Author: Martin KaFai Lau <kafai@fb.com>
Date:   Tue Jan 17 22:06:07 2017 -0800

    net/mlx5e: Support bpf_xdp_adjust_head()
    
    This patch adds bpf_xdp_adjust_head() support to mlx5e.
    
    1. rx_headroom is added to struct mlx5e_rq.  It uses
       an existing 4 byte hole in the struct.
    2. The adjusted data length is checked against
       MLX5E_XDP_MIN_INLINE and MLX5E_SW2HW_MTU(rq->netdev->mtu).
    3. The macro MLX5E_SW2HW_MTU is moved from en_main.c to en.h.
       MLX5E_HW2SW_MTU is also moved to en.h for symmetric reason
       but it is not a must.
    
    v2:
    - Keep the xdp specific logic in mlx5e_xdp_handle()
    - Update dma_len after the sanity checks in mlx5e_xmit_xdp_frame()
    
    Signed-off-by: Martin KaFai Lau <kafai@fb.com>
    Acked-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index a473cea10c16..0d9dd860a295 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -51,6 +51,9 @@
 
 #define MLX5_SET_CFG(p, f, v) MLX5_SET(create_flow_group_in, p, f, v)
 
+#define MLX5E_HW2SW_MTU(hwmtu) ((hwmtu) - (ETH_HLEN + VLAN_HLEN + ETH_FCS_LEN))
+#define MLX5E_SW2HW_MTU(swmtu) ((swmtu) + (ETH_HLEN + VLAN_HLEN + ETH_FCS_LEN))
+
 #define MLX5E_MAX_NUM_TC	8
 
 #define MLX5E_PARAMS_MINIMUM_LOG_SQ_SIZE                0x6
@@ -369,6 +372,7 @@ struct mlx5e_rq {
 
 	unsigned long          state;
 	int                    ix;
+	u16                    rx_headroom;
 
 	struct mlx5e_rx_am     am; /* Adaptive Moderation */
 	struct bpf_prog       *xdp_prog;

commit 30aa60b3bd12bd79b5324b7b595bd3446ab24b52
Author: Eli Cohen <eli@mellanox.com>
Date:   Tue Jan 3 23:55:27 2017 +0200

    IB/mlx5: Support 4k UAR for libmlx5
    
    Add fields to structs to convey to kernel an indication whether the
    library supports multi UARs per page and return to the library the size
    of a UAR based on the queried value.
    
    Signed-off-by: Eli Cohen <eli@mellanox.com>
    Reviewed-by: Matan Barak <matanb@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 3037631570b1..a473cea10c16 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -465,7 +465,6 @@ struct mlx5e_sq {
 	/* read only */
 	struct mlx5_wq_cyc         wq;
 	u32                        dma_fifo_mask;
-	void __iomem              *uar_map;
 	struct netdev_queue       *txq;
 	u32                        sqn;
 	u16                        bf_buf_size;
@@ -479,7 +478,7 @@ struct mlx5e_sq {
 
 	/* control path */
 	struct mlx5_wq_ctrl        wq_ctrl;
-	struct mlx5_uar            uar;
+	struct mlx5_sq_bfreg	   bfreg;
 	struct mlx5e_channel      *channel;
 	int                        tc;
 	u32                        rate_limit;
@@ -806,7 +805,7 @@ void mlx5e_set_rx_cq_mode_params(struct mlx5e_params *params,
 static inline void mlx5e_tx_notify_hw(struct mlx5e_sq *sq,
 				      struct mlx5_wqe_ctrl_seg *ctrl, int bf_sz)
 {
-	u16 ofst = MLX5_BF_OFFSET + sq->bf_offset;
+	u16 ofst = sq->bf_offset;
 
 	/* ensure wqe is visible to device before updating doorbell record */
 	dma_wmb();
@@ -818,9 +817,9 @@ static inline void mlx5e_tx_notify_hw(struct mlx5e_sq *sq,
 	 */
 	wmb();
 	if (bf_sz)
-		__iowrite64_copy(sq->uar_map + ofst, ctrl, bf_sz);
+		__iowrite64_copy(sq->bfreg.map + ofst, ctrl, bf_sz);
 	else
-		mlx5_write64((__be32 *)ctrl, sq->uar_map + ofst, NULL);
+		mlx5_write64((__be32 *)ctrl, sq->bfreg.map + ofst, NULL);
 	/* flush the write-combining mapped buffer */
 	wmb();
 

commit 5fe9dec0d045437e48f112b8fa705197bd7bc3c0
Author: Eli Cohen <eli@mellanox.com>
Date:   Tue Jan 3 23:55:25 2017 +0200

    IB/mlx5: Use blue flame register allocator in mlx5_ib
    
    Make use of the blue flame registers allocator at mlx5_ib. Since blue
    flame was not really supported we remove all the code that is related to
    blue flame and we let all consumers to use the same blue flame register.
    Once blue flame is supported we will add the code. As part of this patch
    we also move the definition of struct mlx5_bf to mlx5_ib.h as it is only
    used by mlx5_ib.
    
    Signed-off-by: Eli Cohen <eli@mellanox.com>
    Reviewed-by: Matan Barak <matanb@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 951dbd58594d..3037631570b1 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -832,7 +832,7 @@ static inline void mlx5e_cq_arm(struct mlx5e_cq *cq)
 	struct mlx5_core_cq *mcq;
 
 	mcq = &cq->mcq;
-	mlx5_cq_arm(mcq, MLX5_CQ_DB_REQ_NOT, mcq->uar->map, NULL, cq->wq.cc);
+	mlx5_cq_arm(mcq, MLX5_CQ_DB_REQ_NOT, mcq->uar->map, cq->wq.cc);
 }
 
 static inline u32 mlx5e_get_wqe_mtt_offset(struct mlx5e_rq *rq, u16 wqe_ix)

commit c63d352f053a788281eb90df0a71cd3a6b2c4040
Merge: 862b3d2090ae bc3913a5378c
Author: David S. Miller <davem@davemloft.net>
Date:   Tue Dec 6 21:33:19 2016 -0500

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net

commit c0f1147d14e4b09018a495c5095094e5707a4f44
Author: Mohamad Haj Yahia <mohamad@mellanox.com>
Date:   Tue Dec 6 17:32:48 2016 +0200

    net/mlx5e: Change the SQ/RQ operational state to positive logic
    
    When using the negative logic (i.e. FLUSH state), after the RQ/SQ reopen
    we will have a time interval that the RQ/SQ is not really ready and the
    state indicates that its not in FLUSH state because the initial SQ/RQ struct
    memory starts as zeros.
    Now we changed the state to indicate if the SQ/RQ is opened and we will
    set the READY state after finishing preparing all the SQ/RQ resources.
    
    Fixes: 6e8dd6d6f4bd ("net/mlx5e: Don't wait for SQ completions on close")
    Fixes: f2fde18c52a7 ("net/mlx5e: Don't wait for RQ completions on close")
    Signed-off-by: Mohamad Haj Yahia <mohamad@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 7a43502a89cc..71382df59fc0 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -241,7 +241,7 @@ struct mlx5e_tstamp {
 };
 
 enum {
-	MLX5E_RQ_STATE_FLUSH,
+	MLX5E_RQ_STATE_ENABLED,
 	MLX5E_RQ_STATE_UMR_WQE_IN_PROGRESS,
 	MLX5E_RQ_STATE_AM,
 };
@@ -394,7 +394,7 @@ struct mlx5e_sq_dma {
 };
 
 enum {
-	MLX5E_SQ_STATE_FLUSH,
+	MLX5E_SQ_STATE_ENABLED,
 	MLX5E_SQ_STATE_BF_ENABLE,
 };
 

commit ec8b9981ad3f3eeb5dcc4f237266e897c363f896
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Wed Nov 30 17:59:39 2016 +0200

    net/mlx5e: Create UMR MKey per RQ
    
    In Striding RQ implementation, we used a single UMR
    (User-Mode Memory Registration) memory key for all RQs.
    When the product of RQs number*size gets high, we hit a
    limitation of u16 field size in FW.
    
    Here we move to using a UMR memory key per RQ, so we can
    scale to any number of rings, with the maximum buffer
    size in each.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index f16f7fbd2044..63dd6390b161 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -77,9 +77,9 @@
 						 MLX5_MPWRQ_WQE_PAGE_ORDER)
 
 #define MLX5_MTT_OCTW(npages) (ALIGN(npages, 8) / 2)
-#define MLX5E_REQUIRED_MTTS(rqs, wqes)\
-	(rqs * wqes * ALIGN(MLX5_MPWRQ_PAGES_PER_WQE, 8))
-#define MLX5E_VALID_NUM_MTTS(num_mtts) (MLX5_MTT_OCTW(num_mtts) <= U16_MAX)
+#define MLX5E_REQUIRED_MTTS(wqes)		\
+	(wqes * ALIGN(MLX5_MPWRQ_PAGES_PER_WQE, 8))
+#define MLX5E_VALID_NUM_MTTS(num_mtts) (MLX5_MTT_OCTW(num_mtts) - 1 <= U16_MAX)
 
 #define MLX5_UMR_ALIGN				(2048)
 #define MLX5_MPWRQ_SMALL_PACKET_THRESHOLD	(128)
@@ -347,7 +347,6 @@ struct mlx5e_rq {
 		struct {
 			struct mlx5e_mpw_info *info;
 			void                  *mtt_no_align;
-			u32                    mtt_offset;
 		} mpwqe;
 	};
 	struct {
@@ -382,6 +381,7 @@ struct mlx5e_rq {
 	u32                    rqn;
 	struct mlx5e_channel  *channel;
 	struct mlx5e_priv     *priv;
+	struct mlx5_core_mkey  umr_mkey;
 } ____cacheline_aligned_in_smp;
 
 struct mlx5e_umr_dma_info {
@@ -689,7 +689,6 @@ struct mlx5e_priv {
 
 	unsigned long              state;
 	struct mutex               state_lock; /* Protects Interface state */
-	struct mlx5_core_mkey      umr_mkey;
 	struct mlx5e_rq            drop_rq;
 
 	struct mlx5e_channel     **channel;
@@ -838,8 +837,7 @@ static inline void mlx5e_cq_arm(struct mlx5e_cq *cq)
 
 static inline u32 mlx5e_get_wqe_mtt_offset(struct mlx5e_rq *rq, u16 wqe_ix)
 {
-	return rq->mpwqe.mtt_offset +
-		wqe_ix * ALIGN(MLX5_MPWRQ_PAGES_PER_WQE, 8);
+	return wqe_ix * ALIGN(MLX5_MPWRQ_PAGES_PER_WQE, 8);
 }
 
 static inline int mlx5e_get_max_num_channels(struct mlx5_core_dev *mdev)

commit 1c1b522808a18402f043c1418b4e48c7355480cc
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Wed Nov 30 17:59:37 2016 +0200

    net/mlx5e: Implement Fragmented Work Queue (WQ)
    
    Add new type of struct mlx5_frag_buf which is used to allocate fragmented
    buffers rather than contiguous, and make the Completion Queues (CQs) use
    it as they are big (default of 2MB per CQ in Striding RQ).
    
    This fixes the failures of type:
    "mlx5e_open_locked: mlx5e_open_channels failed, -12"
    due to dma_zalloc_coherent insufficient contiguous coherent memory to
    satisfy the driver's request when the user tries to setup more or larger
    rings.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Reported-by: Sebastian Ott <sebott@linux.vnet.ibm.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 442dbc3e6be4..f16f7fbd2044 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -286,7 +286,7 @@ struct mlx5e_cq {
 	u16                        decmprs_wqe_counter;
 
 	/* control */
-	struct mlx5_wq_ctrl        wq_ctrl;
+	struct mlx5_frag_wq_ctrl   wq_ctrl;
 } ____cacheline_aligned_in_smp;
 
 struct mlx5e_rq;

commit 9bcc86064bb5006257e3367fc4439f4072d82442
Author: Shaker Daibes <shakerd@mellanox.com>
Date:   Sun Nov 27 17:02:12 2016 +0200

    net/mlx5e: Add CQE compression user control
    
    The user can now override the automatic driver decision using the
    rx_cqe_compress flag, which is the preference for CQE compression.
    The flag is initialized with the automatic driver decision.
    
    Signed-off-by: Shaker Daibes <shakerd@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 84ac78f9087f..442dbc3e6be4 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -171,10 +171,12 @@ extern const char mlx5e_self_tests[][ETH_GSTRING_LEN];
 
 static const char mlx5e_priv_flags[][ETH_GSTRING_LEN] = {
 	"rx_cqe_moder",
+	"rx_cqe_compress",
 };
 
 enum mlx5e_priv_flag {
 	MLX5E_PFLAG_RX_CQE_BASED_MODER = (1 << 0),
+	MLX5E_PFLAG_RX_CQE_COMPRESS = (1 << 1),
 };
 
 #define MLX5E_SET_PFLAG(priv, pflag, enable)			\
@@ -205,8 +207,7 @@ struct mlx5e_params {
 	u16 num_channels;
 	u8  num_tc;
 	u8  rx_cq_period_mode;
-	bool rx_cqe_compress_admin;
-	bool rx_cqe_compress;
+	bool rx_cqe_compress_def;
 	struct mlx5e_cq_moder rx_cq_moderation;
 	struct mlx5e_cq_moder tx_cq_moderation;
 	u16 min_rx_wqes;

commit 59ece1c9693914f969eb896906889591c4d2f172
Author: Shaker Daibes <shakerd@mellanox.com>
Date:   Sun Nov 27 17:02:11 2016 +0200

    net/mlx5e: Moves pflags to priv->params
    
    pflags is a configuration parameter for the netdev, naturally it belongs
    to priv->params.
    Also introduce MLX5E_GET_PFLAG
    
    Signed-off-by: Shaker Daibes <shakerd@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 9cf32d38fb55..84ac78f9087f 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -177,14 +177,16 @@ enum mlx5e_priv_flag {
 	MLX5E_PFLAG_RX_CQE_BASED_MODER = (1 << 0),
 };
 
-#define MLX5E_SET_PRIV_FLAG(priv, pflag, enable)    \
-	do {                                        \
-		if (enable)                         \
-			priv->pflags |= pflag;      \
-		else                                \
-			priv->pflags &= ~pflag;     \
+#define MLX5E_SET_PFLAG(priv, pflag, enable)			\
+	do {							\
+		if (enable)					\
+			(priv)->params.pflags |= (pflag);	\
+		else						\
+			(priv)->params.pflags &= ~(pflag);	\
 	} while (0)
 
+#define MLX5E_GET_PFLAG(priv, pflag) (!!((priv)->params.pflags & (pflag)))
+
 #ifdef CONFIG_MLX5_CORE_EN_DCB
 #define MLX5E_MAX_BW_ALLOC 100 /* Max percentage of BW allocation */
 #endif
@@ -218,6 +220,7 @@ struct mlx5e_params {
 	bool vlan_strip_disable;
 	bool rx_am_enabled;
 	u32 lro_timeout;
+	u32 pflags;
 };
 
 #ifdef CONFIG_MLX5_CORE_EN_DCB
@@ -705,7 +708,6 @@ struct mlx5e_priv {
 	struct work_struct         tx_timeout_work;
 	struct delayed_work        update_stats_work;
 
-	u32                        pflags;
 	struct mlx5_core_dev      *mdev;
 	struct net_device         *netdev;
 	struct mlx5e_stats         stats;

commit 0952da791c97477da52fbdb2242477d781b9f37e
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Sun Nov 27 17:02:10 2016 +0200

    net/mlx5e: Add support for loopback selftest
    
    Extend the self diagnostic tests to support loopback test.
    
    The loopback test doesn't require the offline flag, it will use the
    generic dev_queue_xmit and a dedicated packet_type to capture and verify
    mlx5e selftest loopback packets.
    
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: Kamal Heib <kamalh@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index f7bb4a7fb981..9cf32d38fb55 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -885,7 +885,8 @@ void mlx5e_destroy_tir(struct mlx5_core_dev *mdev,
 		       struct mlx5e_tir *tir);
 int mlx5e_create_mdev_resources(struct mlx5_core_dev *mdev);
 void mlx5e_destroy_mdev_resources(struct mlx5_core_dev *mdev);
-int mlx5e_refresh_tirs_self_loopback_enable(struct mlx5_core_dev *mdev);
+int mlx5e_refresh_tirs_self_loopback(struct mlx5_core_dev *mdev,
+				     bool enable_uc_lb);
 
 struct mlx5_eswitch_rep;
 int mlx5e_vport_rep_load(struct mlx5_eswitch *esw,

commit d605d6686dc7e7c767cd2dbf89a4ebd195891212
Author: Kamal Heib <kamalh@mellanox.com>
Date:   Sun Nov 27 17:02:09 2016 +0200

    net/mlx5e: Add support for ethtool self diagnostics test
    
    The self diagnostics test implementaion include the following features:
    1. Link Test: Check that link is in up state.
    2. Speed Test: Check that link was negotiated correctly.
    3. Health Test: Check the device health.
    
    Signed-off-by: Kamal Heib <kamalh@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 6c954cddfe66..f7bb4a7fb981 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -167,6 +167,8 @@ struct mlx5e_umr_wqe {
 	struct mlx5_wqe_data_seg       data;
 };
 
+extern const char mlx5e_self_tests[][ETH_GSTRING_LEN];
+
 static const char mlx5e_priv_flags[][ETH_GSTRING_LEN] = {
 	"rx_cqe_moder",
 };
@@ -754,6 +756,9 @@ int mlx5e_create_flow_steering(struct mlx5e_priv *priv);
 void mlx5e_destroy_flow_steering(struct mlx5e_priv *priv);
 void mlx5e_init_l2_addr(struct mlx5e_priv *priv);
 void mlx5e_destroy_flow_table(struct mlx5e_flow_table *ft);
+int mlx5e_self_test_num(struct mlx5e_priv *priv);
+void mlx5e_self_test(struct net_device *ndev, struct ethtool_test *etest,
+		     u64 *buf);
 int mlx5e_ethtool_get_flow(struct mlx5e_priv *priv, struct ethtool_rxnfc *info,
 			   int location);
 int mlx5e_ethtool_get_all_flows(struct mlx5e_priv *priv,

commit e207b7e991768b724f6d216de49c9b800e203eed
Author: Huy Nguyen <huyn@mellanox.com>
Date:   Sun Nov 27 17:02:07 2016 +0200

    net/mlx5e: ConnectX-4 firmware support for DCBX
    
    DBCX by default is controlled by firmware where dcbx capability bit
    is set. In this mode, firmware is responsible for reading/sending the
    TLV packets from/to the remote partner.
    
    This patch sets up the infrastructure to move between HOST/FW DCBX
    control mode.
    
    Signed-off-by: Huy Nguyen <huyn@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 60aa13b2b21f..6c954cddfe66 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -234,6 +234,7 @@ enum {
 };
 
 struct mlx5e_dcbx {
+	enum mlx5_dcbx_oper_mode   mode;
 	struct mlx5e_cee_config    cee_cfg; /* pending configuration */
 
 	/* The only setting that cannot be read from FW */
@@ -843,6 +844,7 @@ extern const struct ethtool_ops mlx5e_ethtool_ops;
 #ifdef CONFIG_MLX5_CORE_EN_DCB
 extern const struct dcbnl_rtnl_ops mlx5e_dcbnl_ops;
 int mlx5e_dcbnl_ieee_setets_core(struct mlx5e_priv *priv, struct ieee_ets *ets);
+void mlx5e_dcbnl_initialize(struct mlx5e_priv *priv);
 #endif
 
 #ifndef CONFIG_RFS_ACCEL

commit 820c2c5e773d283ab102f1b64618e2ddfbb975ef
Author: Huy Nguyen <huyn@mellanox.com>
Date:   Sun Nov 27 17:02:05 2016 +0200

    net/mlx5e: Read ETS settings directly from firmware
    
    Issue description:
    Current implementation saves the ETS settings from user in
    a temporal soft copy and returns this settings when user
    queries the ETS settings.
    
    With the new DCBX firmware, the ETS settings can be changed
    by firmware when the DCBX is in firmware controlled mode. Therefore,
    user will obtain wrong values from the temporal soft copy.
    
    Solution:
    1. Read the ETS settings directly from firmware.
    2. For tc_tsa:
       a. Initialize tc_tsa to vendor IEEE_8021QAZ_TSA_VENDOR at netdev
          creation.
       b. When reading ETS setting from FW, if the traffic class bandwidth
          is less than 100, set tc_tsa to IEEE_8021QAZ_TSA_ETS. This
          implementation solves the scenarios when the DCBX is in FW control
          and willing bit is on which means the ETS setting is dictated
          by remote switch.
    
    Also check ETS capability where needed.
    
    Signed-off-by: Huy Nguyen <huyn@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 31387ed9113b..60aa13b2b21f 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -214,9 +214,6 @@ struct mlx5e_params {
 	u8  toeplitz_hash_key[40];
 	u32 indirection_rqt[MLX5E_INDIR_RQT_SIZE];
 	bool vlan_strip_disable;
-#ifdef CONFIG_MLX5_CORE_EN_DCB
-	struct ieee_ets ets;
-#endif
 	bool rx_am_enabled;
 	u32 lro_timeout;
 };
@@ -238,6 +235,9 @@ enum {
 
 struct mlx5e_dcbx {
 	struct mlx5e_cee_config    cee_cfg; /* pending configuration */
+
+	/* The only setting that cannot be read from FW */
+	u8                         tc_tsa[IEEE_8021QAZ_MAX_TCS];
 };
 #endif
 

commit 3a6a931dfb8e49a7377825b465d84e110fe89f68
Author: Huy Nguyen <huyn@mellanox.com>
Date:   Sun Nov 27 17:02:04 2016 +0200

    net/mlx5e: Support DCBX CEE API
    
    Add DCBX CEE API interface for ConnectX-4. Configurations are stored in
    a temporary structure and are applied to the card's firmware when
    the CEE's setall callback function is called.
    
    Note:
      priority group in CEE is equivalent to traffic class in ConnectX-4
      hardware spec.
    
      bw allocation per priority in CEE is not supported because ConnectX-4
      only supports bw allocation per traffic class.
    
      user priority in CEE does not have an equivalent term in ConnectX-4.
      Therefore, user priority to priority mapping in CEE is not supported.
    
    Signed-off-by: Huy Nguyen <huyn@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index a2b32ed24315..31387ed9113b 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -221,6 +221,26 @@ struct mlx5e_params {
 	u32 lro_timeout;
 };
 
+#ifdef CONFIG_MLX5_CORE_EN_DCB
+struct mlx5e_cee_config {
+	/* bw pct for priority group */
+	u8                         pg_bw_pct[CEE_DCBX_MAX_PGS];
+	u8                         prio_to_pg_map[CEE_DCBX_MAX_PRIO];
+	bool                       pfc_setting[CEE_DCBX_MAX_PRIO];
+	bool                       pfc_enable;
+};
+
+enum {
+	MLX5_DCB_CHG_RESET,
+	MLX5_DCB_NO_CHG,
+	MLX5_DCB_CHG_NO_RESET,
+};
+
+struct mlx5e_dcbx {
+	struct mlx5e_cee_config    cee_cfg; /* pending configuration */
+};
+#endif
+
 struct mlx5e_tstamp {
 	rwlock_t                   lock;
 	struct cyclecounter        cycles;
@@ -688,6 +708,10 @@ struct mlx5e_priv {
 	struct mlx5e_stats         stats;
 	struct mlx5e_tstamp        tstamp;
 	u16 q_counter;
+#ifdef CONFIG_MLX5_CORE_EN_DCB
+	struct mlx5e_dcbx          dcbx;
+#endif
+
 	const struct mlx5e_profile *profile;
 	void                      *ppriv;
 };

commit 34e4e99078667d30f71a50c1e5181e4270e9d8bb
Author: Roi Dayan <roid@mellanox.com>
Date:   Tue Nov 22 23:09:58 2016 +0200

    net/mlx5: Enable to query min inline for a specific vport
    
    Also move the inline capablities enum to a shared header vport.h
    
    Signed-off-by: Roi Dayan <roid@mellanox.com>
    Reviewed-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index ebf5dbc85bff..a2b32ed24315 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -150,12 +150,6 @@ static inline int mlx5_max_log_rq_size(int wq_type)
 	}
 }
 
-enum {
-	MLX5E_INLINE_MODE_L2,
-	MLX5E_INLINE_MODE_VPORT_CONTEXT,
-	MLX5_INLINE_MODE_NOT_REQUIRED,
-};
-
 struct mlx5e_tx_wqe {
 	struct mlx5_wqe_ctrl_seg ctrl;
 	struct mlx5_wqe_eth_seg  eth;

commit 370bad0f9a5261da0ef0bc76705f5b0b8af148ab
Author: Or Gerlitz <ogerlitz@mellanox.com>
Date:   Tue Nov 22 23:09:55 2016 +0200

    net/mlx5e: Support HW (offloaded) and SW counters for SRIOV switchdev mode
    
    Switchdev driver net-device port statistics should follow the model introduced
    in commit a5ea31f57309 'Merge branch net-offloaded-stats'.
    
    For VF reps we return the SRIOV eswitch vport stats as the usual ones and SW stats
    if asked. For the PF, if we're in the switchdev mode, we return the uplink stats
    and SW stats if asked, otherwise as before. The uplink stats are implemented using
    the PPCNT 802_3 counters which are already being read/cached by the driver.
    
    Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index ac09767b6984..ebf5dbc85bff 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -874,6 +874,7 @@ int mlx5e_add_sqs_fwd_rules(struct mlx5e_priv *priv);
 void mlx5e_remove_sqs_fwd_rules(struct mlx5e_priv *priv);
 int mlx5e_attr_get(struct net_device *dev, struct switchdev_attr *attr);
 void mlx5e_handle_rx_cqe_rep(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe);
+void mlx5e_update_hw_rep_counters(struct mlx5e_priv *priv);
 
 int mlx5e_create_direct_rqts(struct mlx5e_priv *priv);
 void mlx5e_destroy_rqt(struct mlx5e_priv *priv, struct mlx5e_rqt *rqt);
@@ -890,12 +891,16 @@ struct net_device *mlx5e_create_netdev(struct mlx5_core_dev *mdev,
 void mlx5e_destroy_netdev(struct mlx5_core_dev *mdev, struct mlx5e_priv *priv);
 int mlx5e_attach_netdev(struct mlx5_core_dev *mdev, struct net_device *netdev);
 void mlx5e_detach_netdev(struct mlx5_core_dev *mdev, struct net_device *netdev);
-struct rtnl_link_stats64 *
-mlx5e_get_stats(struct net_device *dev, struct rtnl_link_stats64 *stats);
 u32 mlx5e_choose_lro_timeout(struct mlx5_core_dev *mdev, u32 wanted_timeout);
 void mlx5e_add_vxlan_port(struct net_device *netdev,
 			  struct udp_tunnel_info *ti);
 void mlx5e_del_vxlan_port(struct net_device *netdev,
 			  struct udp_tunnel_info *ti);
 
+int mlx5e_get_offload_stats(int attr_id, const struct net_device *dev,
+			    void *sp);
+bool mlx5e_has_offload_stats(const struct net_device *dev, int attr_id);
+
+bool mlx5e_is_uplink_rep(struct mlx5e_priv *priv);
+bool mlx5e_is_vf_vport_rep(struct mlx5e_priv *priv);
 #endif /* __MLX5_EN_H__ */

commit 4a25730eb202893cd79b885df8f6fa1e8174742f
Author: Hadar Hen Zion <hadarh@mellanox.com>
Date:   Mon Nov 7 15:14:47 2016 +0200

    net/mlx5e: Add ndo_udp_tunnel_add to VF representors
    
    By implementing this ndo, the host stack will set the vxlan udp port
    also to VF representor netdevices. This will allow the TC offload code
    in the driver when it gets a tunnel key set action to identify the UDP
    port as vxlan, and hence the rule will be a candidate for offloading.
    
    Signed-off-by: Hadar Hen Zion <hadarh@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index ea8af475e7d2..ac09767b6984 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -893,5 +893,9 @@ void mlx5e_detach_netdev(struct mlx5_core_dev *mdev, struct net_device *netdev);
 struct rtnl_link_stats64 *
 mlx5e_get_stats(struct net_device *dev, struct rtnl_link_stats64 *stats);
 u32 mlx5e_choose_lro_timeout(struct mlx5_core_dev *mdev, u32 wanted_timeout);
+void mlx5e_add_vxlan_port(struct net_device *netdev,
+			  struct udp_tunnel_info *ti);
+void mlx5e_del_vxlan_port(struct net_device *netdev,
+			  struct udp_tunnel_info *ti);
 
 #endif /* __MLX5_EN_H__ */

commit 0a6ce1e3c1ab71c7b889fae3359c9c3ff3a43aab
Merge: d46b63494544 e37a79e5d4ca
Author: David S. Miller <davem@davemloft.net>
Date:   Sun Oct 30 17:31:12 2016 -0400

    Merge tag 'shared-for-4.10-1' of git://git.kernel.org/pub/scm/linux/kernel/git/leon/linux-rdma
    
    Saeed Mahameed says:
    
    ====================
    Mellanox mlx5 core driver updates 2016-10-25
    
    This series contains some updates and fixes of mlx5 core and
    IB drivers with the addition of two features that demand
    new low level commands and infrastructure updates.
     - SRIOV VF max rate limit support
     - mlx5e tc support for FWD rules with counter.
    
    Needed for both net and rdma subsystems.
    
    Updates and Fixes:
    From Saeed Mahameed (2):
      - mlx5 IB: Skip handling unknown mlx5 events
      - Add ConnectX-5 PCIe 4.0 VF device ID
    
    From Artemy Kovalyov (2):
      - Update struct mlx5_ifc_xrqc_bits
      - Ensure SRQ physical address structure endianness
    
    From Eugenia Emantayev (1):
      - Fix length of async_event_mask
    
    New Features:
    From Mohamad Haj Yahia (3): mlx5 SRIOV VF max rate limit support
      - Introduce TSAR manipulation firmware commands
      - Introduce E-switch QoS management
      - Add SRIOV VF max rate configuration support
    
    From Mark Bloch (7): mlx5e Tc support for FWD rule with counter
      - Don't unlock fte while still using it
      - Use fte status to decide on firmware command
      - Refactor find_flow_rule
      - Group similar rules under the same fte
      - Add multi dest support
      - Add option to add fwd rule with counter
      - mlx5e tc support for FWD rule with counter
      Mark here fixed two trivial issues with the flow steering core, and did
      some refactoring in the flow steering API to support adding mulit destination
      rules to the same hardware flow table entry at once.  In the last two patches
      added the ability to populate a flow rule with a flow counter to the same flow entry.
    
    V2: Dropped some patches that added new structures without adding any usage of them.
        Added SRIOV VF max rate configuration support patch that introduces
        the usage of the TSAR infrastructure.
        Added flow steering fixes and refactoring in addition to mlx5 tc
        support for forward rule with counter.
    ====================
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 74491de937125d0c98c9b9c9208b4105717a3caa
Author: Mark Bloch <markb@mellanox.com>
Date:   Wed Aug 31 11:24:25 2016 +0000

    net/mlx5: Add multi dest support
    
    Currently when calling mlx5_add_flow_rule we accept
    only one flow destination, this commit allows to pass
    multiple destinations.
    
    This change forces us to change the return structure to a more
    flexible one. We introduce a flow handle (struct mlx5_flow_handle),
    it holds internally the number for rules created and holds an array
    where each cell points the to a flow rule.
    
    From the consumers (of mlx5_add_flow_rule) point of view this
    change is only cosmetic and requires only to change the type
    of the returned value they store.
    
    From the core point of view, we now need to use a loop when
    allocating and deleting rules (e.g given to us a flow handler).
    
    Signed-off-by: Mark Bloch <markb@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 460363b66cb1..47ee8ffe987f 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -520,7 +520,7 @@ struct mlx5e_vxlan_db {
 
 struct mlx5e_l2_rule {
 	u8  addr[ETH_ALEN + 2];
-	struct mlx5_flow_rule *rule;
+	struct mlx5_flow_handle *rule;
 };
 
 struct mlx5e_flow_table {
@@ -541,10 +541,10 @@ struct mlx5e_tc_table {
 struct mlx5e_vlan_table {
 	struct mlx5e_flow_table		ft;
 	unsigned long active_vlans[BITS_TO_LONGS(VLAN_N_VID)];
-	struct mlx5_flow_rule	*active_vlans_rule[VLAN_N_VID];
-	struct mlx5_flow_rule	*untagged_rule;
-	struct mlx5_flow_rule	*any_vlan_rule;
-	bool          filter_disabled;
+	struct mlx5_flow_handle	*active_vlans_rule[VLAN_N_VID];
+	struct mlx5_flow_handle	*untagged_rule;
+	struct mlx5_flow_handle	*any_vlan_rule;
+	bool		filter_disabled;
 };
 
 struct mlx5e_l2_table {
@@ -562,14 +562,14 @@ struct mlx5e_l2_table {
 /* L3/L4 traffic type classifier */
 struct mlx5e_ttc_table {
 	struct mlx5e_flow_table  ft;
-	struct mlx5_flow_rule	 *rules[MLX5E_NUM_TT];
+	struct mlx5_flow_handle	 *rules[MLX5E_NUM_TT];
 };
 
 #define ARFS_HASH_SHIFT BITS_PER_BYTE
 #define ARFS_HASH_SIZE BIT(BITS_PER_BYTE)
 struct arfs_table {
 	struct mlx5e_flow_table  ft;
-	struct mlx5_flow_rule    *default_rule;
+	struct mlx5_flow_handle	 *default_rule;
 	struct hlist_head	 rules_hash[ARFS_HASH_SIZE];
 };
 

commit 2b029556667a7fc1aea731c5828e501656f87653
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Tue Oct 25 18:36:29 2016 +0300

    net/mlx5e: Choose best nearest LRO timeout
    
    Instead of predicting the index of the wanted LRO timeout value from
    hardware capabilities, look for the nearest LRO timeout value.
    
    Fixes: 5c50368f3831 ('net/mlx5e: Light-weight netdev open/stop')
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: Mohamad Haj Yahia <mohamad@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 460363b66cb1..7a43502a89cc 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -85,6 +85,9 @@
 #define MLX5_MPWRQ_SMALL_PACKET_THRESHOLD	(128)
 
 #define MLX5E_PARAMS_DEFAULT_LRO_WQE_SZ                 (64 * 1024)
+#define MLX5E_DEFAULT_LRO_TIMEOUT                       32
+#define MLX5E_LRO_TIMEOUT_ARR_SIZE                      4
+
 #define MLX5E_PARAMS_DEFAULT_RX_CQ_MODERATION_USEC      0x10
 #define MLX5E_PARAMS_DEFAULT_RX_CQ_MODERATION_USEC_FROM_CQE 0x3
 #define MLX5E_PARAMS_DEFAULT_RX_CQ_MODERATION_PKTS      0x20
@@ -221,6 +224,7 @@ struct mlx5e_params {
 	struct ieee_ets ets;
 #endif
 	bool rx_am_enabled;
+	u32 lro_timeout;
 };
 
 struct mlx5e_tstamp {
@@ -888,5 +892,6 @@ int mlx5e_attach_netdev(struct mlx5_core_dev *mdev, struct net_device *netdev);
 void mlx5e_detach_netdev(struct mlx5_core_dev *mdev, struct net_device *netdev);
 struct rtnl_link_stats64 *
 mlx5e_get_stats(struct net_device *dev, struct rtnl_link_stats64 *stats);
+u32 mlx5e_choose_lro_timeout(struct mlx5_core_dev *mdev, u32 wanted_timeout);
 
 #endif /* __MLX5_EN_H__ */

commit f5f82476090fd2c6fc4fde03ba61aef984900009
Author: Or Gerlitz <ogerlitz@mellanox.com>
Date:   Thu Sep 22 20:01:47 2016 +0300

    net/mlx5: E-Switch, Support VLAN actions in the offloads mode
    
    Many virtualization systems use a policy under which a vlan tag is
    pushed to packets sent by guests, and popped before the packet is
    forwarded to the VM.
    
    The current generation of the mlx5 HW doesn't fully support that on
    a per flow level. As such, we are addressing the above common use
    case with the SRIOV e-Switch abilities to push vlan into packets
    sent by VFs and pop vlan from packets forwarded to VFs.
    
    The HW can match on the correct vlan being present in packets
    forwarded to VFs (eSwitch steering is done before stripping
    the tag), so this part is offloaded as is.
    
    A common practice for vlans is to avoid both push vlan and pop vlan
    for inter-host VM/VM (east-west) communication because in this case,
    push on egress cancels out with pop on ingress.
    
    For supporting that, we use a global eswitch vlan pop policy, hence
    allowing guest A to communicate with both remote VM B and local VM C.
    This works since the HW pops the vlan only if it exists (e.g for
    C --> A packets but not for B --> A packets).
    
    On the slow path, when a VF vport has an offloaded flow which involves
    pushing vlans, wheres another flow is not currently offloaded, the
    packets from the 2nd flow seen by the VF representor on the host have
    vlan. The VF rep driver removes such vlan before calling into the host
    networking stack.
    
    Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 346015407b70..460363b66cb1 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -869,6 +869,7 @@ void mlx5e_nic_rep_unload(struct mlx5_eswitch *esw,
 int mlx5e_add_sqs_fwd_rules(struct mlx5e_priv *priv);
 void mlx5e_remove_sqs_fwd_rules(struct mlx5e_priv *priv);
 int mlx5e_attr_get(struct net_device *dev, struct switchdev_attr *attr);
+void mlx5e_handle_rx_cqe_rep(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe);
 
 int mlx5e_create_direct_rqts(struct mlx5e_priv *priv);
 void mlx5e_destroy_rqt(struct mlx5e_priv *priv, struct mlx5e_rqt *rqt);

commit 35b510e257f7516546a0a3f725f71dfbccc3f733
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Wed Sep 21 12:19:49 2016 +0300

    net/mlx5e: XDP TX xmit more
    
    Previously we rang XDP SQ doorbell on every forwarded XDP packet.
    
    Here we introduce a xmit more like mechanism that will queue up more
    than one packet into SQ (up to RX napi budget) w/o notifying the hardware.
    
    Once RX napi budget is consumed and we exit napi RX loop, we will
    flush (doorbell) all XDP looped packets in case there are such.
    
    XDP forward packet rate:
    
    Comparing XDP with and w/o xmit more (bulk transmit):
    
    RX Cores    XDP TX       XDP TX (xmit more)
    ---------------------------------------------------
    1           6.5Mpps      12.4Mpps
    2          13.2Mpps      24.2Mpps
    4          25.2Mpps      36.3Mpps*
    8          36.3Mpps*     36.3Mpps*
    
    *My xmitter was limited to 36.3Mpps, so it is the bottleneck.
    It seems that receive side can handle more.
    
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 82eededfc92a..346015407b70 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -433,6 +433,7 @@ struct mlx5e_sq {
 		struct {
 			struct mlx5e_sq_wqe_info  *wqe_info;
 			struct mlx5e_dma_info     *di;
+			bool                       doorbell;
 		} xdp;
 	} db;
 

commit b5503b994ed5ed8dbfe821317e7b5b38acb065c5
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Wed Sep 21 12:19:48 2016 +0300

    net/mlx5e: XDP TX forwarding support
    
    Adding support for XDP_TX forwarding from xdp program.
    Using XDP, now user can loop packets out of the same port.
    
    We create a dedicated TX SQ for each channel that will serve
    XDP programs that return XDP_TX action to loop packets back to
    the wire directly from the channel RQ RX path.
    
    For that RX pages will now need to be mapped bi-directionally,
    and on XDP_TX action we will sync the page back to device then
    queue it into SQ for transmission.  The XDP xmit frame function will
    report back to the RX path if the page was consumed (transmitted), if so,
    RX path will forget about that page as if it were released to the stack.
    Later on, on XDP TX completion, the page will be released back to the
    page cache.
    
    For simplicity this patch will hit a doorbell on every XDP TX packet.
    
    Next patch will introduce a xmit more like mechanism that will
    queue up more than one packet into SQ w/o notifying the hardware,
    once RX napi loop is done we will hit doorbell once for all XDP TX
    packets form the previous loop.  This should drastically improve
    XDP TX performance.
    
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 5917f5e609ae..82eededfc92a 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -104,6 +104,15 @@
 #define MLX5E_ICOSQ_MAX_WQEBBS \
 	(DIV_ROUND_UP(sizeof(struct mlx5e_umr_wqe), MLX5_SEND_WQE_BB))
 
+#define MLX5E_XDP_MIN_INLINE (ETH_HLEN + VLAN_HLEN)
+#define MLX5E_XDP_IHS_DS_COUNT \
+	DIV_ROUND_UP(MLX5E_XDP_MIN_INLINE - 2, MLX5_SEND_WQE_DS)
+#define MLX5E_XDP_TX_DS_COUNT \
+	(MLX5E_XDP_IHS_DS_COUNT + \
+	 (sizeof(struct mlx5e_tx_wqe) / MLX5_SEND_WQE_DS) + 1 /* SG DS */)
+#define MLX5E_XDP_TX_WQEBBS \
+	DIV_ROUND_UP(MLX5E_XDP_TX_DS_COUNT, MLX5_SEND_WQEBB_NUM_DS)
+
 #define MLX5E_NUM_MAIN_GROUPS 9
 
 static inline u16 mlx5_min_rx_wqes(int wq_type, u32 wq_size)
@@ -319,6 +328,7 @@ struct mlx5e_rq {
 	struct {
 		u8             page_order;
 		u32            wqe_sz;    /* wqe data buffer size */
+		u8             map_dir;   /* dma map direction */
 	} buff;
 	__be32                 mkey_be;
 
@@ -384,14 +394,15 @@ enum {
 	MLX5E_SQ_STATE_BF_ENABLE,
 };
 
-struct mlx5e_ico_wqe_info {
+struct mlx5e_sq_wqe_info {
 	u8  opcode;
 	u8  num_wqebbs;
 };
 
 enum mlx5e_sq_type {
 	MLX5E_SQ_TXQ,
-	MLX5E_SQ_ICO
+	MLX5E_SQ_ICO,
+	MLX5E_SQ_XDP
 };
 
 struct mlx5e_sq {
@@ -418,7 +429,11 @@ struct mlx5e_sq {
 			struct mlx5e_sq_dma       *dma_fifo;
 			struct mlx5e_tx_wqe_info  *wqe_info;
 		} txq;
-		struct mlx5e_ico_wqe_info *ico_wqe;
+		struct mlx5e_sq_wqe_info *ico_wqe;
+		struct {
+			struct mlx5e_sq_wqe_info  *wqe_info;
+			struct mlx5e_dma_info     *di;
+		} xdp;
 	} db;
 
 	/* read only */
@@ -458,8 +473,10 @@ enum channel_flags {
 struct mlx5e_channel {
 	/* data path */
 	struct mlx5e_rq            rq;
+	struct mlx5e_sq            xdp_sq;
 	struct mlx5e_sq            sq[MLX5E_MAX_NUM_TC];
 	struct mlx5e_sq            icosq;   /* internal control operations */
+	bool                       xdp;
 	struct napi_struct         napi;
 	struct device             *pdev;
 	struct net_device         *netdev;
@@ -688,7 +705,7 @@ void mlx5e_cq_error_event(struct mlx5_core_cq *mcq, enum mlx5_event event);
 int mlx5e_napi_poll(struct napi_struct *napi, int budget);
 bool mlx5e_poll_tx_cq(struct mlx5e_cq *cq, int napi_budget);
 int mlx5e_poll_rx_cq(struct mlx5e_cq *cq, int budget);
-void mlx5e_free_tx_descs(struct mlx5e_sq *sq);
+void mlx5e_free_sq_descs(struct mlx5e_sq *sq);
 
 void mlx5e_page_release(struct mlx5e_rq *rq, struct mlx5e_dma_info *dma_info,
 			bool recycle);

commit f10b7cc7707f7d598e3ddacd848080b18ba4cbff
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Wed Sep 21 12:19:47 2016 +0300

    net/mlx5e: Have a clear separation between different SQ types
    
    Make a clear separate between Regular SQ (TXQ) and ICO SQ creation,
    destruction and union their mutual information structures.
    
    Don't allocate redundant TXQ skb/wqe_info/dma_fifo arrays for ICO SQ.
    And have a different SQ edge for ICO SQ than TXQ SQ, to be more
    accurate.
    
    In preparation for XDP TX support.
    
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 5e8e669f69c0..5917f5e609ae 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -101,6 +101,9 @@
 #define MLX5E_UPDATE_STATS_INTERVAL    200 /* msecs */
 #define MLX5E_SQ_BF_BUDGET             16
 
+#define MLX5E_ICOSQ_MAX_WQEBBS \
+	(DIV_ROUND_UP(sizeof(struct mlx5e_umr_wqe), MLX5_SEND_WQE_BB))
+
 #define MLX5E_NUM_MAIN_GROUPS 9
 
 static inline u16 mlx5_min_rx_wqes(int wq_type, u32 wq_size)
@@ -386,6 +389,11 @@ struct mlx5e_ico_wqe_info {
 	u8  num_wqebbs;
 };
 
+enum mlx5e_sq_type {
+	MLX5E_SQ_TXQ,
+	MLX5E_SQ_ICO
+};
+
 struct mlx5e_sq {
 	/* data path */
 
@@ -403,10 +411,15 @@ struct mlx5e_sq {
 
 	struct mlx5e_cq            cq;
 
-	/* pointers to per packet info: write@xmit, read@completion */
-	struct sk_buff           **skb;
-	struct mlx5e_sq_dma       *dma_fifo;
-	struct mlx5e_tx_wqe_info  *wqe_info;
+	/* pointers to per tx element info: write@xmit, read@completion */
+	union {
+		struct {
+			struct sk_buff           **skb;
+			struct mlx5e_sq_dma       *dma_fifo;
+			struct mlx5e_tx_wqe_info  *wqe_info;
+		} txq;
+		struct mlx5e_ico_wqe_info *ico_wqe;
+	} db;
 
 	/* read only */
 	struct mlx5_wq_cyc         wq;
@@ -428,8 +441,8 @@ struct mlx5e_sq {
 	struct mlx5_uar            uar;
 	struct mlx5e_channel      *channel;
 	int                        tc;
-	struct mlx5e_ico_wqe_info *ico_wqe_info;
 	u32                        rate_limit;
+	u8                         type;
 } ____cacheline_aligned_in_smp;
 
 static inline bool mlx5e_sq_has_room_for(struct mlx5e_sq *sq, u16 n)

commit 86994156c736978d113e7927455d4eeeb2128b9f
Author: Rana Shahout <ranas@mellanox.com>
Date:   Wed Sep 21 12:19:46 2016 +0300

    net/mlx5e: XDP fast RX drop bpf programs support
    
    Add support for the BPF_PROG_TYPE_PHYS_DEV hook in mlx5e driver.
    
    When XDP is on we make sure to change channels RQs type to
    MLX5_WQ_TYPE_LINKED_LIST rather than "striding RQ" type to
    ensure "page per packet".
    
    On XDP set, we fail if HW LRO is set and request from user to turn it
    off.  Since on ConnectX4-LX HW LRO is always on by default, this will be
    annoying, but we prefer not to enforce LRO off from XDP set function.
    
    Full channels reset (close/open) is required only when setting XDP
    on/off.
    
    When XDP set is called just to exchange programs, we will update
    each RQ xdp program on the fly and for synchronization with current
    data path RX activity of that RQ, we temporally disable that RQ and
    ensure RX path is not running, quickly update and re-enable that RQ,
    for that we do:
            - rq.state = disabled
            - napi_synnchronize
            - xchg(rq->xdp_prg)
            - rq.state = enabled
            - napi_schedule // Just in case we've missed an IRQ
    
    Packet rate performance testing was done with pktgen 64B packets and on
    TX side and, TC drop action on RX side compared to XDP fast drop.
    
    CPU: Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz
    
    Comparison is done between:
            1. Baseline, Before this patch with TC drop action
            2. This patch with TC drop action
            3. This patch with XDP RX fast drop
    
    RX Cores  Baseline(TC drop)    TC drop    XDP fast Drop
    --------------------------------------------------------------
    1            5.3Mpps           5.3Mpps     16.5Mpps
    2           10.2Mpps          10.2Mpps     31.3Mpps
    4           20.5Mpps          19.9Mpps     36.3Mpps*
    
    *My xmitter was limited to 36.3Mpps, so it is the bottleneck.
    It seems that receive side can handle more.
    
    Signed-off-by: Rana Shahout <ranas@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index e3331237ce0e..5e8e669f69c0 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -334,6 +334,7 @@ struct mlx5e_rq {
 	int                    ix;
 
 	struct mlx5e_rx_am     am; /* Adaptive Moderation */
+	struct bpf_prog       *xdp_prog;
 
 	/* control */
 	struct mlx5_wq_ctrl    wq_ctrl;
@@ -627,6 +628,7 @@ struct mlx5e_priv {
 	/* priv data path fields - start */
 	struct mlx5e_sq            **txq_to_sq_map;
 	int channeltc_to_txq_map[MLX5E_MAX_NUM_CHANNELS][MLX5E_MAX_NUM_TC];
+	struct bpf_prog *xdp_prog;
 	/* priv data path fields - end */
 
 	unsigned long              state;

commit 21c59685dd176dd6b2c4fc5e18dc65730cfd546a
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Wed Sep 21 12:19:43 2016 +0300

    net/mlx5e: Union RQ RX info per RQ type
    
    We have two types of RX RQs, and they use two separate sets of
    info arrays and structures in RX data path function.  Today those
    structures are mutually exclusive per RQ type, hence one kind is
    allocated on RQ creation according to the RQ type.
    
    For better cache locality and to minimalize the
    sizeof(struct mlx5e_rq), in this patch we define them as a union.
    
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 4d06c1b9348b..e3331237ce0e 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -305,9 +305,14 @@ struct mlx5e_rq {
 	/* data path */
 	struct mlx5_wq_ll      wq;
 
-	struct mlx5e_dma_info *dma_info;
-	struct mlx5e_mpw_info *wqe_info;
-	void                  *mtt_no_align;
+	union {
+		struct mlx5e_dma_info *dma_info;
+		struct {
+			struct mlx5e_mpw_info *info;
+			void                  *mtt_no_align;
+			u32                    mtt_offset;
+		} mpwqe;
+	};
 	struct {
 		u8             page_order;
 		u32            wqe_sz;    /* wqe data buffer size */
@@ -327,7 +332,6 @@ struct mlx5e_rq {
 
 	unsigned long          state;
 	int                    ix;
-	u32                    mpwqe_mtt_offset;
 
 	struct mlx5e_rx_am     am; /* Adaptive Moderation */
 
@@ -770,7 +774,7 @@ static inline void mlx5e_cq_arm(struct mlx5e_cq *cq)
 
 static inline u32 mlx5e_get_wqe_mtt_offset(struct mlx5e_rq *rq, u16 wqe_ix)
 {
-	return rq->mpwqe_mtt_offset +
+	return rq->mpwqe.mtt_offset +
 		wqe_ix * ALIGN(MLX5_MPWRQ_PAGES_PER_WQE, 8);
 }
 

commit 1bfecfca565c0505d04dbf5fdd3d2fbb951827c0
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Wed Sep 21 12:19:42 2016 +0300

    net/mlx5e: Build RX SKB on demand
    
    For non-striding RQ configuration before this patch we had a ring
    with pre-allocated SKBs and mapped the SKB->data buffers for
    device.
    
    For robustness and better RX data buffers management, we allocate a
    page per packet and build_skb around it.
    
    This patch (which is a prerequisite for XDP) will actually reduce
    performance for normal stack usage, because we are now hitting a bottleneck
    in the page allocator. We use the page-cache to restore or even improve
    performance in comparison to the old RX scheme.
    
    Packet rate performance testing was done with pktgen 64B packets on xmit
    side and TC ingress dropping action on RX side.
    
    CPU: Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz
    
    Comparison is done between:
     1.Baseline, before 'net/mlx5e: Build RX SKB on demand'
     2.Build SKB with RX page cache (This patch)
    
    RX Cores  Baseline    Build SKB+page-cache    Improvement
    -----------------------------------------------------------
    1          4.16Mpps       5.33Mpps                28%
    2          7.16Mpps      10.24Mpps                43%
    4         13.61Mpps      20.51Mpps                51%
    8         25.32Mpps      32.00Mpps                26%
    
    All respective cores were 100% utilized.
    
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 7dd4763e726e..4d06c1b9348b 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -65,6 +65,8 @@
 #define MLX5E_PARAMS_DEFAULT_LOG_RQ_SIZE_MPW            0x3
 #define MLX5E_PARAMS_MAXIMUM_LOG_RQ_SIZE_MPW            0x6
 
+#define MLX5_RX_HEADROOM NET_SKB_PAD
+
 #define MLX5_MPWRQ_LOG_STRIDE_SIZE		6  /* >= 6, HW restriction */
 #define MLX5_MPWRQ_LOG_STRIDE_SIZE_CQE_COMPRESS	8  /* >= 6, HW restriction */
 #define MLX5_MPWRQ_LOG_WQE_SZ			18
@@ -302,10 +304,14 @@ struct mlx5e_page_cache {
 struct mlx5e_rq {
 	/* data path */
 	struct mlx5_wq_ll      wq;
-	u32                    wqe_sz;
-	struct sk_buff       **skb;
+
+	struct mlx5e_dma_info *dma_info;
 	struct mlx5e_mpw_info *wqe_info;
 	void                  *mtt_no_align;
+	struct {
+		u8             page_order;
+		u32            wqe_sz;    /* wqe data buffer size */
+	} buff;
 	__be32                 mkey_be;
 
 	struct device         *pdev;

commit 4415a0319f92ea0d624fe11c917faf9114f89187
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Thu Sep 15 16:08:38 2016 +0300

    net/mlx5e: Implement RX mapped page cache for page recycle
    
    Instead of reallocating and mapping pages for RX data-path,
    recycle already used pages in a per ring cache.
    
    Performance tests:
    The following results were measured on a freshly booted system,
    giving optimal baseline performance, as high-order pages are yet to
    be fragmented and depleted.
    
    We ran pktgen single-stream benchmarks, with iptables-raw-drop:
    
    Single stride, 64 bytes:
    * 4,739,057 - baseline
    * 4,749,550 - order0 no cache
    * 4,786,899 - order0 with cache
    1% gain
    
    Larger packets, no page cross, 1024 bytes:
    * 3,982,361 - baseline
    * 3,845,682 - order0 no cache
    * 4,127,852 - order0 with cache
    3.7% gain
    
    Larger packets, every 3rd packet crosses a page, 1500 bytes:
    * 3,731,189 - baseline
    * 3,579,414 - order0 no cache
    * 3,931,708 - order0 with cache
    5.4% gain
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 401b2f7b165f..7dd4763e726e 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -287,6 +287,18 @@ struct mlx5e_rx_am { /* Adaptive Moderation */
 	u8					tired;
 };
 
+/* a single cache unit is capable to serve one napi call (for non-striding rq)
+ * or a MPWQE (for striding rq).
+ */
+#define MLX5E_CACHE_UNIT	(MLX5_MPWRQ_PAGES_PER_WQE > NAPI_POLL_WEIGHT ? \
+				 MLX5_MPWRQ_PAGES_PER_WQE : NAPI_POLL_WEIGHT)
+#define MLX5E_CACHE_SIZE	(2 * roundup_pow_of_two(MLX5E_CACHE_UNIT))
+struct mlx5e_page_cache {
+	u32 head;
+	u32 tail;
+	struct mlx5e_dma_info page_cache[MLX5E_CACHE_SIZE];
+};
+
 struct mlx5e_rq {
 	/* data path */
 	struct mlx5_wq_ll      wq;
@@ -301,6 +313,8 @@ struct mlx5e_rq {
 	struct mlx5e_tstamp   *tstamp;
 	struct mlx5e_rq_stats  stats;
 	struct mlx5e_cq        cq;
+	struct mlx5e_page_cache page_cache;
+
 	mlx5e_fp_handle_rx_cqe handle_rx_cqe;
 	mlx5e_fp_alloc_wqe     alloc_wqe;
 	mlx5e_fp_dealloc_wqe   dealloc_wqe;
@@ -651,6 +665,8 @@ bool mlx5e_poll_tx_cq(struct mlx5e_cq *cq, int napi_budget);
 int mlx5e_poll_rx_cq(struct mlx5e_cq *cq, int budget);
 void mlx5e_free_tx_descs(struct mlx5e_sq *sq);
 
+void mlx5e_page_release(struct mlx5e_rq *rq, struct mlx5e_dma_info *dma_info,
+			bool recycle);
 void mlx5e_handle_rx_cqe(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe);
 void mlx5e_handle_rx_cqe_mpwrq(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe);
 bool mlx5e_post_rx_wqes(struct mlx5e_rq *rq);

commit 7e426671704d2266757dff9c4254b788561aa11e
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Thu Sep 15 16:08:36 2016 +0300

    net/mlx5e: Single flow order-0 pages for Striding RQ
    
    To improve the memory consumption scheme, we omit the flow that
    demands and splits high-order pages in Striding RQ, and stay
    with a single Striding RQ flow that uses order-0 pages.
    
    Moving to fragmented memory allows the use of larger MPWQEs,
    which reduces the number of UMR posts and filler CQEs.
    
    Moving to a single flow allows several optimizations that improve
    performance, especially in production servers where we would
    anyway fallback to order-0 allocations:
    - inline functions that were called via function pointers.
    - improve the UMR post process.
    
    This patch alone is expected to give a slight performance reduction.
    However, the new memory scheme gives the possibility to use a page-cache
    of a fair size, that doesn't inflate the memory footprint, which will
    dramatically fix the reduction and even give a performance gain.
    
    Performance tests:
    The following results were measured on a freshly booted system,
    giving optimal baseline performance, as high-order pages are yet to
    be fragmented and depleted.
    
    We ran pktgen single-stream benchmarks, with iptables-raw-drop:
    
    Single stride, 64 bytes:
    * 4,739,057 - baseline
    * 4,749,550 - this patch
    no reduction
    
    Larger packets, no page cross, 1024 bytes:
    * 3,982,361 - baseline
    * 3,845,682 - this patch
    3.5% reduction
    
    Larger packets, every 3rd packet crosses a page, 1500 bytes:
    * 3,731,189 - baseline
    * 3,579,414 - this patch
    4% reduction
    
    Fixes: 461017cb006a ("net/mlx5e: Support RX multi-packet WQE (Striding RQ)")
    Fixes: bc77b240b3c5 ("net/mlx5e: Add fragmented memory support for RX multi packet WQE")
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index a9358cf7386a..401b2f7b165f 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -62,12 +62,12 @@
 #define MLX5E_PARAMS_MAXIMUM_LOG_RQ_SIZE                0xd
 
 #define MLX5E_PARAMS_MINIMUM_LOG_RQ_SIZE_MPW            0x1
-#define MLX5E_PARAMS_DEFAULT_LOG_RQ_SIZE_MPW            0x4
+#define MLX5E_PARAMS_DEFAULT_LOG_RQ_SIZE_MPW            0x3
 #define MLX5E_PARAMS_MAXIMUM_LOG_RQ_SIZE_MPW            0x6
 
 #define MLX5_MPWRQ_LOG_STRIDE_SIZE		6  /* >= 6, HW restriction */
 #define MLX5_MPWRQ_LOG_STRIDE_SIZE_CQE_COMPRESS	8  /* >= 6, HW restriction */
-#define MLX5_MPWRQ_LOG_WQE_SZ			17
+#define MLX5_MPWRQ_LOG_WQE_SZ			18
 #define MLX5_MPWRQ_WQE_PAGE_ORDER  (MLX5_MPWRQ_LOG_WQE_SZ - PAGE_SHIFT > 0 ? \
 				    MLX5_MPWRQ_LOG_WQE_SZ - PAGE_SHIFT : 0)
 #define MLX5_MPWRQ_PAGES_PER_WQE		BIT(MLX5_MPWRQ_WQE_PAGE_ORDER)
@@ -293,8 +293,8 @@ struct mlx5e_rq {
 	u32                    wqe_sz;
 	struct sk_buff       **skb;
 	struct mlx5e_mpw_info *wqe_info;
+	void                  *mtt_no_align;
 	__be32                 mkey_be;
-	__be32                 umr_mkey_be;
 
 	struct device         *pdev;
 	struct net_device     *netdev;
@@ -323,32 +323,15 @@ struct mlx5e_rq {
 
 struct mlx5e_umr_dma_info {
 	__be64                *mtt;
-	__be64                *mtt_no_align;
 	dma_addr_t             mtt_addr;
-	struct mlx5e_dma_info *dma_info;
+	struct mlx5e_dma_info  dma_info[MLX5_MPWRQ_PAGES_PER_WQE];
+	struct mlx5e_umr_wqe   wqe;
 };
 
 struct mlx5e_mpw_info {
-	union {
-		struct mlx5e_dma_info     dma_info;
-		struct mlx5e_umr_dma_info umr;
-	};
+	struct mlx5e_umr_dma_info umr;
 	u16 consumed_strides;
 	u16 skbs_frags[MLX5_MPWRQ_PAGES_PER_WQE];
-
-	void (*dma_pre_sync)(struct device *pdev,
-			     struct mlx5e_mpw_info *wi,
-			     u32 wqe_offset, u32 len);
-	void (*add_skb_frag)(struct mlx5e_rq *rq,
-			     struct sk_buff *skb,
-			     struct mlx5e_mpw_info *wi,
-			     u32 page_idx, u32 frag_offset, u32 len);
-	void (*copy_skb_header)(struct device *pdev,
-				struct sk_buff *skb,
-				struct mlx5e_mpw_info *wi,
-				u32 page_idx, u32 offset,
-				u32 headlen);
-	void (*free_wqe)(struct mlx5e_rq *rq, struct mlx5e_mpw_info *wi);
 };
 
 struct mlx5e_tx_wqe_info {
@@ -672,24 +655,11 @@ void mlx5e_handle_rx_cqe(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe);
 void mlx5e_handle_rx_cqe_mpwrq(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe);
 bool mlx5e_post_rx_wqes(struct mlx5e_rq *rq);
 int mlx5e_alloc_rx_wqe(struct mlx5e_rq *rq, struct mlx5e_rx_wqe *wqe, u16 ix);
-int mlx5e_alloc_rx_mpwqe(struct mlx5e_rq *rq, struct mlx5e_rx_wqe *wqe, u16 ix);
+int mlx5e_alloc_rx_mpwqe(struct mlx5e_rq *rq, struct mlx5e_rx_wqe *wqe,	u16 ix);
 void mlx5e_dealloc_rx_wqe(struct mlx5e_rq *rq, u16 ix);
 void mlx5e_dealloc_rx_mpwqe(struct mlx5e_rq *rq, u16 ix);
-void mlx5e_post_rx_fragmented_mpwqe(struct mlx5e_rq *rq);
-void mlx5e_complete_rx_linear_mpwqe(struct mlx5e_rq *rq,
-				    struct mlx5_cqe64 *cqe,
-				    u16 byte_cnt,
-				    struct mlx5e_mpw_info *wi,
-				    struct sk_buff *skb);
-void mlx5e_complete_rx_fragmented_mpwqe(struct mlx5e_rq *rq,
-					struct mlx5_cqe64 *cqe,
-					u16 byte_cnt,
-					struct mlx5e_mpw_info *wi,
-					struct sk_buff *skb);
-void mlx5e_free_rx_linear_mpwqe(struct mlx5e_rq *rq,
-				struct mlx5e_mpw_info *wi);
-void mlx5e_free_rx_fragmented_mpwqe(struct mlx5e_rq *rq,
-				    struct mlx5e_mpw_info *wi);
+void mlx5e_post_rx_mpwqe(struct mlx5e_rq *rq);
+void mlx5e_free_rx_mpwqe(struct mlx5e_rq *rq, struct mlx5e_mpw_info *wi);
 struct mlx5_cqe64 *mlx5e_get_cqe(struct mlx5e_cq *cq);
 
 void mlx5e_rx_am(struct mlx5e_rq *rq);
@@ -776,6 +746,12 @@ static inline void mlx5e_cq_arm(struct mlx5e_cq *cq)
 	mlx5_cq_arm(mcq, MLX5_CQ_DB_REQ_NOT, mcq->uar->map, NULL, cq->wq.cc);
 }
 
+static inline u32 mlx5e_get_wqe_mtt_offset(struct mlx5e_rq *rq, u16 wqe_ix)
+{
+	return rq->mpwqe_mtt_offset +
+		wqe_ix * ALIGN(MLX5_MPWRQ_PAGES_PER_WQE, 8);
+}
+
 static inline int mlx5e_get_max_num_channels(struct mlx5_core_dev *mdev)
 {
 	return min_t(int, mdev->priv.eq_table.num_comp_vectors,

commit 26e59d8077a31972dc81fe5ff75aa4fd5b260b71
Author: Mohamad Haj Yahia <mohamad@mellanox.com>
Date:   Fri Sep 9 17:35:25 2016 +0300

    net/mlx5e: Implement mlx5e interface attach/detach callbacks
    
    Needed to support seamless and lightweight PCI/Internal error recovery.
    Implement the attach/detach interface callbacks.
    In attach callback we only allocate HW resources.
    In detach callback we only deallocate HW resources.
    All SW/kernel objects initialzing/destroying is kept in add/remove
    callbacks.
    
    Signed-off-by: Mohamad Haj Yahia <mohamad@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 96995609f205..a9358cf7386a 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -844,9 +844,12 @@ void mlx5e_cleanup_nic_tx(struct mlx5e_priv *priv);
 int mlx5e_close(struct net_device *netdev);
 int mlx5e_open(struct net_device *netdev);
 void mlx5e_update_stats_work(struct work_struct *work);
-void *mlx5e_create_netdev(struct mlx5_core_dev *mdev,
-			  const struct mlx5e_profile *profile, void *ppriv);
+struct net_device *mlx5e_create_netdev(struct mlx5_core_dev *mdev,
+				       const struct mlx5e_profile *profile,
+				       void *ppriv);
 void mlx5e_destroy_netdev(struct mlx5_core_dev *mdev, struct mlx5e_priv *priv);
+int mlx5e_attach_netdev(struct mlx5_core_dev *mdev, struct net_device *netdev);
+void mlx5e_detach_netdev(struct mlx5_core_dev *mdev, struct net_device *netdev);
 struct rtnl_link_stats64 *
 mlx5e_get_stats(struct net_device *dev, struct rtnl_link_stats64 *stats);
 

commit 6abdd5f5935fff978f950561f3c5175eb34dad73
Merge: 0b498a527783 e4e98c460ad3
Author: David S. Miller <davem@davemloft.net>
Date:   Tue Aug 30 00:54:02 2016 -0400

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    All three conflicts were cases of simple overlapping
    changes.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 6e8dd6d6f4bd2fd6fefdbf2e73bf251e36db59af
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Mon Aug 29 01:13:45 2016 +0300

    net/mlx5e: Don't wait for SQ completions on close
    
    Instead of asking the firmware to flush the SQ (Send Queue) via
    asynchronous completions when moved to error, we handle SQ flush
    manually (mlx5e_free_tx_descs) same as we did when SQ flush got
    timed out or on tx_timeout.
    
    This will reduce SQs flush time and speedup interface down procedure.
    
    Moved mlx5e_free_tx_descs to the end of en_tx.c for tx
    critical code locality.
    
    Fixes: 29429f3300a3 ('net/mlx5e: Timeout if SQ doesn't flush during close')
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 26a7ec7073f2..bf722aa88cf0 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -369,9 +369,8 @@ struct mlx5e_sq_dma {
 };
 
 enum {
-	MLX5E_SQ_STATE_WAKE_TXQ_ENABLE,
+	MLX5E_SQ_STATE_FLUSH,
 	MLX5E_SQ_STATE_BF_ENABLE,
-	MLX5E_SQ_STATE_TX_TIMEOUT,
 };
 
 struct mlx5e_ico_wqe_info {

commit f2fde18c52a7367a8f6cf6855e2a7174e601c8ee
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Mon Aug 29 01:13:43 2016 +0300

    net/mlx5e: Don't wait for RQ completions on close
    
    This will significantly reduce receive queue flush time on interface
    down.
    
    Instead of asking the firmware to flush the RQ (Receive Queue) via
    asynchronous completions when moved to error, we handle RQ flush
    manually (mlx5e_free_rx_descs) same as we did when RQ flush got timed
    out.
    
    This will reduce RQs flush time and speedup interface down procedure
    (ifconfig down) from 6 sec to 0.3 sec on a 48 cores system.
    
    Moved mlx5e_free_rx_descs en_main.c where it is needed, to keep en_rx.c
    free form non critical data path code for better code locality.
    
    Fixes: 6cd392a082de ('net/mlx5e: Handle RQ flush in error cases')
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index d63a1b8f9c13..26a7ec7073f2 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -223,9 +223,8 @@ struct mlx5e_tstamp {
 };
 
 enum {
-	MLX5E_RQ_STATE_POST_WQES_ENABLE,
+	MLX5E_RQ_STATE_FLUSH,
 	MLX5E_RQ_STATE_UMR_WQE_IN_PROGRESS,
-	MLX5E_RQ_STATE_FLUSH_TIMEOUT,
 	MLX5E_RQ_STATE_AM,
 };
 
@@ -703,7 +702,6 @@ int mlx5e_napi_poll(struct napi_struct *napi, int budget);
 bool mlx5e_poll_tx_cq(struct mlx5e_cq *cq, int napi_budget);
 int mlx5e_poll_rx_cq(struct mlx5e_cq *cq, int budget);
 void mlx5e_free_tx_descs(struct mlx5e_sq *sq);
-void mlx5e_free_rx_descs(struct mlx5e_rq *rq);
 
 void mlx5e_handle_rx_cqe(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe);
 void mlx5e_handle_rx_cqe_mpwrq(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe);

commit fe4c988bdd1cc60402a4e3ca3976a686ea991b5a
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Mon Aug 29 01:13:42 2016 +0300

    net/mlx5e: Limit UMR length to the device's limitation
    
    ConnectX-4 UMR (User Memory Region) MTT translation table offset in WQE
    is limited to U16_MAX, before this patch we ignored that limitation and
    requested the maximum possible UMR translation length that the netdev
    might need (MAX channels * MAX pages per channel).
    In case of a system with #cores > 32 and when linear WQE allocation fails,
    falling back to using UMR WQEs will cause the RQ (Receive Queue) to get
    stuck.
    
    Here we limit UMR length to min(U16_MAX, max required pages) (while
    considering the required alignments) on driver load, by default U16_MAX is
    sufficient since the default RX rings value guarantees that we are in
    range, dynamically (on set_ringparam/set_channels) we will check if the
    new required UMR length (num mtts) is still in range, if not, fail the
    request.
    
    Fixes: bc77b240b3c5 ('net/mlx5e: Add fragmented memory support for RX multi packet WQE')
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 1b495efa7490..d63a1b8f9c13 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -73,8 +73,12 @@
 #define MLX5_MPWRQ_PAGES_PER_WQE		BIT(MLX5_MPWRQ_WQE_PAGE_ORDER)
 #define MLX5_MPWRQ_STRIDES_PER_PAGE		(MLX5_MPWRQ_NUM_STRIDES >> \
 						 MLX5_MPWRQ_WQE_PAGE_ORDER)
-#define MLX5_CHANNEL_MAX_NUM_MTTS (ALIGN(MLX5_MPWRQ_PAGES_PER_WQE, 8) * \
-				   BIT(MLX5E_PARAMS_MAXIMUM_LOG_RQ_SIZE_MPW))
+
+#define MLX5_MTT_OCTW(npages) (ALIGN(npages, 8) / 2)
+#define MLX5E_REQUIRED_MTTS(rqs, wqes)\
+	(rqs * wqes * ALIGN(MLX5_MPWRQ_PAGES_PER_WQE, 8))
+#define MLX5E_VALID_NUM_MTTS(num_mtts) (MLX5_MTT_OCTW(num_mtts) <= U16_MAX)
+
 #define MLX5_UMR_ALIGN				(2048)
 #define MLX5_MPWRQ_SMALL_PACKET_THRESHOLD	(128)
 
@@ -304,6 +308,7 @@ struct mlx5e_rq {
 
 	unsigned long          state;
 	int                    ix;
+	u32                    mpwqe_mtt_offset;
 
 	struct mlx5e_rx_am     am; /* Adaptive Moderation */
 
@@ -814,11 +819,6 @@ static inline int mlx5e_get_max_num_channels(struct mlx5_core_dev *mdev)
 		     MLX5E_MAX_NUM_CHANNELS);
 }
 
-static inline int mlx5e_get_mtt_octw(int npages)
-{
-	return ALIGN(npages, 8) / 2;
-}
-
 extern const struct ethtool_ops mlx5e_ethtool_ops;
 #ifdef CONFIG_MLX5_CORE_EN_DCB
 extern const struct dcbnl_rtnl_ops mlx5e_dcbnl_ops;

commit 8cca30a7f914fe363fa9700715619ca5c8cb38cc
Author: Noa Osherovich <noaos@mellanox.com>
Date:   Sun Jun 26 12:43:24 2016 +0300

    net/mlx5: Expose mlx5e_link_mode
    
    The mlx5e_link_mode enumeration will also be used in mlx5_ib for RoCE.
    This patch moves the enumeration to the mlx5 driver port header file.
    
    Signed-off-by: Noa Osherovich <noaos@mellanox.com>
    Signed-off-by: Eran Ben Elisha <eranbe@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: Leon Romanovsky <leon@kernel.org>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 1b495efa7490..61902b147339 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -651,40 +651,6 @@ struct mlx5e_priv {
 	void                      *ppriv;
 };
 
-enum mlx5e_link_mode {
-	MLX5E_1000BASE_CX_SGMII	 = 0,
-	MLX5E_1000BASE_KX	 = 1,
-	MLX5E_10GBASE_CX4	 = 2,
-	MLX5E_10GBASE_KX4	 = 3,
-	MLX5E_10GBASE_KR	 = 4,
-	MLX5E_20GBASE_KR2	 = 5,
-	MLX5E_40GBASE_CR4	 = 6,
-	MLX5E_40GBASE_KR4	 = 7,
-	MLX5E_56GBASE_R4	 = 8,
-	MLX5E_10GBASE_CR	 = 12,
-	MLX5E_10GBASE_SR	 = 13,
-	MLX5E_10GBASE_ER	 = 14,
-	MLX5E_40GBASE_SR4	 = 15,
-	MLX5E_40GBASE_LR4	 = 16,
-	MLX5E_50GBASE_SR2	 = 18,
-	MLX5E_100GBASE_CR4	 = 20,
-	MLX5E_100GBASE_SR4	 = 21,
-	MLX5E_100GBASE_KR4	 = 22,
-	MLX5E_100GBASE_LR4	 = 23,
-	MLX5E_100BASE_TX	 = 24,
-	MLX5E_1000BASE_T	 = 25,
-	MLX5E_10GBASE_T		 = 26,
-	MLX5E_25GBASE_CR	 = 27,
-	MLX5E_25GBASE_KR	 = 28,
-	MLX5E_25GBASE_SR	 = 29,
-	MLX5E_50GBASE_CR2	 = 30,
-	MLX5E_50GBASE_KR2	 = 31,
-	MLX5E_LINK_MODES_NUMBER,
-};
-
-#define MLX5E_PROT_MASK(link_mode) (1 << link_mode)
-
-
 void mlx5e_build_ptys2ethtool_map(void);
 
 void mlx5e_send_nop(struct mlx5e_sq *sq, bool notify_hw);

commit cff92d7c7ebd7ceddd4def6b39e0302585b1eb14
Author: Hadar Hen Zion <hadarh@mellanox.com>
Date:   Sun Jul 24 16:12:40 2016 +0300

    net/mlx5e: Query minimum required header copy during xmit
    
    Add support for query the minimum inline mode from the Firmware.
    It is required for correct TX steering according to L3/L4 packet
    headers.
    
    Each send queue (SQ) has inline mode that defines the minimal required
    headers that needs to be copied into the SQ WQE.
    The driver asks the Firmware for the wqe_inline_mode device capability
    value.  In case the device capability defined as "vport context" the
    driver must check the reported min inline mode from the vport context
    before creating its SQs.
    
    Signed-off-by: Hadar Hen Zion <hadarh@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 2c20c7b14043..1b495efa7490 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -129,6 +129,12 @@ static inline int mlx5_max_log_rq_size(int wq_type)
 	}
 }
 
+enum {
+	MLX5E_INLINE_MODE_L2,
+	MLX5E_INLINE_MODE_VPORT_CONTEXT,
+	MLX5_INLINE_MODE_NOT_REQUIRED,
+};
+
 struct mlx5e_tx_wqe {
 	struct mlx5_wqe_ctrl_seg ctrl;
 	struct mlx5_wqe_eth_seg  eth;
@@ -188,6 +194,7 @@ struct mlx5e_params {
 	bool lro_en;
 	u32 lro_wqe_sz;
 	u16 tx_max_inline;
+	u8  tx_min_inline_mode;
 	u8  rss_hfunc;
 	u8  toeplitz_hash_key[40];
 	u32 indirection_rqt[MLX5E_INDIR_RQT_SIZE];

commit ae76715d153e33c249b6850361e4d8d775388b5a
Author: Hadar Hen Zion <hadarh@mellanox.com>
Date:   Sun Jul 24 16:12:39 2016 +0300

    net/mlx5e: Check the minimum inline header mode before xmit
    
    Each send queue (SQ) has inline mode that defines the minimal required
    inline headers in the SQ WQE.
    Before sending each packet check that the minimum required headers
    on the WQE are copied.
    
    Signed-off-by: Hadar Hen Zion <hadarh@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 4cbd452fec25..2c20c7b14043 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -398,6 +398,7 @@ struct mlx5e_sq {
 	u32                        sqn;
 	u16                        bf_buf_size;
 	u16                        max_inline;
+	u8                         min_inline_mode;
 	u16                        edge;
 	struct device             *pdev;
 	struct mlx5e_tstamp       *tstamp;

commit 30d0844bdcea9fb8b0b3c8abfa5547bc3bcf8baa
Merge: ae3e4562e2ce bc86765181aa
Author: David S. Miller <davem@davemloft.net>
Date:   Wed Jul 6 10:35:22 2016 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Conflicts:
            drivers/net/ethernet/mellanox/mlx5/core/en.h
            drivers/net/ethernet/mellanox/mlx5/core/en_main.c
            drivers/net/usb/r8152.c
    
    All three conflicts were overlapping changes.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit f913a72aa008777d4a92f82acafb17cce9aed4dc
Author: Maor Gottlieb <maorg@mellanox.com>
Date:   Mon Jul 4 17:23:10 2016 +0300

    net/mlx5e: Add support to get ethtool flow rules
    
    Enhance the existing get_rxnfc callback:
    1. Get flow rule of specific ID.
    2. Get all flow rules.
    3. Get number of rules.
    
    Signed-off-by: Maor Gottlieb <maorg@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 9842594f5980..1365cdc81838 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -717,6 +717,10 @@ int mlx5e_create_flow_steering(struct mlx5e_priv *priv);
 void mlx5e_destroy_flow_steering(struct mlx5e_priv *priv);
 void mlx5e_init_l2_addr(struct mlx5e_priv *priv);
 void mlx5e_destroy_flow_table(struct mlx5e_flow_table *ft);
+int mlx5e_ethtool_get_flow(struct mlx5e_priv *priv, struct ethtool_rxnfc *info,
+			   int location);
+int mlx5e_ethtool_get_all_flows(struct mlx5e_priv *priv,
+				struct ethtool_rxnfc *info, u32 *rule_locs);
 int mlx5e_ethtool_flow_replace(struct mlx5e_priv *priv,
 			       struct ethtool_rx_flow_spec *fs);
 int mlx5e_ethtool_flow_remove(struct mlx5e_priv *priv,

commit 1174fce8d1410d13b665cb7693250cc789637b9a
Author: Maor Gottlieb <maorg@mellanox.com>
Date:   Mon Jul 4 17:23:09 2016 +0300

    net/mlx5e: Support l3/l4 flow type specs in ethtool flow steering
    
    Add support to add flow steering rules with ethtool
    of L3/L4 flow types (ip4/tcp4/udp4).
    Those rules will be in higher priority than l2 flow rules, in order
    to prefer more specific rules.
    
    Mask is not supported for l3/l4 flow types.
    
    Signed-off-by: Maor Gottlieb <maorg@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 357320e79de2..9842594f5980 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -549,9 +549,11 @@ struct mlx5e_ethtool_table {
 	int                    num_rules;
 };
 
+#define ETHTOOL_NUM_L3_L4_FTS 7
 #define ETHTOOL_NUM_L2_FTS 4
 
 struct mlx5e_ethtool_steering {
+	struct mlx5e_ethtool_table      l3_l4_ft[ETHTOOL_NUM_L3_L4_FTS];
 	struct mlx5e_ethtool_table      l2_ft[ETHTOOL_NUM_L2_FTS];
 	struct list_head                rules;
 	int                             tot_num_rules;

commit 6dc6071cfcde6cf687f8d288c9cef9ee6ee24dc7
Author: Maor Gottlieb <maorg@mellanox.com>
Date:   Mon Jul 4 17:23:08 2016 +0300

    net/mlx5e: Add ethtool flow steering support
    
    Implement etrhtool set_rxnfc callback to support ethtool flow spec
    direct steering. This patch adds only the support of ether flow type
    spec. L3/L4 flow specs support will be added in downstream patches.
    
    Signed-off-by: Maor Gottlieb <maorg@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 00643a116492..357320e79de2 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -544,8 +544,22 @@ enum {
 	MLX5E_ARFS_FT_LEVEL
 };
 
+struct mlx5e_ethtool_table {
+	struct mlx5_flow_table *ft;
+	int                    num_rules;
+};
+
+#define ETHTOOL_NUM_L2_FTS 4
+
+struct mlx5e_ethtool_steering {
+	struct mlx5e_ethtool_table      l2_ft[ETHTOOL_NUM_L2_FTS];
+	struct list_head                rules;
+	int                             tot_num_rules;
+};
+
 struct mlx5e_flow_steering {
 	struct mlx5_flow_namespace      *ns;
+	struct mlx5e_ethtool_steering   ethtool;
 	struct mlx5e_tc_table           tc;
 	struct mlx5e_vlan_table         vlan;
 	struct mlx5e_l2_table           l2;
@@ -701,6 +715,12 @@ int mlx5e_create_flow_steering(struct mlx5e_priv *priv);
 void mlx5e_destroy_flow_steering(struct mlx5e_priv *priv);
 void mlx5e_init_l2_addr(struct mlx5e_priv *priv);
 void mlx5e_destroy_flow_table(struct mlx5e_flow_table *ft);
+int mlx5e_ethtool_flow_replace(struct mlx5e_priv *priv,
+			       struct ethtool_rx_flow_spec *fs);
+int mlx5e_ethtool_flow_remove(struct mlx5e_priv *priv,
+			      int location);
+void mlx5e_ethtool_init_steering(struct mlx5e_priv *priv);
+void mlx5e_ethtool_cleanup_steering(struct mlx5e_priv *priv);
 void mlx5e_set_rx_mode_work(struct work_struct *work);
 
 void mlx5e_fill_hwstamp(struct mlx5e_tstamp *clock, u64 timestamp,

commit cb67b832921cfa20ad79bafdc51f1745339d0557
Author: Hadar Hen Zion <hadarh@mellanox.com>
Date:   Fri Jul 1 14:51:09 2016 +0300

    net/mlx5e: Introduce SRIOV VF representors
    
    Implement the relevant profile functions to create mlx5e driver instance
    serving as VF representor. When SRIOV offloads mode is enabled, each VF
    will have a representor netdevice instance on the host.
    
    To do that, we also export set of shared service functions from en_main.c,
    such that they can be used by both NIC and repsresentors netdevs.
    
    The newly created representor netdevice has a basic set of net_device_ops
    which are the same ndo functions as the NIC netdevice and an ndo of it's
    own for phys port name.
    
    The profiling infrastructure allow sharing code between the NIC and the
    vport representor even though the representor has only a subset of the
    NIC functionality.
    
    The VF reps and the PF which is used in that mode to represent the uplink,
    expose switchdev ops. Currently the only op supposed is attr get for the
    port parent ID which here serves to identify net-devices belonging to the
    same HW E-Switch. Other than that, no offloading is implemented and hence
    switching functionality is achieved if one sets SW switching rules, e.g
    using tc, bridge or ovs.
    
    Port phys name (ndo_get_phys_port_name) is implemented to allow exporting
    to user-space the VF vport number and along with the switchdev port parent
    id (phys_switch_id) enable a udev base consistent naming scheme:
    
    SUBSYSTEM=="net", ACTION=="add", ATTR{phys_switch_id}=="<phys_switch_id>", \
            ATTR{phys_port_name}!="", NAME="$PF_NIC$attr{phys_port_name}"
    
    where phys_switch_id is exposed by the PF (and VF reps) and $PF_NIC is
    the name of the PF netdevice.
    
    Signed-off-by: Hadar Hen Zion <hadarh@mellanox.com>
    Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 081259a4edc0..00643a116492 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -44,6 +44,7 @@
 #include <linux/mlx5/vport.h>
 #include <linux/mlx5/transobj.h>
 #include <linux/rhashtable.h>
+#include <net/switchdev.h>
 #include "wq.h"
 #include "mlx5_core.h"
 #include "en_stats.h"
@@ -816,4 +817,31 @@ int mlx5e_create_mdev_resources(struct mlx5_core_dev *mdev);
 void mlx5e_destroy_mdev_resources(struct mlx5_core_dev *mdev);
 int mlx5e_refresh_tirs_self_loopback_enable(struct mlx5_core_dev *mdev);
 
+struct mlx5_eswitch_rep;
+int mlx5e_vport_rep_load(struct mlx5_eswitch *esw,
+			 struct mlx5_eswitch_rep *rep);
+void mlx5e_vport_rep_unload(struct mlx5_eswitch *esw,
+			    struct mlx5_eswitch_rep *rep);
+int mlx5e_nic_rep_load(struct mlx5_eswitch *esw, struct mlx5_eswitch_rep *rep);
+void mlx5e_nic_rep_unload(struct mlx5_eswitch *esw,
+			  struct mlx5_eswitch_rep *rep);
+int mlx5e_add_sqs_fwd_rules(struct mlx5e_priv *priv);
+void mlx5e_remove_sqs_fwd_rules(struct mlx5e_priv *priv);
+int mlx5e_attr_get(struct net_device *dev, struct switchdev_attr *attr);
+
+int mlx5e_create_direct_rqts(struct mlx5e_priv *priv);
+void mlx5e_destroy_rqt(struct mlx5e_priv *priv, struct mlx5e_rqt *rqt);
+int mlx5e_create_direct_tirs(struct mlx5e_priv *priv);
+void mlx5e_destroy_direct_tirs(struct mlx5e_priv *priv);
+int mlx5e_create_tises(struct mlx5e_priv *priv);
+void mlx5e_cleanup_nic_tx(struct mlx5e_priv *priv);
+int mlx5e_close(struct net_device *netdev);
+int mlx5e_open(struct net_device *netdev);
+void mlx5e_update_stats_work(struct work_struct *work);
+void *mlx5e_create_netdev(struct mlx5_core_dev *mdev,
+			  const struct mlx5e_profile *profile, void *ppriv);
+void mlx5e_destroy_netdev(struct mlx5_core_dev *mdev, struct mlx5e_priv *priv);
+struct rtnl_link_stats64 *
+mlx5e_get_stats(struct net_device *dev, struct rtnl_link_stats64 *stats);
+
 #endif /* __MLX5_EN_H__ */

commit 127ea380acc9de16c2cbd57ed99475944c9917ec
Author: Hadar Hen Zion <hadarh@mellanox.com>
Date:   Fri Jul 1 14:51:08 2016 +0300

    net/mlx5: Add Representors registration API
    
    Introduce E-Switch registration/unregister representors functions.
    
    Those functions are called by the mlx5e driver when the PF NIC is
    created upon pci probe action regardless of the E-Switch mode (NONE,
    LEGACY or OFFLOADS).
    
    Adding basic E-Switch database that will hold the vport represntors
    upon creation.
    
    This patch doesn't add any new functionality.
    
    Signed-off-by: Hadar Hen Zion <hadarh@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index edfc9beb1d2c..081259a4edc0 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -571,7 +571,7 @@ enum {
 struct mlx5e_profile {
 	void	(*init)(struct mlx5_core_dev *mdev,
 			struct net_device *netdev,
-			const struct mlx5e_profile *profile);
+			const struct mlx5e_profile *profile, void *ppriv);
 	void	(*cleanup)(struct mlx5e_priv *priv);
 	int	(*init_rx)(struct mlx5e_priv *priv);
 	void	(*cleanup_rx)(struct mlx5e_priv *priv);
@@ -618,6 +618,7 @@ struct mlx5e_priv {
 	struct mlx5e_tstamp        tstamp;
 	u16 q_counter;
 	const struct mlx5e_profile *profile;
+	void                      *ppriv;
 };
 
 enum mlx5e_link_mode {

commit 6bfd390ba5466675f6f02f77a3e957bd4e6075ee
Author: Hadar Hen Zion <hadarh@mellanox.com>
Date:   Fri Jul 1 14:51:07 2016 +0300

    net/mlx5e: Add support for multiple profiles
    
    To allow support in representor netdevices where we create more than one
    netdevice per NIC, add profiles to the mlx5e driver. The profiling
    allows for creation of mlx5e instances with different characteristics.
    
    Each profile implements its own behavior using set of function pointers
    defined in struct mlx5e_profile. This is done to allow for avoiding complex
    per profix branching in the code.
    
    Currently only the profile for the conventional NIC is implemented,
    which is of use when a netdev is created upon pci probe.
    
    This patch doesn't add any new functionality.
    
    Signed-off-by: Hadar Hen Zion <hadarh@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 91c6bbeca656..edfc9beb1d2c 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -568,6 +568,22 @@ enum {
 	MLX5E_NIC_PRIO
 };
 
+struct mlx5e_profile {
+	void	(*init)(struct mlx5_core_dev *mdev,
+			struct net_device *netdev,
+			const struct mlx5e_profile *profile);
+	void	(*cleanup)(struct mlx5e_priv *priv);
+	int	(*init_rx)(struct mlx5e_priv *priv);
+	void	(*cleanup_rx)(struct mlx5e_priv *priv);
+	int	(*init_tx)(struct mlx5e_priv *priv);
+	void	(*cleanup_tx)(struct mlx5e_priv *priv);
+	void	(*enable)(struct mlx5e_priv *priv);
+	void	(*disable)(struct mlx5e_priv *priv);
+	void	(*update_stats)(struct mlx5e_priv *priv);
+	int	(*max_nch)(struct mlx5_core_dev *mdev);
+	int	max_tc;
+};
+
 struct mlx5e_priv {
 	/* priv data path fields - start */
 	struct mlx5e_sq            **txq_to_sq_map;
@@ -601,6 +617,7 @@ struct mlx5e_priv {
 	struct mlx5e_stats         stats;
 	struct mlx5e_tstamp        tstamp;
 	u16 q_counter;
+	const struct mlx5e_profile *profile;
 };
 
 enum mlx5e_link_mode {

commit 398f33511e97aad7f259e864a1596fc8ef559dc1
Author: Hadar Hen Zion <hadarh@mellanox.com>
Date:   Fri Jul 1 14:51:06 2016 +0300

    net/mlx5e: Mark enabled RQTs instances explicitly
    
    In the current driver implementation two types of receive queue
    tables (RQTs) are in use - direct and indirect.
    
    Change the driver to mark each new created RQT (direct or indirect)
    as "enabled". This behaviour is needed for introducing new mlx5e
    instances which serve to represent SRIOV VFs.
    
    The VF representors will have only one type of RQTs (direct).
    
    An "enabled" flag is added to each RQT to allow better handling
    and code sharing between the representors and the nic netdevices.
    
    This patch doesn't add any new functionality.
    
    Signed-off-by: Hadar Hen Zion <hadarh@mellanox.com>
    Reviewed-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 8dad50caa4c9..91c6bbeca656 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -552,10 +552,15 @@ struct mlx5e_flow_steering {
 	struct mlx5e_arfs_tables        arfs;
 };
 
-struct mlx5e_tir {
-	u32              tirn;
+struct mlx5e_rqt {
 	u32              rqtn;
-	struct list_head list;
+	bool		 enabled;
+};
+
+struct mlx5e_tir {
+	u32		  tirn;
+	struct mlx5e_rqt  rqt;
+	struct list_head  list;
 };
 
 enum {
@@ -576,7 +581,7 @@ struct mlx5e_priv {
 
 	struct mlx5e_channel     **channel;
 	u32                        tisn[MLX5E_MAX_NUM_TC];
-	u32                        indir_rqtn;
+	struct mlx5e_rqt           indir_rqt;
 	struct mlx5e_tir           indir_tir[MLX5E_NUM_INDIR_TIRS];
 	struct mlx5e_tir           direct_tir[MLX5E_MAX_NUM_CHANNELS];
 	u32                        tx_rates[MLX5E_MAX_NUM_SQS];

commit 724b2aa15126d9e24b36650c5cad9cf468c20785
Author: Hadar Hen Zion <hadarh@mellanox.com>
Date:   Fri Jul 1 14:51:05 2016 +0300

    net/mlx5e: TIRs management refactoring
    
    The current refresh tirs self loopback mechanism, refreshes all the tirs
    belonging to the same mlx5e instance to prevent self loopback by packets
    sent over any ring of that instance. This mechanism relies on all the
    tirs/tises of an instance to be created with the same transport domain
    number (tdn).
    
    Change the driver to refresh all the tirs created under the same tdn
    regardless of which mlx5e netdev instance they belong to.
    
    This behaviour is needed for introducing new mlx5e instances which serve
    to represent SRIOV VFs. The representors and the PF share vport used for
    E-Switch management, and we want to avoid NIC level HW loopback between
    them, e.g when sending broadcast packets. To achieve that, both the
    representors and the PF NIC will share the tdn.
    
    This patch doesn't add any new functionality.
    
    Signed-off-by: Hadar Hen Zion <hadarh@mellanox.com>
    Reviewed-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 3226b92a344d..8dad50caa4c9 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -552,9 +552,10 @@ struct mlx5e_flow_steering {
 	struct mlx5e_arfs_tables        arfs;
 };
 
-struct mlx5e_direct_tir {
+struct mlx5e_tir {
 	u32              tirn;
 	u32              rqtn;
+	struct list_head list;
 };
 
 enum {
@@ -576,8 +577,8 @@ struct mlx5e_priv {
 	struct mlx5e_channel     **channel;
 	u32                        tisn[MLX5E_MAX_NUM_TC];
 	u32                        indir_rqtn;
-	u32                        indir_tirn[MLX5E_NUM_INDIR_TIRS];
-	struct mlx5e_direct_tir    direct_tir[MLX5E_MAX_NUM_CHANNELS];
+	struct mlx5e_tir           indir_tir[MLX5E_NUM_INDIR_TIRS];
+	struct mlx5e_tir           direct_tir[MLX5E_MAX_NUM_CHANNELS];
 	u32                        tx_rates[MLX5E_MAX_NUM_SQS];
 
 	struct mlx5e_flow_steering fs;
@@ -784,7 +785,12 @@ int mlx5e_rx_flow_steer(struct net_device *dev, const struct sk_buff *skb,
 #endif
 
 u16 mlx5e_get_max_inline_cap(struct mlx5_core_dev *mdev);
+int mlx5e_create_tir(struct mlx5_core_dev *mdev,
+		     struct mlx5e_tir *tir, u32 *in, int inlen);
+void mlx5e_destroy_tir(struct mlx5_core_dev *mdev,
+		       struct mlx5e_tir *tir);
 int mlx5e_create_mdev_resources(struct mlx5_core_dev *mdev);
 void mlx5e_destroy_mdev_resources(struct mlx5_core_dev *mdev);
+int mlx5e_refresh_tirs_self_loopback_enable(struct mlx5_core_dev *mdev);
 
 #endif /* __MLX5_EN_H__ */

commit b50d292b4399f4eb11e82d0430aacf62dd5d5365
Author: Hadar Hen Zion <hadarh@mellanox.com>
Date:   Fri Jul 1 14:51:04 2016 +0300

    net/mlx5e: Create NIC global resources only once
    
    To allow creating more than one netdev over the same PCI function, we
    change the driver such that global NIC resources are created once and
    later be shared amongst all the mlx5e netdevs running over that port.
    
    Move the CQ UAR, PD (pdn), Transport Domain (tdn), MKey resources from
    being kept in the mlx5e priv part to a new resources structure
    (mlx5e_resources) placed under the mlx5_core device.
    
    This patch doesn't add any new functionality.
    
    Signed-off-by: Hadar Hen Zion <hadarh@mellanox.com>
    Reviewed-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index b97511bf4c7b..3226b92a344d 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -570,10 +570,6 @@ struct mlx5e_priv {
 
 	unsigned long              state;
 	struct mutex               state_lock; /* Protects Interface state */
-	struct mlx5_uar            cq_uar;
-	u32                        pdn;
-	u32                        tdn;
-	struct mlx5_core_mkey      mkey;
 	struct mlx5_core_mkey      umr_mkey;
 	struct mlx5e_rq            drop_rq;
 
@@ -788,5 +784,7 @@ int mlx5e_rx_flow_steer(struct net_device *dev, const struct sk_buff *skb,
 #endif
 
 u16 mlx5e_get_max_inline_cap(struct mlx5_core_dev *mdev);
+int mlx5e_create_mdev_resources(struct mlx5_core_dev *mdev);
+void mlx5e_destroy_mdev_resources(struct mlx5_core_dev *mdev);
 
 #endif /* __MLX5_EN_H__ */

commit cdcf11212b22ff8e3de88ced1a6eda5bb950e120
Author: Rana Shahout <ranas@mellanox.com>
Date:   Thu Jun 30 17:34:49 2016 +0300

    net/mlx5e: Validate BW weight values of ETS
    
    Valid weight assigned to ETS TClass values are 1-100
    
    Fixes: 08fb1dacdd76 ('net/mlx5e: Support DCBNL IEEE ETS')
    Signed-off-by: Rana Shahout <ranas@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index b429591894eb..943b1bd434bf 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -145,7 +145,6 @@ struct mlx5e_umr_wqe {
 
 #ifdef CONFIG_MLX5_CORE_EN_DCB
 #define MLX5E_MAX_BW_ALLOC 100 /* Max percentage of BW allocation */
-#define MLX5E_MIN_BW_ALLOC 1   /* Min percentage of BW allocation */
 #endif
 
 struct mlx5e_params {

commit 6cd392a082deca8accec5c50b5b3fc1a9de5bfa2
Author: Daniel Jurgens <danielj@mellanox.com>
Date:   Thu Jun 30 17:34:46 2016 +0300

    net/mlx5e: Handle RQ flush in error cases
    
    Add a timeout to avoid an infinite loop waiting for RQ's to flush. This
    occurs during AER/EEH and will also happen if the device stops posting
    completions due to internal error or reset, or if moving the RQ to the
    error state fails. Also cleanup posted receive resources when closing
    the RQ.
    
    Fixes: f62b8bb8f2d3 ('net/mlx5: Extend mlx5_core to support ConnectX-4 Ethernet functionality')
    Signed-off-by: Daniel Jurgens <danielj@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 244aced4fe70..b429591894eb 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -191,6 +191,7 @@ struct mlx5e_tstamp {
 enum {
 	MLX5E_RQ_STATE_POST_WQES_ENABLE,
 	MLX5E_RQ_STATE_UMR_WQE_IN_PROGRESS,
+	MLX5E_RQ_STATE_FLUSH_TIMEOUT,
 };
 
 struct mlx5e_cq {
@@ -220,6 +221,8 @@ typedef void (*mlx5e_fp_handle_rx_cqe)(struct mlx5e_rq *rq,
 typedef int (*mlx5e_fp_alloc_wqe)(struct mlx5e_rq *rq, struct mlx5e_rx_wqe *wqe,
 				  u16 ix);
 
+typedef void (*mlx5e_fp_dealloc_wqe)(struct mlx5e_rq *rq, u16 ix);
+
 struct mlx5e_dma_info {
 	struct page	*page;
 	dma_addr_t	addr;
@@ -241,6 +244,7 @@ struct mlx5e_rq {
 	struct mlx5e_cq        cq;
 	mlx5e_fp_handle_rx_cqe handle_rx_cqe;
 	mlx5e_fp_alloc_wqe     alloc_wqe;
+	mlx5e_fp_dealloc_wqe   dealloc_wqe;
 
 	unsigned long          state;
 	int                    ix;
@@ -592,12 +596,15 @@ int mlx5e_napi_poll(struct napi_struct *napi, int budget);
 bool mlx5e_poll_tx_cq(struct mlx5e_cq *cq, int napi_budget);
 int mlx5e_poll_rx_cq(struct mlx5e_cq *cq, int budget);
 void mlx5e_free_tx_descs(struct mlx5e_sq *sq);
+void mlx5e_free_rx_descs(struct mlx5e_rq *rq);
 
 void mlx5e_handle_rx_cqe(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe);
 void mlx5e_handle_rx_cqe_mpwrq(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe);
 bool mlx5e_post_rx_wqes(struct mlx5e_rq *rq);
 int mlx5e_alloc_rx_wqe(struct mlx5e_rq *rq, struct mlx5e_rx_wqe *wqe, u16 ix);
 int mlx5e_alloc_rx_mpwqe(struct mlx5e_rq *rq, struct mlx5e_rx_wqe *wqe, u16 ix);
+void mlx5e_dealloc_rx_wqe(struct mlx5e_rq *rq, u16 ix);
+void mlx5e_dealloc_rx_mpwqe(struct mlx5e_rq *rq, u16 ix);
 void mlx5e_post_rx_fragmented_mpwqe(struct mlx5e_rq *rq);
 void mlx5e_complete_rx_linear_mpwqe(struct mlx5e_rq *rq,
 				    struct mlx5_cqe64 *cqe,

commit 3947ca1859999ac00698c0da0d233813a93d288a
Author: Daniel Jurgens <danielj@mellanox.com>
Date:   Thu Jun 30 17:34:45 2016 +0300

    net/mlx5e: Implement ndo_tx_timeout callback
    
    Add callback to handle TX timeouts.
    
    Fixes: f62b8bb8f2d3 ('net/mlx5: Extend mlx5_core to support ConnectX-4 Ethernet functionality')
    Signed-off-by: Daniel Jurgens <danielj@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index c22d8c8a627e..244aced4fe70 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -539,6 +539,7 @@ struct mlx5e_priv {
 	struct workqueue_struct    *wq;
 	struct work_struct         update_carrier_work;
 	struct work_struct         set_rx_mode_work;
+	struct work_struct         tx_timeout_work;
 	struct delayed_work        update_stats_work;
 
 	struct mlx5_core_dev      *mdev;

commit 29429f3300a378f7c29583b4c2c2ef29e2190a69
Author: Daniel Jurgens <danielj@mellanox.com>
Date:   Thu Jun 30 17:34:44 2016 +0300

    net/mlx5e: Timeout if SQ doesn't flush during close
    
    Avoid an infinite loop by timing out waiting for the SQ to flush. Also
    clean up the TX descriptors if that happens.
    
    Fixes: f62b8bb8f2d3 ('net/mlx5: Extend mlx5_core to support ConnectX-4 Ethernet functionality')
    Signed-off-by: Daniel Jurgens <danielj@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index baa991a23475..c22d8c8a627e 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -305,6 +305,7 @@ struct mlx5e_sq_dma {
 enum {
 	MLX5E_SQ_STATE_WAKE_TXQ_ENABLE,
 	MLX5E_SQ_STATE_BF_ENABLE,
+	MLX5E_SQ_STATE_TX_TIMEOUT,
 };
 
 struct mlx5e_ico_wqe_info {
@@ -589,6 +590,7 @@ void mlx5e_cq_error_event(struct mlx5_core_cq *mcq, enum mlx5_event event);
 int mlx5e_napi_poll(struct napi_struct *napi, int budget);
 bool mlx5e_poll_tx_cq(struct mlx5e_cq *cq, int napi_budget);
 int mlx5e_poll_rx_cq(struct mlx5e_cq *cq, int budget);
+void mlx5e_free_tx_descs(struct mlx5e_sq *sq);
 
 void mlx5e_handle_rx_cqe(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe);
 void mlx5e_handle_rx_cqe_mpwrq(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe);

commit ee58b57100ca953da7320c285315a95db2f7053d
Merge: 6f30e8b022c8 e7bdea7750eb
Author: David S. Miller <davem@davemloft.net>
Date:   Thu Jun 30 05:03:36 2016 -0400

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Several cases of overlapping changes, except the packet scheduler
    conflicts which deal with the addition of the free list parameter
    to qdisc_enqueue().
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit e0f46eb9f64ca2e97d242b6e02b8ae2c7fe6dc56
Author: Eli Cohen <eli@mellanox.com>
Date:   Mon Jun 27 12:08:34 2016 +0300

    net/mlx5e: Change enum to better reflect usage
    
    Change MLX5E_STATE_ASYNC_EVENTS_ENABLE to
    MLX5E_STATE_ASYNC_EVENTS_ENABLED since it represent a state and not an
    operation.
    
    Fixes: acff797cd1874 ('net/mlx5: Extend mlx5_core to support ConnectX-4 Ethernet functionality')
    Signed-off-by: Eli Cohen <eli@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index e8a6c3325b39..baa991a23475 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -401,7 +401,7 @@ enum mlx5e_traffic_types {
 };
 
 enum {
-	MLX5E_STATE_ASYNC_EVENTS_ENABLE,
+	MLX5E_STATE_ASYNC_EVENTS_ENABLED,
 	MLX5E_STATE_OPENED,
 	MLX5E_STATE_DESTROYING,
 };

commit 665bc53969d79af9cfd080e25b91b0415a2b5eec
Author: Gal Pressman <galp@mellanox.com>
Date:   Thu Jun 23 17:02:45 2016 +0300

    net/mlx5e: Use new ethtool get/set link ksettings API
    
    Use new get/set link ksettings and remove get/set settings legacy
    callbacks.
    This allows us to use bitmasks longer than 32 bit for supported and
    advertised link modes and use modes that were previously not supported.
    
    Signed-off-by: Gal Pressman <galp@mellanox.com>
    CC: Ben Hutchings <bwh@kernel.org>
    CC: David Decotigny <decot@googlers.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index b8732e6d2b6b..da885c0dfebe 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -634,6 +634,9 @@ enum mlx5e_link_mode {
 
 #define MLX5E_PROT_MASK(link_mode) (1 << link_mode)
 
+
+void mlx5e_build_ptys2ethtool_map(void);
+
 void mlx5e_send_nop(struct mlx5e_sq *sq, bool notify_hw);
 u16 mlx5e_select_queue(struct net_device *dev, struct sk_buff *skb,
 		       void *accel_priv, select_queue_fallback_t fallback);

commit 4a50e35b04c9008558a73ed4e349b3b483ef6739
Author: Gal Pressman <galp@mellanox.com>
Date:   Thu Jun 23 17:02:44 2016 +0300

    net/mlx5e: Add missing 50G baseSR2 link mode
    
    Add MLX5E_50GBASE_SR2 as ETHTOOL_LINK_MODE_50000baseSR2_Full_BIT.
    
    Signed-off-by: Gal Pressman <galp@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Cc: Ben Hutchings <bwh@kernel.org>
    Cc: David Decotigny <decot@googlers.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index aa36a3a65e72..b8732e6d2b6b 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -616,6 +616,7 @@ enum mlx5e_link_mode {
 	MLX5E_10GBASE_ER	 = 14,
 	MLX5E_40GBASE_SR4	 = 15,
 	MLX5E_40GBASE_LR4	 = 16,
+	MLX5E_50GBASE_SR2	 = 18,
 	MLX5E_100GBASE_CR4	 = 20,
 	MLX5E_100GBASE_SR4	 = 21,
 	MLX5E_100GBASE_KR4	 = 22,

commit cb3c7fd4f8393e0c42cbb13367b60454ae4e05f7
Author: Gil Rockah <gilr@mellanox.com>
Date:   Thu Jun 23 17:02:41 2016 +0300

    net/mlx5e: Support adaptive RX coalescing
    
    Striving for high message rate and low interrupt rate.
    
    Usage:
            ethtool -C <interface> adaptive-rx on/off
    
    Signed-off-by: Gil Rockah <gilr@mellanox.com>
    Signed-off-by: Achiad Shochat <achiad@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    CC: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 36f625d3c736..aa36a3a65e72 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -195,6 +195,7 @@ struct mlx5e_params {
 #ifdef CONFIG_MLX5_CORE_EN_DCB
 	struct ieee_ets ets;
 #endif
+	bool rx_am_enabled;
 };
 
 struct mlx5e_tstamp {
@@ -213,6 +214,7 @@ struct mlx5e_tstamp {
 enum {
 	MLX5E_RQ_STATE_POST_WQES_ENABLE,
 	MLX5E_RQ_STATE_UMR_WQE_IN_PROGRESS,
+	MLX5E_RQ_STATE_AM,
 };
 
 struct mlx5e_cq {
@@ -220,6 +222,7 @@ struct mlx5e_cq {
 	struct mlx5_cqwq           wq;
 
 	/* data path - accessed per napi poll */
+	u16                        event_ctr;
 	struct napi_struct        *napi;
 	struct mlx5_core_cq        mcq;
 	struct mlx5e_channel      *channel;
@@ -247,6 +250,30 @@ struct mlx5e_dma_info {
 	dma_addr_t	addr;
 };
 
+struct mlx5e_rx_am_stats {
+	int ppms; /* packets per msec */
+	int epms; /* events per msec */
+};
+
+struct mlx5e_rx_am_sample {
+	ktime_t		time;
+	unsigned int	pkt_ctr;
+	u16		event_ctr;
+};
+
+struct mlx5e_rx_am { /* Adaptive Moderation */
+	u8					state;
+	struct mlx5e_rx_am_stats		prev_stats;
+	struct mlx5e_rx_am_sample		start_sample;
+	struct work_struct			work;
+	u8					profile_ix;
+	u8					mode;
+	u8					tune_state;
+	u8					steps_right;
+	u8					steps_left;
+	u8					tired;
+};
+
 struct mlx5e_rq {
 	/* data path */
 	struct mlx5_wq_ll      wq;
@@ -267,6 +294,8 @@ struct mlx5e_rq {
 	unsigned long          state;
 	int                    ix;
 
+	struct mlx5e_rx_am     am; /* Adaptive Moderation */
+
 	/* control */
 	struct mlx5_wq_ctrl    wq_ctrl;
 	u8                     wq_type;
@@ -637,6 +666,10 @@ void mlx5e_free_rx_fragmented_mpwqe(struct mlx5e_rq *rq,
 				    struct mlx5e_mpw_info *wi);
 struct mlx5_cqe64 *mlx5e_get_cqe(struct mlx5e_cq *cq);
 
+void mlx5e_rx_am(struct mlx5e_rq *rq);
+void mlx5e_rx_am_work(struct work_struct *work);
+struct mlx5e_cq_moder mlx5e_am_get_def_profile(u8 rx_cq_period_mode);
+
 void mlx5e_update_stats(struct mlx5e_priv *priv);
 
 int mlx5e_create_flow_steering(struct mlx5e_priv *priv);

commit 9908aa292971ee3320ea13a71d75f90a52929892
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Thu Jun 23 17:02:40 2016 +0300

    net/mlx5e: CQE based moderation
    
    In this mode the moderation timer will restart upon
    new completion (CQE) generation rather than upon interrupt
    generation.
    
    The outcome is that for bursty traffic the period timer will never
    expire and thus only the moderation frames counter will dictate
    interrupt generation, thus the interrupt rate will be relative
    to the incoming packets size.
    If the burst seizes for "moderation period" time then an interrupt
    will be issued immediately.
    
    CQE based moderation is off by default and can be controlled
    via ethtool set_priv_flags.
    
    Performance tested on ConnectX4-Lx 50G.
    
    Less packet loss in netperf UDP and TCP tests, with no bw degradation,
    for both single and multi streams, with message sizes of
    64, 1024, 1472 and 32768 byte.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Achiad Shochat <achiad@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: Gal Pressman <galp@mellanox.com>
    Signed-off-by: Gil Rockah <gilr@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 02fa4daef59d..36f625d3c736 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -79,6 +79,7 @@
 
 #define MLX5E_PARAMS_DEFAULT_LRO_WQE_SZ                 (64 * 1024)
 #define MLX5E_PARAMS_DEFAULT_RX_CQ_MODERATION_USEC      0x10
+#define MLX5E_PARAMS_DEFAULT_RX_CQ_MODERATION_USEC_FROM_CQE 0x3
 #define MLX5E_PARAMS_DEFAULT_RX_CQ_MODERATION_PKTS      0x20
 #define MLX5E_PARAMS_DEFAULT_TX_CQ_MODERATION_USEC      0x10
 #define MLX5E_PARAMS_DEFAULT_TX_CQ_MODERATION_PKTS      0x20
@@ -145,11 +146,11 @@ struct mlx5e_umr_wqe {
 };
 
 static const char mlx5e_priv_flags[][ETH_GSTRING_LEN] = {
-	"nop",
+	"rx_cqe_moder",
 };
 
 enum mlx5e_priv_flag {
-	MLX5E_PFLAG_NOP = (1 << 0),
+	MLX5E_PFLAG_RX_CQE_BASED_MODER = (1 << 0),
 };
 
 #define MLX5E_SET_PRIV_FLAG(priv, pflag, enable)    \
@@ -165,6 +166,11 @@ enum mlx5e_priv_flag {
 #define MLX5E_MIN_BW_ALLOC 1   /* Min percentage of BW allocation */
 #endif
 
+struct mlx5e_cq_moder {
+	u16 usec;
+	u16 pkts;
+};
+
 struct mlx5e_params {
 	u8  log_sq_size;
 	u8  rq_wq_type;
@@ -173,12 +179,11 @@ struct mlx5e_params {
 	u8  log_rq_size;
 	u16 num_channels;
 	u8  num_tc;
+	u8  rx_cq_period_mode;
 	bool rx_cqe_compress_admin;
 	bool rx_cqe_compress;
-	u16 rx_cq_moderation_usec;
-	u16 rx_cq_moderation_pkts;
-	u16 tx_cq_moderation_usec;
-	u16 tx_cq_moderation_pkts;
+	struct mlx5e_cq_moder rx_cq_moderation;
+	struct mlx5e_cq_moder tx_cq_moderation;
 	u16 min_rx_wqes;
 	bool lro_en;
 	u32 lro_wqe_sz;
@@ -667,6 +672,9 @@ void mlx5e_build_default_indir_rqt(struct mlx5_core_dev *mdev,
 				   int num_channels);
 int mlx5e_get_max_linkspeed(struct mlx5_core_dev *mdev, u32 *speed);
 
+void mlx5e_set_rx_cq_mode_params(struct mlx5e_params *params,
+				 u8 cq_period_mode);
+
 static inline void mlx5e_tx_notify_hw(struct mlx5e_sq *sq,
 				      struct mlx5_wqe_ctrl_seg *ctrl, int bf_sz)
 {

commit 4e59e288813901815b39c82fc00d4e9fe78ce16b
Author: Gal Pressman <galp@mellanox.com>
Date:   Thu Jun 23 17:02:39 2016 +0300

    net/mlx5e: Introduce net device priv flags infrastructure
    
    Introduce an infrastructure for getting/setting private net device
    flags.
    
    Currently a 'nop' priv flag is added, following patches will override
    the flag will actual feature specific flags.
    
    Signed-off-by: Gal Pressman <galp@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 017e047965a6..02fa4daef59d 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -144,6 +144,22 @@ struct mlx5e_umr_wqe {
 	struct mlx5_wqe_data_seg       data;
 };
 
+static const char mlx5e_priv_flags[][ETH_GSTRING_LEN] = {
+	"nop",
+};
+
+enum mlx5e_priv_flag {
+	MLX5E_PFLAG_NOP = (1 << 0),
+};
+
+#define MLX5E_SET_PRIV_FLAG(priv, pflag, enable)    \
+	do {                                        \
+		if (enable)                         \
+			priv->pflags |= pflag;      \
+		else                                \
+			priv->pflags &= ~pflag;     \
+	} while (0)
+
 #ifdef CONFIG_MLX5_CORE_EN_DCB
 #define MLX5E_MAX_BW_ALLOC 100 /* Max percentage of BW allocation */
 #define MLX5E_MIN_BW_ALLOC 1   /* Min percentage of BW allocation */
@@ -543,6 +559,7 @@ struct mlx5e_priv {
 	struct work_struct         set_rx_mode_work;
 	struct delayed_work        update_stats_work;
 
+	u32                        pflags;
 	struct mlx5_core_dev      *mdev;
 	struct net_device         *netdev;
 	struct mlx5e_stats         stats;

commit 507f0c817f7a28bbf4facb3a8dca72a68bc25248
Author: Yevgeny Petrilin <yevgenyp@mellanox.com>
Date:   Thu Jun 23 17:02:38 2016 +0300

    net/mlx5e: Add TXQ set max rate support
    
    Implement set_maxrate ndo.
    Use the rate index from the hardware table to attach to channel SQ/TXQ.
    In case of failure to configure new rate, the queue remains with
    unlimited rate.
    
    We save the configuration on priv structure and apply it each time
    Send Queues are being reinitialized (after open/close) operations.
    
    Signed-off-by: Yevgeny Petrilin <yevgenyp@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index e8a6c3325b39..017e047965a6 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -88,6 +88,7 @@
 #define MLX5E_LOG_INDIR_RQT_SIZE       0x7
 #define MLX5E_INDIR_RQT_SIZE           BIT(MLX5E_LOG_INDIR_RQT_SIZE)
 #define MLX5E_MAX_NUM_CHANNELS         (MLX5E_INDIR_RQT_SIZE >> 1)
+#define MLX5E_MAX_NUM_SQS              (MLX5E_MAX_NUM_CHANNELS * MLX5E_MAX_NUM_TC)
 #define MLX5E_TX_CQ_POLL_BUDGET        128
 #define MLX5E_UPDATE_STATS_INTERVAL    200 /* msecs */
 #define MLX5E_SQ_BF_BUDGET             16
@@ -354,6 +355,7 @@ struct mlx5e_sq {
 	struct mlx5e_channel      *channel;
 	int                        tc;
 	struct mlx5e_ico_wqe_info *ico_wqe_info;
+	u32                        rate_limit;
 } ____cacheline_aligned_in_smp;
 
 static inline bool mlx5e_sq_has_room_for(struct mlx5e_sq *sq, u16 n)
@@ -530,6 +532,7 @@ struct mlx5e_priv {
 	u32                        indir_rqtn;
 	u32                        indir_tirn[MLX5E_NUM_INDIR_TIRS];
 	struct mlx5e_direct_tir    direct_tir[MLX5E_MAX_NUM_CHANNELS];
+	u32                        tx_rates[MLX5E_MAX_NUM_SQS];
 
 	struct mlx5e_flow_steering fs;
 	struct mlx5e_vxlan_db      vxlan;

commit b797a684b0ddeb6f78193c9b1bfae6bd824ec01a
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Wed May 11 00:29:16 2016 +0300

    net/mlx5e: Enable CQE compression when PCI is slower than link
    
    We turn the feature ON, only for servers with PCI BW < MAX LINK BW, as it
    helps reducing PCI pressure on weak PCI slots, but it adds some software
    overhead.
    
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index e05abad50c7b..e8a6c3325b39 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -645,6 +645,7 @@ int mlx5e_close_locked(struct net_device *netdev);
 void mlx5e_build_default_indir_rqt(struct mlx5_core_dev *mdev,
 				   u32 *indirection_rqt, int len,
 				   int num_channels);
+int mlx5e_get_max_linkspeed(struct mlx5_core_dev *mdev, u32 *speed);
 
 static inline void mlx5e_tx_notify_hw(struct mlx5e_sq *sq,
 				      struct mlx5_wqe_ctrl_seg *ctrl, int bf_sz)

commit d9d9f156f3807b07bb84c1cfb074d620383c016a
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Wed May 11 00:29:15 2016 +0300

    net/mlx5e: Expand WQE stride when CQE compression is enabled
    
    Make the MPWQE/Striding RQ default configuration dynamic and not
    statically set at compile time.  Now at driver load we set
    stride size and num strides dynamically.
    
    By default we use same values as before, but when CQE compression
    is enabled, we set larger stride size to benefit from CQE
    compression for larger packets.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 19f0d8db27ce..e05abad50c7b 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -64,12 +64,9 @@
 #define MLX5E_PARAMS_DEFAULT_LOG_RQ_SIZE_MPW            0x4
 #define MLX5E_PARAMS_MAXIMUM_LOG_RQ_SIZE_MPW            0x6
 
-#define MLX5_MPWRQ_LOG_NUM_STRIDES		11 /* >= 9, HW restriction */
 #define MLX5_MPWRQ_LOG_STRIDE_SIZE		6  /* >= 6, HW restriction */
-#define MLX5_MPWRQ_NUM_STRIDES			BIT(MLX5_MPWRQ_LOG_NUM_STRIDES)
-#define MLX5_MPWRQ_STRIDE_SIZE			BIT(MLX5_MPWRQ_LOG_STRIDE_SIZE)
-#define MLX5_MPWRQ_LOG_WQE_SZ			(MLX5_MPWRQ_LOG_NUM_STRIDES +\
-						 MLX5_MPWRQ_LOG_STRIDE_SIZE)
+#define MLX5_MPWRQ_LOG_STRIDE_SIZE_CQE_COMPRESS	8  /* >= 6, HW restriction */
+#define MLX5_MPWRQ_LOG_WQE_SZ			17
 #define MLX5_MPWRQ_WQE_PAGE_ORDER  (MLX5_MPWRQ_LOG_WQE_SZ - PAGE_SHIFT > 0 ? \
 				    MLX5_MPWRQ_LOG_WQE_SZ - PAGE_SHIFT : 0)
 #define MLX5_MPWRQ_PAGES_PER_WQE		BIT(MLX5_MPWRQ_WQE_PAGE_ORDER)
@@ -154,6 +151,8 @@ struct mlx5e_umr_wqe {
 struct mlx5e_params {
 	u8  log_sq_size;
 	u8  rq_wq_type;
+	u8  mpwqe_log_stride_sz;
+	u8  mpwqe_log_num_strides;
 	u8  log_rq_size;
 	u16 num_channels;
 	u8  num_tc;
@@ -249,6 +248,8 @@ struct mlx5e_rq {
 	/* control */
 	struct mlx5_wq_ctrl    wq_ctrl;
 	u8                     wq_type;
+	u32                    mpwqe_stride_sz;
+	u32                    mpwqe_num_strides;
 	u32                    rqn;
 	struct mlx5e_channel  *channel;
 	struct mlx5e_priv     *priv;
@@ -272,7 +273,7 @@ struct mlx5e_mpw_info {
 	void (*dma_pre_sync)(struct device *pdev,
 			     struct mlx5e_mpw_info *wi,
 			     u32 wqe_offset, u32 len);
-	void (*add_skb_frag)(struct device *pdev,
+	void (*add_skb_frag)(struct mlx5e_rq *rq,
 			     struct sk_buff *skb,
 			     struct mlx5e_mpw_info *wi,
 			     u32 page_idx, u32 frag_offset, u32 len);

commit 7219ab34f184b5d864be38f5ada7cdff1ab5b18a
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Wed May 11 00:29:14 2016 +0300

    net/mlx5e: CQE compression
    
    CQE compression feature is meant to save PCIe bandwidth by
    compressing few CQEs into smaller amount of bytes on PCIe.
    CQE compression can be selectively enabled per CQ.  By default
    is disabled for now and will be enabled later on.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Eugenia Emantayev <eugenia@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index bfa5daaaf5aa..19f0d8db27ce 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -157,6 +157,8 @@ struct mlx5e_params {
 	u8  log_rq_size;
 	u16 num_channels;
 	u8  num_tc;
+	bool rx_cqe_compress_admin;
+	bool rx_cqe_compress;
 	u16 rx_cq_moderation_usec;
 	u16 rx_cq_moderation_pkts;
 	u16 tx_cq_moderation_usec;
@@ -202,6 +204,13 @@ struct mlx5e_cq {
 	struct mlx5e_channel      *channel;
 	struct mlx5e_priv         *priv;
 
+	/* cqe decompression */
+	struct mlx5_cqe64          title;
+	struct mlx5_mini_cqe8      mini_arr[MLX5_MINI_CQE_ARRAY_SIZE];
+	u8                         mini_arr_idx;
+	u16                        decmprs_left;
+	u16                        decmprs_wqe_counter;
+
 	/* control */
 	struct mlx5_wq_ctrl        wq_ctrl;
 } ____cacheline_aligned_in_smp;
@@ -616,6 +625,7 @@ void mlx5e_timestamp_init(struct mlx5e_priv *priv);
 void mlx5e_timestamp_cleanup(struct mlx5e_priv *priv);
 int mlx5e_hwstamp_set(struct net_device *dev, struct ifreq *ifr);
 int mlx5e_hwstamp_get(struct net_device *dev, struct ifreq *ifr);
+void mlx5e_modify_rx_cqe_compression(struct mlx5e_priv *priv, bool val);
 
 int mlx5e_vlan_rx_add_vid(struct net_device *dev, __always_unused __be16 proto,
 			  u16 vid);

commit 1de1d449c6cf701e09d2941d042b28328118ad1a
Author: David S. Miller <davem@davemloft.net>
Date:   Mon May 9 22:05:13 2016 -0400

    mlx5: Fix merge errors.
    
    I accidently let Arnd's VXLAN dependency changes slip into net-next,
    they are only appropriate for net.
    
    Also the flow steering structural changes to mlx5e_priv got scrambled
    during the merge resolution as well.
    
    Fix that all up.
    
    Reported-by: Stephen Rothwell <sfr@canb.auug.org.au>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 7aea32e085b3..bfa5daaaf5aa 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -522,12 +522,7 @@ struct mlx5e_priv {
 	struct mlx5e_direct_tir    direct_tir[MLX5E_MAX_NUM_CHANNELS];
 
 	struct mlx5e_flow_steering fs;
-	struct mlx5e_flow_tables   fts;
-	struct mlx5e_eth_addr_db   eth_addr;
-	struct mlx5e_vlan_db       vlan;
-#ifdef CONFIG_MLX5_CORE_EN_VXLAN
 	struct mlx5e_vxlan_db      vxlan;
-#endif
 
 	struct mlx5e_params        params;
 	struct workqueue_struct    *wq;

commit e800072c18f0d7b89a80fa46dceb3d080c80e09c
Merge: e8ed77dfa90d b507146bb6b9
Author: David S. Miller <davem@davemloft.net>
Date:   Mon May 9 15:59:24 2016 -0400

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    In netdevice.h we removed the structure in net-next that is being
    changes in 'net'.  In macsec.c and rtnetlink.c we have overlaps
    between fixes in 'net' and the u64 attribute changes in 'net-next'.
    
    The mlx5 conflicts have to do with vxlan support dependencies.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 7dbb29172d415ccccad1166700d6be78dee9f2bc
Author: Arnd Bergmann <arnd@arndb.de>
Date:   Sun May 8 14:55:25 2016 +0300

    net/mlx5e: make VXLAN support conditional
    
    VXLAN can be disabled at compile-time or it can be a loadable
    module while mlx5 is built-in, which leads to a link error:
    
    drivers/net/built-in.o: In function `mlx5e_create_netdev':
    ntb_netdev.c:(.text+0x106de4): undefined reference to `vxlan_get_rx_port'
    
    This avoids the link error and makes the vxlan code optional,
    like the other ethernet drivers do as well.
    
    Link: https://patchwork.ozlabs.org/patch/589296/
    Fixes: b3f63c3d5e2c ("net/mlx5e: Add netdev support for VXLAN tunneling")
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 3881dce0cc30..24344aafbd36 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -564,7 +564,9 @@ struct mlx5e_priv {
 	struct mlx5e_flow_tables   fts;
 	struct mlx5e_eth_addr_db   eth_addr;
 	struct mlx5e_vlan_db       vlan;
+#ifdef CONFIG_MLX5_CORE_EN_VXLAN
 	struct mlx5e_vxlan_db      vxlan;
+#endif
 
 	struct mlx5e_params        params;
 	struct workqueue_struct    *wq;

commit cba653210056cf47cc1969f831f05ddfb99ee2bd
Merge: 26879da58711 7391daf2ffc7
Author: David S. Miller <davem@davemloft.net>
Date:   Wed May 4 00:52:29 2016 -0400

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Conflicts:
            net/ipv4/ip_gre.c
    
    Minor conflicts between tunnel bug fixes in net and
    ipv6 tunnel cleanups in net-next.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 7bb2975599210097115021e542b6137781a09588
Author: Matthew Finlay <matt@mellanox.com>
Date:   Sun May 1 22:59:56 2016 +0300

    net/mlx5e: Implement a mlx5e workqueue
    
    Implement a mlx5e workqueue to handle all mlx5e specific tasks.  Move
    all tasks currently using the system workqueue to the new workqueue.
    This is in preparation for vxlan using the mlx5e workqueue in order to
    schedule port add/remove operations.
    
    Signed-off-by: Matthew Finlay <matt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index e80ce94b5dcf..3881dce0cc30 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -567,6 +567,7 @@ struct mlx5e_priv {
 	struct mlx5e_vxlan_db      vxlan;
 
 	struct mlx5e_params        params;
+	struct workqueue_struct    *wq;
 	struct work_struct         update_carrier_work;
 	struct work_struct         set_rx_mode_work;
 	struct delayed_work        update_stats_work;

commit 45bf454ae88414e80b80979ebb2c22bd66ea7d1b
Author: Maor Gottlieb <maorg@mellanox.com>
Date:   Fri Apr 29 01:36:42 2016 +0300

    net/mlx5e: Enabling aRFS mechanism
    
    Accelerated RFS requires that ntuple filtering is enabled via
    ethtool and driver supports ndo_rx_flow_steer.
    When the ntuple filtering is enabled, we modify the l3_l4 ttc
    rules to point on the aRFS flow tables and when the filtering
    is disabled, we modify the l3_l4 ttc rules to point on the RSS
    TIRs.
    
    Signed-off-by: Maor Gottlieb <maorg@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 21c38419ad89..34523c48444e 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -690,9 +690,21 @@ static inline int mlx5e_arfs_create_tables(struct mlx5e_priv *priv)
 }
 
 static inline void mlx5e_arfs_destroy_tables(struct mlx5e_priv *priv) {}
+
+static inline int mlx5e_arfs_enable(struct mlx5e_priv *priv)
+{
+	return -ENOTSUPP;
+}
+
+static inline int mlx5e_arfs_disable(struct mlx5e_priv *priv)
+{
+	return -ENOTSUPP;
+}
 #else
 int mlx5e_arfs_create_tables(struct mlx5e_priv *priv);
 void mlx5e_arfs_destroy_tables(struct mlx5e_priv *priv);
+int mlx5e_arfs_enable(struct mlx5e_priv *priv);
+int mlx5e_arfs_disable(struct mlx5e_priv *priv);
 int mlx5e_rx_flow_steer(struct net_device *dev, const struct sk_buff *skb,
 			u16 rxq_index, u32 flow_id);
 #endif

commit 18c908e477dcc94ede69323a6b876b5d8cfb40ff
Author: Maor Gottlieb <maorg@mellanox.com>
Date:   Fri Apr 29 01:36:41 2016 +0300

    net/mlx5e: Add accelerated RFS support
    
    Implement ndo_rx_flow_steer ndo.
    A new flow steering rule will be composed from the
    skb 4-tuple and added to the hardware aRFS flow table.
    
    Each rule is stored in an internal hash table, if such
    skb 4-tuple rule already exists we update the corresponding
    hardware steering rule with the new destination.
    
    For garbage collection rps_may_expire_flow will be
    invoked for a limited amount of old rules upon any
    ndo_rx_flow_steer invocation.
    
    Signed-off-by: Maor Gottlieb <maorg@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 999e05826490..21c38419ad89 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -448,9 +448,12 @@ struct mlx5e_ttc_table {
 	struct mlx5_flow_rule	 *rules[MLX5E_NUM_TT];
 };
 
+#define ARFS_HASH_SHIFT BITS_PER_BYTE
+#define ARFS_HASH_SIZE BIT(BITS_PER_BYTE)
 struct arfs_table {
 	struct mlx5e_flow_table  ft;
 	struct mlx5_flow_rule    *default_rule;
+	struct hlist_head	 rules_hash[ARFS_HASH_SIZE];
 };
 
 enum  arfs_type {
@@ -463,6 +466,11 @@ enum  arfs_type {
 
 struct mlx5e_arfs_tables {
 	struct arfs_table arfs_tables[ARFS_NUM_TYPES];
+	/* Protect aRFS rules list */
+	spinlock_t                     arfs_lock;
+	struct list_head               rules;
+	int                            last_filter_id;
+	struct workqueue_struct        *wq;
 };
 
 /* NIC prio FTS */
@@ -685,6 +693,8 @@ static inline void mlx5e_arfs_destroy_tables(struct mlx5e_priv *priv) {}
 #else
 int mlx5e_arfs_create_tables(struct mlx5e_priv *priv);
 void mlx5e_arfs_destroy_tables(struct mlx5e_priv *priv);
+int mlx5e_rx_flow_steer(struct net_device *dev, const struct sk_buff *skb,
+			u16 rxq_index, u32 flow_id);
 #endif
 
 u16 mlx5e_get_max_inline_cap(struct mlx5_core_dev *mdev);

commit 1cabe6b0965ec067ac60e8f182f16d479a3b9a5c
Author: Maor Gottlieb <maorg@mellanox.com>
Date:   Fri Apr 29 01:36:40 2016 +0300

    net/mlx5e: Create aRFS flow tables
    
    Create the following four flow tables for aRFS usage:
    1. IPv4 TCP - filtering 4-tuple of IPv4 TCP packets.
    2. IPv6 TCP - filtering 4-tuple of IPv6 TCP packets.
    3. IPv4 UDP - filtering 4-tuple of IPv4 UDP packets.
    4. IPv6 UDP - filtering 4-tuple of IPv6 UDP packets.
    
    Each flow table has two flow groups: one for the 4-tuple
    filtering (full match)  and the other contains * rule for miss rule.
    
    Full match rule means a hit for aRFS and packet will be forwarded
    to the dedicated RQ/Core, miss rule packets will be forwarded to
    default RSS hashing.
    
    Signed-off-by: Maor Gottlieb <maorg@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 2c9879c011b1..999e05826490 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -48,6 +48,8 @@
 #include "mlx5_core.h"
 #include "en_stats.h"
 
+#define MLX5_SET_CFG(p, f, v) MLX5_SET(create_flow_group_in, p, f, v)
+
 #define MLX5E_MAX_NUM_TC	8
 
 #define MLX5E_PARAMS_MINIMUM_LOG_SQ_SIZE                0x6
@@ -446,12 +448,38 @@ struct mlx5e_ttc_table {
 	struct mlx5_flow_rule	 *rules[MLX5E_NUM_TT];
 };
 
+struct arfs_table {
+	struct mlx5e_flow_table  ft;
+	struct mlx5_flow_rule    *default_rule;
+};
+
+enum  arfs_type {
+	ARFS_IPV4_TCP,
+	ARFS_IPV6_TCP,
+	ARFS_IPV4_UDP,
+	ARFS_IPV6_UDP,
+	ARFS_NUM_TYPES,
+};
+
+struct mlx5e_arfs_tables {
+	struct arfs_table arfs_tables[ARFS_NUM_TYPES];
+};
+
+/* NIC prio FTS */
+enum {
+	MLX5E_VLAN_FT_LEVEL = 0,
+	MLX5E_L2_FT_LEVEL,
+	MLX5E_TTC_FT_LEVEL,
+	MLX5E_ARFS_FT_LEVEL
+};
+
 struct mlx5e_flow_steering {
 	struct mlx5_flow_namespace      *ns;
 	struct mlx5e_tc_table           tc;
 	struct mlx5e_vlan_table         vlan;
 	struct mlx5e_l2_table           l2;
 	struct mlx5e_ttc_table          ttc;
+	struct mlx5e_arfs_tables        arfs;
 };
 
 struct mlx5e_direct_tir {
@@ -570,6 +598,7 @@ void mlx5e_update_stats(struct mlx5e_priv *priv);
 int mlx5e_create_flow_steering(struct mlx5e_priv *priv);
 void mlx5e_destroy_flow_steering(struct mlx5e_priv *priv);
 void mlx5e_init_l2_addr(struct mlx5e_priv *priv);
+void mlx5e_destroy_flow_table(struct mlx5e_flow_table *ft);
 void mlx5e_set_rx_mode_work(struct work_struct *work);
 
 void mlx5e_fill_hwstamp(struct mlx5e_tstamp *clock, u64 timestamp,
@@ -646,6 +675,18 @@ extern const struct dcbnl_rtnl_ops mlx5e_dcbnl_ops;
 int mlx5e_dcbnl_ieee_setets_core(struct mlx5e_priv *priv, struct ieee_ets *ets);
 #endif
 
+#ifndef CONFIG_RFS_ACCEL
+static inline int mlx5e_arfs_create_tables(struct mlx5e_priv *priv)
+{
+	return 0;
+}
+
+static inline void mlx5e_arfs_destroy_tables(struct mlx5e_priv *priv) {}
+#else
+int mlx5e_arfs_create_tables(struct mlx5e_priv *priv);
+void mlx5e_arfs_destroy_tables(struct mlx5e_priv *priv);
+#endif
+
 u16 mlx5e_get_max_inline_cap(struct mlx5_core_dev *mdev);
 
 #endif /* __MLX5_EN_H__ */

commit 33cfaaa8f36ffbee9ad259264334325b5449f5fe
Author: Maor Gottlieb <maorg@mellanox.com>
Date:   Fri Apr 29 01:36:38 2016 +0300

    net/mlx5e: Split the main flow steering table
    
    Currently, the main flow table is used for two purposes:
    One is to do mac filtering and the other is to classify
    the packet l3-l4 header in order to steer the packet to
    the right RSS TIR.
    
    This design is very complex, for each configured mac address we
    have to add eleven rules (rule for each traffic type), the same if the
    device is put to promiscuous/allmulti mode.
    This scheme isn't scalable for future features like aRFS.
    
    In order to simplify it, the main flow table is split to two flow
    tables:
    1. l2 table - filter the packet dmac address, if there is a match
    we forward to the ttc flow table.
    
    2. TTC (Traffic Type Classifier) table - classify the traffic
    type of the packet and steer the packet to the right TIR.
    
    In this new design, when new mac address is added, the driver adds
    only one flow rule instead of eleven.
    
    Signed-off-by: Maor Gottlieb <maorg@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 02b964402156..2c9879c011b1 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -399,31 +399,18 @@ struct mlx5e_vxlan_db {
 	struct radix_tree_root		tree;
 };
 
-struct mlx5e_eth_addr_info {
+struct mlx5e_l2_rule {
 	u8  addr[ETH_ALEN + 2];
-	u32 tt_vec;
-	struct mlx5_flow_rule *ft_rule[MLX5E_NUM_TT];
+	struct mlx5_flow_rule *rule;
 };
 
-#define MLX5E_ETH_ADDR_HASH_SIZE (1 << BITS_PER_BYTE)
-
 struct mlx5e_flow_table {
 	int num_groups;
 	struct mlx5_flow_table *t;
 	struct mlx5_flow_group **g;
 };
 
-struct mlx5e_main_table {
-	struct mlx5e_flow_table	   ft;
-	struct hlist_head          netdev_uc[MLX5E_ETH_ADDR_HASH_SIZE];
-	struct hlist_head          netdev_mc[MLX5E_ETH_ADDR_HASH_SIZE];
-	struct mlx5e_eth_addr_info broadcast;
-	struct mlx5e_eth_addr_info allmulti;
-	struct mlx5e_eth_addr_info promisc;
-	bool                       broadcast_enabled;
-	bool                       allmulti_enabled;
-	bool                       promisc_enabled;
-};
+#define MLX5E_L2_ADDR_HASH_SIZE BIT(BITS_PER_BYTE)
 
 struct mlx5e_tc_table {
 	struct mlx5_flow_table		*t;
@@ -441,11 +428,30 @@ struct mlx5e_vlan_table {
 	bool          filter_disabled;
 };
 
+struct mlx5e_l2_table {
+	struct mlx5e_flow_table    ft;
+	struct hlist_head          netdev_uc[MLX5E_L2_ADDR_HASH_SIZE];
+	struct hlist_head          netdev_mc[MLX5E_L2_ADDR_HASH_SIZE];
+	struct mlx5e_l2_rule	   broadcast;
+	struct mlx5e_l2_rule	   allmulti;
+	struct mlx5e_l2_rule	   promisc;
+	bool                       broadcast_enabled;
+	bool                       allmulti_enabled;
+	bool                       promisc_enabled;
+};
+
+/* L3/L4 traffic type classifier */
+struct mlx5e_ttc_table {
+	struct mlx5e_flow_table  ft;
+	struct mlx5_flow_rule	 *rules[MLX5E_NUM_TT];
+};
+
 struct mlx5e_flow_steering {
 	struct mlx5_flow_namespace      *ns;
 	struct mlx5e_tc_table           tc;
 	struct mlx5e_vlan_table         vlan;
-	struct mlx5e_main_table         main;
+	struct mlx5e_l2_table           l2;
+	struct mlx5e_ttc_table          ttc;
 };
 
 struct mlx5e_direct_tir {
@@ -563,7 +569,7 @@ void mlx5e_update_stats(struct mlx5e_priv *priv);
 
 int mlx5e_create_flow_steering(struct mlx5e_priv *priv);
 void mlx5e_destroy_flow_steering(struct mlx5e_priv *priv);
-void mlx5e_init_eth_addr(struct mlx5e_priv *priv);
+void mlx5e_init_l2_addr(struct mlx5e_priv *priv);
 void mlx5e_set_rx_mode_work(struct work_struct *work);
 
 void mlx5e_fill_hwstamp(struct mlx5e_tstamp *clock, u64 timestamp,

commit acff797cd187402d73cf9f290531a41250613294
Author: Maor Gottlieb <maorg@mellanox.com>
Date:   Fri Apr 29 01:36:37 2016 +0300

    net/mlx5e: Refactor mlx5e flow steering structs
    
    Slightly refactor and re-order the flow steering structs,
    tables and data-bases for better self-containment and
    flexibility to add more future steering phases
    (tables/rules/data bases) e.g: aRFS.
    
    Changes:
    1. Move the vlan DB and address DB into their table structs.
    2. Rename steering table structs to unique format: mlx5e_*_table,
    e.g: mlx5e_vlan_table.
    
    Signed-off-by: Maor Gottlieb <maorg@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 5c8e98c277dc..02b964402156 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -388,6 +388,17 @@ enum mlx5e_traffic_types {
 	MLX5E_NUM_INDIR_TIRS = MLX5E_TT_ANY,
 };
 
+enum {
+	MLX5E_STATE_ASYNC_EVENTS_ENABLE,
+	MLX5E_STATE_OPENED,
+	MLX5E_STATE_DESTROYING,
+};
+
+struct mlx5e_vxlan_db {
+	spinlock_t			lock; /* protect vxlan table */
+	struct radix_tree_root		tree;
+};
+
 struct mlx5e_eth_addr_info {
 	u8  addr[ETH_ALEN + 2];
 	u32 tt_vec;
@@ -396,7 +407,14 @@ struct mlx5e_eth_addr_info {
 
 #define MLX5E_ETH_ADDR_HASH_SIZE (1 << BITS_PER_BYTE)
 
-struct mlx5e_eth_addr_db {
+struct mlx5e_flow_table {
+	int num_groups;
+	struct mlx5_flow_table *t;
+	struct mlx5_flow_group **g;
+};
+
+struct mlx5e_main_table {
+	struct mlx5e_flow_table	   ft;
 	struct hlist_head          netdev_uc[MLX5E_ETH_ADDR_HASH_SIZE];
 	struct hlist_head          netdev_mc[MLX5E_ETH_ADDR_HASH_SIZE];
 	struct mlx5e_eth_addr_info broadcast;
@@ -407,13 +425,15 @@ struct mlx5e_eth_addr_db {
 	bool                       promisc_enabled;
 };
 
-enum {
-	MLX5E_STATE_ASYNC_EVENTS_ENABLE,
-	MLX5E_STATE_OPENED,
-	MLX5E_STATE_DESTROYING,
+struct mlx5e_tc_table {
+	struct mlx5_flow_table		*t;
+
+	struct rhashtable_params        ht_params;
+	struct rhashtable               ht;
 };
 
-struct mlx5e_vlan_db {
+struct mlx5e_vlan_table {
+	struct mlx5e_flow_table		ft;
 	unsigned long active_vlans[BITS_TO_LONGS(VLAN_N_VID)];
 	struct mlx5_flow_rule	*active_vlans_rule[VLAN_N_VID];
 	struct mlx5_flow_rule	*untagged_rule;
@@ -421,29 +441,11 @@ struct mlx5e_vlan_db {
 	bool          filter_disabled;
 };
 
-struct mlx5e_vxlan_db {
-	spinlock_t			lock; /* protect vxlan table */
-	struct radix_tree_root		tree;
-};
-
-struct mlx5e_flow_table {
-	int num_groups;
-	struct mlx5_flow_table		*t;
-	struct mlx5_flow_group		**g;
-};
-
-struct mlx5e_tc_flow_table {
-	struct mlx5_flow_table		*t;
-
-	struct rhashtable_params        ht_params;
-	struct rhashtable               ht;
-};
-
-struct mlx5e_flow_tables {
-	struct mlx5_flow_namespace	*ns;
-	struct mlx5e_tc_flow_table	tc;
-	struct mlx5e_flow_table		vlan;
-	struct mlx5e_flow_table		main;
+struct mlx5e_flow_steering {
+	struct mlx5_flow_namespace      *ns;
+	struct mlx5e_tc_table           tc;
+	struct mlx5e_vlan_table         vlan;
+	struct mlx5e_main_table         main;
 };
 
 struct mlx5e_direct_tir {
@@ -451,6 +453,11 @@ struct mlx5e_direct_tir {
 	u32              rqtn;
 };
 
+enum {
+	MLX5E_TC_PRIO = 0,
+	MLX5E_NIC_PRIO
+};
+
 struct mlx5e_priv {
 	/* priv data path fields - start */
 	struct mlx5e_sq            **txq_to_sq_map;
@@ -472,9 +479,7 @@ struct mlx5e_priv {
 	u32                        indir_tirn[MLX5E_NUM_INDIR_TIRS];
 	struct mlx5e_direct_tir    direct_tir[MLX5E_MAX_NUM_CHANNELS];
 
-	struct mlx5e_flow_tables   fts;
-	struct mlx5e_eth_addr_db   eth_addr;
-	struct mlx5e_vlan_db       vlan;
+	struct mlx5e_flow_steering fs;
 	struct mlx5e_vxlan_db      vxlan;
 
 	struct mlx5e_params        params;
@@ -556,8 +561,8 @@ struct mlx5_cqe64 *mlx5e_get_cqe(struct mlx5e_cq *cq);
 
 void mlx5e_update_stats(struct mlx5e_priv *priv);
 
-int mlx5e_create_flow_tables(struct mlx5e_priv *priv);
-void mlx5e_destroy_flow_tables(struct mlx5e_priv *priv);
+int mlx5e_create_flow_steering(struct mlx5e_priv *priv);
+void mlx5e_destroy_flow_steering(struct mlx5e_priv *priv);
 void mlx5e_init_eth_addr(struct mlx5e_priv *priv);
 void mlx5e_set_rx_mode_work(struct work_struct *work);
 

commit 1da366964ec907fccdea0339d81c2e9ef3d803f8
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Fri Apr 29 01:36:32 2016 +0300

    net/mlx5e: Direct TIR per RQ
    
    Introduce new TIRs for direct access per RQ.
    Now we have 2 available kinds of TIRs:
            - indirect TIR per traffic type, each points to one RQT (RSS RQT)
              same as before.
            - New direct TIR per RQ, each points to RQT with a size of one
              that forwards packets to that RQ only.
    
    Driver will open max channels (num cores) direct TIRs by default,
    they will be filled with the actual RQs once channels are allocated.
    
    Needed for downstream aRFS and ethtool direct steering functionalities.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index bbc01a49db02..5c8e98c277dc 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -385,14 +385,7 @@ enum mlx5e_traffic_types {
 	MLX5E_TT_IPV6,
 	MLX5E_TT_ANY,
 	MLX5E_NUM_TT,
-};
-
-#define IS_HASHING_TT(tt) (tt != MLX5E_TT_ANY)
-
-enum mlx5e_rqt_ix {
-	MLX5E_INDIRECTION_RQT,
-	MLX5E_SINGLE_RQ_RQT,
-	MLX5E_NUM_RQT,
+	MLX5E_NUM_INDIR_TIRS = MLX5E_TT_ANY,
 };
 
 struct mlx5e_eth_addr_info {
@@ -453,6 +446,11 @@ struct mlx5e_flow_tables {
 	struct mlx5e_flow_table		main;
 };
 
+struct mlx5e_direct_tir {
+	u32              tirn;
+	u32              rqtn;
+};
+
 struct mlx5e_priv {
 	/* priv data path fields - start */
 	struct mlx5e_sq            **txq_to_sq_map;
@@ -470,8 +468,9 @@ struct mlx5e_priv {
 
 	struct mlx5e_channel     **channel;
 	u32                        tisn[MLX5E_MAX_NUM_TC];
-	u32                        rqtn[MLX5E_NUM_RQT];
-	u32                        tirn[MLX5E_NUM_TT];
+	u32                        indir_rqtn;
+	u32                        indir_tirn[MLX5E_NUM_INDIR_TIRS];
+	struct mlx5e_direct_tir    direct_tir[MLX5E_MAX_NUM_CHANNELS];
 
 	struct mlx5e_flow_tables   fts;
 	struct mlx5e_eth_addr_db   eth_addr;
@@ -578,7 +577,7 @@ void mlx5e_disable_vlan_filter(struct mlx5e_priv *priv);
 
 int mlx5e_modify_rqs_vsd(struct mlx5e_priv *priv, bool vsd);
 
-int mlx5e_redirect_rqt(struct mlx5e_priv *priv, enum mlx5e_rqt_ix rqt_ix);
+int mlx5e_redirect_rqt(struct mlx5e_priv *priv, u32 rqtn, int sz, int ix);
 void mlx5e_build_tir_ctx_hash(void *tirc, struct mlx5e_priv *priv);
 
 int mlx5e_open_locked(struct net_device *netdev);

commit c0cc53162a0644dd57dce5e2fbb9bbafdc57d183
Merge: 8c14586fc320 f28f20da704d
Author: David S. Miller <davem@davemloft.net>
Date:   Wed Apr 27 15:43:10 2016 -0400

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Minor overlapping changes in the conflicts.
    
    In the macsec case, the change of the default ID macro
    name overlapped with the 64-bit netlink attribute alignment
    fixes in net-next.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 363501145e3faa650193722fe7047b767ed87172
Author: Gal Pressman <galp@mellanox.com>
Date:   Sun Apr 24 22:51:55 2016 +0300

    net/mlx5e: Add ethtool support for rxvlan-offload (vlan stripping)
    
    Use ethtool -K <interface> rxvlan <on/off> to enable/disable
    C-TAG vlan stripping by hardware.
    
    Signed-off-by: Gal Pressman <galp@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index e903eff2574f..8abc289ac1fb 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -166,6 +166,7 @@ struct mlx5e_params {
 	u8  rss_hfunc;
 	u8  toeplitz_hash_key[40];
 	u32 indirection_rqt[MLX5E_INDIR_RQT_SIZE];
+	bool vlan_strip_disable;
 #ifdef CONFIG_MLX5_CORE_EN_DCB
 	struct ieee_ets ets;
 #endif
@@ -575,6 +576,8 @@ int mlx5e_vlan_rx_kill_vid(struct net_device *dev, __always_unused __be16 proto,
 void mlx5e_enable_vlan_filter(struct mlx5e_priv *priv);
 void mlx5e_disable_vlan_filter(struct mlx5e_priv *priv);
 
+int mlx5e_modify_rqs_vsd(struct mlx5e_priv *priv, bool vsd);
+
 int mlx5e_redirect_rqt(struct mlx5e_priv *priv, enum mlx5e_rqt_ix rqt_ix);
 void mlx5e_build_tir_ctx_hash(void *tirc, struct mlx5e_priv *priv);
 

commit 9218b44dcc059e08e249f6f7614b8e391eb041d8
Author: Gal Pressman <galp@mellanox.com>
Date:   Sun Apr 24 22:51:47 2016 +0300

    net/mlx5e: Statistics handling refactoring
    
    Redesign ethtool statistics handling and reporting in the driver:
    1. Move counters to a separate file (en_stats.h).
    2. Remove unnecessary dependencies between stats and strings.
    3. Use counter descriptors which hold a name and offset for each counter,
       and will be used to decide which counters will be exposed.
    
    For example when adding a new software counter to ethtool, instead of:
    1. Add to stats struct.
    2. Add to strings struct in the same order.
    3. Change macro defining number of software counters.
    The only thing needed is to link the new counter to a counter descriptor.
    
    VPort counters are a set of hardware traffic counters created automatically
    for each virtual port opened.
    PPort counters are a set of counters describing per physical port
    performance statistics.
    These counters are gathered from hardware register and divided to groups
    according to different protocols.
    
    Signed-off-by: Gal Pressman <galp@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 6e24e821a1d8..e903eff2574f 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -46,6 +46,7 @@
 #include <linux/rhashtable.h>
 #include "wq.h"
 #include "mlx5_core.h"
+#include "en_stats.h"
 
 #define MLX5E_MAX_NUM_TC	8
 
@@ -148,245 +149,6 @@ struct mlx5e_umr_wqe {
 #define MLX5E_MIN_BW_ALLOC 1   /* Min percentage of BW allocation */
 #endif
 
-static const char vport_strings[][ETH_GSTRING_LEN] = {
-	/* vport statistics */
-	"rx_packets",
-	"rx_bytes",
-	"tx_packets",
-	"tx_bytes",
-	"rx_error_packets",
-	"rx_error_bytes",
-	"tx_error_packets",
-	"tx_error_bytes",
-	"rx_unicast_packets",
-	"rx_unicast_bytes",
-	"tx_unicast_packets",
-	"tx_unicast_bytes",
-	"rx_multicast_packets",
-	"rx_multicast_bytes",
-	"tx_multicast_packets",
-	"tx_multicast_bytes",
-	"rx_broadcast_packets",
-	"rx_broadcast_bytes",
-	"tx_broadcast_packets",
-	"tx_broadcast_bytes",
-
-	/* SW counters */
-	"tso_packets",
-	"tso_bytes",
-	"tso_inner_packets",
-	"tso_inner_bytes",
-	"lro_packets",
-	"lro_bytes",
-	"rx_csum_good",
-	"rx_csum_none",
-	"rx_csum_sw",
-	"tx_csum_offload",
-	"tx_csum_inner",
-	"tx_queue_stopped",
-	"tx_queue_wake",
-	"tx_queue_dropped",
-	"rx_wqe_err",
-	"rx_mpwqe_filler",
-	"rx_mpwqe_frag",
-	"rx_buff_alloc_err",
-};
-
-struct mlx5e_vport_stats {
-	/* HW counters */
-	u64 rx_packets;
-	u64 rx_bytes;
-	u64 tx_packets;
-	u64 tx_bytes;
-	u64 rx_error_packets;
-	u64 rx_error_bytes;
-	u64 tx_error_packets;
-	u64 tx_error_bytes;
-	u64 rx_unicast_packets;
-	u64 rx_unicast_bytes;
-	u64 tx_unicast_packets;
-	u64 tx_unicast_bytes;
-	u64 rx_multicast_packets;
-	u64 rx_multicast_bytes;
-	u64 tx_multicast_packets;
-	u64 tx_multicast_bytes;
-	u64 rx_broadcast_packets;
-	u64 rx_broadcast_bytes;
-	u64 tx_broadcast_packets;
-	u64 tx_broadcast_bytes;
-
-	/* SW counters */
-	u64 tso_packets;
-	u64 tso_bytes;
-	u64 tso_inner_packets;
-	u64 tso_inner_bytes;
-	u64 lro_packets;
-	u64 lro_bytes;
-	u64 rx_csum_good;
-	u64 rx_csum_none;
-	u64 rx_csum_sw;
-	u64 tx_csum_offload;
-	u64 tx_csum_inner;
-	u64 tx_queue_stopped;
-	u64 tx_queue_wake;
-	u64 tx_queue_dropped;
-	u64 rx_wqe_err;
-	u64 rx_mpwqe_filler;
-	u64 rx_mpwqe_frag;
-	u64 rx_buff_alloc_err;
-
-#define NUM_VPORT_COUNTERS     38
-};
-
-static const char pport_strings[][ETH_GSTRING_LEN] = {
-	/* IEEE802.3 counters */
-	"frames_tx",
-	"frames_rx",
-	"check_seq_err",
-	"alignment_err",
-	"octets_tx",
-	"octets_received",
-	"multicast_xmitted",
-	"broadcast_xmitted",
-	"multicast_rx",
-	"broadcast_rx",
-	"in_range_len_errors",
-	"out_of_range_len",
-	"too_long_errors",
-	"symbol_err",
-	"mac_control_tx",
-	"mac_control_rx",
-	"unsupported_op_rx",
-	"pause_ctrl_rx",
-	"pause_ctrl_tx",
-
-	/* RFC2863 counters */
-	"in_octets",
-	"in_ucast_pkts",
-	"in_discards",
-	"in_errors",
-	"in_unknown_protos",
-	"out_octets",
-	"out_ucast_pkts",
-	"out_discards",
-	"out_errors",
-	"in_multicast_pkts",
-	"in_broadcast_pkts",
-	"out_multicast_pkts",
-	"out_broadcast_pkts",
-
-	/* RFC2819 counters */
-	"drop_events",
-	"octets",
-	"pkts",
-	"broadcast_pkts",
-	"multicast_pkts",
-	"crc_align_errors",
-	"undersize_pkts",
-	"oversize_pkts",
-	"fragments",
-	"jabbers",
-	"collisions",
-	"p64octets",
-	"p65to127octets",
-	"p128to255octets",
-	"p256to511octets",
-	"p512to1023octets",
-	"p1024to1518octets",
-	"p1519to2047octets",
-	"p2048to4095octets",
-	"p4096to8191octets",
-	"p8192to10239octets",
-};
-
-#define NUM_IEEE_802_3_COUNTERS		19
-#define NUM_RFC_2863_COUNTERS		13
-#define NUM_RFC_2819_COUNTERS		21
-#define NUM_PPORT_COUNTERS		(NUM_IEEE_802_3_COUNTERS + \
-					 NUM_RFC_2863_COUNTERS + \
-					 NUM_RFC_2819_COUNTERS)
-
-struct mlx5e_pport_stats {
-	__be64 IEEE_802_3_counters[NUM_IEEE_802_3_COUNTERS];
-	__be64 RFC_2863_counters[NUM_RFC_2863_COUNTERS];
-	__be64 RFC_2819_counters[NUM_RFC_2819_COUNTERS];
-};
-
-static const char qcounter_stats_strings[][ETH_GSTRING_LEN] = {
-	"rx_out_of_buffer",
-};
-
-struct mlx5e_qcounter_stats {
-	u32 rx_out_of_buffer;
-#define NUM_Q_COUNTERS 1
-};
-
-static const char rq_stats_strings[][ETH_GSTRING_LEN] = {
-	"packets",
-	"bytes",
-	"csum_none",
-	"csum_sw",
-	"lro_packets",
-	"lro_bytes",
-	"wqe_err",
-	"mpwqe_filler",
-	"mpwqe_frag",
-	"buff_alloc_err",
-};
-
-struct mlx5e_rq_stats {
-	u64 packets;
-	u64 bytes;
-	u64 csum_none;
-	u64 csum_sw;
-	u64 lro_packets;
-	u64 lro_bytes;
-	u64 wqe_err;
-	u64 mpwqe_filler;
-	u64 mpwqe_frag;
-	u64 buff_alloc_err;
-#define NUM_RQ_STATS 10
-};
-
-static const char sq_stats_strings[][ETH_GSTRING_LEN] = {
-	"packets",
-	"bytes",
-	"tso_packets",
-	"tso_bytes",
-	"tso_inner_packets",
-	"tso_inner_bytes",
-	"csum_offload_inner",
-	"nop",
-	"csum_offload_none",
-	"stopped",
-	"wake",
-	"dropped",
-};
-
-struct mlx5e_sq_stats {
-	/* commonly accessed in data path */
-	u64 packets;
-	u64 bytes;
-	u64 tso_packets;
-	u64 tso_bytes;
-	u64 tso_inner_packets;
-	u64 tso_inner_bytes;
-	u64 csum_offload_inner;
-	u64 nop;
-	/* less likely accessed in data path */
-	u64 csum_offload_none;
-	u64 stopped;
-	u64 wake;
-	u64 dropped;
-#define NUM_SQ_STATS 12
-};
-
-struct mlx5e_stats {
-	struct mlx5e_vport_stats   vport;
-	struct mlx5e_pport_stats   pport;
-	struct mlx5e_qcounter_stats qcnt;
-};
-
 struct mlx5e_params {
 	u8  log_sq_size;
 	u8  rq_wq_type;

commit 6e4c21894673baabdbef03c3ac2458a28246128b
Author: Rana Shahout <ranas@mellanox.com>
Date:   Fri Apr 22 00:33:01 2016 +0300

    net/mlx5e: Fix MLX5E_100BASE_T define
    
    Bit 25 of eth_proto_capability in PTYS register is
    1000Base-TT and not 100Base-T.
    
    Fixes: f62b8bb8f2d3 ('net/mlx5: Extend mlx5_core to
    support ConnectX-4 Ethernet functionality')
    Signed-off-by: Rana Shahout <ranas@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 879e6276c473..e80ce94b5dcf 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -609,7 +609,7 @@ enum mlx5e_link_mode {
 	MLX5E_100GBASE_KR4	 = 22,
 	MLX5E_100GBASE_LR4	 = 23,
 	MLX5E_100BASE_TX	 = 24,
-	MLX5E_100BASE_T		 = 25,
+	MLX5E_1000BASE_T	 = 25,
 	MLX5E_10GBASE_T		 = 26,
 	MLX5E_25GBASE_CR	 = 27,
 	MLX5E_25GBASE_KR	 = 28,

commit 54984407564ef6b35488f52654f828c17b9d6fa8
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Wed Apr 20 22:02:19 2016 +0300

    net/mlx5e: Add ethtool counter for RX buffer allocation failures
    
    Counts the number of RX buffer allocation failures and shows it
    in ethtool statistics.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 303e6cdf9fcd..6e24e821a1d8 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -189,6 +189,7 @@ static const char vport_strings[][ETH_GSTRING_LEN] = {
 	"rx_wqe_err",
 	"rx_mpwqe_filler",
 	"rx_mpwqe_frag",
+	"rx_buff_alloc_err",
 };
 
 struct mlx5e_vport_stats {
@@ -232,8 +233,9 @@ struct mlx5e_vport_stats {
 	u64 rx_wqe_err;
 	u64 rx_mpwqe_filler;
 	u64 rx_mpwqe_frag;
+	u64 rx_buff_alloc_err;
 
-#define NUM_VPORT_COUNTERS     37
+#define NUM_VPORT_COUNTERS     38
 };
 
 static const char pport_strings[][ETH_GSTRING_LEN] = {
@@ -329,6 +331,7 @@ static const char rq_stats_strings[][ETH_GSTRING_LEN] = {
 	"wqe_err",
 	"mpwqe_filler",
 	"mpwqe_frag",
+	"buff_alloc_err",
 };
 
 struct mlx5e_rq_stats {
@@ -341,7 +344,8 @@ struct mlx5e_rq_stats {
 	u64 wqe_err;
 	u64 mpwqe_filler;
 	u64 mpwqe_frag;
-#define NUM_RQ_STATS 9
+	u64 buff_alloc_err;
+#define NUM_RQ_STATS 10
 };
 
 static const char sq_stats_strings[][ETH_GSTRING_LEN] = {

commit c5adb96f6c4a22aceff2e8220612c5b9239ffeb2
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Wed Apr 20 22:02:16 2016 +0300

    net/mlx5e: Use napi_alloc_skb for RX SKB allocations
    
    Instead of netdev_alloc_skb, we use the napi_alloc_skb function
    which is designated to allocate skbuff's for RX in a
    channel-specific NAPI instance, and implies the IP packet alignment.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index c99fdff74c97..303e6cdf9fcd 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -93,7 +93,6 @@
 #define MLX5E_SQ_BF_BUDGET             16
 
 #define MLX5E_NUM_MAIN_GROUPS 9
-#define MLX5E_NET_IP_ALIGN 2
 
 static inline u16 mlx5_min_rx_wqes(int wq_type, u32 wq_size)
 {

commit bc77b240b3c57236cdcc08d64ca390655d3a16ff
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Wed Apr 20 22:02:15 2016 +0300

    net/mlx5e: Add fragmented memory support for RX multi packet WQE
    
    If the allocation of a linear (physically continuous) MPWQE fails,
    we allocate a fragmented MPWQE.
    
    This is implemented via device's UMR (User Memory Registration)
    which allows to register multiple memory fragments into ConnectX
    hardware as a continuous buffer.
    UMR registration is an asynchronous operation and is done via
    ICO SQs.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index a757fcf19332..c99fdff74c97 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -72,6 +72,9 @@
 #define MLX5_MPWRQ_PAGES_PER_WQE		BIT(MLX5_MPWRQ_WQE_PAGE_ORDER)
 #define MLX5_MPWRQ_STRIDES_PER_PAGE		(MLX5_MPWRQ_NUM_STRIDES >> \
 						 MLX5_MPWRQ_WQE_PAGE_ORDER)
+#define MLX5_CHANNEL_MAX_NUM_MTTS (ALIGN(MLX5_MPWRQ_PAGES_PER_WQE, 8) * \
+				   BIT(MLX5E_PARAMS_MAXIMUM_LOG_RQ_SIZE_MPW))
+#define MLX5_UMR_ALIGN				(2048)
 #define MLX5_MPWRQ_SMALL_PACKET_THRESHOLD	(128)
 
 #define MLX5E_PARAMS_DEFAULT_LRO_WQE_SZ                 (64 * 1024)
@@ -134,6 +137,13 @@ struct mlx5e_rx_wqe {
 	struct mlx5_wqe_data_seg      data;
 };
 
+struct mlx5e_umr_wqe {
+	struct mlx5_wqe_ctrl_seg       ctrl;
+	struct mlx5_wqe_umr_ctrl_seg   uctrl;
+	struct mlx5_mkey_seg           mkc;
+	struct mlx5_wqe_data_seg       data;
+};
+
 #ifdef CONFIG_MLX5_CORE_EN_DCB
 #define MLX5E_MAX_BW_ALLOC 100 /* Max percentage of BW allocation */
 #define MLX5E_MIN_BW_ALLOC 1   /* Min percentage of BW allocation */
@@ -179,6 +189,7 @@ static const char vport_strings[][ETH_GSTRING_LEN] = {
 	"tx_queue_dropped",
 	"rx_wqe_err",
 	"rx_mpwqe_filler",
+	"rx_mpwqe_frag",
 };
 
 struct mlx5e_vport_stats {
@@ -221,8 +232,9 @@ struct mlx5e_vport_stats {
 	u64 tx_queue_dropped;
 	u64 rx_wqe_err;
 	u64 rx_mpwqe_filler;
+	u64 rx_mpwqe_frag;
 
-#define NUM_VPORT_COUNTERS     36
+#define NUM_VPORT_COUNTERS     37
 };
 
 static const char pport_strings[][ETH_GSTRING_LEN] = {
@@ -317,6 +329,7 @@ static const char rq_stats_strings[][ETH_GSTRING_LEN] = {
 	"lro_bytes",
 	"wqe_err",
 	"mpwqe_filler",
+	"mpwqe_frag",
 };
 
 struct mlx5e_rq_stats {
@@ -328,7 +341,8 @@ struct mlx5e_rq_stats {
 	u64 lro_bytes;
 	u64 wqe_err;
 	u64 mpwqe_filler;
-#define NUM_RQ_STATS 8
+	u64 mpwqe_frag;
+#define NUM_RQ_STATS 9
 };
 
 static const char sq_stats_strings[][ETH_GSTRING_LEN] = {
@@ -407,6 +421,7 @@ struct mlx5e_tstamp {
 
 enum {
 	MLX5E_RQ_STATE_POST_WQES_ENABLE,
+	MLX5E_RQ_STATE_UMR_WQE_IN_PROGRESS,
 };
 
 struct mlx5e_cq {
@@ -434,18 +449,14 @@ struct mlx5e_dma_info {
 	dma_addr_t	addr;
 };
 
-struct mlx5e_mpw_info {
-	struct mlx5e_dma_info dma_info;
-	u16 consumed_strides;
-	u16 skbs_frags[MLX5_MPWRQ_PAGES_PER_WQE];
-};
-
 struct mlx5e_rq {
 	/* data path */
 	struct mlx5_wq_ll      wq;
 	u32                    wqe_sz;
 	struct sk_buff       **skb;
 	struct mlx5e_mpw_info *wqe_info;
+	__be32                 mkey_be;
+	__be32                 umr_mkey_be;
 
 	struct device         *pdev;
 	struct net_device     *netdev;
@@ -466,6 +477,36 @@ struct mlx5e_rq {
 	struct mlx5e_priv     *priv;
 } ____cacheline_aligned_in_smp;
 
+struct mlx5e_umr_dma_info {
+	__be64                *mtt;
+	__be64                *mtt_no_align;
+	dma_addr_t             mtt_addr;
+	struct mlx5e_dma_info *dma_info;
+};
+
+struct mlx5e_mpw_info {
+	union {
+		struct mlx5e_dma_info     dma_info;
+		struct mlx5e_umr_dma_info umr;
+	};
+	u16 consumed_strides;
+	u16 skbs_frags[MLX5_MPWRQ_PAGES_PER_WQE];
+
+	void (*dma_pre_sync)(struct device *pdev,
+			     struct mlx5e_mpw_info *wi,
+			     u32 wqe_offset, u32 len);
+	void (*add_skb_frag)(struct device *pdev,
+			     struct sk_buff *skb,
+			     struct mlx5e_mpw_info *wi,
+			     u32 page_idx, u32 frag_offset, u32 len);
+	void (*copy_skb_header)(struct device *pdev,
+				struct sk_buff *skb,
+				struct mlx5e_mpw_info *wi,
+				u32 page_idx, u32 offset,
+				u32 headlen);
+	void (*free_wqe)(struct mlx5e_rq *rq, struct mlx5e_mpw_info *wi);
+};
+
 struct mlx5e_tx_wqe_info {
 	u32 num_bytes;
 	u8  num_wqebbs;
@@ -658,6 +699,7 @@ struct mlx5e_priv {
 	u32                        pdn;
 	u32                        tdn;
 	struct mlx5_core_mkey      mkey;
+	struct mlx5_core_mkey      umr_mkey;
 	struct mlx5e_rq            drop_rq;
 
 	struct mlx5e_channel     **channel;
@@ -730,6 +772,21 @@ void mlx5e_handle_rx_cqe_mpwrq(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe);
 bool mlx5e_post_rx_wqes(struct mlx5e_rq *rq);
 int mlx5e_alloc_rx_wqe(struct mlx5e_rq *rq, struct mlx5e_rx_wqe *wqe, u16 ix);
 int mlx5e_alloc_rx_mpwqe(struct mlx5e_rq *rq, struct mlx5e_rx_wqe *wqe, u16 ix);
+void mlx5e_post_rx_fragmented_mpwqe(struct mlx5e_rq *rq);
+void mlx5e_complete_rx_linear_mpwqe(struct mlx5e_rq *rq,
+				    struct mlx5_cqe64 *cqe,
+				    u16 byte_cnt,
+				    struct mlx5e_mpw_info *wi,
+				    struct sk_buff *skb);
+void mlx5e_complete_rx_fragmented_mpwqe(struct mlx5e_rq *rq,
+					struct mlx5_cqe64 *cqe,
+					u16 byte_cnt,
+					struct mlx5e_mpw_info *wi,
+					struct sk_buff *skb);
+void mlx5e_free_rx_linear_mpwqe(struct mlx5e_rq *rq,
+				struct mlx5e_mpw_info *wi);
+void mlx5e_free_rx_fragmented_mpwqe(struct mlx5e_rq *rq,
+				    struct mlx5e_mpw_info *wi);
 struct mlx5_cqe64 *mlx5e_get_cqe(struct mlx5e_cq *cq);
 
 void mlx5e_update_stats(struct mlx5e_priv *priv);
@@ -763,7 +820,7 @@ void mlx5e_build_default_indir_rqt(struct mlx5_core_dev *mdev,
 				   int num_channels);
 
 static inline void mlx5e_tx_notify_hw(struct mlx5e_sq *sq,
-				      struct mlx5e_tx_wqe *wqe, int bf_sz)
+				      struct mlx5_wqe_ctrl_seg *ctrl, int bf_sz)
 {
 	u16 ofst = MLX5_BF_OFFSET + sq->bf_offset;
 
@@ -777,9 +834,9 @@ static inline void mlx5e_tx_notify_hw(struct mlx5e_sq *sq,
 	 */
 	wmb();
 	if (bf_sz)
-		__iowrite64_copy(sq->uar_map + ofst, &wqe->ctrl, bf_sz);
+		__iowrite64_copy(sq->uar_map + ofst, ctrl, bf_sz);
 	else
-		mlx5_write64((__be32 *)&wqe->ctrl, sq->uar_map + ofst, NULL);
+		mlx5_write64((__be32 *)ctrl, sq->uar_map + ofst, NULL);
 	/* flush the write-combining mapped buffer */
 	wmb();
 
@@ -800,6 +857,11 @@ static inline int mlx5e_get_max_num_channels(struct mlx5_core_dev *mdev)
 		     MLX5E_MAX_NUM_CHANNELS);
 }
 
+static inline int mlx5e_get_mtt_octw(int npages)
+{
+	return ALIGN(npages, 8) / 2;
+}
+
 extern const struct ethtool_ops mlx5e_ethtool_ops;
 #ifdef CONFIG_MLX5_CORE_EN_DCB
 extern const struct dcbnl_rtnl_ops mlx5e_dcbnl_ops;

commit d3c9bc2743dc95b273ed0e6a3394a71ca314813c
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Wed Apr 20 22:02:14 2016 +0300

    net/mlx5e: Added ICO SQs
    
    Added ICO (Internal Control Operations) SQ per channel to be used
    for driver internal operations such as memory registration for
    fragmented memory and nop requests upon ifconfig up.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index f519148d7dcc..a757fcf19332 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -488,6 +488,11 @@ enum {
 	MLX5E_SQ_STATE_BF_ENABLE,
 };
 
+struct mlx5e_ico_wqe_info {
+	u8  opcode;
+	u8  num_wqebbs;
+};
+
 struct mlx5e_sq {
 	/* data path */
 
@@ -529,6 +534,7 @@ struct mlx5e_sq {
 	struct mlx5_uar            uar;
 	struct mlx5e_channel      *channel;
 	int                        tc;
+	struct mlx5e_ico_wqe_info *ico_wqe_info;
 } ____cacheline_aligned_in_smp;
 
 static inline bool mlx5e_sq_has_room_for(struct mlx5e_sq *sq, u16 n)
@@ -545,6 +551,7 @@ struct mlx5e_channel {
 	/* data path */
 	struct mlx5e_rq            rq;
 	struct mlx5e_sq            sq[MLX5E_MAX_NUM_TC];
+	struct mlx5e_sq            icosq;   /* internal control operations */
 	struct napi_struct         napi;
 	struct device             *pdev;
 	struct net_device         *netdev;

commit 461017cb006aa1b39b0f647ae0ee2d9d84eef05b
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Wed Apr 20 22:02:13 2016 +0300

    net/mlx5e: Support RX multi-packet WQE (Striding RQ)
    
    Introduce the feature of multi-packet WQE (RX Work Queue Element)
    referred to as (MPWQE or Striding RQ), in which WQEs are larger
    and serve multiple packets each.
    
    Every WQE consists of many strides of the same size, every received
    packet is aligned to a beginning of a stride and is written to
    consecutive strides within a WQE.
    
    In the regular approach, each regular WQE is big enough to be capable
    of serving one received packet of any size up to MTU or 64K in case of
    device LRO is enabled, making it very wasteful when dealing with
    small packets or device LRO is enabled.
    
    For its flexibility, MPWQE allows a better memory utilization
    (implying improvements in CPU utilization and packet rate) as packets
    consume strides according to their size, preserving the rest of
    the WQE to be available for other packets.
    
    MPWQE default configuration:
            Num of WQEs     = 16
            Strides Per WQE = 2048
            Stride Size     = 64 byte
    
    The default WQEs memory footprint went from 1024*mtu (~1.5MB) to
    16 * 2048 * 64 = 2MB per ring.
    However, HW LRO can now be supported at no additional cost in memory
    footprint, and hence we turn it on by default and get an even better
    performance.
    
    Performance tested on ConnectX4-Lx 50G.
    To isolate the feature under test, the numbers below were measured with
    HW LRO turned off. We verified that the performance just improves when
    LRO is turned back on.
    
    * Netperf single TCP stream:
    - BW raised by 10-15% for representative packet sizes:
      default, 64B, 1024B, 1478B, 65536B.
    
    * Netperf multi TCP stream:
    - No degradation, line rate reached.
    
    * Pktgen: packet rate raised by 2-10% for traffic of different message
    sizes: 64B, 128B, 256B, 1024B, and 1500B.
    
    * Pktgen: packet loss in bursts of small messages (64byte),
    single stream:
    - | num packets | packets loss before | packets loss after
      |     2K      |       ~ 1K          |       0
      |     8K      |       ~ 6K          |       0
      |     16K     |       ~13K          |       0
      |     32K     |       ~28K          |       0
      |     64K     |       ~57K          |     ~24K
    
    As expected as the driver can receive as many small packets (<=64B) as
    the number of total strides in the ring (default = 2048 * 16) vs. 1024
    (default ring size regardless of packets size) before this feature.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Achiad Shochat <achiad@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 61e249d8d7f8..f519148d7dcc 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -57,12 +57,30 @@
 #define MLX5E_PARAMS_DEFAULT_LOG_RQ_SIZE                0xa
 #define MLX5E_PARAMS_MAXIMUM_LOG_RQ_SIZE                0xd
 
+#define MLX5E_PARAMS_MINIMUM_LOG_RQ_SIZE_MPW            0x1
+#define MLX5E_PARAMS_DEFAULT_LOG_RQ_SIZE_MPW            0x4
+#define MLX5E_PARAMS_MAXIMUM_LOG_RQ_SIZE_MPW            0x6
+
+#define MLX5_MPWRQ_LOG_NUM_STRIDES		11 /* >= 9, HW restriction */
+#define MLX5_MPWRQ_LOG_STRIDE_SIZE		6  /* >= 6, HW restriction */
+#define MLX5_MPWRQ_NUM_STRIDES			BIT(MLX5_MPWRQ_LOG_NUM_STRIDES)
+#define MLX5_MPWRQ_STRIDE_SIZE			BIT(MLX5_MPWRQ_LOG_STRIDE_SIZE)
+#define MLX5_MPWRQ_LOG_WQE_SZ			(MLX5_MPWRQ_LOG_NUM_STRIDES +\
+						 MLX5_MPWRQ_LOG_STRIDE_SIZE)
+#define MLX5_MPWRQ_WQE_PAGE_ORDER  (MLX5_MPWRQ_LOG_WQE_SZ - PAGE_SHIFT > 0 ? \
+				    MLX5_MPWRQ_LOG_WQE_SZ - PAGE_SHIFT : 0)
+#define MLX5_MPWRQ_PAGES_PER_WQE		BIT(MLX5_MPWRQ_WQE_PAGE_ORDER)
+#define MLX5_MPWRQ_STRIDES_PER_PAGE		(MLX5_MPWRQ_NUM_STRIDES >> \
+						 MLX5_MPWRQ_WQE_PAGE_ORDER)
+#define MLX5_MPWRQ_SMALL_PACKET_THRESHOLD	(128)
+
 #define MLX5E_PARAMS_DEFAULT_LRO_WQE_SZ                 (64 * 1024)
 #define MLX5E_PARAMS_DEFAULT_RX_CQ_MODERATION_USEC      0x10
 #define MLX5E_PARAMS_DEFAULT_RX_CQ_MODERATION_PKTS      0x20
 #define MLX5E_PARAMS_DEFAULT_TX_CQ_MODERATION_USEC      0x10
 #define MLX5E_PARAMS_DEFAULT_TX_CQ_MODERATION_PKTS      0x20
 #define MLX5E_PARAMS_DEFAULT_MIN_RX_WQES                0x80
+#define MLX5E_PARAMS_DEFAULT_MIN_RX_WQES_MPW            0x2
 
 #define MLX5E_LOG_INDIR_RQT_SIZE       0x7
 #define MLX5E_INDIR_RQT_SIZE           BIT(MLX5E_LOG_INDIR_RQT_SIZE)
@@ -74,6 +92,38 @@
 #define MLX5E_NUM_MAIN_GROUPS 9
 #define MLX5E_NET_IP_ALIGN 2
 
+static inline u16 mlx5_min_rx_wqes(int wq_type, u32 wq_size)
+{
+	switch (wq_type) {
+	case MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ:
+		return min_t(u16, MLX5E_PARAMS_DEFAULT_MIN_RX_WQES_MPW,
+			     wq_size / 2);
+	default:
+		return min_t(u16, MLX5E_PARAMS_DEFAULT_MIN_RX_WQES,
+			     wq_size / 2);
+	}
+}
+
+static inline int mlx5_min_log_rq_size(int wq_type)
+{
+	switch (wq_type) {
+	case MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ:
+		return MLX5E_PARAMS_MINIMUM_LOG_RQ_SIZE_MPW;
+	default:
+		return MLX5E_PARAMS_MINIMUM_LOG_RQ_SIZE;
+	}
+}
+
+static inline int mlx5_max_log_rq_size(int wq_type)
+{
+	switch (wq_type) {
+	case MLX5_WQ_TYPE_LINKED_LIST_STRIDING_RQ:
+		return MLX5E_PARAMS_MAXIMUM_LOG_RQ_SIZE_MPW;
+	default:
+		return MLX5E_PARAMS_MAXIMUM_LOG_RQ_SIZE;
+	}
+}
+
 struct mlx5e_tx_wqe {
 	struct mlx5_wqe_ctrl_seg ctrl;
 	struct mlx5_wqe_eth_seg  eth;
@@ -128,6 +178,7 @@ static const char vport_strings[][ETH_GSTRING_LEN] = {
 	"tx_queue_wake",
 	"tx_queue_dropped",
 	"rx_wqe_err",
+	"rx_mpwqe_filler",
 };
 
 struct mlx5e_vport_stats {
@@ -169,8 +220,9 @@ struct mlx5e_vport_stats {
 	u64 tx_queue_wake;
 	u64 tx_queue_dropped;
 	u64 rx_wqe_err;
+	u64 rx_mpwqe_filler;
 
-#define NUM_VPORT_COUNTERS     35
+#define NUM_VPORT_COUNTERS     36
 };
 
 static const char pport_strings[][ETH_GSTRING_LEN] = {
@@ -263,7 +315,8 @@ static const char rq_stats_strings[][ETH_GSTRING_LEN] = {
 	"csum_sw",
 	"lro_packets",
 	"lro_bytes",
-	"wqe_err"
+	"wqe_err",
+	"mpwqe_filler",
 };
 
 struct mlx5e_rq_stats {
@@ -274,7 +327,8 @@ struct mlx5e_rq_stats {
 	u64 lro_packets;
 	u64 lro_bytes;
 	u64 wqe_err;
-#define NUM_RQ_STATS 7
+	u64 mpwqe_filler;
+#define NUM_RQ_STATS 8
 };
 
 static const char sq_stats_strings[][ETH_GSTRING_LEN] = {
@@ -318,6 +372,7 @@ struct mlx5e_stats {
 
 struct mlx5e_params {
 	u8  log_sq_size;
+	u8  rq_wq_type;
 	u8  log_rq_size;
 	u16 num_channels;
 	u8  num_tc;
@@ -374,11 +429,23 @@ typedef void (*mlx5e_fp_handle_rx_cqe)(struct mlx5e_rq *rq,
 typedef int (*mlx5e_fp_alloc_wqe)(struct mlx5e_rq *rq, struct mlx5e_rx_wqe *wqe,
 				  u16 ix);
 
+struct mlx5e_dma_info {
+	struct page	*page;
+	dma_addr_t	addr;
+};
+
+struct mlx5e_mpw_info {
+	struct mlx5e_dma_info dma_info;
+	u16 consumed_strides;
+	u16 skbs_frags[MLX5_MPWRQ_PAGES_PER_WQE];
+};
+
 struct mlx5e_rq {
 	/* data path */
 	struct mlx5_wq_ll      wq;
 	u32                    wqe_sz;
 	struct sk_buff       **skb;
+	struct mlx5e_mpw_info *wqe_info;
 
 	struct device         *pdev;
 	struct net_device     *netdev;
@@ -393,6 +460,7 @@ struct mlx5e_rq {
 
 	/* control */
 	struct mlx5_wq_ctrl    wq_ctrl;
+	u8                     wq_type;
 	u32                    rqn;
 	struct mlx5e_channel  *channel;
 	struct mlx5e_priv     *priv;
@@ -649,9 +717,12 @@ void mlx5e_cq_error_event(struct mlx5_core_cq *mcq, enum mlx5_event event);
 int mlx5e_napi_poll(struct napi_struct *napi, int budget);
 bool mlx5e_poll_tx_cq(struct mlx5e_cq *cq, int napi_budget);
 int mlx5e_poll_rx_cq(struct mlx5e_cq *cq, int budget);
+
 void mlx5e_handle_rx_cqe(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe);
+void mlx5e_handle_rx_cqe_mpwrq(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe);
 bool mlx5e_post_rx_wqes(struct mlx5e_rq *rq);
 int mlx5e_alloc_rx_wqe(struct mlx5e_rq *rq, struct mlx5e_rx_wqe *wqe, u16 ix);
+int mlx5e_alloc_rx_mpwqe(struct mlx5e_rq *rq, struct mlx5e_rx_wqe *wqe, u16 ix);
 struct mlx5_cqe64 *mlx5e_get_cqe(struct mlx5e_cq *cq);
 
 void mlx5e_update_stats(struct mlx5e_priv *priv);

commit 2f48af128d9aa64dd4e8c6fe97491b0bde3681b2
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Wed Apr 20 22:02:12 2016 +0300

    net/mlx5e: Use function pointers for RX data path handling
    
    In preparation for Striding RQ feature, which will need its own
    RX handlers.
    This patch does not change any functionality.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Achiad Shochat <achiad@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 7f19644689f2..61e249d8d7f8 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -72,6 +72,17 @@
 #define MLX5E_SQ_BF_BUDGET             16
 
 #define MLX5E_NUM_MAIN_GROUPS 9
+#define MLX5E_NET_IP_ALIGN 2
+
+struct mlx5e_tx_wqe {
+	struct mlx5_wqe_ctrl_seg ctrl;
+	struct mlx5_wqe_eth_seg  eth;
+};
+
+struct mlx5e_rx_wqe {
+	struct mlx5_wqe_srq_next_seg  next;
+	struct mlx5_wqe_data_seg      data;
+};
 
 #ifdef CONFIG_MLX5_CORE_EN_DCB
 #define MLX5E_MAX_BW_ALLOC 100 /* Max percentage of BW allocation */
@@ -357,6 +368,12 @@ struct mlx5e_cq {
 	struct mlx5_wq_ctrl        wq_ctrl;
 } ____cacheline_aligned_in_smp;
 
+struct mlx5e_rq;
+typedef void (*mlx5e_fp_handle_rx_cqe)(struct mlx5e_rq *rq,
+				       struct mlx5_cqe64 *cqe);
+typedef int (*mlx5e_fp_alloc_wqe)(struct mlx5e_rq *rq, struct mlx5e_rx_wqe *wqe,
+				  u16 ix);
+
 struct mlx5e_rq {
 	/* data path */
 	struct mlx5_wq_ll      wq;
@@ -368,6 +385,8 @@ struct mlx5e_rq {
 	struct mlx5e_tstamp   *tstamp;
 	struct mlx5e_rq_stats  stats;
 	struct mlx5e_cq        cq;
+	mlx5e_fp_handle_rx_cqe handle_rx_cqe;
+	mlx5e_fp_alloc_wqe     alloc_wqe;
 
 	unsigned long          state;
 	int                    ix;
@@ -588,18 +607,6 @@ struct mlx5e_priv {
 	u16 q_counter;
 };
 
-#define MLX5E_NET_IP_ALIGN 2
-
-struct mlx5e_tx_wqe {
-	struct mlx5_wqe_ctrl_seg ctrl;
-	struct mlx5_wqe_eth_seg  eth;
-};
-
-struct mlx5e_rx_wqe {
-	struct mlx5_wqe_srq_next_seg  next;
-	struct mlx5_wqe_data_seg      data;
-};
-
 enum mlx5e_link_mode {
 	MLX5E_1000BASE_CX_SGMII	 = 0,
 	MLX5E_1000BASE_KX	 = 1,
@@ -642,7 +649,9 @@ void mlx5e_cq_error_event(struct mlx5_core_cq *mcq, enum mlx5_event event);
 int mlx5e_napi_poll(struct napi_struct *napi, int budget);
 bool mlx5e_poll_tx_cq(struct mlx5e_cq *cq, int napi_budget);
 int mlx5e_poll_rx_cq(struct mlx5e_cq *cq, int budget);
+void mlx5e_handle_rx_cqe(struct mlx5e_rq *rq, struct mlx5_cqe64 *cqe);
 bool mlx5e_post_rx_wqes(struct mlx5e_rq *rq);
+int mlx5e_alloc_rx_wqe(struct mlx5e_rq *rq, struct mlx5e_rx_wqe *wqe, u16 ix);
 struct mlx5_cqe64 *mlx5e_get_cqe(struct mlx5e_cq *cq);
 
 void mlx5e_update_stats(struct mlx5e_priv *priv);

commit d8c9660dac6287490ef450bc892593f05d364531
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Wed Apr 20 22:02:11 2016 +0300

    net/mlx5e: Use only close NUMA node for default RSS
    
    Distribute default RSS table uniformly over the rings of the
    close NUMA node, instead of all available channels.
    This way we enforce the preference of close rings over far ones.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index c4ddbe8501a7..7f19644689f2 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -671,7 +671,8 @@ void mlx5e_build_tir_ctx_hash(void *tirc, struct mlx5e_priv *priv);
 
 int mlx5e_open_locked(struct net_device *netdev);
 int mlx5e_close_locked(struct net_device *netdev);
-void mlx5e_build_default_indir_rqt(u32 *indirection_rqt, int len,
+void mlx5e_build_default_indir_rqt(struct mlx5_core_dev *mdev,
+				   u32 *indirection_rqt, int len,
 				   int num_channels);
 
 static inline void mlx5e_tx_notify_hw(struct mlx5e_sq *sq,

commit 593cf33829adfd3d5c75d42879cc42afded1b626
Author: Rana Shahout <ranas@mellanox.com>
Date:   Wed Apr 20 22:02:10 2016 +0300

    net/mlx5e: Allocate set of queue counters per netdev
    
    Connect all netdev RQs to this set of queue counters.
    Also, add an "rx_out_of_buffer" counter to ethtool,
    which indicates RX packet drops due to lack of receive
    buffers.
    
    Signed-off-by: Rana Shahout <ranas@mellanox.com>
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 879e6276c473..c4ddbe8501a7 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -236,6 +236,15 @@ struct mlx5e_pport_stats {
 	__be64 RFC_2819_counters[NUM_RFC_2819_COUNTERS];
 };
 
+static const char qcounter_stats_strings[][ETH_GSTRING_LEN] = {
+	"rx_out_of_buffer",
+};
+
+struct mlx5e_qcounter_stats {
+	u32 rx_out_of_buffer;
+#define NUM_Q_COUNTERS 1
+};
+
 static const char rq_stats_strings[][ETH_GSTRING_LEN] = {
 	"packets",
 	"bytes",
@@ -293,6 +302,7 @@ struct mlx5e_sq_stats {
 struct mlx5e_stats {
 	struct mlx5e_vport_stats   vport;
 	struct mlx5e_pport_stats   pport;
+	struct mlx5e_qcounter_stats qcnt;
 };
 
 struct mlx5e_params {
@@ -575,6 +585,7 @@ struct mlx5e_priv {
 	struct net_device         *netdev;
 	struct mlx5e_stats         stats;
 	struct mlx5e_tstamp        tstamp;
+	u16 q_counter;
 };
 
 #define MLX5E_NET_IP_ALIGN 2

commit 1200b6809dfd9d73bc4c7db76d288c35fa4b2ebe
Merge: 6b5f04b6cf8e fe30937b6535
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Mar 19 10:05:34 2016 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next
    
    Pull networking updates from David Miller:
     "Highlights:
    
       1) Support more Realtek wireless chips, from Jes Sorenson.
    
       2) New BPF types for per-cpu hash and arrap maps, from Alexei
          Starovoitov.
    
       3) Make several TCP sysctls per-namespace, from Nikolay Borisov.
    
       4) Allow the use of SO_REUSEPORT in order to do per-thread processing
       of incoming TCP/UDP connections.  The muxing can be done using a
       BPF program which hashes the incoming packet.  From Craig Gallek.
    
       5) Add a multiplexer for TCP streams, to provide a messaged based
          interface.  BPF programs can be used to determine the message
          boundaries.  From Tom Herbert.
    
       6) Add 802.1AE MACSEC support, from Sabrina Dubroca.
    
       7) Avoid factorial complexity when taking down an inetdev interface
          with lots of configured addresses.  We were doing things like
          traversing the entire address less for each address removed, and
          flushing the entire netfilter conntrack table for every address as
          well.
    
       8) Add and use SKB bulk free infrastructure, from Jesper Brouer.
    
       9) Allow offloading u32 classifiers to hardware, and implement for
          ixgbe, from John Fastabend.
    
      10) Allow configuring IRQ coalescing parameters on a per-queue basis,
          from Kan Liang.
    
      11) Extend ethtool so that larger link mode masks can be supported.
          From David Decotigny.
    
      12) Introduce devlink, which can be used to configure port link types
          (ethernet vs Infiniband, etc.), port splitting, and switch device
          level attributes as a whole.  From Jiri Pirko.
    
      13) Hardware offload support for flower classifiers, from Amir Vadai.
    
      14) Add "Local Checksum Offload".  Basically, for a tunneled packet
          the checksum of the outer header is 'constant' (because with the
          checksum field filled into the inner protocol header, the payload
          of the outer frame checksums to 'zero'), and we can take advantage
          of that in various ways.  From Edward Cree"
    
    * git://git.kernel.org/pub/scm/linux/kernel/git/davem/net-next: (1548 commits)
      bonding: fix bond_get_stats()
      net: bcmgenet: fix dma api length mismatch
      net/mlx4_core: Fix backward compatibility on VFs
      phy: mdio-thunder: Fix some Kconfig typos
      lan78xx: add ndo_get_stats64
      lan78xx: handle statistics counter rollover
      RDS: TCP: Remove unused constant
      RDS: TCP: Add sysctl tunables for sndbuf/rcvbuf on rds-tcp socket
      net: smc911x: convert pxa dma to dmaengine
      team: remove duplicate set of flag IFF_MULTICAST
      bonding: remove duplicate set of flag IFF_MULTICAST
      net: fix a comment typo
      ethernet: micrel: fix some error codes
      ip_tunnels, bpf: define IP_TUNNEL_OPTS_MAX and use it
      bpf, dst: add and use dst_tclassid helper
      bpf: make skb->tc_classid also readable
      net: mvneta: bm: clarify dependencies
      cls_bpf: reset class and reuse major in da
      ldmvsw: Checkpatch sunvnet.c and sunvnet_common.c
      ldmvsw: Add ldmvsw.c driver code
      ...

commit 9ea446352047d8350553250db51da2c73a610688
Merge: 9dffdb38d864 082eaa50838c
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Mar 18 09:39:22 2016 -0700

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/dledford/rdma
    
    Pull rdma updates from Doug Ledford:
     "Initial roundup of 4.6 merge window patches.
    
      This is the first of two pull requests.  It is the smaller request,
      but touches for more different things (this is everything but what is
      in or going into staging).  The pull request for the code in
      staging/rdma is on hold until after we decide what to do on the
      write/writev API issue and may be partially deferred until 4.7 as a
      result.
    
      Summary:
    
       - cxgb4 updates
       - nes updates
       - unification of iwarp portmapper code to core
       - add drain_cq API
       - various ib_core updates
       - minor ipoib updates
       - minor mlx4 updates
       - more significant mlx5 updates (including a minor merge conflict
         with net-next tree...merge is simple to resolve and Stephen's
         resolution was confirmed by Mellanox)
       - trivial net/9p rdma conversion
       - ocrdma RoCEv2 update
       - srpt updates"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/dledford/rdma: (85 commits)
      iwpm: crash fix for large connections test
      iw_cxgb3: support for iWARP port mapping
      iw_cxgb4: remove port mapper related code
      iw_nes: remove port mapper related code
      iwcm: common code for port mapper
      net/9p: convert to new CQ API
      IB/mlx5: Add support for don't trap rules
      net/mlx5_core: Introduce forward to next priority action
      net/mlx5_core: Create anchor of last flow table
      iser: Accept arbitrary sg lists mapping if the device supports it
      mlx5: Add arbitrary sg list support
      IB/core: Add arbitrary sg_list support
      IB/mlx5: Expose correct max_fast_reg_page_list_len
      IB/mlx5: Make coding style more consistent
      IB/mlx5: Convert UMR CQ to new CQ API
      IB/ocrdma: Skip using unneeded intermediate variable
      IB/ocrdma: Skip using unneeded intermediate variable
      IB/ocrdma: Delete unnecessary variable initialisations in 11 functions
      IB/core: Documentation fix in the MAD header file
      IB/core: trivial prink cleanup.
      ...

commit 8ec736e556e3340b4b4295c7567b0766d6629702
Author: Jesper Dangaard Brouer <brouer@redhat.com>
Date:   Fri Mar 11 09:44:17 2016 +0100

    mlx5: use napi_consume_skb API to get bulk free operations
    
    Bulk free of SKBs happen transparently by the API call napi_consume_skb().
    The napi budget parameter is needed by napi_consume_skb() to detect
    if called from netpoll.
    
    Signed-off-by: Jesper Dangaard Brouer <brouer@redhat.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 0f76d321030f..0cb4a093958b 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -629,7 +629,7 @@ netdev_tx_t mlx5e_xmit(struct sk_buff *skb, struct net_device *dev);
 void mlx5e_completion_event(struct mlx5_core_cq *mcq);
 void mlx5e_cq_error_event(struct mlx5_core_cq *mcq, enum mlx5_event event);
 int mlx5e_napi_poll(struct napi_struct *napi, int budget);
-bool mlx5e_poll_tx_cq(struct mlx5e_cq *cq);
+bool mlx5e_poll_tx_cq(struct mlx5e_cq *cq, int napi_budget);
 int mlx5e_poll_rx_cq(struct mlx5e_cq *cq, int budget);
 bool mlx5e_post_rx_wqes(struct mlx5e_rq *rq);
 struct mlx5_cqe64 *mlx5e_get_cqe(struct mlx5e_cq *cq);

commit e8f887ac6a45ecb7f881f278a3fed1fbf002ef0b
Author: Amir Vadai <amir@vadai.me>
Date:   Tue Mar 8 12:42:36 2016 +0200

    net/mlx5e: Introduce tc offload support
    
    Extend ndo_setup_tc() to support ingress tc offloading. Will be used by
    later patches to offload tc flower filter.
    
    Feature is off by default and could be enabled by issuing:
     # ethtool  -K eth0 hw-tc-offload on
    
    Offloads flow table is dynamically created when first filter is
    added.
    Rules are saved in a hash table that is maintained by the consumer (for
    example - the flower offload in the next patch).
    When last filter is removed and no filters exist in the hash table, the
    offload flow table is destroyed.
    
    Signed-off-by: Amir Vadai <amir@vadai.me>
    Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index dbc2fb89e067..0f76d321030f 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -43,6 +43,7 @@
 #include <linux/mlx5/port.h>
 #include <linux/mlx5/vport.h>
 #include <linux/mlx5/transobj.h>
+#include <linux/rhashtable.h>
 #include "wq.h"
 #include "mlx5_core.h"
 
@@ -527,8 +528,16 @@ struct mlx5e_flow_table {
 	struct mlx5_flow_group		**g;
 };
 
+struct mlx5e_tc_flow_table {
+	struct mlx5_flow_table		*t;
+
+	struct rhashtable_params        ht_params;
+	struct rhashtable               ht;
+};
+
 struct mlx5e_flow_tables {
 	struct mlx5_flow_namespace	*ns;
+	struct mlx5e_tc_flow_table	tc;
 	struct mlx5e_flow_table		vlan;
 	struct mlx5e_flow_table		main;
 };

commit 810813c47a564416f6306ae214e2661366c987a7
Merge: d66ab5144221 e2857b8f11a2
Author: David S. Miller <davem@davemloft.net>
Date:   Tue Mar 8 12:34:12 2016 -0500

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Several cases of overlapping changes, as well as one instance
    (vxlan) of a bug fix in 'net' overlapping with code movement
    in 'net-next'.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit b081da5ee1860d1874381a413a1b71d8f9f67e83
Author: Gal Pressman <galp@mellanox.com>
Date:   Mon Feb 29 21:17:14 2016 +0200

    net/mlx5e: Add rx/tx bytes software counters
    
    Sum up rx/tx bytes in software as we do for rx/tx packets, to be reported
    in upcoming statistics fix.
    
    Signed-off-by: Gal Pressman <galp@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index d0a57d53e125..5b1753233c5d 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -223,6 +223,7 @@ struct mlx5e_pport_stats {
 
 static const char rq_stats_strings[][ETH_GSTRING_LEN] = {
 	"packets",
+	"bytes",
 	"csum_none",
 	"csum_sw",
 	"lro_packets",
@@ -232,16 +233,18 @@ static const char rq_stats_strings[][ETH_GSTRING_LEN] = {
 
 struct mlx5e_rq_stats {
 	u64 packets;
+	u64 bytes;
 	u64 csum_none;
 	u64 csum_sw;
 	u64 lro_packets;
 	u64 lro_bytes;
 	u64 wqe_err;
-#define NUM_RQ_STATS 6
+#define NUM_RQ_STATS 7
 };
 
 static const char sq_stats_strings[][ETH_GSTRING_LEN] = {
 	"packets",
+	"bytes",
 	"tso_packets",
 	"tso_bytes",
 	"csum_offload_none",
@@ -253,6 +256,7 @@ static const char sq_stats_strings[][ETH_GSTRING_LEN] = {
 
 struct mlx5e_sq_stats {
 	u64 packets;
+	u64 bytes;
 	u64 tso_packets;
 	u64 tso_bytes;
 	u64 csum_offload_none;
@@ -260,7 +264,7 @@ struct mlx5e_sq_stats {
 	u64 wake;
 	u64 dropped;
 	u64 nop;
-#define NUM_SQ_STATS 8
+#define NUM_SQ_STATS 9
 };
 
 struct mlx5e_stats {

commit 85082dba0a5059c538cfa786d07f5ec5370d22fe
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Mon Feb 29 21:17:13 2016 +0200

    net/mlx5e: Correctly handle RSS indirection table when changing number of channels
    
    Upon changing num_channels, reset the RSS indirection table to
    match the new value.
    
    Fixes: 2d75b2bc8a8c ('net/mlx5e: Add ethtool RSS configuration options')
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 976bddbddb5d..d0a57d53e125 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -619,6 +619,8 @@ void mlx5e_build_tir_ctx_hash(void *tirc, struct mlx5e_priv *priv);
 
 int mlx5e_open_locked(struct net_device *netdev);
 int mlx5e_close_locked(struct net_device *netdev);
+void mlx5e_build_default_indir_rqt(u32 *indirection_rqt, int len,
+				   int num_channels);
 
 static inline void mlx5e_tx_notify_hw(struct mlx5e_sq *sq,
 				      struct mlx5e_tx_wqe *wqe, int bf_sz)

commit bdfc028de1b3cd59490d5413a5c87b0fa50040c2
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Mon Feb 29 21:17:12 2016 +0200

    net/mlx5e: Fix ethtool RX hash func configuration change
    
    We should modify TIRs explicitly to apply the new RSS configuration.
    The light ndo close/open calls do not "refresh" them.
    
    Fixes: 2d75b2bc8a8c ('net/mlx5e: Add ethtool RSS configuration options')
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 614a6022db25..976bddbddb5d 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -447,6 +447,8 @@ enum mlx5e_traffic_types {
 	MLX5E_NUM_TT,
 };
 
+#define IS_HASHING_TT(tt) (tt != MLX5E_TT_ANY)
+
 enum mlx5e_rqt_ix {
 	MLX5E_INDIRECTION_RQT,
 	MLX5E_SINGLE_RQ_RQT,
@@ -613,6 +615,7 @@ void mlx5e_enable_vlan_filter(struct mlx5e_priv *priv);
 void mlx5e_disable_vlan_filter(struct mlx5e_priv *priv);
 
 int mlx5e_redirect_rqt(struct mlx5e_priv *priv, enum mlx5e_rqt_ix rqt_ix);
+void mlx5e_build_tir_ctx_hash(void *tirc, struct mlx5e_priv *priv);
 
 int mlx5e_open_locked(struct net_device *netdev);
 int mlx5e_close_locked(struct net_device *netdev);

commit 59a7c2fd336eaafb030aac9c91ac21d136a99f33
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Mon Feb 29 21:17:09 2016 +0200

    net/mlx5e: Remove wrong poll CQ optimization
    
    With the MLX5E_CQ_HAS_CQES optimization flag, the following buggy
    flow might occur:
    - Suppose RX is always busy, TX has a single packet every second.
    - We poll a single TX cqe and clear its flag.
    - We never arm it again as RX is always busy.
    - TX CQ flag is never changed, and new TX cqes are not polled.
    
    We revert this optimization.
    
    Fixes: e586b3b0baee ('net/mlx5: Ethernet Datapath files')
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index aac071a7e830..614a6022db25 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -304,14 +304,9 @@ enum {
 	MLX5E_RQ_STATE_POST_WQES_ENABLE,
 };
 
-enum cq_flags {
-	MLX5E_CQ_HAS_CQES = 1,
-};
-
 struct mlx5e_cq {
 	/* data path - accessed per cqe */
 	struct mlx5_cqwq           wq;
-	unsigned long              flags;
 
 	/* data path - accessed per napi poll */
 	struct napi_struct        *napi;

commit 0ba422410bbf7081c3c7d7b2dcc10e9eb5cb46f7
Author: Moshe Lazer <moshel@mellanox.com>
Date:   Wed Mar 2 00:13:40 2016 +0200

    net/mlx5: Fix global UAR mapping
    
    Avoid double mapping of io mapped memory, Device page may be
    mapped to non-cached(NC) or to write-combining(WC).
    The code before this fix tries to map it both to WC and NC
    contrary to what stated in Intel's software developer manual.
    
    Here we remove the global WC mapping of all UARS
    "dev->priv.bf_mapping", since UAR mapping should be decided
    per UAR (e.g we want different mappings for EQs, CQs vs QPs).
    
    Caller will now have to choose whether to map via
    write-combining API or not.
    
    mlx5e SQs will choose write-combining in order to perform
    BlueFlame writes.
    
    Fixes: 88a85f99e51f ('TX latency optimization to save DMA reads')
    Signed-off-by: Moshe Lazer <moshel@mellanox.com>
    Reviewed-by: Achiad Shochat <achiad@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index b289660568cf..9c0e80e64b43 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -388,6 +388,7 @@ struct mlx5e_sq_dma {
 
 enum {
 	MLX5E_SQ_STATE_WAKE_TXQ_ENABLE,
+	MLX5E_SQ_STATE_BF_ENABLE,
 };
 
 struct mlx5e_sq {
@@ -416,7 +417,6 @@ struct mlx5e_sq {
 	struct mlx5_wq_cyc         wq;
 	u32                        dma_fifo_mask;
 	void __iomem              *uar_map;
-	void __iomem              *uar_bf_map;
 	struct netdev_queue       *txq;
 	u32                        sqn;
 	u16                        bf_buf_size;
@@ -664,16 +664,12 @@ static inline void mlx5e_tx_notify_hw(struct mlx5e_sq *sq,
 	 * doorbell
 	 */
 	wmb();
-
-	if (bf_sz) {
-		__iowrite64_copy(sq->uar_bf_map + ofst, &wqe->ctrl, bf_sz);
-
-		/* flush the write-combining mapped buffer */
-		wmb();
-
-	} else {
+	if (bf_sz)
+		__iowrite64_copy(sq->uar_map + ofst, &wqe->ctrl, bf_sz);
+	else
 		mlx5_write64((__be32 *)&wqe->ctrl, sq->uar_map + ofst, NULL);
-	}
+	/* flush the write-combining mapped buffer */
+	wmb();
 
 	sq->bf_offset ^= sq->bf_buf_size;
 }

commit c89fb18b657b07d6c98810182057f844e71d3eee
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Wed Mar 2 00:13:35 2016 +0200

    net/mlx5e: Move common case counters within sq_stats struct
    
    For data cache locality considerations, we moved the nop and
    csum_offload_inner within sq_stats struct as they are more
    commonly accessed in xmit path.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 451198444ace..b289660568cf 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -260,26 +260,28 @@ static const char sq_stats_strings[][ETH_GSTRING_LEN] = {
 	"tso_bytes",
 	"tso_inner_packets",
 	"tso_inner_bytes",
-	"csum_offload_none",
 	"csum_offload_inner",
+	"nop",
+	"csum_offload_none",
 	"stopped",
 	"wake",
 	"dropped",
-	"nop"
 };
 
 struct mlx5e_sq_stats {
+	/* commonly accessed in data path */
 	u64 packets;
 	u64 tso_packets;
 	u64 tso_bytes;
 	u64 tso_inner_packets;
 	u64 tso_inner_bytes;
-	u64 csum_offload_none;
 	u64 csum_offload_inner;
+	u64 nop;
+	/* less likely accessed in data path */
+	u64 csum_offload_none;
 	u64 stopped;
 	u64 wake;
 	u64 dropped;
-	u64 nop;
 #define NUM_SQ_STATS 11
 };
 

commit daa21560a29673f63263e907045f513a8d66dc33
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Wed Mar 2 00:13:32 2016 +0200

    net/mlx5e: Replace async events spinlock with synchronize_irq()
    
    We only need to flush the irq handler to make sure it does not
    queue a work into the global work queue after we start to flush it.
    So using synchronize_irq() is more appropriate than a spin lock.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 1dca3dcf90f5..451198444ace 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -555,7 +555,6 @@ struct mlx5e_priv {
 	struct mlx5e_vxlan_db      vxlan;
 
 	struct mlx5e_params        params;
-	spinlock_t                 async_events_spinlock; /* sync hw events */
 	struct work_struct         update_carrier_work;
 	struct work_struct         set_rx_mode_work;
 	struct delayed_work        update_stats_work;

commit a606b0f6691daf861482f8b77326f672238ffbfd
Author: Matan Barak <matanb@mellanox.com>
Date:   Mon Feb 29 18:05:28 2016 +0200

    net/mlx5: Refactor mlx5_core_mr to mkey
    
    Mlx5's mkey mechanism is also used for memory windows.
    The current code base uses MR (memory region) naming, which is
    inaccurate. Changing MR to mkey in order to represent its different
    usages more accurately.
    
    Signed-off-by: Matan Barak <matanb@mellanox.com>
    Reviewed-by: Yishai Hadas <yishaih@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index aac071a7e830..6ef0bfded2ba 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -515,7 +515,7 @@ struct mlx5e_priv {
 	struct mlx5_uar            cq_uar;
 	u32                        pdn;
 	u32                        tdn;
-	struct mlx5_core_mr        mr;
+	struct mlx5_core_mkey      mkey;
 	struct mlx5e_rq            drop_rq;
 
 	struct mlx5e_channel     **channel;

commit 89db09eb5979b74f1f50ee2fb2ce519ee8414c48
Author: Matthew Finlay <matt@mellanox.com>
Date:   Mon Feb 22 18:17:34 2016 +0200

    net/mlx5e: Add TX inner packet counters
    
    Add TSO and TX checksum counters for tunneled, inner packets
    
    Signed-off-by: Matthew Finlay <matt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index a700c5713226..1dca3dcf90f5 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -103,12 +103,15 @@ static const char vport_strings[][ETH_GSTRING_LEN] = {
 	/* SW counters */
 	"tso_packets",
 	"tso_bytes",
+	"tso_inner_packets",
+	"tso_inner_bytes",
 	"lro_packets",
 	"lro_bytes",
 	"rx_csum_good",
 	"rx_csum_none",
 	"rx_csum_sw",
 	"tx_csum_offload",
+	"tx_csum_inner",
 	"tx_queue_stopped",
 	"tx_queue_wake",
 	"tx_queue_dropped",
@@ -141,18 +144,21 @@ struct mlx5e_vport_stats {
 	/* SW counters */
 	u64 tso_packets;
 	u64 tso_bytes;
+	u64 tso_inner_packets;
+	u64 tso_inner_bytes;
 	u64 lro_packets;
 	u64 lro_bytes;
 	u64 rx_csum_good;
 	u64 rx_csum_none;
 	u64 rx_csum_sw;
 	u64 tx_csum_offload;
+	u64 tx_csum_inner;
 	u64 tx_queue_stopped;
 	u64 tx_queue_wake;
 	u64 tx_queue_dropped;
 	u64 rx_wqe_err;
 
-#define NUM_VPORT_COUNTERS     32
+#define NUM_VPORT_COUNTERS     35
 };
 
 static const char pport_strings[][ETH_GSTRING_LEN] = {
@@ -252,7 +258,10 @@ static const char sq_stats_strings[][ETH_GSTRING_LEN] = {
 	"packets",
 	"tso_packets",
 	"tso_bytes",
+	"tso_inner_packets",
+	"tso_inner_bytes",
 	"csum_offload_none",
+	"csum_offload_inner",
 	"stopped",
 	"wake",
 	"dropped",
@@ -263,12 +272,15 @@ struct mlx5e_sq_stats {
 	u64 packets;
 	u64 tso_packets;
 	u64 tso_bytes;
+	u64 tso_inner_packets;
+	u64 tso_inner_bytes;
 	u64 csum_offload_none;
+	u64 csum_offload_inner;
 	u64 stopped;
 	u64 wake;
 	u64 dropped;
 	u64 nop;
-#define NUM_SQ_STATS 8
+#define NUM_SQ_STATS 11
 };
 
 struct mlx5e_stats {

commit b3f63c3d5e2cbb9c800516ef47e32d8cb0cf237b
Author: Matthew Finlay <matt@mellanox.com>
Date:   Mon Feb 22 18:17:32 2016 +0200

    net/mlx5e: Add netdev support for VXLAN tunneling
    
    If a VXLAN udp dport is added to device it will:
    
       - Configure the hardware to offload the port (up to the max
         supported).
       - Advertise NETIF_F_GSO_UDP_TUNNEL and supported hw_enc_features.
    
    Signed-off-by: Matthew Finlay <matt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 786a2471ec0e..a700c5713226 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -501,6 +501,11 @@ struct mlx5e_vlan_db {
 	bool          filter_disabled;
 };
 
+struct mlx5e_vxlan_db {
+	spinlock_t			lock; /* protect vxlan table */
+	struct radix_tree_root		tree;
+};
+
 struct mlx5e_flow_table {
 	int num_groups;
 	struct mlx5_flow_table		*t;
@@ -535,6 +540,7 @@ struct mlx5e_priv {
 	struct mlx5e_flow_tables   fts;
 	struct mlx5e_eth_addr_db   eth_addr;
 	struct mlx5e_vlan_db       vlan;
+	struct mlx5e_vxlan_db      vxlan;
 
 	struct mlx5e_params        params;
 	spinlock_t                 async_events_spinlock; /* sync hw events */

commit 1afff42c062fe1d1f8d99423dddfb8ca5f100574
Author: Matthew Finlay <matt@mellanox.com>
Date:   Mon Feb 22 18:17:31 2016 +0200

    net/mlx5e: Protect en header file from redefinitions
    
    add ifndef to en.h. needed for upcoming vxlan patchset.
    
    Signed-off-by: Matthew Finlay <matt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index dfbc4e54efd4..786a2471ec0e 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -1,5 +1,5 @@
 /*
- * Copyright (c) 2015, Mellanox Technologies. All rights reserved.
+ * Copyright (c) 2015-2016, Mellanox Technologies. All rights reserved.
  *
  * This software is available to you under a choice of one of two
  * licenses.  You may choose to be licensed under the terms of the GNU
@@ -29,6 +29,8 @@
  * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
  * SOFTWARE.
  */
+#ifndef __MLX5_EN_H__
+#define __MLX5_EN_H__
 
 #include <linux/if_vlan.h>
 #include <linux/etherdevice.h>
@@ -678,3 +680,5 @@ int mlx5e_dcbnl_ieee_setets_core(struct mlx5e_priv *priv, struct ieee_ets *ets);
 #endif
 
 u16 mlx5e_get_max_inline_cap(struct mlx5_core_dev *mdev);
+
+#endif /* __MLX5_EN_H__ */

commit 08fb1dacdd763431436d648fd9dc4246e3d5517e
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Mon Feb 22 18:17:26 2016 +0200

    net/mlx5e: Support DCBNL IEEE ETS
    
    Support the ndo_setup_tc callback and the needed methods
    for multi TC/UP support, and removed the default_vlan_prio
    from mlx5e_priv which is always 0, it was replaced with
    hardcoded "0" in the new select queue method.
    
    For that we now create MAX_NUM_TC num of TISs (one per prio)
    on netdevice creation instead of priv->params.num_tc which
    was always 1.
    
    So far each channel had a single TXQ, Now each channel has a
    TXQ per TC (Traffic Class).
    
    Added en_dcbnl.c which implements the set/get DCBNL IEEE ETS,
    set/get dcbx and registers the mlx5e dcbnl ops.
    
    We still use the kernel's default TXQ selection method to select the
    channel to transmit through but now we use our own method to select
    the TXQ inside the channel based on VLAN priority.
    
    In mlx5, as opposed to mlx4, tc group N gets lower priority than
    tc group N+1.
    
    CC: John Fastabend <john.fastabend@gmail.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: Rana Shahout <ranas@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 15f6cdb842d5..dfbc4e54efd4 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -70,6 +70,11 @@
 
 #define MLX5E_NUM_MAIN_GROUPS 9
 
+#ifdef CONFIG_MLX5_CORE_EN_DCB
+#define MLX5E_MAX_BW_ALLOC 100 /* Max percentage of BW allocation */
+#define MLX5E_MIN_BW_ALLOC 1   /* Min percentage of BW allocation */
+#endif
+
 static const char vport_strings[][ETH_GSTRING_LEN] = {
 	/* vport statistics */
 	"rx_packets",
@@ -273,7 +278,6 @@ struct mlx5e_params {
 	u8  log_sq_size;
 	u8  log_rq_size;
 	u16 num_channels;
-	u8  default_vlan_prio;
 	u8  num_tc;
 	u16 rx_cq_moderation_usec;
 	u16 rx_cq_moderation_pkts;
@@ -286,6 +290,9 @@ struct mlx5e_params {
 	u8  rss_hfunc;
 	u8  toeplitz_hash_key[40];
 	u32 indirection_rqt[MLX5E_INDIR_RQT_SIZE];
+#ifdef CONFIG_MLX5_CORE_EN_DCB
+	struct ieee_ets ets;
+#endif
 };
 
 struct mlx5e_tstamp {
@@ -506,7 +513,6 @@ struct mlx5e_flow_tables {
 
 struct mlx5e_priv {
 	/* priv data path fields - start */
-	int                        default_vlan_prio;
 	struct mlx5e_sq            **txq_to_sq_map;
 	int channeltc_to_txq_map[MLX5E_MAX_NUM_CHANNELS][MLX5E_MAX_NUM_TC];
 	/* priv data path fields - end */
@@ -666,4 +672,9 @@ static inline int mlx5e_get_max_num_channels(struct mlx5_core_dev *mdev)
 }
 
 extern const struct ethtool_ops mlx5e_ethtool_ops;
+#ifdef CONFIG_MLX5_CORE_EN_DCB
+extern const struct dcbnl_rtnl_ops mlx5e_dcbnl_ops;
+int mlx5e_dcbnl_ieee_setets_core(struct mlx5e_priv *priv, struct ieee_ets *ets);
+#endif
+
 u16 mlx5e_get_max_inline_cap(struct mlx5_core_dev *mdev);

commit ada68c31ba9c02d7aabdd87db979fe670b499d54
Author: Achiad Shochat <achiad@mellanox.com>
Date:   Mon Feb 22 18:17:23 2016 +0200

    net/mlx5: Introduce a new header file for physical port functions
    
    All the device physical port access functions are implemented in the
    port.c file.
    We just extract the exposure of these functions from driver.h into a
    dedicated header file called port.h.
    
    Signed-off-by: Achiad Shochat <achiad@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index aac071a7e830..15f6cdb842d5 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -38,6 +38,7 @@
 #include <linux/mlx5/driver.h>
 #include <linux/mlx5/qp.h>
 #include <linux/mlx5/cq.h>
+#include <linux/mlx5/port.h>
 #include <linux/mlx5/vport.h>
 #include <linux/mlx5/transobj.h>
 #include "wq.h"

commit 048ccca8c1c8f583deec3367d7df521bb1f542ae
Merge: b3e27d5d4a29 34356f64ac0d
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Jan 23 18:45:06 2016 -0800

    Merge tag 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/dledford/rdma
    
    Pull rdma updates from Doug Ledford:
     "Initial roundup of 4.5 merge window patches
    
       - Remove usage of ib_query_device and instead store attributes in
         ib_device struct
    
       - Move iopoll out of block and into lib, rename to irqpoll, and use
         in several places in the rdma stack as our new completion queue
         polling library mechanism.  Update the other block drivers that
         already used iopoll to use the new mechanism too.
    
       - Replace the per-entry GID table locks with a single GID table lock
    
       - IPoIB multicast cleanup
    
       - Cleanups to the IB MR facility
    
       - Add support for 64bit extended IB counters
    
       - Fix for netlink oops while parsing RDMA nl messages
    
       - RoCEv2 support for the core IB code
    
       - mlx4 RoCEv2 support
    
       - mlx5 RoCEv2 support
    
       - Cross Channel support for mlx5
    
       - Timestamp support for mlx5
    
       - Atomic support for mlx5
    
       - Raw QP support for mlx5
    
       - MAINTAINERS update for mlx4/mlx5
    
       - Misc ocrdma, qib, nes, usNIC, cxgb3, cxgb4, mlx4, mlx5 updates
    
       - Add support for remote invalidate to the iSER driver (pushed
         through the RDMA tree due to dependencies, acknowledged by nab)
    
       - Update to NFSoRDMA (pushed through the RDMA tree due to
         dependencies, acknowledged by Bruce)"
    
    * tag 'for-linus' of git://git.kernel.org/pub/scm/linux/kernel/git/dledford/rdma: (169 commits)
      IB/mlx5: Unify CQ create flags check
      IB/mlx5: Expose Raw Packet QP to user space consumers
      {IB, net}/mlx5: Move the modify QP operation table to mlx5_ib
      IB/mlx5: Support setting Ethernet priority for Raw Packet QPs
      IB/mlx5: Add Raw Packet QP query functionality
      IB/mlx5: Add create and destroy functionality for Raw Packet QP
      IB/mlx5: Refactor mlx5_ib_qp to accommodate other QP types
      IB/mlx5: Allocate a Transport Domain for each ucontext
      net/mlx5_core: Warn on unsupported events of QP/RQ/SQ
      net/mlx5_core: Add RQ and SQ event handling
      net/mlx5_core: Export transport objects
      IB/mlx5: Expose CQE version to user-space
      IB/mlx5: Add CQE version 1 support to user QPs and SRQs
      IB/mlx5: Fix data validation in mlx5_ib_alloc_ucontext
      IB/sa: Fix netlink local service GFP crash
      IB/srpt: Remove redundant wc array
      IB/qib: Improve ipoib UD performance
      IB/mlx4: Advertise RoCE v2 support
      IB/mlx4: Create and use another QP1 for RoCEv2
      IB/mlx4: Enable send of RoCE QP1 packets with IP/UDP headers
      ...

commit 8d7f9ecb371a15e48754fa816e3f716517df7b13
Author: majd@mellanox.com <majd@mellanox.com>
Date:   Thu Jan 14 19:12:59 2016 +0200

    net/mlx5_core: Export transport objects
    
    To be used by mlx5_ib in the following patches for implementing
    RAW PACKET QP.
    
    Add mlx5_core_ prefix to alloc and delloc transport_domain since
    they are exposed now.
    
    Signed-off-by: Majd Dibbiny <majd@mellanox.com>
    Reviewed-by: Matan Barak <matanb@mellanox.com>
    Signed-off-by: Doug Ledford <dledford@redhat.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 22e72bf1ae48..034ccab6d9a8 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -36,8 +36,8 @@
 #include <linux/mlx5/qp.h>
 #include <linux/mlx5/cq.h>
 #include <linux/mlx5/vport.h>
+#include <linux/mlx5/transobj.h>
 #include "wq.h"
-#include "transobj.h"
 #include "mlx5_core.h"
 
 #define MLX5E_MAX_NUM_TC	8

commit 3d8c38af149309feb2541b995b3a45df170d6da3
Author: Eran Ben Elisha <eranbe@mellanox.com>
Date:   Tue Dec 29 14:58:32 2015 +0200

    net/mlx5e: Add PTP Hardware Clock (PHC) support
    
    Add a PHC support to the mlx5_en driver. Use reader/writer spinlocks to
    protect the timecounter since every packet received needs to call
    timecounter_cycle2time() when timestamping is enabled.  This can become
    a performance bottleneck with RSS and multiple receive queues if normal
    spinlocks are used.
    
    The driver has been tested with both Documentation/ptp/testptp and the
    linuxptp project (http://linuxptp.sourceforge.net/) on a Mellanox
    ConnectX-4 card.
    
    Signed-off-by: Eran Ben Elisha <eranbe@mellanox.com>
    Cc: Richard Cochran <richardcochran@gmail.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Acked-by: Richard Cochran <richardcochran@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 477e24884012..9ea49a893323 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -34,6 +34,7 @@
 #include <linux/etherdevice.h>
 #include <linux/timecounter.h>
 #include <linux/net_tstamp.h>
+#include <linux/ptp_clock_kernel.h>
 #include <linux/mlx5/driver.h>
 #include <linux/mlx5/qp.h>
 #include <linux/mlx5/cq.h>
@@ -295,6 +296,8 @@ struct mlx5e_tstamp {
 	unsigned long              overflow_period;
 	struct delayed_work        overflow_work;
 	struct mlx5_core_dev      *mdev;
+	struct ptp_clock          *ptp;
+	struct ptp_clock_info      ptp_info;
 };
 
 enum {

commit ef9814deafd0c83a358d49e3709c3e16cc352118
Author: Eran Ben Elisha <eranbe@mellanox.com>
Date:   Tue Dec 29 14:58:31 2015 +0200

    net/mlx5e: Add HW timestamping (TS) support
    
    Add support for enable/disable HW timestamping for incoming and/or
    outgoing packets. To enable/disable HW timestamping appropriate
    ioctl should be used. Currently HWTSTAMP_FILTER_ALL/NONE and
    HWTSAMP_TX_ON/OFF only are supported. Make all relevant changes in
    RX/TX flows to consider TS request and plant HW timestamps into
    relevant structures.
    
    Add internal clock for converting hardware timestamp to nanoseconds. In
    addition, add a service task to catch internal clock overflow, to make
    sure timestamping is accurate.
    
    Signed-off-by: Eran Ben Elisha <eranbe@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Acked-by: Richard Cochran <richardcochran@gmail.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index ae3f0e326c79..477e24884012 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -32,6 +32,8 @@
 
 #include <linux/if_vlan.h>
 #include <linux/etherdevice.h>
+#include <linux/timecounter.h>
+#include <linux/net_tstamp.h>
 #include <linux/mlx5/driver.h>
 #include <linux/mlx5/qp.h>
 #include <linux/mlx5/cq.h>
@@ -284,6 +286,17 @@ struct mlx5e_params {
 	u32 indirection_rqt[MLX5E_INDIR_RQT_SIZE];
 };
 
+struct mlx5e_tstamp {
+	rwlock_t                   lock;
+	struct cyclecounter        cycles;
+	struct timecounter         clock;
+	struct hwtstamp_config     hwtstamp_config;
+	u32                        nominal_c_mult;
+	unsigned long              overflow_period;
+	struct delayed_work        overflow_work;
+	struct mlx5_core_dev      *mdev;
+};
+
 enum {
 	MLX5E_RQ_STATE_POST_WQES_ENABLE,
 };
@@ -315,6 +328,7 @@ struct mlx5e_rq {
 
 	struct device         *pdev;
 	struct net_device     *netdev;
+	struct mlx5e_tstamp   *tstamp;
 	struct mlx5e_rq_stats  stats;
 	struct mlx5e_cq        cq;
 
@@ -382,6 +396,7 @@ struct mlx5e_sq {
 	u16                        max_inline;
 	u16                        edge;
 	struct device             *pdev;
+	struct mlx5e_tstamp       *tstamp;
 	__be32                     mkey_be;
 	unsigned long              state;
 
@@ -518,6 +533,7 @@ struct mlx5e_priv {
 	struct mlx5_core_dev      *mdev;
 	struct net_device         *netdev;
 	struct mlx5e_stats         stats;
+	struct mlx5e_tstamp        tstamp;
 };
 
 #define MLX5E_NET_IP_ALIGN 2
@@ -584,6 +600,13 @@ void mlx5e_destroy_flow_tables(struct mlx5e_priv *priv);
 void mlx5e_init_eth_addr(struct mlx5e_priv *priv);
 void mlx5e_set_rx_mode_work(struct work_struct *work);
 
+void mlx5e_fill_hwstamp(struct mlx5e_tstamp *clock, u64 timestamp,
+			struct skb_shared_hwtstamps *hwts);
+void mlx5e_timestamp_init(struct mlx5e_priv *priv);
+void mlx5e_timestamp_cleanup(struct mlx5e_priv *priv);
+int mlx5e_hwstamp_set(struct net_device *dev, struct ifreq *ifr);
+int mlx5e_hwstamp_get(struct net_device *dev, struct ifreq *ifr);
+
 int mlx5e_vlan_rx_add_vid(struct net_device *dev, __always_unused __be16 proto,
 			  u16 vid);
 int mlx5e_vlan_rx_kill_vid(struct net_device *dev, __always_unused __be16 proto,

commit 34802a42b3528b0e18ea4517c8b23e1214a09332
Author: Achiad Shochat <achiad@mellanox.com>
Date:   Tue Dec 29 14:58:29 2015 +0200

    net/mlx5e: Do not modify the TX SKB
    
    If the SKB is cloned, or has an elevated users count, someone else
    can be looking at it at the same time.
    
    Signed-off-by: Achiad Shochat <achiad@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index f689ce580b44..ae3f0e326c79 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -328,14 +328,12 @@ struct mlx5e_rq {
 	struct mlx5e_priv     *priv;
 } ____cacheline_aligned_in_smp;
 
-struct mlx5e_tx_skb_cb {
+struct mlx5e_tx_wqe_info {
 	u32 num_bytes;
 	u8  num_wqebbs;
 	u8  num_dma;
 };
 
-#define MLX5E_TX_SKB_CB(__skb) ((struct mlx5e_tx_skb_cb *)__skb->cb)
-
 enum mlx5e_dma_map_type {
 	MLX5E_DMA_MAP_SINGLE,
 	MLX5E_DMA_MAP_PAGE
@@ -371,6 +369,7 @@ struct mlx5e_sq {
 	/* pointers to per packet info: write@xmit, read@completion */
 	struct sk_buff           **skb;
 	struct mlx5e_sq_dma       *dma_fifo;
+	struct mlx5e_tx_wqe_info  *wqe_info;
 
 	/* read only */
 	struct mlx5_wq_cyc         wq;

commit 86d722ad2c3bd2f0536b196b7fd67ae2a7e2a492
Author: Maor Gottlieb <maorg@mellanox.com>
Date:   Thu Dec 10 17:12:44 2015 +0200

    net/mlx5: Use flow steering infrastructure for mlx5_en
    
    Expose the new flow steering API and remove the old
    one.
    
    Few changes are required:
    
    1. The Ethernet flow steering follows the existing implementation, but uses
    the new steering API. The old flow steering implementation is removed.
    
    2. Move the E-switch FDB management to use the new API.
    
    3. When driver is loaded call to mlx5_init_fs which initialize
    the flow steering tree structure, open namespaces for NIC receive
    and for E-switch FDB.
    
    4. Call to mlx5_cleanup_fs when the driver is unloaded.
    
    Signed-off-by: Maor Gottlieb <maorg@mellanox.com>
    Signed-off-by: Moni Shoua <monis@mellanox.com>
    Signed-off-by: Matan Barak <matanb@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 89313d46952d..f689ce580b44 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -64,6 +64,8 @@
 #define MLX5E_UPDATE_STATS_INTERVAL    200 /* msecs */
 #define MLX5E_SQ_BF_BUDGET             16
 
+#define MLX5E_NUM_MAIN_GROUPS 9
+
 static const char vport_strings[][ETH_GSTRING_LEN] = {
 	/* vport statistics */
 	"rx_packets",
@@ -442,7 +444,7 @@ enum mlx5e_rqt_ix {
 struct mlx5e_eth_addr_info {
 	u8  addr[ETH_ALEN + 2];
 	u32 tt_vec;
-	u32 ft_ix[MLX5E_NUM_TT]; /* flow table index per traffic type */
+	struct mlx5_flow_rule *ft_rule[MLX5E_NUM_TT];
 };
 
 #define MLX5E_ETH_ADDR_HASH_SIZE (1 << BITS_PER_BYTE)
@@ -466,15 +468,22 @@ enum {
 
 struct mlx5e_vlan_db {
 	unsigned long active_vlans[BITS_TO_LONGS(VLAN_N_VID)];
-	u32           active_vlans_ft_ix[VLAN_N_VID];
-	u32           untagged_rule_ft_ix;
-	u32           any_vlan_rule_ft_ix;
+	struct mlx5_flow_rule	*active_vlans_rule[VLAN_N_VID];
+	struct mlx5_flow_rule	*untagged_rule;
+	struct mlx5_flow_rule	*any_vlan_rule;
 	bool          filter_disabled;
 };
 
 struct mlx5e_flow_table {
-	void *vlan;
-	void *main;
+	int num_groups;
+	struct mlx5_flow_table		*t;
+	struct mlx5_flow_group		**g;
+};
+
+struct mlx5e_flow_tables {
+	struct mlx5_flow_namespace	*ns;
+	struct mlx5e_flow_table		vlan;
+	struct mlx5e_flow_table		main;
 };
 
 struct mlx5e_priv {
@@ -497,7 +506,7 @@ struct mlx5e_priv {
 	u32                        rqtn[MLX5E_NUM_RQT];
 	u32                        tirn[MLX5E_NUM_TT];
 
-	struct mlx5e_flow_table    ft;
+	struct mlx5e_flow_tables   fts;
 	struct mlx5e_eth_addr_db   eth_addr;
 	struct mlx5e_vlan_db       vlan;
 

commit aad9e6e41e5e595528e316871bbc5be78a8a1eb4
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Tue Dec 1 18:03:17 2015 +0200

    net/mlx5e: Write vlan list into vport context
    
    Each Vport/vNIC must notify underlying e-Switch layer
    for vlan table changes in-order to update SR-IOV FDB tables.
    
    We do that at vlan_rx_add_vid and vlan_rx_kill_vid ndos.
    
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 69f1c1a412b4..89313d46952d 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -465,6 +465,7 @@ enum {
 };
 
 struct mlx5e_vlan_db {
+	unsigned long active_vlans[BITS_TO_LONGS(VLAN_N_VID)];
 	u32           active_vlans_ft_ix[VLAN_N_VID];
 	u32           untagged_rule_ft_ix;
 	u32           any_vlan_rule_ft_ix;

commit 44fb6fbbac23ee304d113ceb5eed45666d8a32f9
Author: Eric Dumazet <edumazet@google.com>
Date:   Wed Nov 18 06:30:56 2015 -0800

    mlx5: support napi_complete_done()
    
    A NAPI poll handler should return number of RX packets processed,
    instead of 0 / budget.
    
    This allows proper busy poll accounting through LINUX_MIB_BUSYPOLLRXPACKETS
    SNMP counter.
    
    napi_complete_done() allows /sys/class/net/ethX/gro_flush_timeout
    to be used for finer GRO aggregation control.
    
    Tested:
    
    Enabled busy polling, and checked TcpExtBusyPollRxPackets counter is increasing.
    
    echo 70 >/proc/sys/net/core/busy_read
    nstat >/dev/null
    netperf -H target -t TCP_RR >/dev/null
    nstat | grep TcpExtBusyPollRxPackets
    TcpExtBusyPollRxPackets         490958             0.0
    
    Signed-off-by: Eric Dumazet <edumazet@google.com>
    Cc: Eli Cohen <eli@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 22e72bf1ae48..69f1c1a412b4 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -564,7 +564,7 @@ void mlx5e_completion_event(struct mlx5_core_cq *mcq);
 void mlx5e_cq_error_event(struct mlx5_core_cq *mcq, enum mlx5_event event);
 int mlx5e_napi_poll(struct napi_struct *napi, int budget);
 bool mlx5e_poll_tx_cq(struct mlx5e_cq *cq);
-bool mlx5e_poll_rx_cq(struct mlx5e_cq *cq, int budget);
+int mlx5e_poll_rx_cq(struct mlx5e_cq *cq, int budget);
 bool mlx5e_post_rx_wqes(struct mlx5e_rq *rq);
 struct mlx5_cqe64 *mlx5e_get_cqe(struct mlx5e_cq *cq);
 

commit d4e28cbd24c8cb004960ddb8b22124953f6c220c
Author: Achiad Shochat <achiad@mellanox.com>
Date:   Thu Nov 12 19:35:28 2015 +0200

    net/mlx5e: Use the right DMA free function on TX path
    
    On xmit path we use skb_frag_dma_map() which is using dma_map_page(),
    while upon completion we dma-unmap the skb fragments using
    dma_unmap_single() rather than dma_unmap_page().
    
    To fix this, we now save the dma map type on xmit path and use this
    info to call the right dma unmap method upon TX completion.
    
    Signed-off-by: Achiad Shochat <achiad@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index f2ae62dd8c09..22e72bf1ae48 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -334,9 +334,15 @@ struct mlx5e_tx_skb_cb {
 
 #define MLX5E_TX_SKB_CB(__skb) ((struct mlx5e_tx_skb_cb *)__skb->cb)
 
+enum mlx5e_dma_map_type {
+	MLX5E_DMA_MAP_SINGLE,
+	MLX5E_DMA_MAP_PAGE
+};
+
 struct mlx5e_sq_dma {
-	dma_addr_t addr;
-	u32        size;
+	dma_addr_t              addr;
+	u32                     size;
+	enum mlx5e_dma_map_type type;
 };
 
 enum {

commit 3435ab59d3febe0c00af2a4d40a6df401effe3de
Author: Achiad Shochat <achiad@mellanox.com>
Date:   Tue Nov 3 08:07:21 2015 +0200

    net/mlx5e: Don't allow more than max supported channels
    
    Consider MLX5E_MAX_NUM_CHANNELS @ethtool set/get_channels
    
    Signed-off-by: Achiad Shochat <achiad@mellanox.com>
    Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 0983a208b299..f2ae62dd8c09 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -617,5 +617,11 @@ static inline void mlx5e_cq_arm(struct mlx5e_cq *cq)
 	mlx5_cq_arm(mcq, MLX5_CQ_DB_REQ_NOT, mcq->uar->map, NULL, cq->wq.cc);
 }
 
+static inline int mlx5e_get_max_num_channels(struct mlx5_core_dev *mdev)
+{
+	return min_t(int, mdev->priv.eq_table.num_comp_vectors,
+		     MLX5E_MAX_NUM_CHANNELS);
+}
+
 extern const struct ethtool_ops mlx5e_ethtool_ops;
 u16 mlx5e_get_max_inline_cap(struct mlx5_core_dev *mdev);

commit 5283af899adec1a7765e413e43b27a335fadac5d
Author: Rana Shahout <ranas@mellanox.com>
Date:   Sun Aug 23 16:12:14 2015 +0300

    net/mlx5e: Avoid accessing NULL pointer at ndo_select_queue
    
    To avoid multiply/division operations on the data path,
    we hold a {channel, tc}==>txq mapping table.
    We held this mapping table inside the channel object that is
    being destroyed upon some configuration operations (e.g MTU change).
    So in case ndo_select_queue occurs during such a configuration operation,
    it may access a NULL channel pointer, resulting in kernel panic.
    To fix this issue we moved the {channel, tc}==>txq mapping table
    outside the channel object so that it will be available also
    during such configuration operations.
    
    Signed-off-by: Rana Shahout <ranas@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 27ca4596775a..0983a208b299 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -405,7 +405,6 @@ struct mlx5e_channel {
 	__be32                     mkey_be;
 	u8                         num_tc;
 	unsigned long              flags;
-	int                        tc_to_txq_map[MLX5E_MAX_NUM_TC];
 
 	/* control */
 	struct mlx5e_priv         *priv;
@@ -475,6 +474,7 @@ struct mlx5e_priv {
 	/* priv data path fields - start */
 	int                        default_vlan_prio;
 	struct mlx5e_sq            **txq_to_sq_map;
+	int channeltc_to_txq_map[MLX5E_MAX_NUM_CHANNELS][MLX5E_MAX_NUM_TC];
 	/* priv data path fields - end */
 
 	unsigned long              state;

commit bbceefce9adfc9cf7b4110727c96c841ba8c7964
Author: Achiad Shochat <achiad@mellanox.com>
Date:   Sun Aug 16 16:04:52 2015 +0300

    net/mlx5e: Support RX CHECKSUM_COMPLETE
    
    Only for packets with first ethertype set to IPv4/6 for now.
    
    Signed-off-by: Achiad Shochat <achiad@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 298f0bf476a7..27ca4596775a 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -94,6 +94,7 @@ static const char vport_strings[][ETH_GSTRING_LEN] = {
 	"lro_bytes",
 	"rx_csum_good",
 	"rx_csum_none",
+	"rx_csum_sw",
 	"tx_csum_offload",
 	"tx_queue_stopped",
 	"tx_queue_wake",
@@ -131,13 +132,14 @@ struct mlx5e_vport_stats {
 	u64 lro_bytes;
 	u64 rx_csum_good;
 	u64 rx_csum_none;
+	u64 rx_csum_sw;
 	u64 tx_csum_offload;
 	u64 tx_queue_stopped;
 	u64 tx_queue_wake;
 	u64 tx_queue_dropped;
 	u64 rx_wqe_err;
 
-#define NUM_VPORT_COUNTERS     31
+#define NUM_VPORT_COUNTERS     32
 };
 
 static const char pport_strings[][ETH_GSTRING_LEN] = {
@@ -217,6 +219,7 @@ struct mlx5e_pport_stats {
 static const char rq_stats_strings[][ETH_GSTRING_LEN] = {
 	"packets",
 	"csum_none",
+	"csum_sw",
 	"lro_packets",
 	"lro_bytes",
 	"wqe_err"
@@ -225,10 +228,11 @@ static const char rq_stats_strings[][ETH_GSTRING_LEN] = {
 struct mlx5e_rq_stats {
 	u64 packets;
 	u64 csum_none;
+	u64 csum_sw;
 	u64 lro_packets;
 	u64 lro_bytes;
 	u64 wqe_err;
-#define NUM_RQ_STATS 5
+#define NUM_RQ_STATS 6
 };
 
 static const char sq_stats_strings[][ETH_GSTRING_LEN] = {

commit d9a40271cfe625950dc4ef0cbd8429c0d28e62d1
Author: Achiad Shochat <achiad@mellanox.com>
Date:   Sun Aug 16 16:04:49 2015 +0300

    net/mlx5e: HW LRO changes/fixes
    
    - Change the maximum LRO session size from 16KB to 64KB
    - Reduce the LRO session timeout from 512us to 32us in
      order to reduce the TCP latency of non-LRO'ed flows.
    - Fix skb_shinfo(skb)->gso_size and set skb_shinfo(skb)->gso_type.
    - Fix a bug accessing un-initialized mdev pointer.
    
    Signed-off-by: Achiad Shochat <achiad@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index d32a005c601e..298f0bf476a7 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -50,7 +50,7 @@
 #define MLX5E_PARAMS_DEFAULT_LOG_RQ_SIZE                0xa
 #define MLX5E_PARAMS_MAXIMUM_LOG_RQ_SIZE                0xd
 
-#define MLX5E_PARAMS_DEFAULT_LRO_WQE_SZ                 (16 * 1024)
+#define MLX5E_PARAMS_DEFAULT_LRO_WQE_SZ                 (64 * 1024)
 #define MLX5E_PARAMS_DEFAULT_RX_CQ_MODERATION_USEC      0x10
 #define MLX5E_PARAMS_DEFAULT_RX_CQ_MODERATION_PKTS      0x20
 #define MLX5E_PARAMS_DEFAULT_TX_CQ_MODERATION_USEC      0x10

commit e842b1001dc9b90cc62b489b07ff18e123c8c858
Author: Achiad Shochat <achiad@mellanox.com>
Date:   Sun Aug 16 16:04:48 2015 +0300

    net/mlx5e: Support smaller RX/TX ring sizes
    
    We un-intentionally limited the minimum rings size too much.
    
    TX minimum ring size reduced from 128 to 64.
    RX minimum ring size reduced from 128 to 2.
    
    Signed-off-by: Achiad Shochat <achiad@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 3b659c360d08..d32a005c601e 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -42,11 +42,11 @@
 
 #define MLX5E_MAX_NUM_TC	8
 
-#define MLX5E_PARAMS_MINIMUM_LOG_SQ_SIZE                0x7
+#define MLX5E_PARAMS_MINIMUM_LOG_SQ_SIZE                0x6
 #define MLX5E_PARAMS_DEFAULT_LOG_SQ_SIZE                0xa
 #define MLX5E_PARAMS_MAXIMUM_LOG_SQ_SIZE                0xd
 
-#define MLX5E_PARAMS_MINIMUM_LOG_RQ_SIZE                0x7
+#define MLX5E_PARAMS_MINIMUM_LOG_RQ_SIZE                0x1
 #define MLX5E_PARAMS_DEFAULT_LOG_RQ_SIZE                0xa
 #define MLX5E_PARAMS_MAXIMUM_LOG_RQ_SIZE                0xd
 

commit 2d75b2bc8a8c0ce5567a6ecef52e194d117efe3f
Author: Achiad Shochat <achiad@mellanox.com>
Date:   Sun Aug 16 16:04:47 2015 +0300

    net/mlx5e: Add ethtool RSS configuration options
    
    - get_rxfh_key_size
    - get_rxfh_indir_size
    - get/set_rxfh indirection table and RSS Toeplitz hash key
    - get_rxnfc
    
    Signed-off-by: Achiad Shochat <achiad@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 0b8a270308ff..3b659c360d08 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -272,9 +272,10 @@ struct mlx5e_params {
 	u16 min_rx_wqes;
 	bool lro_en;
 	u32 lro_wqe_sz;
-	u8  rss_hfunc;
 	u16 tx_max_inline;
-	u8 toeplitz_hash_key[40];
+	u8  rss_hfunc;
+	u8  toeplitz_hash_key[40];
+	u32 indirection_rqt[MLX5E_INDIR_RQT_SIZE];
 };
 
 enum {
@@ -571,6 +572,8 @@ int mlx5e_vlan_rx_kill_vid(struct net_device *dev, __always_unused __be16 proto,
 void mlx5e_enable_vlan_filter(struct mlx5e_priv *priv);
 void mlx5e_disable_vlan_filter(struct mlx5e_priv *priv);
 
+int mlx5e_redirect_rqt(struct mlx5e_priv *priv, enum mlx5e_rqt_ix rqt_ix);
+
 int mlx5e_open_locked(struct net_device *netdev);
 int mlx5e_close_locked(struct net_device *netdev);
 

commit 936896e9080b84b0e8b9b23e84fb734e91f5ba6d
Author: Achiad Shochat <achiad@mellanox.com>
Date:   Sun Aug 16 16:04:46 2015 +0300

    net/mlx5e: Make RSS indirection table size a constant
    
    The indirection table size was defined by a variable that
    was actually assigned a constant value.
    Since we do not have any forseen intension to make it configurable
    we simply made it a constant.
    
    We also limit the number of channels such that the RSS indirection
    table could always populate all RX rings.
    
    Signed-off-by: Achiad Shochat <achiad@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 7b2b74398ead..0b8a270308ff 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -56,8 +56,10 @@
 #define MLX5E_PARAMS_DEFAULT_TX_CQ_MODERATION_USEC      0x10
 #define MLX5E_PARAMS_DEFAULT_TX_CQ_MODERATION_PKTS      0x20
 #define MLX5E_PARAMS_DEFAULT_MIN_RX_WQES                0x80
-#define MLX5E_PARAMS_DEFAULT_RX_HASH_LOG_TBL_SZ         0x7
 
+#define MLX5E_LOG_INDIR_RQT_SIZE       0x7
+#define MLX5E_INDIR_RQT_SIZE           BIT(MLX5E_LOG_INDIR_RQT_SIZE)
+#define MLX5E_MAX_NUM_CHANNELS         (MLX5E_INDIR_RQT_SIZE >> 1)
 #define MLX5E_TX_CQ_POLL_BUDGET        128
 #define MLX5E_UPDATE_STATS_INTERVAL    200 /* msecs */
 #define MLX5E_SQ_BF_BUDGET             16
@@ -268,7 +270,6 @@ struct mlx5e_params {
 	u16 tx_cq_moderation_usec;
 	u16 tx_cq_moderation_pkts;
 	u16 min_rx_wqes;
-	u16 rx_hash_log_tbl_sz;
 	bool lro_en;
 	u32 lro_wqe_sz;
 	u8  rss_hfunc;

commit 57afead5445de85658e3fa3811ef0cdbf5ed73bf
Author: Achiad Shochat <achiad@mellanox.com>
Date:   Sun Aug 16 16:04:45 2015 +0300

    net/mlx5e: Have a single RSS Toeplitz hash key
    
    No need to generate a unique key per TIR.
    Generating a single key per netdev and copying it to all
    its TIRs.
    
    Signed-off-by: Achiad Shochat <achiad@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index e9d7d90363a8..7b2b74398ead 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -273,6 +273,7 @@ struct mlx5e_params {
 	u32 lro_wqe_sz;
 	u8  rss_hfunc;
 	u16 tx_max_inline;
+	u8 toeplitz_hash_key[40];
 };
 
 enum {

commit efea389d3cc6427a9a94e92b2d7bf4c862f2cfcf
Author: Gal Pressman <galp@mellanox.com>
Date:   Tue Aug 4 14:05:47 2015 +0300

    net/mlx5_core: Support physical port counters
    
    Added physical port counters in the following standard formats to
    ethtool statistics:
      - IEEE 802.3
      - RFC2863
      - RFC2819
    
    Signed-off-by: Gal Pressman <galp@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: Amir Vadai <amirv@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 35c33907a9ff..e9d7d90363a8 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -138,6 +138,80 @@ struct mlx5e_vport_stats {
 #define NUM_VPORT_COUNTERS     31
 };
 
+static const char pport_strings[][ETH_GSTRING_LEN] = {
+	/* IEEE802.3 counters */
+	"frames_tx",
+	"frames_rx",
+	"check_seq_err",
+	"alignment_err",
+	"octets_tx",
+	"octets_received",
+	"multicast_xmitted",
+	"broadcast_xmitted",
+	"multicast_rx",
+	"broadcast_rx",
+	"in_range_len_errors",
+	"out_of_range_len",
+	"too_long_errors",
+	"symbol_err",
+	"mac_control_tx",
+	"mac_control_rx",
+	"unsupported_op_rx",
+	"pause_ctrl_rx",
+	"pause_ctrl_tx",
+
+	/* RFC2863 counters */
+	"in_octets",
+	"in_ucast_pkts",
+	"in_discards",
+	"in_errors",
+	"in_unknown_protos",
+	"out_octets",
+	"out_ucast_pkts",
+	"out_discards",
+	"out_errors",
+	"in_multicast_pkts",
+	"in_broadcast_pkts",
+	"out_multicast_pkts",
+	"out_broadcast_pkts",
+
+	/* RFC2819 counters */
+	"drop_events",
+	"octets",
+	"pkts",
+	"broadcast_pkts",
+	"multicast_pkts",
+	"crc_align_errors",
+	"undersize_pkts",
+	"oversize_pkts",
+	"fragments",
+	"jabbers",
+	"collisions",
+	"p64octets",
+	"p65to127octets",
+	"p128to255octets",
+	"p256to511octets",
+	"p512to1023octets",
+	"p1024to1518octets",
+	"p1519to2047octets",
+	"p2048to4095octets",
+	"p4096to8191octets",
+	"p8192to10239octets",
+};
+
+#define NUM_IEEE_802_3_COUNTERS		19
+#define NUM_RFC_2863_COUNTERS		13
+#define NUM_RFC_2819_COUNTERS		21
+#define NUM_PPORT_COUNTERS		(NUM_IEEE_802_3_COUNTERS + \
+					 NUM_RFC_2863_COUNTERS + \
+					 NUM_RFC_2819_COUNTERS)
+
+struct mlx5e_pport_stats {
+	__be64 IEEE_802_3_counters[NUM_IEEE_802_3_COUNTERS];
+	__be64 RFC_2863_counters[NUM_RFC_2863_COUNTERS];
+	__be64 RFC_2819_counters[NUM_RFC_2819_COUNTERS];
+};
+
 static const char rq_stats_strings[][ETH_GSTRING_LEN] = {
 	"packets",
 	"csum_none",
@@ -180,6 +254,7 @@ struct mlx5e_sq_stats {
 
 struct mlx5e_stats {
 	struct mlx5e_vport_stats   vport;
+	struct mlx5e_pport_stats   pport;
 };
 
 struct mlx5e_params {

commit 9b37b07fcb0e00a9c0b605b7b28c2d200f4eb064
Author: Achiad Shochat <achiad@mellanox.com>
Date:   Tue Aug 4 14:05:46 2015 +0300

    net/mlx5e: Take advantage of the light-weight netdev open/stop
    
    Now that TIRs, TISs and flow tables are kept alive while the netdev is
    stopped (after executing ndo_stop()) we can do the following
    improvements:
    
    - Obsolete the active_vlans SW shadow.
    - Do not delete/add flow table rules upon ndo_stop/open.
      In addition to simplifying the flow, this change also fastens
      the ndo_open/close operations.
    - Obsolete synchronization of threads accessing the flow tables
      with the netdev stop/open threads.
    
    Signed-off-by: Achiad Shochat <achiad@mellanox.com>
    Signed-off-by: Amir Vadai <amirv@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index a6c4bd3265a5..35c33907a9ff 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -374,10 +374,10 @@ struct mlx5e_eth_addr_db {
 enum {
 	MLX5E_STATE_ASYNC_EVENTS_ENABLE,
 	MLX5E_STATE_OPENED,
+	MLX5E_STATE_DESTROYING,
 };
 
 struct mlx5e_vlan_db {
-	unsigned long active_vlans[BITS_TO_LONGS(VLAN_N_VID)];
 	u32           active_vlans_ft_ix[VLAN_N_VID];
 	u32           untagged_rule_ft_ix;
 	u32           any_vlan_rule_ft_ix;
@@ -485,7 +485,6 @@ void mlx5e_update_stats(struct mlx5e_priv *priv);
 int mlx5e_create_flow_tables(struct mlx5e_priv *priv);
 void mlx5e_destroy_flow_tables(struct mlx5e_priv *priv);
 void mlx5e_init_eth_addr(struct mlx5e_priv *priv);
-void mlx5e_set_rx_mode_core(struct mlx5e_priv *priv);
 void mlx5e_set_rx_mode_work(struct work_struct *work);
 
 int mlx5e_vlan_rx_add_vid(struct net_device *dev, __always_unused __be16 proto,
@@ -494,8 +493,6 @@ int mlx5e_vlan_rx_kill_vid(struct net_device *dev, __always_unused __be16 proto,
 			   u16 vid);
 void mlx5e_enable_vlan_filter(struct mlx5e_priv *priv);
 void mlx5e_disable_vlan_filter(struct mlx5e_priv *priv);
-int mlx5e_add_all_vlan_rules(struct mlx5e_priv *priv);
-void mlx5e_del_all_vlan_rules(struct mlx5e_priv *priv);
 
 int mlx5e_open_locked(struct net_device *netdev);
 int mlx5e_close_locked(struct net_device *netdev);

commit 40ab6a6ebeebbcfc313233f5aa0d55930734f529
Author: Achiad Shochat <achiad@mellanox.com>
Date:   Tue Aug 4 14:05:44 2015 +0300

    net/mlx5e: Rename/move functions following the ndo_stop flow change
    
    Rename some functions that used to be invoked upon ndo_open/stop and
    are now invoked upon create/destroy_netdev() in order to better hint
    their place in the flow.
    
    Change some functions location in the file so that functions involved
    in ndo_open/stop flow will not be interleaved with other functions.
    
    This is a cosmetic change, no logical change here.
    
    Signed-off-by: Achiad Shochat <achiad@mellanox.com>
    Signed-off-by: Amir Vadai <amirv@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 31e9610926fe..a6c4bd3265a5 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -482,8 +482,8 @@ struct mlx5_cqe64 *mlx5e_get_cqe(struct mlx5e_cq *cq);
 
 void mlx5e_update_stats(struct mlx5e_priv *priv);
 
-int mlx5e_open_flow_table(struct mlx5e_priv *priv);
-void mlx5e_close_flow_table(struct mlx5e_priv *priv);
+int mlx5e_create_flow_tables(struct mlx5e_priv *priv);
+void mlx5e_destroy_flow_tables(struct mlx5e_priv *priv);
 void mlx5e_init_eth_addr(struct mlx5e_priv *priv);
 void mlx5e_set_rx_mode_core(struct mlx5e_priv *priv);
 void mlx5e_set_rx_mode_work(struct work_struct *work);

commit 50cfa25aba67c658979c5a3188d514ee6780364b
Author: Achiad Shochat <achiad@mellanox.com>
Date:   Tue Aug 4 14:05:41 2015 +0300

    net/mlx5e: Introduce the "Drop RQ"
    
    RX traffic routed to this RQ will be silently dropped, at the NIC HW
    level.
    
    This is in preparation for netdev "light-weight" open/stop flow
    change described in previous commit.
    
    Signed-off-by: Achiad Shochat <achiad@mellanox.com>
    Signed-off-by: Amir Vadai <amirv@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index af5791296584..31e9610926fe 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -217,6 +217,7 @@ struct mlx5e_cq {
 	struct napi_struct        *napi;
 	struct mlx5_core_cq        mcq;
 	struct mlx5e_channel      *channel;
+	struct mlx5e_priv         *priv;
 
 	/* control */
 	struct mlx5_wq_ctrl        wq_ctrl;
@@ -240,6 +241,7 @@ struct mlx5e_rq {
 	struct mlx5_wq_ctrl    wq_ctrl;
 	u32                    rqn;
 	struct mlx5e_channel  *channel;
+	struct mlx5e_priv     *priv;
 } ____cacheline_aligned_in_smp;
 
 struct mlx5e_tx_skb_cb {
@@ -399,6 +401,7 @@ struct mlx5e_priv {
 	u32                        pdn;
 	u32                        tdn;
 	struct mlx5_core_mr        mr;
+	struct mlx5e_rq            drop_rq;
 
 	struct mlx5e_channel     **channel;
 	u32                        tisn[MLX5E_MAX_NUM_TC];

commit 4cbeaff54f00f39493c4251bf115d02e26ac8bf2
Author: Achiad Shochat <achiad@mellanox.com>
Date:   Tue Aug 4 14:05:40 2015 +0300

    net/mlx5e: Unify the RX flow
    
    Generally an RX packet flows through the following objects:
    Flow table --> TIR --> RQT --> RQ
    
    Where:
    - TIR stands for "Transport Interface Receive", defining the RSS and
      LRO paramaters.
    - RQT stands for "RQ Table", implementing the RSS indirection table.
    - RQ stands for "Receive Queue"
    
    For flows that do not need LRO, nor RSS, the driver made a shortcut to
    the above RX flow by pointing to the RQ directly from the TIR, yielding
    this flow:
    Flow table --> TIR --> RQ
    
    In this commit we remove this shortcut by "inserting" a single-RQ RQT
    between the TIR and the RQ, i.e RX packets will reach the same RQ but
    will go through an RQT of size 1, pointing to just a single RQ.
    
    This way the RX traffic re-direction to/from the "Drop RQ" will be more
    uniform (AKA "one flow"), as it will involve only RQTs re-direction and
    no TIRs re-direction.
    
    Signed-off-by: Achiad Shochat <achiad@mellanox.com>
    Signed-off-by: Amir Vadai <amirv@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 45f6dc75c0df..af5791296584 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -344,10 +344,10 @@ enum mlx5e_traffic_types {
 	MLX5E_NUM_TT,
 };
 
-enum {
-	MLX5E_RQT_SPREADING  = 0,
-	MLX5E_RQT_DEFAULT_RQ = 1,
-	MLX5E_NUM_RQT        = 2,
+enum mlx5e_rqt_ix {
+	MLX5E_INDIRECTION_RQT,
+	MLX5E_SINGLE_RQ_RQT,
+	MLX5E_NUM_RQT,
 };
 
 struct mlx5e_eth_addr_info {
@@ -402,7 +402,7 @@ struct mlx5e_priv {
 
 	struct mlx5e_channel     **channel;
 	u32                        tisn[MLX5E_MAX_NUM_TC];
-	u32                        rqtn;
+	u32                        rqtn[MLX5E_NUM_RQT];
 	u32                        tirn[MLX5E_NUM_TT];
 
 	struct mlx5e_flow_table    ft;

commit 98e81b0ad6f25e2c4e2ff1680f50f9c66bec9e08
Author: Achiad Shochat <achiad@mellanox.com>
Date:   Wed Jul 29 15:05:46 2015 +0300

    net/mlx5e: Remove the mlx5e_update_priv_params() function
    
    It was used to update netdev priv parameters that require stopping
    and re-opening the device in a generic way - it got the new
    parameters and did: ndo_stop(), copy new parameters into current
    parameters, ndo_open().
    
    We chose to remove it for two reasons:
    1) It requires additional instance of struct mlx5e_params on the
       stack and looking forward we expect this struct to grow.
    2) Sometimes we want to do additional operations (besides
       just updating the priv parameters) while the netdev is stopped.
       For example, updating netdev->mtu @mlx5e_change_mtu() should
       be done while the netdev is stopped (done in this commit).
    
    Signed-off-by: Achiad Shochat <achiad@mellanox.com>
    Signed-off-by: Amir Vadai <amirv@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 7b5a679ab185..45f6dc75c0df 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -496,8 +496,6 @@ void mlx5e_del_all_vlan_rules(struct mlx5e_priv *priv);
 
 int mlx5e_open_locked(struct net_device *netdev);
 int mlx5e_close_locked(struct net_device *netdev);
-int mlx5e_update_priv_params(struct mlx5e_priv *priv,
-			     struct mlx5e_params *new_params);
 
 static inline void mlx5e_tx_notify_hw(struct mlx5e_sq *sq,
 				      struct mlx5e_tx_wqe *wqe, int bf_sz)

commit a4418a6c36cd9eb080cde0473d15f5cee4c3f35d
Author: Achiad Shochat <achiad@mellanox.com>
Date:   Wed Jul 29 15:05:41 2015 +0300

    net/mlx5e: Remove redundant field mlx5e_priv->num_tc
    
    This field already exists under the mlx5e_params struct
    
    Signed-off-by: Achiad Shochat <achiad@mellanox.com>
    Signed-off-by: Amir Vadai <amirv@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index b710e9bade38..7b5a679ab185 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -389,7 +389,6 @@ struct mlx5e_flow_table {
 
 struct mlx5e_priv {
 	/* priv data path fields - start */
-	int                        num_tc;
 	int                        default_vlan_prio;
 	struct mlx5e_sq            **txq_to_sq_map;
 	/* priv data path fields - end */

commit a741749f214afd05402aeed0ecdca90eeaffc303
Author: Achiad Shochat <achiad@mellanox.com>
Date:   Thu Jul 23 23:36:01 2015 +0300

    net/mlx5e: Input IPSEC.SPI into the RX RSS hash function
    
    In addition to the source/destination IP which are already hashed.
    Only for unicast traffic for now.
    
    Signed-off-by: Achiad Shochat <achiad@mellanox.com>
    Signed-off-by: Amir Vadai <amirv@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 39294f2fbaac..b710e9bade38 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -334,6 +334,10 @@ enum mlx5e_traffic_types {
 	MLX5E_TT_IPV6_TCP,
 	MLX5E_TT_IPV4_UDP,
 	MLX5E_TT_IPV6_UDP,
+	MLX5E_TT_IPV4_IPSEC_AH,
+	MLX5E_TT_IPV6_IPSEC_AH,
+	MLX5E_TT_IPV4_IPSEC_ESP,
+	MLX5E_TT_IPV6_IPSEC_ESP,
 	MLX5E_TT_IPV4,
 	MLX5E_TT_IPV6,
 	MLX5E_TT_ANY,

commit 5a6f8aef16c53ffeb9c1497f0b583269695c067d
Author: Achiad Shochat <achiad@mellanox.com>
Date:   Thu Jul 23 23:36:00 2015 +0300

    net/mlx5e: Cosmetics: use BIT() instead of "1 <<", and others
    
    No logical change in this commit.
    
    Signed-off-by: Achiad Shochat <achiad@mellanox.com>
    Signed-off-by: Amir Vadai <amirv@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index b66edd2c5a61..39294f2fbaac 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -330,14 +330,14 @@ struct mlx5e_channel {
 };
 
 enum mlx5e_traffic_types {
-	MLX5E_TT_IPV4_TCP = 0,
-	MLX5E_TT_IPV6_TCP = 1,
-	MLX5E_TT_IPV4_UDP = 2,
-	MLX5E_TT_IPV6_UDP = 3,
-	MLX5E_TT_IPV4     = 4,
-	MLX5E_TT_IPV6     = 5,
-	MLX5E_TT_ANY      = 6,
-	MLX5E_NUM_TT      = 7,
+	MLX5E_TT_IPV4_TCP,
+	MLX5E_TT_IPV6_TCP,
+	MLX5E_TT_IPV4_UDP,
+	MLX5E_TT_IPV6_UDP,
+	MLX5E_TT_IPV4,
+	MLX5E_TT_IPV6,
+	MLX5E_TT_ANY,
+	MLX5E_NUM_TT,
 };
 
 enum {

commit 88a85f99e51fb2373259ab83c8bb130a9bbf3804
Author: Achiad Shochat <achiad@mellanox.com>
Date:   Thu Jul 23 23:35:59 2015 +0300

    net/mlx5e: TX latency optimization to save DMA reads
    
    A regular TX WQE execution involves two or more DMA reads -
    one to fetch the WQE, and another one per WQE gather entry.
    
    These DMA reads obviously increase the TX latency.
    There are two mlx5 mechanisms to bypass these DMA reads:
    1) Inline WQE
    2) Blue Flame (BF)
    
    An inline WQE contains a whole packet, thus saves the DMA read/s
    of the regular WQE gather entry/s. Inline WQE support was already
    added in the previous commit.
    
    A BF WQE is written directly to the device I/O mapped memory, thus
    enables saving the DMA read that fetches the WQE.
    
    The BF WQE I/O write must be in cache line granularity, thus uses
    the CPU write combining mechanism.
    A BF WQE I/O write acts also as a TX doorbell for notifying the
    device of new TX WQEs.
    A BF WQE is written to the same I/O mapped address as the regular TX
    doorbell, thus this address is being mapped twice - once by ioremap()
    and once by io_mapping_map_wc().
    
    While both mechanisms reduce the TX latency, they both consume more CPU
    cycles than a regular WQE:
    - A BF WQE must still be written to host memory, in addition to being
      written directly to the device I/O mapped memory.
    - An inline WQE involves copying the SKB data into it.
    
    To handle this tradeoff, we introduce here a heuristic algorithm that
    strives to avoid using these two mechanisms in case the TX queue is
    being back-pressured by the device, and limit their usage rate otherwise.
    
    An inline WQE will always be "Blue Flamed" (written directly to the
    device I/O mapped memory) while a BF WQE may not be inlined (may contain
    gather entries).
    
    Preliminary testing using netperf UDP_RR shows that the latency goes down
    from 17.5us to 16.9us, while the message rate (tested with pktgen) stays
    the same.
    
    Signed-off-by: Achiad Shochat <achiad@mellanox.com>
    Signed-off-by: Amir Vadai <amirv@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index d9dc506188c8..b66edd2c5a61 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -60,6 +60,7 @@
 
 #define MLX5E_TX_CQ_POLL_BUDGET        128
 #define MLX5E_UPDATE_STATS_INTERVAL    200 /* msecs */
+#define MLX5E_SQ_BF_BUDGET             16
 
 static const char vport_strings[][ETH_GSTRING_LEN] = {
 	/* vport statistics */
@@ -268,7 +269,9 @@ struct mlx5e_sq {
 	/* dirtied @xmit */
 	u16                        pc ____cacheline_aligned_in_smp;
 	u32                        dma_fifo_pc;
-	u32                        bf_offset;
+	u16                        bf_offset;
+	u16                        prev_cc;
+	u8                         bf_budget;
 	struct mlx5e_sq_stats      stats;
 
 	struct mlx5e_cq            cq;
@@ -281,9 +284,10 @@ struct mlx5e_sq {
 	struct mlx5_wq_cyc         wq;
 	u32                        dma_fifo_mask;
 	void __iomem              *uar_map;
+	void __iomem              *uar_bf_map;
 	struct netdev_queue       *txq;
 	u32                        sqn;
-	u32                        bf_buf_size;
+	u16                        bf_buf_size;
 	u16                        max_inline;
 	u16                        edge;
 	struct device             *pdev;
@@ -493,8 +497,10 @@ int mlx5e_update_priv_params(struct mlx5e_priv *priv,
 			     struct mlx5e_params *new_params);
 
 static inline void mlx5e_tx_notify_hw(struct mlx5e_sq *sq,
-				      struct mlx5e_tx_wqe *wqe)
+				      struct mlx5e_tx_wqe *wqe, int bf_sz)
 {
+	u16 ofst = MLX5_BF_OFFSET + sq->bf_offset;
+
 	/* ensure wqe is visible to device before updating doorbell record */
 	dma_wmb();
 
@@ -505,9 +511,15 @@ static inline void mlx5e_tx_notify_hw(struct mlx5e_sq *sq,
 	 */
 	wmb();
 
-	mlx5_write64((__be32 *)&wqe->ctrl,
-		     sq->uar_map + MLX5_BF_OFFSET + sq->bf_offset,
-		     NULL);
+	if (bf_sz) {
+		__iowrite64_copy(sq->uar_bf_map + ofst, &wqe->ctrl, bf_sz);
+
+		/* flush the write-combining mapped buffer */
+		wmb();
+
+	} else {
+		mlx5_write64((__be32 *)&wqe->ctrl, sq->uar_map + ofst, NULL);
+	}
 
 	sq->bf_offset ^= sq->bf_buf_size;
 }

commit 58d522912ac7d25b63f468fa4a4e8bb059c5144e
Author: Achiad Shochat <achiad@mellanox.com>
Date:   Thu Jul 23 23:35:58 2015 +0300

    net/mlx5e: Support TX packet copy into WQE
    
    AKA inline WQE.
    A TX latency optimization to save data gather DMA reads.
    Controlled by ETHTOOL_TX_COPYBREAK.
    
    Signed-off-by: Achiad Shochat <achiad@mellanox.com>
    Signed-off-by: Amir Vadai <amirv@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 61d8433392aa..d9dc506188c8 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -196,6 +196,7 @@ struct mlx5e_params {
 	bool lro_en;
 	u32 lro_wqe_sz;
 	u8  rss_hfunc;
+	u16 tx_max_inline;
 };
 
 enum {
@@ -520,3 +521,4 @@ static inline void mlx5e_cq_arm(struct mlx5e_cq *cq)
 }
 
 extern const struct ethtool_ops mlx5e_ethtool_ops;
+u16 mlx5e_get_max_inline_cap(struct mlx5_core_dev *mdev);

commit 2be6967cdbc95a9960b620defedbf5e02e2af619
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Thu Jul 23 23:35:56 2015 +0300

    net/mlx5e: Support ETH_RSS_HASH_XOR
    
    The ConnectX-4 HW implements inverted XOR8.
    To make it act as XOR we re-order the HW RSS indirection table.
    
    Set XOR to be the default RSS hash function and add ethtool API to
    control it.
    
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: Amir Vadai <amirv@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 3d23bd657e3c..61d8433392aa 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -195,6 +195,7 @@ struct mlx5e_params {
 	u16 rx_hash_log_tbl_sz;
 	bool lro_en;
 	u32 lro_wqe_sz;
+	u8  rss_hfunc;
 };
 
 enum {

commit e33910548a4090671094336397ed383c252463ff
Author: Achiad Shochat <achiad@mellanox.com>
Date:   Tue Jun 23 17:14:20 2015 +0300

    net/mlx5e: Remove mlx5e_cq.sqrq back-pointer
    
    Use container_of() instead.
    
    Signed-off-by: Achiad Shochat <achiad@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 17069791f680..3d23bd657e3c 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -208,7 +208,6 @@ enum cq_flags {
 struct mlx5e_cq {
 	/* data path - accessed per cqe */
 	struct mlx5_cqwq           wq;
-	void                      *sqrq;
 	unsigned long              flags;
 
 	/* data path - accessed per napi poll */

commit 03289b88e3bcbdf2c8e0f1c92ce4e5a4cfbabf77
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Tue Jun 23 17:14:14 2015 +0300

    net/mlx5e: Static mapping of netdev priv resources to/from netdev TX queues
    
    To save per-packet calculations, we use the following static mappings:
    1) priv {channel, tc} to netdev txq (used @mlx5e_selec_queue())
    2) netdev txq to priv sq (used @mlx5e_xmit())
    
    Thanks to these static mappings, no more need for a separate implementation
    of ndo_start_xmit when multiple TCs are configured.
    We believe the performance improvement of such separation would be negligible, if any.
    The previous way of dynamically calculating the above mappings required
    allocating more TX queues than actually used (@alloc_etherdev_mqs()),
    which is now no longer needed.
    
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index e14120eccf04..17069791f680 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -316,6 +316,7 @@ struct mlx5e_channel {
 	__be32                     mkey_be;
 	u8                         num_tc;
 	unsigned long              flags;
+	int                        tc_to_txq_map[MLX5E_MAX_NUM_TC];
 
 	/* control */
 	struct mlx5e_priv         *priv;
@@ -379,10 +380,9 @@ struct mlx5e_flow_table {
 
 struct mlx5e_priv {
 	/* priv data path fields - start */
-	int                        order_base_2_num_channels;
-	int                        queue_mapping_channel_mask;
 	int                        num_tc;
 	int                        default_vlan_prio;
+	struct mlx5e_sq            **txq_to_sq_map;
 	/* priv data path fields - end */
 
 	unsigned long              state;
@@ -460,7 +460,6 @@ void mlx5e_send_nop(struct mlx5e_sq *sq, bool notify_hw);
 u16 mlx5e_select_queue(struct net_device *dev, struct sk_buff *skb,
 		       void *accel_priv, select_queue_fallback_t fallback);
 netdev_tx_t mlx5e_xmit(struct sk_buff *skb, struct net_device *dev);
-netdev_tx_t mlx5e_xmit_multi_tc(struct sk_buff *skb, struct net_device *dev);
 
 void mlx5e_completion_event(struct mlx5_core_cq *mcq);
 void mlx5e_cq_error_event(struct mlx5_core_cq *mcq, enum mlx5_event event);

commit 3191e05feafaae58c04973ae8a157393eaa30519
Author: Achiad Shochat <achiad@mellanox.com>
Date:   Thu Jun 11 14:47:33 2015 +0300

    net/mlx5e: Add transport domain to the ethernet TIRs/TISs
    
    Allocate and use transport domain by the Ethernet driver code.
    
    Signed-off-by: Achiad Shochat <achiad@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 23cc3bf55991..e14120eccf04 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -389,6 +389,7 @@ struct mlx5e_priv {
 	struct mutex               state_lock; /* Protects Interface state */
 	struct mlx5_uar            cq_uar;
 	u32                        pdn;
+	u32                        tdn;
 	struct mlx5_core_mr        mr;
 
 	struct mlx5e_channel     **channel;

commit 12be4b2190f0e22dc8f7c8ca173bc233d0d46246
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Thu Jun 11 14:47:31 2015 +0300

    net/mlx5e: Support NETIF_F_SG
    
    When NETIF_F_SG is set, each send WQE may have a different size since
    each skb can have different number of fragments as of LSO header etc.
    
    This implies that a given WQE may wrap around the send queue, i.e begin
    at its end and continue at its start. While it is legal by the device spec,
    we preferred a solution that avoids it - when building of current WQE is
    done, if the next WQE may wrap around the send queue, fill the send queue
    with NOPs WQEs till its end, so that the next WQE will begin at send queue
    start.
    
    NOP WQE for itself cannot wrap around the send queue since it is of
    minimal size - 64 bytes, and all send WQEs are a multiple of that size.
    
    Signed-off-by: Achiad Shochat <achiad@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index 71f38bb5dbfc..23cc3bf55991 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -283,6 +283,8 @@ struct mlx5e_sq {
 	struct netdev_queue       *txq;
 	u32                        sqn;
 	u32                        bf_buf_size;
+	u16                        max_inline;
+	u16                        edge;
 	struct device             *pdev;
 	__be32                     mkey_be;
 	unsigned long              state;
@@ -453,6 +455,7 @@ enum mlx5e_link_mode {
 
 #define MLX5E_PROT_MASK(link_mode) (1 << link_mode)
 
+void mlx5e_send_nop(struct mlx5e_sq *sq, bool notify_hw);
 u16 mlx5e_select_queue(struct net_device *dev, struct sk_buff *skb,
 		       void *accel_priv, select_queue_fallback_t fallback);
 netdev_tx_t mlx5e_xmit(struct sk_buff *skb, struct net_device *dev);

commit facc9699f0fe7d65a92cc09e175662659306066d
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Thu Jun 11 14:47:27 2015 +0300

    net/mlx5e: Fix HW MTU settings
    
    Previously we configured HW MTU to be netdev->mtu, actually we
    need to configure netdev->mtu + (ETH_HLEN + VLAN_HLEN + ETH_FCS_LEN).
    
    Also, query MTU can not fail, hence make the relevant helper a
    void functionm, add mlx5e_set_dev_port_mtu, helper function to
    handle MTU setting.
    
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index e9edb7210de1..71f38bb5dbfc 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -57,7 +57,6 @@
 #define MLX5E_PARAMS_DEFAULT_TX_CQ_MODERATION_PKTS      0x20
 #define MLX5E_PARAMS_DEFAULT_MIN_RX_WQES                0x80
 #define MLX5E_PARAMS_DEFAULT_RX_HASH_LOG_TBL_SZ         0x7
-#define MLX5E_PARAMS_MIN_MTU                            46
 
 #define MLX5E_TX_CQ_POLL_BUDGET        128
 #define MLX5E_UPDATE_STATS_INTERVAL    200 /* msecs */

commit d18a9470f89727f870db944a36223bf1bb15bdc1
Author: Majd Dibbiny <majd@mellanox.com>
Date:   Thu Jun 4 19:30:40 2015 +0300

    net/mlx5_core: Make the vport helpers available for the IB driver too
    
    Move the vport header file to be under include/linux/mlx5, such that
    the mlx5 IB can use it as well.
    
    Also add nic_ prefix to the vport NIC commands to differeniate between
    HCA vport commands and NIC vport commands.
    
    Signed-off-by: Majd Dibbiny <majd@mellanox.com>
    Signed-off-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
index cbb3c7cb53f7..e9edb7210de1 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -35,7 +35,7 @@
 #include <linux/mlx5/driver.h>
 #include <linux/mlx5/qp.h>
 #include <linux/mlx5/cq.h>
-#include "vport.h"
+#include <linux/mlx5/vport.h>
 #include "wq.h"
 #include "transobj.h"
 #include "mlx5_core.h"

commit f62b8bb8f2d30582f30f51e85a8c0e1260125d7e
Author: Amir Vadai <amirv@mellanox.com>
Date:   Thu May 28 22:28:48 2015 +0300

    net/mlx5: Extend mlx5_core to support ConnectX-4 Ethernet functionality
    
    This is the Ethernet part of the driver for the Mellanox ConnectX(R)-4
    Single/Dual-Port Adapter supporting 100Gb/s with VPI.  The driver
    extends the existing mlx5 driver with Ethernet functionality.
    
    This patch contains the driver entry points but does not include
    transmit and receive (see the previous patch in the series) routines.
    
    It also adds the option MLX5_CORE_EN to Kconfig to enable/disable the
    Ethernet functionality. Currently, Kconfig is programmed to make
    Ethernet and Infiniband functionality mutally exclusive.
    Also changed MLX5_INFINIBAND to be depandant on MLX5_CORE instead of
    selecting it, since MLX5_CORE could be selected without MLX5_INFINIBAND
    being selected.
    
    Signed-off-by: Amir Vadai <amirv@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en.h b/drivers/net/ethernet/mellanox/mlx5/core/en.h
new file mode 100644
index 000000000000..cbb3c7cb53f7
--- /dev/null
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en.h
@@ -0,0 +1,520 @@
+/*
+ * Copyright (c) 2015, Mellanox Technologies. All rights reserved.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the
+ * OpenIB.org BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+
+#include <linux/if_vlan.h>
+#include <linux/etherdevice.h>
+#include <linux/mlx5/driver.h>
+#include <linux/mlx5/qp.h>
+#include <linux/mlx5/cq.h>
+#include "vport.h"
+#include "wq.h"
+#include "transobj.h"
+#include "mlx5_core.h"
+
+#define MLX5E_MAX_NUM_TC	8
+
+#define MLX5E_PARAMS_MINIMUM_LOG_SQ_SIZE                0x7
+#define MLX5E_PARAMS_DEFAULT_LOG_SQ_SIZE                0xa
+#define MLX5E_PARAMS_MAXIMUM_LOG_SQ_SIZE                0xd
+
+#define MLX5E_PARAMS_MINIMUM_LOG_RQ_SIZE                0x7
+#define MLX5E_PARAMS_DEFAULT_LOG_RQ_SIZE                0xa
+#define MLX5E_PARAMS_MAXIMUM_LOG_RQ_SIZE                0xd
+
+#define MLX5E_PARAMS_DEFAULT_LRO_WQE_SZ                 (16 * 1024)
+#define MLX5E_PARAMS_DEFAULT_RX_CQ_MODERATION_USEC      0x10
+#define MLX5E_PARAMS_DEFAULT_RX_CQ_MODERATION_PKTS      0x20
+#define MLX5E_PARAMS_DEFAULT_TX_CQ_MODERATION_USEC      0x10
+#define MLX5E_PARAMS_DEFAULT_TX_CQ_MODERATION_PKTS      0x20
+#define MLX5E_PARAMS_DEFAULT_MIN_RX_WQES                0x80
+#define MLX5E_PARAMS_DEFAULT_RX_HASH_LOG_TBL_SZ         0x7
+#define MLX5E_PARAMS_MIN_MTU                            46
+
+#define MLX5E_TX_CQ_POLL_BUDGET        128
+#define MLX5E_UPDATE_STATS_INTERVAL    200 /* msecs */
+
+static const char vport_strings[][ETH_GSTRING_LEN] = {
+	/* vport statistics */
+	"rx_packets",
+	"rx_bytes",
+	"tx_packets",
+	"tx_bytes",
+	"rx_error_packets",
+	"rx_error_bytes",
+	"tx_error_packets",
+	"tx_error_bytes",
+	"rx_unicast_packets",
+	"rx_unicast_bytes",
+	"tx_unicast_packets",
+	"tx_unicast_bytes",
+	"rx_multicast_packets",
+	"rx_multicast_bytes",
+	"tx_multicast_packets",
+	"tx_multicast_bytes",
+	"rx_broadcast_packets",
+	"rx_broadcast_bytes",
+	"tx_broadcast_packets",
+	"tx_broadcast_bytes",
+
+	/* SW counters */
+	"tso_packets",
+	"tso_bytes",
+	"lro_packets",
+	"lro_bytes",
+	"rx_csum_good",
+	"rx_csum_none",
+	"tx_csum_offload",
+	"tx_queue_stopped",
+	"tx_queue_wake",
+	"tx_queue_dropped",
+	"rx_wqe_err",
+};
+
+struct mlx5e_vport_stats {
+	/* HW counters */
+	u64 rx_packets;
+	u64 rx_bytes;
+	u64 tx_packets;
+	u64 tx_bytes;
+	u64 rx_error_packets;
+	u64 rx_error_bytes;
+	u64 tx_error_packets;
+	u64 tx_error_bytes;
+	u64 rx_unicast_packets;
+	u64 rx_unicast_bytes;
+	u64 tx_unicast_packets;
+	u64 tx_unicast_bytes;
+	u64 rx_multicast_packets;
+	u64 rx_multicast_bytes;
+	u64 tx_multicast_packets;
+	u64 tx_multicast_bytes;
+	u64 rx_broadcast_packets;
+	u64 rx_broadcast_bytes;
+	u64 tx_broadcast_packets;
+	u64 tx_broadcast_bytes;
+
+	/* SW counters */
+	u64 tso_packets;
+	u64 tso_bytes;
+	u64 lro_packets;
+	u64 lro_bytes;
+	u64 rx_csum_good;
+	u64 rx_csum_none;
+	u64 tx_csum_offload;
+	u64 tx_queue_stopped;
+	u64 tx_queue_wake;
+	u64 tx_queue_dropped;
+	u64 rx_wqe_err;
+
+#define NUM_VPORT_COUNTERS     31
+};
+
+static const char rq_stats_strings[][ETH_GSTRING_LEN] = {
+	"packets",
+	"csum_none",
+	"lro_packets",
+	"lro_bytes",
+	"wqe_err"
+};
+
+struct mlx5e_rq_stats {
+	u64 packets;
+	u64 csum_none;
+	u64 lro_packets;
+	u64 lro_bytes;
+	u64 wqe_err;
+#define NUM_RQ_STATS 5
+};
+
+static const char sq_stats_strings[][ETH_GSTRING_LEN] = {
+	"packets",
+	"tso_packets",
+	"tso_bytes",
+	"csum_offload_none",
+	"stopped",
+	"wake",
+	"dropped",
+	"nop"
+};
+
+struct mlx5e_sq_stats {
+	u64 packets;
+	u64 tso_packets;
+	u64 tso_bytes;
+	u64 csum_offload_none;
+	u64 stopped;
+	u64 wake;
+	u64 dropped;
+	u64 nop;
+#define NUM_SQ_STATS 8
+};
+
+struct mlx5e_stats {
+	struct mlx5e_vport_stats   vport;
+};
+
+struct mlx5e_params {
+	u8  log_sq_size;
+	u8  log_rq_size;
+	u16 num_channels;
+	u8  default_vlan_prio;
+	u8  num_tc;
+	u16 rx_cq_moderation_usec;
+	u16 rx_cq_moderation_pkts;
+	u16 tx_cq_moderation_usec;
+	u16 tx_cq_moderation_pkts;
+	u16 min_rx_wqes;
+	u16 rx_hash_log_tbl_sz;
+	bool lro_en;
+	u32 lro_wqe_sz;
+};
+
+enum {
+	MLX5E_RQ_STATE_POST_WQES_ENABLE,
+};
+
+enum cq_flags {
+	MLX5E_CQ_HAS_CQES = 1,
+};
+
+struct mlx5e_cq {
+	/* data path - accessed per cqe */
+	struct mlx5_cqwq           wq;
+	void                      *sqrq;
+	unsigned long              flags;
+
+	/* data path - accessed per napi poll */
+	struct napi_struct        *napi;
+	struct mlx5_core_cq        mcq;
+	struct mlx5e_channel      *channel;
+
+	/* control */
+	struct mlx5_wq_ctrl        wq_ctrl;
+} ____cacheline_aligned_in_smp;
+
+struct mlx5e_rq {
+	/* data path */
+	struct mlx5_wq_ll      wq;
+	u32                    wqe_sz;
+	struct sk_buff       **skb;
+
+	struct device         *pdev;
+	struct net_device     *netdev;
+	struct mlx5e_rq_stats  stats;
+	struct mlx5e_cq        cq;
+
+	unsigned long          state;
+	int                    ix;
+
+	/* control */
+	struct mlx5_wq_ctrl    wq_ctrl;
+	u32                    rqn;
+	struct mlx5e_channel  *channel;
+} ____cacheline_aligned_in_smp;
+
+struct mlx5e_tx_skb_cb {
+	u32 num_bytes;
+	u8  num_wqebbs;
+	u8  num_dma;
+};
+
+#define MLX5E_TX_SKB_CB(__skb) ((struct mlx5e_tx_skb_cb *)__skb->cb)
+
+struct mlx5e_sq_dma {
+	dma_addr_t addr;
+	u32        size;
+};
+
+enum {
+	MLX5E_SQ_STATE_WAKE_TXQ_ENABLE,
+};
+
+struct mlx5e_sq {
+	/* data path */
+
+	/* dirtied @completion */
+	u16                        cc;
+	u32                        dma_fifo_cc;
+
+	/* dirtied @xmit */
+	u16                        pc ____cacheline_aligned_in_smp;
+	u32                        dma_fifo_pc;
+	u32                        bf_offset;
+	struct mlx5e_sq_stats      stats;
+
+	struct mlx5e_cq            cq;
+
+	/* pointers to per packet info: write@xmit, read@completion */
+	struct sk_buff           **skb;
+	struct mlx5e_sq_dma       *dma_fifo;
+
+	/* read only */
+	struct mlx5_wq_cyc         wq;
+	u32                        dma_fifo_mask;
+	void __iomem              *uar_map;
+	struct netdev_queue       *txq;
+	u32                        sqn;
+	u32                        bf_buf_size;
+	struct device             *pdev;
+	__be32                     mkey_be;
+	unsigned long              state;
+
+	/* control path */
+	struct mlx5_wq_ctrl        wq_ctrl;
+	struct mlx5_uar            uar;
+	struct mlx5e_channel      *channel;
+	int                        tc;
+} ____cacheline_aligned_in_smp;
+
+static inline bool mlx5e_sq_has_room_for(struct mlx5e_sq *sq, u16 n)
+{
+	return (((sq->wq.sz_m1 & (sq->cc - sq->pc)) >= n) ||
+		(sq->cc  == sq->pc));
+}
+
+enum channel_flags {
+	MLX5E_CHANNEL_NAPI_SCHED = 1,
+};
+
+struct mlx5e_channel {
+	/* data path */
+	struct mlx5e_rq            rq;
+	struct mlx5e_sq            sq[MLX5E_MAX_NUM_TC];
+	struct napi_struct         napi;
+	struct device             *pdev;
+	struct net_device         *netdev;
+	__be32                     mkey_be;
+	u8                         num_tc;
+	unsigned long              flags;
+
+	/* control */
+	struct mlx5e_priv         *priv;
+	int                        ix;
+	int                        cpu;
+};
+
+enum mlx5e_traffic_types {
+	MLX5E_TT_IPV4_TCP = 0,
+	MLX5E_TT_IPV6_TCP = 1,
+	MLX5E_TT_IPV4_UDP = 2,
+	MLX5E_TT_IPV6_UDP = 3,
+	MLX5E_TT_IPV4     = 4,
+	MLX5E_TT_IPV6     = 5,
+	MLX5E_TT_ANY      = 6,
+	MLX5E_NUM_TT      = 7,
+};
+
+enum {
+	MLX5E_RQT_SPREADING  = 0,
+	MLX5E_RQT_DEFAULT_RQ = 1,
+	MLX5E_NUM_RQT        = 2,
+};
+
+struct mlx5e_eth_addr_info {
+	u8  addr[ETH_ALEN + 2];
+	u32 tt_vec;
+	u32 ft_ix[MLX5E_NUM_TT]; /* flow table index per traffic type */
+};
+
+#define MLX5E_ETH_ADDR_HASH_SIZE (1 << BITS_PER_BYTE)
+
+struct mlx5e_eth_addr_db {
+	struct hlist_head          netdev_uc[MLX5E_ETH_ADDR_HASH_SIZE];
+	struct hlist_head          netdev_mc[MLX5E_ETH_ADDR_HASH_SIZE];
+	struct mlx5e_eth_addr_info broadcast;
+	struct mlx5e_eth_addr_info allmulti;
+	struct mlx5e_eth_addr_info promisc;
+	bool                       broadcast_enabled;
+	bool                       allmulti_enabled;
+	bool                       promisc_enabled;
+};
+
+enum {
+	MLX5E_STATE_ASYNC_EVENTS_ENABLE,
+	MLX5E_STATE_OPENED,
+};
+
+struct mlx5e_vlan_db {
+	unsigned long active_vlans[BITS_TO_LONGS(VLAN_N_VID)];
+	u32           active_vlans_ft_ix[VLAN_N_VID];
+	u32           untagged_rule_ft_ix;
+	u32           any_vlan_rule_ft_ix;
+	bool          filter_disabled;
+};
+
+struct mlx5e_flow_table {
+	void *vlan;
+	void *main;
+};
+
+struct mlx5e_priv {
+	/* priv data path fields - start */
+	int                        order_base_2_num_channels;
+	int                        queue_mapping_channel_mask;
+	int                        num_tc;
+	int                        default_vlan_prio;
+	/* priv data path fields - end */
+
+	unsigned long              state;
+	struct mutex               state_lock; /* Protects Interface state */
+	struct mlx5_uar            cq_uar;
+	u32                        pdn;
+	struct mlx5_core_mr        mr;
+
+	struct mlx5e_channel     **channel;
+	u32                        tisn[MLX5E_MAX_NUM_TC];
+	u32                        rqtn;
+	u32                        tirn[MLX5E_NUM_TT];
+
+	struct mlx5e_flow_table    ft;
+	struct mlx5e_eth_addr_db   eth_addr;
+	struct mlx5e_vlan_db       vlan;
+
+	struct mlx5e_params        params;
+	spinlock_t                 async_events_spinlock; /* sync hw events */
+	struct work_struct         update_carrier_work;
+	struct work_struct         set_rx_mode_work;
+	struct delayed_work        update_stats_work;
+
+	struct mlx5_core_dev      *mdev;
+	struct net_device         *netdev;
+	struct mlx5e_stats         stats;
+};
+
+#define MLX5E_NET_IP_ALIGN 2
+
+struct mlx5e_tx_wqe {
+	struct mlx5_wqe_ctrl_seg ctrl;
+	struct mlx5_wqe_eth_seg  eth;
+};
+
+struct mlx5e_rx_wqe {
+	struct mlx5_wqe_srq_next_seg  next;
+	struct mlx5_wqe_data_seg      data;
+};
+
+enum mlx5e_link_mode {
+	MLX5E_1000BASE_CX_SGMII	 = 0,
+	MLX5E_1000BASE_KX	 = 1,
+	MLX5E_10GBASE_CX4	 = 2,
+	MLX5E_10GBASE_KX4	 = 3,
+	MLX5E_10GBASE_KR	 = 4,
+	MLX5E_20GBASE_KR2	 = 5,
+	MLX5E_40GBASE_CR4	 = 6,
+	MLX5E_40GBASE_KR4	 = 7,
+	MLX5E_56GBASE_R4	 = 8,
+	MLX5E_10GBASE_CR	 = 12,
+	MLX5E_10GBASE_SR	 = 13,
+	MLX5E_10GBASE_ER	 = 14,
+	MLX5E_40GBASE_SR4	 = 15,
+	MLX5E_40GBASE_LR4	 = 16,
+	MLX5E_100GBASE_CR4	 = 20,
+	MLX5E_100GBASE_SR4	 = 21,
+	MLX5E_100GBASE_KR4	 = 22,
+	MLX5E_100GBASE_LR4	 = 23,
+	MLX5E_100BASE_TX	 = 24,
+	MLX5E_100BASE_T		 = 25,
+	MLX5E_10GBASE_T		 = 26,
+	MLX5E_25GBASE_CR	 = 27,
+	MLX5E_25GBASE_KR	 = 28,
+	MLX5E_25GBASE_SR	 = 29,
+	MLX5E_50GBASE_CR2	 = 30,
+	MLX5E_50GBASE_KR2	 = 31,
+	MLX5E_LINK_MODES_NUMBER,
+};
+
+#define MLX5E_PROT_MASK(link_mode) (1 << link_mode)
+
+u16 mlx5e_select_queue(struct net_device *dev, struct sk_buff *skb,
+		       void *accel_priv, select_queue_fallback_t fallback);
+netdev_tx_t mlx5e_xmit(struct sk_buff *skb, struct net_device *dev);
+netdev_tx_t mlx5e_xmit_multi_tc(struct sk_buff *skb, struct net_device *dev);
+
+void mlx5e_completion_event(struct mlx5_core_cq *mcq);
+void mlx5e_cq_error_event(struct mlx5_core_cq *mcq, enum mlx5_event event);
+int mlx5e_napi_poll(struct napi_struct *napi, int budget);
+bool mlx5e_poll_tx_cq(struct mlx5e_cq *cq);
+bool mlx5e_poll_rx_cq(struct mlx5e_cq *cq, int budget);
+bool mlx5e_post_rx_wqes(struct mlx5e_rq *rq);
+struct mlx5_cqe64 *mlx5e_get_cqe(struct mlx5e_cq *cq);
+
+void mlx5e_update_stats(struct mlx5e_priv *priv);
+
+int mlx5e_open_flow_table(struct mlx5e_priv *priv);
+void mlx5e_close_flow_table(struct mlx5e_priv *priv);
+void mlx5e_init_eth_addr(struct mlx5e_priv *priv);
+void mlx5e_set_rx_mode_core(struct mlx5e_priv *priv);
+void mlx5e_set_rx_mode_work(struct work_struct *work);
+
+int mlx5e_vlan_rx_add_vid(struct net_device *dev, __always_unused __be16 proto,
+			  u16 vid);
+int mlx5e_vlan_rx_kill_vid(struct net_device *dev, __always_unused __be16 proto,
+			   u16 vid);
+void mlx5e_enable_vlan_filter(struct mlx5e_priv *priv);
+void mlx5e_disable_vlan_filter(struct mlx5e_priv *priv);
+int mlx5e_add_all_vlan_rules(struct mlx5e_priv *priv);
+void mlx5e_del_all_vlan_rules(struct mlx5e_priv *priv);
+
+int mlx5e_open_locked(struct net_device *netdev);
+int mlx5e_close_locked(struct net_device *netdev);
+int mlx5e_update_priv_params(struct mlx5e_priv *priv,
+			     struct mlx5e_params *new_params);
+
+static inline void mlx5e_tx_notify_hw(struct mlx5e_sq *sq,
+				      struct mlx5e_tx_wqe *wqe)
+{
+	/* ensure wqe is visible to device before updating doorbell record */
+	dma_wmb();
+
+	*sq->wq.db = cpu_to_be32(sq->pc);
+
+	/* ensure doorbell record is visible to device before ringing the
+	 * doorbell
+	 */
+	wmb();
+
+	mlx5_write64((__be32 *)&wqe->ctrl,
+		     sq->uar_map + MLX5_BF_OFFSET + sq->bf_offset,
+		     NULL);
+
+	sq->bf_offset ^= sq->bf_buf_size;
+}
+
+static inline void mlx5e_cq_arm(struct mlx5e_cq *cq)
+{
+	struct mlx5_core_cq *mcq;
+
+	mcq = &cq->mcq;
+	mlx5_cq_arm(mcq, MLX5_CQ_DB_REQ_NOT, mcq->uar->map, NULL, cq->wq.cc);
+}
+
+extern const struct ethtool_ops mlx5e_ethtool_ops;
