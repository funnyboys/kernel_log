commit a152b85984a03e7f83b9d8bcf908c29597d898fc
Merge: 1e6a70526640 a5dfaa2ab940
Author: David S. Miller <davem@davemloft.net>
Date:   Fri May 22 18:30:34 2020 -0700

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/bpf/bpf-next
    
    Daniel Borkmann says:
    
    ====================
    pull-request: bpf-next 2020-05-23
    
    The following pull-request contains BPF updates for your *net-next* tree.
    
    We've added 50 non-merge commits during the last 8 day(s) which contain
    a total of 109 files changed, 2776 insertions(+), 2887 deletions(-).
    
    The main changes are:
    
    1) Add a new AF_XDP buffer allocation API to the core in order to help
       lowering the bar for drivers adopting AF_XDP support. i40e, ice, ixgbe
       as well as mlx5 have been moved over to the new API and also gained a
       small improvement in performance, from Björn Töpel and Magnus Karlsson.
    
    2) Add getpeername()/getsockname() attach types for BPF sock_addr programs
       in order to allow for e.g. reverse translation of load-balancer backend
       to service address/port tuple from a connected peer, from Daniel Borkmann.
    
    3) Improve the BPF verifier is_branch_taken() logic to evaluate pointers
       being non-NULL, e.g. if after an initial test another non-NULL test on
       that pointer follows in a given path, then it can be pruned right away,
       from John Fastabend.
    
    4) Larger rework of BPF sockmap selftests to make output easier to understand
       and to reduce overall runtime as well as adding new BPF kTLS selftests
       that run in combination with sockmap, also from John Fastabend.
    
    5) Batch of misc updates to BPF selftests including fixing up test_align
       to match verifier output again and moving it under test_progs, allowing
       bpf_iter selftest to compile on machines with older vmlinux.h, and
       updating config options for lirc and v6 segment routing helpers, from
       Stanislav Fomichev, Andrii Nakryiko and Alan Maguire.
    
    6) Conversion of BPF tracing samples outdated internal BPF loader to use
       libbpf API instead, from Daniel T. Lee.
    
    7) Follow-up to BPF kernel test infrastructure in order to fix a flake in
       the XDP selftests, from Jesper Dangaard Brouer.
    
    8) Minor improvements to libbpf's internal hashmap implementation, from
       Ian Rogers.
    ====================
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 39d6443c8daf9fefcfcf89de7ae87240956a0b84
Author: Björn Töpel <bjorn.topel@intel.com>
Date:   Wed May 20 21:20:59 2020 +0200

    mlx5, xsk: Migrate to new MEM_TYPE_XSK_BUFF_POOL
    
    Use the new MEM_TYPE_XSK_BUFF_POOL API in lieu of MEM_TYPE_ZERO_COPY in
    mlx5e. It allows to drop a lot of code from the driver (which is now
    common in AF_XDP core and was related to XSK RX frame allocation, DMA
    mapping, etc.) and slightly improve performance (RX +0.8 Mpps, TX +0.4
    Mpps).
    
    rfc->v1: Put back the sanity check for XSK params, use XSK API to get
             the total headroom size. (Maxim)
    
    v1->v2: Fix DMA address handling, set XDP metadata to invalid. (Maxim)
    
    v2->v3: Handle frame_sz, use xsk_buff_xdp_get_frame_dma, use xsk_buff
            API for DMA sync on TX, add performance numbers. (Maxim)
    
    v3->v4: Remove unused variable num_xsk_frames. (Jakub)
    
    Signed-off-by: Björn Töpel <bjorn.topel@intel.com>
    Signed-off-by: Maxim Mikityanskiy <maximmi@mellanox.com>
    Signed-off-by: Alexei Starovoitov <ast@kernel.org>
    Link: https://lore.kernel.org/bpf/20200520192103.355233-12-bjorn.topel@gmail.com

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h b/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
index e2e01f064c1e..2e4e117aeb49 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
@@ -63,7 +63,7 @@
 struct mlx5e_xsk_param;
 int mlx5e_xdp_max_mtu(struct mlx5e_params *params, struct mlx5e_xsk_param *xsk);
 bool mlx5e_xdp_handle(struct mlx5e_rq *rq, struct mlx5e_dma_info *di,
-		      void *va, u16 *rx_headroom, u32 *len, bool xsk);
+		      u32 *len, struct xdp_buff *xdp);
 void mlx5e_xdp_mpwqe_complete(struct mlx5e_xdpsq *sq);
 bool mlx5e_poll_xdpsq_cq(struct mlx5e_cq *cq);
 void mlx5e_free_xdpsq_descs(struct mlx5e_xdpsq *sq);

commit 5ffb4d858b7051720f20bcbb92dce0b433e60d88
Author: Maxim Mikityanskiy <maximmi@mellanox.com>
Date:   Mon Mar 30 16:28:34 2020 +0300

    net/mlx5e: Calculate SQ stop room in a robust way
    
    Currently, different formulas are used to estimate the space that may be
    taken by WQEs in the SQ during a single packet transmit. This space is
    called stop room, and it's checked in the end of packet transmit to find
    out if the next packet could overflow the SQ. If it could, the driver
    tells the kernel to stop sending next packets.
    
    Many factors affect the stop room:
    
    1. Padding with NOPs to avoid WQEs spanning over page boundaries.
    
    2. Enabled and disabled offloads (TLS, upcoming MPWQE).
    
    3. The maximum size of a WQE.
    
    The padding is performed before every WQE if it doesn't fit the current
    page.
    
    The current formula assumes that only one padding will be required per
    packet, and it doesn't take into account that the WQEs posted during the
    transmission of a single packet might exceed the page size in very rare
    circumstances. For example, to hit this condition with 4096-byte pages,
    TLS offload will have to interrupt an almost-full MPWQE session, be in
    the resync flow and try to transmit a near to maximum amount of data.
    
    To avoid SQ overflows in such rare cases after MPWQE is added, this
    patch introduces a more robust formula to estimate the stop room. The
    new formula uses the fact that a WQE of size X will not require more
    than X-1 WQEBBs of padding. More exact estimations are possible, but
    they result in much more complex and error-prone code for little gain.
    
    Before this patch, the TLS stop room included space for both INNOVA and
    ConnectX TLS offloads that couldn't run at the same time anyway, so this
    patch accounts only for the active one.
    
    Signed-off-by: Maxim Mikityanskiy <maximmi@mellanox.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h b/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
index e2e01f064c1e..be64eb68f4e5 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
@@ -40,8 +40,6 @@
 	(sizeof(struct mlx5e_tx_wqe) / MLX5_SEND_WQE_DS)
 #define MLX5E_XDP_TX_DS_COUNT (MLX5E_XDP_TX_EMPTY_DS_COUNT + 1 /* SG DS */)
 
-#define MLX5E_XDPSQ_STOP_ROOM (MLX5E_SQ_STOP_ROOM)
-
 #define MLX5E_XDP_INLINE_WQE_SZ_THRSD (256 - sizeof(struct mlx5_wqe_inline_seg))
 #define MLX5E_XDP_INLINE_WQE_MAX_DS_CNT \
 	DIV_ROUND_UP(MLX5E_XDP_INLINE_WQE_SZ_THRSD, MLX5_SEND_WQE_DS)

commit 05dfd570826f1ae408e1a3faeddb753ff06fed14
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Thu Apr 9 13:43:43 2020 +0300

    net/mlx5e: Take TX WQE info structures out of general EN header
    
    Into the txrx header file.
    The mlx5e_sq_wqe_info structure describes WQE info for the ICOSQ,
    rename it to better reflect this.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Reviewed-by: Maxim Mikityanskiy <maximmi@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h b/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
index ed6f045febeb..e2e01f064c1e 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
@@ -137,6 +137,11 @@ mlx5e_xdp_no_room_for_inline_pkt(struct mlx5e_xdp_mpwqe *session)
 	       session->ds_count + MLX5E_XDP_INLINE_WQE_MAX_DS_CNT > MLX5E_XDP_MPW_MAX_NUM_DS;
 }
 
+struct mlx5e_xdp_wqe_info {
+	u8 num_wqebbs;
+	u8 num_pkts;
+};
+
 static inline void
 mlx5e_xdp_mpwqe_add_dseg(struct mlx5e_xdpsq *sq,
 			 struct mlx5e_xdp_xmit_data *xdptxd,

commit ec9cdca0663a543ede2072ff091beec1787e3374
Author: Maxim Mikityanskiy <maximmi@mellanox.com>
Date:   Thu Apr 16 11:29:49 2020 +0300

    net/mlx5e: Unify reserving space for WQEs
    
    In our fast-path design, a WQE (Work Queue Element) must not cross the
    page boundary. To enforce that, for WQEs consisting of more than one BB
    (Basic Block), the driver checks the available contiguous space in the
    WQ in advance, and if it's not enough, it pads it with NOPs.
    
    This patch modifies the code that calculates the position of next WQE,
    considering the padding, and prepares the WQE. This code is common for
    all SQ types. In this patch it's reorganized in a way that makes the
    usage pattern unified for all SQ types, and makes the implementations
    self-contained and look almost the same, preparing the repeating code to
    further attempts to deduplicate it.
    
    One place is left as is: mlx5e_sq_xmit and mlx5e_fill_sq_frag_edge call
    inside, because it is special in a way that it may also copy WQE's cseg
    and eseg when reserving space. This will be eliminated in one of the
    following patches, and this place will be converted to the new approach,
    too.
    
    Signed-off-by: Maxim Mikityanskiy <maximmi@mellanox.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h b/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
index 4fd0ff47bdc3..ed6f045febeb 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
@@ -137,23 +137,6 @@ mlx5e_xdp_no_room_for_inline_pkt(struct mlx5e_xdp_mpwqe *session)
 	       session->ds_count + MLX5E_XDP_INLINE_WQE_MAX_DS_CNT > MLX5E_XDP_MPW_MAX_NUM_DS;
 }
 
-static inline void
-mlx5e_fill_xdpsq_frag_edge(struct mlx5e_xdpsq *sq, struct mlx5_wq_cyc *wq,
-			   u16 pi, u16 nnops)
-{
-	struct mlx5e_xdp_wqe_info *edge_wi, *wi = &sq->db.wqe_info[pi];
-
-	edge_wi = wi + nnops;
-	/* fill sq frag edge with nops to avoid wqe wrapping two pages */
-	for (; wi < edge_wi; wi++) {
-		wi->num_wqebbs = 1;
-		wi->num_pkts   = 0;
-		mlx5e_post_nop(wq, sq->sqn, &sq->pc);
-	}
-
-	sq->stats->nops += nnops;
-}
-
 static inline void
 mlx5e_xdp_mpwqe_add_dseg(struct mlx5e_xdpsq *sq,
 			 struct mlx5e_xdp_xmit_data *xdptxd,

commit fed0c6cfcd58f29ff60f47559b88a6289b6b680a
Author: Maxim Mikityanskiy <maximmi@mellanox.com>
Date:   Fri Nov 15 13:48:38 2019 +0200

    net/mlx5e: Fetch WQE: reuse code and enforce typing
    
    There are multiple functions mlx5{e,i}_*_fetch_wqe that contain the same
    code, that is repeated, because they operate on different SQ struct
    types. mlx5e_sq_fetch_wqe also returns void *, instead of the concrete
    WQE type.
    
    This commit generalizes the fetch WQE operation by putting this code
    into a single function. To simplify calls of the generic function in
    concrete use cases, macros are provided that substitute the right WQE
    size and cast the return type.
    
    Before this patch, fetch_wqe used to calculate pi itself, but the value
    was often known to the caller. This calculation is moved outside to
    eliminate this unnecessary step and prepare for the fill_frag_edge
    refactoring in the next patch.
    
    Signed-off-by: Maxim Mikityanskiy <maximmi@mellanox.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h b/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
index d7587f40ecae..4fd0ff47bdc3 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
@@ -186,19 +186,6 @@ mlx5e_xdp_mpwqe_add_dseg(struct mlx5e_xdpsq *sq,
 	session->ds_count++;
 }
 
-static inline struct mlx5e_tx_wqe *
-mlx5e_xdpsq_fetch_wqe(struct mlx5e_xdpsq *sq, u16 *pi)
-{
-	struct mlx5_wq_cyc *wq = &sq->wq;
-	struct mlx5e_tx_wqe *wqe;
-
-	*pi = mlx5_wq_cyc_ctr2ix(wq, sq->pc);
-	wqe = mlx5_wq_cyc_get_wqe(wq, *pi);
-	memset(wqe, 0, sizeof(*wqe));
-
-	return wqe;
-}
-
 static inline void
 mlx5e_xdpi_fifo_push(struct mlx5e_xdp_info_fifo *fifo,
 		     struct mlx5e_xdp_info *xi)

commit 9cf88808ad6a0f1e958e00abd9a081295fe6da0c
Author: Maxim Mikityanskiy <maximmi@mellanox.com>
Date:   Tue Dec 17 16:20:44 2019 +0000

    net/mlx5e: Fix concurrency issues between config flow and XSK
    
    After disabling resources necessary for XSK (the XDP program, channels,
    XSK queues), use synchronize_rcu to wait until the XSK wakeup function
    finishes, before freeing the resources.
    
    Suspend XSK wakeups during switching channels. If the XDP program is
    being removed, synchronize_rcu before closing the old channels to allow
    XSK wakeup to complete.
    
    Signed-off-by: Maxim Mikityanskiy <maximmi@mellanox.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>
    Link: https://lore.kernel.org/bpf/20191217162023.16011-3-maximmi@mellanox.com

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h b/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
index 36ac1e3816b9..d7587f40ecae 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
@@ -75,12 +75,18 @@ int mlx5e_xdp_xmit(struct net_device *dev, int n, struct xdp_frame **frames,
 static inline void mlx5e_xdp_tx_enable(struct mlx5e_priv *priv)
 {
 	set_bit(MLX5E_STATE_XDP_TX_ENABLED, &priv->state);
+
+	if (priv->channels.params.xdp_prog)
+		set_bit(MLX5E_STATE_XDP_ACTIVE, &priv->state);
 }
 
 static inline void mlx5e_xdp_tx_disable(struct mlx5e_priv *priv)
 {
+	if (priv->channels.params.xdp_prog)
+		clear_bit(MLX5E_STATE_XDP_ACTIVE, &priv->state);
+
 	clear_bit(MLX5E_STATE_XDP_TX_ENABLED, &priv->state);
-	/* let other device's napi(s) see our new state */
+	/* Let other device's napi(s) and XSK wakeups see our new state. */
 	synchronize_rcu();
 }
 
@@ -89,19 +95,9 @@ static inline bool mlx5e_xdp_tx_is_enabled(struct mlx5e_priv *priv)
 	return test_bit(MLX5E_STATE_XDP_TX_ENABLED, &priv->state);
 }
 
-static inline void mlx5e_xdp_set_open(struct mlx5e_priv *priv)
-{
-	set_bit(MLX5E_STATE_XDP_OPEN, &priv->state);
-}
-
-static inline void mlx5e_xdp_set_closed(struct mlx5e_priv *priv)
-{
-	clear_bit(MLX5E_STATE_XDP_OPEN, &priv->state);
-}
-
-static inline bool mlx5e_xdp_is_open(struct mlx5e_priv *priv)
+static inline bool mlx5e_xdp_is_active(struct mlx5e_priv *priv)
 {
-	return test_bit(MLX5E_STATE_XDP_OPEN, &priv->state);
+	return test_bit(MLX5E_STATE_XDP_ACTIVE, &priv->state);
 }
 
 static inline void mlx5e_xmit_xdp_doorbell(struct mlx5e_xdpsq *sq)

commit 7cf6f811b72aced0c48e1065fe059d604ef6363d
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Sun Jul 14 17:50:51 2019 +0300

    net/mlx5e: XDP, Slight enhancement for WQE fetch function
    
    Instead of passing an output param, let function return the
    WQE pointer.
    In addition, pass &pi so it gets its value in the function,
    and save the redundant assignment that comes after it.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h b/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
index e0ed7710f5f1..36ac1e3816b9 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
@@ -190,14 +190,17 @@ mlx5e_xdp_mpwqe_add_dseg(struct mlx5e_xdpsq *sq,
 	session->ds_count++;
 }
 
-static inline void mlx5e_xdpsq_fetch_wqe(struct mlx5e_xdpsq *sq,
-					 struct mlx5e_tx_wqe **wqe)
+static inline struct mlx5e_tx_wqe *
+mlx5e_xdpsq_fetch_wqe(struct mlx5e_xdpsq *sq, u16 *pi)
 {
 	struct mlx5_wq_cyc *wq = &sq->wq;
-	u16 pi = mlx5_wq_cyc_ctr2ix(wq, sq->pc);
+	struct mlx5e_tx_wqe *wqe;
 
-	*wqe = mlx5_wq_cyc_get_wqe(wq, pi);
-	memset(*wqe, 0, sizeof(**wqe));
+	*pi = mlx5_wq_cyc_ctr2ix(wq, sq->pc);
+	wqe = mlx5_wq_cyc_get_wqe(wq, *pi);
+	memset(wqe, 0, sizeof(*wqe));
+
+	return wqe;
 }
 
 static inline void

commit 6c085a8aab5183d8658c9a692bcfda3e24195b7a
Author: Shay Agroskin <shayag@mellanox.com>
Date:   Sun May 12 18:28:27 2019 +0300

    net/mlx5e: XDP, Close TX MPWQE session when no room for inline packet left
    
    In MPWQE mode, when transmitting packets with XDP, a packet that is smaller
    than a certain size (set to 256 bytes) would be sent inline within its WQE
    TX descriptor (mem-copied), in case the hardware tx queue is congested
    beyond a pre-defined water-mark.
    
    If a MPWQE cannot contain an additional inline packet, we close this
    MPWQE session, and send the packet inlined within the next MPWQE.
    To save some MPWQE session close+open operations, we don't open MPWQE
    sessions that are contiguously smaller than certain size (set to the
    HW MPWQE maximum size). If there isn't enough contiguous room in the
    send queue, we fill it with NOPs and wrap the send queue index around.
    
    This way, qualified packets are always sent inline.
    
    Perf tests:
    Tested packet rate for UDP 64Byte multi-stream
    over two dual port ConnectX-5 100Gbps NICs.
    CPU: Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz
    
    XDP_TX:
    
    With 24 channels:
    | ------ | bounced packets | inlined packets | inline ratio |
    | before | 113.6Mpps       | 96.3Mpps        | 84%          |
    | after  |   115Mpps       | 99.5Mpps        | 86%          |
    
    With one channel:
    
    | ------ | bounced packets | inlined packets | inline ratio |
    | before | 6.7Mpps         | 0pps            | 0%           |
    | after  | 6.8Mpps         | 0pps            | 0%           |
    
    As we can see, there is improvement in both inline ratio and overall
    packet rate for 24 channels. Also, we see no degradation for the
    one-channel case.
    
    Signed-off-by: Shay Agroskin <shayag@mellanox.com>
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h b/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
index b90923932668..e0ed7710f5f1 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
@@ -40,6 +40,26 @@
 	(sizeof(struct mlx5e_tx_wqe) / MLX5_SEND_WQE_DS)
 #define MLX5E_XDP_TX_DS_COUNT (MLX5E_XDP_TX_EMPTY_DS_COUNT + 1 /* SG DS */)
 
+#define MLX5E_XDPSQ_STOP_ROOM (MLX5E_SQ_STOP_ROOM)
+
+#define MLX5E_XDP_INLINE_WQE_SZ_THRSD (256 - sizeof(struct mlx5_wqe_inline_seg))
+#define MLX5E_XDP_INLINE_WQE_MAX_DS_CNT \
+	DIV_ROUND_UP(MLX5E_XDP_INLINE_WQE_SZ_THRSD, MLX5_SEND_WQE_DS)
+
+/* The mult of MLX5_SEND_WQE_MAX_WQEBBS * MLX5_SEND_WQEBB_NUM_DS
+ * (16 * 4 == 64) does not fit in the 6-bit DS field of Ctrl Segment.
+ * We use a bound lower that MLX5_SEND_WQE_MAX_WQEBBS to let a
+ * full-session WQE be cache-aligned.
+ */
+#if L1_CACHE_BYTES < 128
+#define MLX5E_XDP_MPW_MAX_WQEBBS (MLX5_SEND_WQE_MAX_WQEBBS - 1)
+#else
+#define MLX5E_XDP_MPW_MAX_WQEBBS (MLX5_SEND_WQE_MAX_WQEBBS - 2)
+#endif
+
+#define MLX5E_XDP_MPW_MAX_NUM_DS \
+	(MLX5E_XDP_MPW_MAX_WQEBBS * MLX5_SEND_WQEBB_NUM_DS)
+
 struct mlx5e_xsk_param;
 int mlx5e_xdp_max_mtu(struct mlx5e_params *params, struct mlx5e_xsk_param *xsk);
 bool mlx5e_xdp_handle(struct mlx5e_rq *rq, struct mlx5e_dma_info *di,
@@ -114,6 +134,30 @@ static inline void mlx5e_xdp_update_inline_state(struct mlx5e_xdpsq *sq)
 		session->inline_on = 1;
 }
 
+static inline bool
+mlx5e_xdp_no_room_for_inline_pkt(struct mlx5e_xdp_mpwqe *session)
+{
+	return session->inline_on &&
+	       session->ds_count + MLX5E_XDP_INLINE_WQE_MAX_DS_CNT > MLX5E_XDP_MPW_MAX_NUM_DS;
+}
+
+static inline void
+mlx5e_fill_xdpsq_frag_edge(struct mlx5e_xdpsq *sq, struct mlx5_wq_cyc *wq,
+			   u16 pi, u16 nnops)
+{
+	struct mlx5e_xdp_wqe_info *edge_wi, *wi = &sq->db.wqe_info[pi];
+
+	edge_wi = wi + nnops;
+	/* fill sq frag edge with nops to avoid wqe wrapping two pages */
+	for (; wi < edge_wi; wi++) {
+		wi->num_wqebbs = 1;
+		wi->num_pkts   = 0;
+		mlx5e_post_nop(wq, sq->sqn, &sq->pc);
+	}
+
+	sq->stats->nops += nnops;
+}
+
 static inline void
 mlx5e_xdp_mpwqe_add_dseg(struct mlx5e_xdpsq *sq,
 			 struct mlx5e_xdp_xmit_data *xdptxd,
@@ -126,20 +170,12 @@ mlx5e_xdp_mpwqe_add_dseg(struct mlx5e_xdpsq *sq,
 
 	session->pkt_count++;
 
-#define MLX5E_XDP_INLINE_WQE_SZ_THRSD (256 - sizeof(struct mlx5_wqe_inline_seg))
-
 	if (session->inline_on && dma_len <= MLX5E_XDP_INLINE_WQE_SZ_THRSD) {
 		struct mlx5_wqe_inline_seg *inline_dseg =
 			(struct mlx5_wqe_inline_seg *)dseg;
 		u16 ds_len = sizeof(*inline_dseg) + dma_len;
 		u16 ds_cnt = DIV_ROUND_UP(ds_len, MLX5_SEND_WQE_DS);
 
-		if (unlikely(session->ds_count + ds_cnt > session->max_ds_count)) {
-			/* Not enough space for inline wqe, send with memory pointer */
-			session->complete = true;
-			goto no_inline;
-		}
-
 		inline_dseg->byte_count = cpu_to_be32(dma_len | MLX5_INLINE_SEG);
 		memcpy(inline_dseg->data, xdptxd->data, dma_len);
 
@@ -148,7 +184,6 @@ mlx5e_xdp_mpwqe_add_dseg(struct mlx5e_xdpsq *sq,
 		return;
 	}
 
-no_inline:
 	dseg->addr       = cpu_to_be64(xdptxd->dma_addr);
 	dseg->byte_count = cpu_to_be32(dma_len);
 	dseg->lkey       = sq->mkey_be;

commit 542578c6793698a98cd0b7c77a96b8c6bfdfcb88
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Fri Jul 5 18:30:15 2019 +0300

    net/mlx5e: Move helper functions to a new txrx datapath header
    
    Take datapath helper functions to a new header file en/txrx.h.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Eran Ben Elisha <eranbe@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h b/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
index 2d934c8d3807..b90923932668 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
@@ -33,6 +33,7 @@
 #define __MLX5_EN_XDP_H__
 
 #include "en.h"
+#include "en/txrx.h"
 
 #define MLX5E_XDP_MIN_INLINE (ETH_HLEN + VLAN_HLEN)
 #define MLX5E_XDP_TX_EMPTY_DS_COUNT \

commit db05815b36cbd486c86fd002dfa81c9af6245e25
Author: Maxim Mikityanskiy <maximmi@mellanox.com>
Date:   Wed Jun 26 17:35:38 2019 +0300

    net/mlx5e: Add XSK zero-copy support
    
    This commit adds support for AF_XDP zero-copy RX and TX.
    
    We create a dedicated XSK RQ inside the channel, it means that two
    RQs are running simultaneously: one for non-XSK traffic and the other
    for XSK traffic. The regular and XSK RQs use a single ID namespace split
    into two halves: the lower half is regular RQs, and the upper half is
    XSK RQs. When any zero-copy AF_XDP socket is active, changing the number
    of channels is not allowed, because it would break to mapping between
    XSK RQ IDs and channels.
    
    XSK requires different page allocation and release routines. Such
    functions as mlx5e_{alloc,free}_rx_mpwqe and mlx5e_{get,put}_rx_frag are
    generic enough to be used for both regular and XSK RQs, and they use the
    mlx5e_page_{alloc,release} wrappers around the real allocation
    functions. Function pointers are not used to avoid losing the
    performance with retpolines. Wherever it's certain that the regular
    (non-XSK) page release function should be used, it's called directly.
    
    Only the stats that could be meaningful for XSK are exposed to the
    userspace. Those that don't take part in the XSK flow are not
    considered.
    
    Note that we don't wait for WQEs on the XSK RQ (unlike the regular RQ),
    because the newer xdpsock sample doesn't provide any Fill Ring entries
    at the setup stage.
    
    We create a dedicated XSK SQ in the channel. This separation has its
    advantages:
    
    1. When the UMEM is closed, the XSK SQ can also be closed and stop
    receiving completions. If an existing SQ was used for XSK, it would
    continue receiving completions for the packets of the closed socket. If
    a new UMEM was opened at that point, it would start getting completions
    that don't belong to it.
    
    2. Calculating statistics separately.
    
    When the userspace kicks the TX, the driver triggers a hardware
    interrupt by posting a NOP to a dedicated XSK ICO (internal control
    operations) SQ, in order to trigger NAPI on the right CPU core. This XSK
    ICO SQ is protected by a spinlock, as the userspace application may kick
    the TX from any core.
    
    Store the pointers to the UMEMs in the net device private context,
    independently from the kernel. This way the driver can distinguish
    between the zero-copy and non-zero-copy UMEMs. The kernel function
    xdp_get_umem_from_qid does not care about this difference, but the
    driver is only interested in zero-copy UMEMs, particularly, on the
    cleanup it determines whether to close the XSK RQ and SQ or not by
    looking at the presence of the UMEM. Use state_lock to protect the
    access to this area of UMEM pointers.
    
    LRO isn't compatible with XDP, but there may be active UMEMs while
    XDP is off. If this is the case, don't allow LRO to ensure XDP can
    be reenabled at any time.
    
    The validation of XSK parameters typically happens when XSK queues
    open. However, when the interface is down or the XDP program isn't
    set, it's still possible to have active AF_XDP sockets and even to
    open new, but the XSK queues will be closed. To cover these cases,
    perform the validation also in these flows:
    
    1. A new UMEM is registered, but the XSK queues aren't going to be
    created due to missing XDP program or interface being down.
    
    2. MTU changes while there are UMEMs registered.
    
    Having this early check prevents mlx5e_open_channels from failing
    at a later stage, where recovery is impossible and the application
    has no chance to handle the error, because it got the successful
    return value for an MTU change or XSK open operation.
    
    The performance testing was performed on a machine with the following
    configuration:
    
    - 24 cores of Intel Xeon E5-2620 v3 @ 2.40 GHz
    - Mellanox ConnectX-5 Ex with 100 Gbit/s link
    
    The results with retpoline disabled, single stream:
    
    txonly: 33.3 Mpps (21.5 Mpps with queue and app pinned to the same CPU)
    rxdrop: 12.2 Mpps
    l2fwd: 9.4 Mpps
    
    The results with retpoline enabled, single stream:
    
    txonly: 21.3 Mpps (14.1 Mpps with queue and app pinned to the same CPU)
    rxdrop: 9.9 Mpps
    l2fwd: 6.8 Mpps
    
    Signed-off-by: Maxim Mikityanskiy <maximmi@mellanox.com>
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Acked-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h b/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
index 9200cb9f499b..2d934c8d3807 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
@@ -42,7 +42,8 @@
 struct mlx5e_xsk_param;
 int mlx5e_xdp_max_mtu(struct mlx5e_params *params, struct mlx5e_xsk_param *xsk);
 bool mlx5e_xdp_handle(struct mlx5e_rq *rq, struct mlx5e_dma_info *di,
-		      void *va, u16 *rx_headroom, u32 *len);
+		      void *va, u16 *rx_headroom, u32 *len, bool xsk);
+void mlx5e_xdp_mpwqe_complete(struct mlx5e_xdpsq *sq);
 bool mlx5e_poll_xdpsq_cq(struct mlx5e_cq *cq);
 void mlx5e_free_xdpsq_descs(struct mlx5e_xdpsq *sq);
 void mlx5e_set_xmit_fp(struct mlx5e_xdpsq *sq, bool is_mpw);
@@ -67,6 +68,21 @@ static inline bool mlx5e_xdp_tx_is_enabled(struct mlx5e_priv *priv)
 	return test_bit(MLX5E_STATE_XDP_TX_ENABLED, &priv->state);
 }
 
+static inline void mlx5e_xdp_set_open(struct mlx5e_priv *priv)
+{
+	set_bit(MLX5E_STATE_XDP_OPEN, &priv->state);
+}
+
+static inline void mlx5e_xdp_set_closed(struct mlx5e_priv *priv)
+{
+	clear_bit(MLX5E_STATE_XDP_OPEN, &priv->state);
+}
+
+static inline bool mlx5e_xdp_is_open(struct mlx5e_priv *priv)
+{
+	return test_bit(MLX5E_STATE_XDP_OPEN, &priv->state);
+}
+
 static inline void mlx5e_xmit_xdp_doorbell(struct mlx5e_xdpsq *sq)
 {
 	if (sq->doorbell_cseg) {

commit a011b49f4ed7813777a15da12a426ab939c58f14
Author: Maxim Mikityanskiy <maximmi@mellanox.com>
Date:   Wed Jun 26 17:35:35 2019 +0300

    net/mlx5e: Consider XSK in XDP MTU limit calculation
    
    Use the existing mlx5e_get_linear_rq_headroom function to calculate the
    headroom for mlx5e_xdp_max_mtu. This function takes the XSK headroom
    into consideration, which will be used in the following patches.
    
    Signed-off-by: Maxim Mikityanskiy <maximmi@mellanox.com>
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Acked-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h b/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
index 86db5ad49a42..9200cb9f499b 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
@@ -39,7 +39,8 @@
 	(sizeof(struct mlx5e_tx_wqe) / MLX5_SEND_WQE_DS)
 #define MLX5E_XDP_TX_DS_COUNT (MLX5E_XDP_TX_EMPTY_DS_COUNT + 1 /* SG DS */)
 
-int mlx5e_xdp_max_mtu(struct mlx5e_params *params);
+struct mlx5e_xsk_param;
+int mlx5e_xdp_max_mtu(struct mlx5e_params *params, struct mlx5e_xsk_param *xsk);
 bool mlx5e_xdp_handle(struct mlx5e_rq *rq, struct mlx5e_dma_info *di,
 		      void *va, u16 *rx_headroom, u32 *len);
 bool mlx5e_poll_xdpsq_cq(struct mlx5e_cq *cq);

commit b9673cf5558c0ae1be787611884d4131633f31a8
Author: Maxim Mikityanskiy <maximmi@mellanox.com>
Date:   Wed Jun 26 17:35:33 2019 +0300

    net/mlx5e: Share the XDP SQ for XDP_TX between RQs
    
    Put the XDP SQ that is used for XDP_TX into the channel. It used to be a
    part of the RQ, but with introduction of AF_XDP there will be one more
    RQ that could share the same XDP SQ. This patch is a preparation for
    that change.
    
    Separate XDP_TX statistics per RQ were implemented in one of the previous
    patches.
    
    Signed-off-by: Maxim Mikityanskiy <maximmi@mellanox.com>
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Acked-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h b/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
index 2a5158993349..86db5ad49a42 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
@@ -42,8 +42,8 @@
 int mlx5e_xdp_max_mtu(struct mlx5e_params *params);
 bool mlx5e_xdp_handle(struct mlx5e_rq *rq, struct mlx5e_dma_info *di,
 		      void *va, u16 *rx_headroom, u32 *len);
-bool mlx5e_poll_xdpsq_cq(struct mlx5e_cq *cq, struct mlx5e_rq *rq);
-void mlx5e_free_xdpsq_descs(struct mlx5e_xdpsq *sq, struct mlx5e_rq *rq);
+bool mlx5e_poll_xdpsq_cq(struct mlx5e_cq *cq);
+void mlx5e_free_xdpsq_descs(struct mlx5e_xdpsq *sq);
 void mlx5e_set_xmit_fp(struct mlx5e_xdpsq *sq, bool is_mpw);
 void mlx5e_xdp_rx_poll_complete(struct mlx5e_rq *rq);
 int mlx5e_xdp_xmit(struct net_device *dev, int n, struct xdp_frame **frames,

commit d963fa15113076f69d8a021de393ab4613620cd9
Author: Maxim Mikityanskiy <maximmi@mellanox.com>
Date:   Wed Jun 26 17:35:32 2019 +0300

    net/mlx5e: Refactor struct mlx5e_xdp_info
    
    Currently, struct mlx5e_xdp_info has some issues that have to be cleaned
    up before the upcoming AF_XDP support makes things too complicated and
    messy. This structure is used both when sending the packet and on
    completion. Moreover, the cleanup procedure on completion depends on the
    origin of the packet (XDP_REDIRECT, XDP_TX). Adding AF_XDP support will
    add new flows that use this structure even differently. To avoid
    overcomplicating the code, this commit refactors the usage of this
    structure in the following ways:
    
    1. struct mlx5e_xdp_info is split into two different structures. One is
    struct mlx5e_xdp_xmit_data, a transient structure that doesn't need to
    be stored and is only used while sending the packet. The other is still
    struct mlx5e_xdp_info that is stored in a FIFO and contains the fields
    needed on completion.
    
    2. The fields of struct mlx5e_xdp_info that are used in different flows
    are put into a union. A special enum indicates the cleanup mode and
    helps choose the right union member. This approach is clear and
    explicit. Although it could be possible to "guess" the mode by looking
    at the values of the fields and at the XDP SQ type, it wouldn't be that
    clear and extendable and would require looking through the whole chain
    to understand what's going on.
    
    For the reference, there are the fields of struct mlx5e_xdp_info that
    are used in different flows (including AF_XDP ones):
    
    Packet origin          | Fields used on completion | Cleanup steps
    -----------------------+---------------------------+------------------
    XDP_REDIRECT,          | xdpf, dma_addr            | DMA unmap and
    XDP_TX from XSK RQ     |                           | xdp_return_frame.
    -----------------------+---------------------------+------------------
    XDP_TX from regular RQ | di                        | Recycle page.
    -----------------------+---------------------------+------------------
    AF_XDP TX              | (none)                    | Increment the
                           |                           | producer index in
                           |                           | Completion Ring.
    
    On send, the same set of mlx5e_xdp_xmit_data fields is used in all
    flows: DMA and virtual addresses and length.
    
    Signed-off-by: Maxim Mikityanskiy <maximmi@mellanox.com>
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Acked-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: Daniel Borkmann <daniel@iogearbox.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h b/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
index 8b537a4b0840..2a5158993349 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
@@ -97,15 +97,14 @@ static inline void mlx5e_xdp_update_inline_state(struct mlx5e_xdpsq *sq)
 }
 
 static inline void
-mlx5e_xdp_mpwqe_add_dseg(struct mlx5e_xdpsq *sq, struct mlx5e_xdp_info *xdpi,
+mlx5e_xdp_mpwqe_add_dseg(struct mlx5e_xdpsq *sq,
+			 struct mlx5e_xdp_xmit_data *xdptxd,
 			 struct mlx5e_xdpsq_stats *stats)
 {
 	struct mlx5e_xdp_mpwqe *session = &sq->mpwqe;
-	dma_addr_t dma_addr    = xdpi->dma_addr;
-	struct xdp_frame *xdpf = xdpi->xdpf;
 	struct mlx5_wqe_data_seg *dseg =
 		(struct mlx5_wqe_data_seg *)session->wqe + session->ds_count;
-	u16 dma_len = xdpf->len;
+	u32 dma_len = xdptxd->len;
 
 	session->pkt_count++;
 
@@ -124,7 +123,7 @@ mlx5e_xdp_mpwqe_add_dseg(struct mlx5e_xdpsq *sq, struct mlx5e_xdp_info *xdpi,
 		}
 
 		inline_dseg->byte_count = cpu_to_be32(dma_len | MLX5_INLINE_SEG);
-		memcpy(inline_dseg->data, xdpf->data, dma_len);
+		memcpy(inline_dseg->data, xdptxd->data, dma_len);
 
 		session->ds_count += ds_cnt;
 		stats->inlnw++;
@@ -132,7 +131,7 @@ mlx5e_xdp_mpwqe_add_dseg(struct mlx5e_xdpsq *sq, struct mlx5e_xdp_info *xdpi,
 	}
 
 no_inline:
-	dseg->addr       = cpu_to_be64(dma_addr);
+	dseg->addr       = cpu_to_be64(xdptxd->dma_addr);
 	dseg->byte_count = cpu_to_be32(dma_len);
 	dseg->lkey       = sq->mkey_be;
 	session->ds_count++;

commit 8b4483658364f05b2e32845c8f445cdfd9452286
Merge: c049d56eb219 cd8dead0c394
Author: David S. Miller <davem@davemloft.net>
Date:   Thu Apr 25 23:52:29 2019 -0400

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Two easy cases of overlapping changes.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit c2273219baa5097a4d7c1c162b992623534f34c1
Author: Shay Agroskin <shayag@mellanox.com>
Date:   Thu Mar 14 14:54:07 2019 +0200

    net/mlx5e: XDP, Inline small packets into the TX MPWQE in XDP xmit flow
    
    Upon high packet rate with multiple CPUs TX workloads, much of the HCA's
    resources are spent on prefetching TX descriptors, thus affecting
    transmission rates.
    This patch comes to mitigate this problem by moving some workload to the
    CPU and reducing the HW data prefetch overhead for small packets (<= 256B).
    
    When forwarding packets with XDP, a packet that is smaller
    than a certain size (set to ~256 bytes) would be sent inline within
    its WQE TX descrptor (mem-copied), when the hardware tx queue is congested
    beyond a pre-defined water-mark.
    
    This is added to better utilize the HW resources (which now makes
    one less packet data prefetch) and allow better scalability, on the
    account of CPU usage (which now 'memcpy's the packet into the WQE).
    
    To load balance between HW and CPU and get max packet rate, we use
    watermarks to detect how much the HW is congested and move the work
    loads back and forth between HW and CPU.
    
    Performance:
    Tested packet rate for UDP 64Byte multi-stream
    over two dual port ConnectX-5 100Gbps NICs.
    CPU: Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz
    
    * Tested with hyper-threading disabled
    
    XDP_TX:
    
    |          | before | after   |       |
    | 24 rings | 51Mpps | 116Mpps | +126% |
    | 1 ring   | 12Mpps | 12Mpps  | same  |
    
    XDP_REDIRECT:
    
    ** Below is the transmit rate, not the redirection rate
    which might be larger, and is not affected by this patch.
    
    |          | before  | after   |      |
    | 32 rings | 64Mpps  | 92Mpps  | +43% |
    | 1 ring   | 6.4Mpps | 6.4Mpps | same |
    
    As we can see, feature significantly improves scaling, without
    hurting single ring performance.
    
    Signed-off-by: Shay Agroskin <shayag@mellanox.com>
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h b/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
index ee27a7c8cd87..858e7a2a13ca 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
@@ -75,16 +75,68 @@ static inline void mlx5e_xmit_xdp_doorbell(struct mlx5e_xdpsq *sq)
 	}
 }
 
+/* Enable inline WQEs to shift some load from a congested HCA (HW) to
+ * a less congested cpu (SW).
+ */
+static inline void mlx5e_xdp_update_inline_state(struct mlx5e_xdpsq *sq)
+{
+	u16 outstanding = sq->xdpi_fifo_pc - sq->xdpi_fifo_cc;
+	struct mlx5e_xdp_mpwqe *session = &sq->mpwqe;
+
+#define MLX5E_XDP_INLINE_WATERMARK_LOW	10
+#define MLX5E_XDP_INLINE_WATERMARK_HIGH 128
+
+	if (session->inline_on) {
+		if (outstanding <= MLX5E_XDP_INLINE_WATERMARK_LOW)
+			session->inline_on = 0;
+		return;
+	}
+
+	/* inline is false */
+	if (outstanding >= MLX5E_XDP_INLINE_WATERMARK_HIGH)
+		session->inline_on = 1;
+}
+
 static inline void
-mlx5e_xdp_mpwqe_add_dseg(struct mlx5e_xdpsq *sq, dma_addr_t dma_addr, u16 dma_len)
+mlx5e_xdp_mpwqe_add_dseg(struct mlx5e_xdpsq *sq, struct mlx5e_xdp_info *xdpi,
+			 struct mlx5e_xdpsq_stats *stats)
 {
 	struct mlx5e_xdp_mpwqe *session = &sq->mpwqe;
+	dma_addr_t dma_addr    = xdpi->dma_addr;
+	struct xdp_frame *xdpf = xdpi->xdpf;
 	struct mlx5_wqe_data_seg *dseg =
-		(struct mlx5_wqe_data_seg *)session->wqe + session->ds_count++;
+		(struct mlx5_wqe_data_seg *)session->wqe + session->ds_count;
+	u16 dma_len = xdpf->len;
 
+	session->pkt_count++;
+
+#define MLX5E_XDP_INLINE_WQE_SZ_THRSD (256 - sizeof(struct mlx5_wqe_inline_seg))
+
+	if (session->inline_on && dma_len <= MLX5E_XDP_INLINE_WQE_SZ_THRSD) {
+		struct mlx5_wqe_inline_seg *inline_dseg =
+			(struct mlx5_wqe_inline_seg *)dseg;
+		u16 ds_len = sizeof(*inline_dseg) + dma_len;
+		u16 ds_cnt = DIV_ROUND_UP(ds_len, MLX5_SEND_WQE_DS);
+
+		if (unlikely(session->ds_count + ds_cnt > session->max_ds_count)) {
+			/* Not enough space for inline wqe, send with memory pointer */
+			session->complete = true;
+			goto no_inline;
+		}
+
+		inline_dseg->byte_count = cpu_to_be32(dma_len | MLX5_INLINE_SEG);
+		memcpy(inline_dseg->data, xdpf->data, dma_len);
+
+		session->ds_count += ds_cnt;
+		stats->inlnw++;
+		return;
+	}
+
+no_inline:
 	dseg->addr       = cpu_to_be64(dma_addr);
 	dseg->byte_count = cpu_to_be32(dma_len);
 	dseg->lkey       = sq->mkey_be;
+	session->ds_count++;
 }
 
 static inline void mlx5e_xdpsq_fetch_wqe(struct mlx5e_xdpsq *sq,
@@ -111,5 +163,4 @@ mlx5e_xdpi_fifo_pop(struct mlx5e_xdp_info_fifo *fifo)
 {
 	return fifo->xi[(*fifo->cc)++ & fifo->mask];
 }
-
 #endif

commit d460c2718906252a2a69bc6f89b537071f792e6e
Author: Maxim Mikityanskiy <maximmi@mellanox.com>
Date:   Mon Apr 8 15:12:45 2019 +0300

    net/mlx5e: Fix the max MTU check in case of XDP
    
    MLX5E_XDP_MAX_MTU was calculated incorrectly. It didn't account for
    NET_IP_ALIGN and MLX5E_HW2SW_MTU, and it also misused MLX5_SKB_FRAG_SZ.
    This commit fixes the calculations and adds a brief explanation for the
    formula used.
    
    Fixes: a26a5bdf3ee2d ("net/mlx5e: Restrict the combination of large MTU and XDP")
    Signed-off-by: Maxim Mikityanskiy <maximmi@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h b/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
index ee27a7c8cd87..553956cadc8a 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
@@ -34,13 +34,12 @@
 
 #include "en.h"
 
-#define MLX5E_XDP_MAX_MTU ((int)(PAGE_SIZE - \
-				 MLX5_SKB_FRAG_SZ(XDP_PACKET_HEADROOM)))
 #define MLX5E_XDP_MIN_INLINE (ETH_HLEN + VLAN_HLEN)
 #define MLX5E_XDP_TX_EMPTY_DS_COUNT \
 	(sizeof(struct mlx5e_tx_wqe) / MLX5_SEND_WQE_DS)
 #define MLX5E_XDP_TX_DS_COUNT (MLX5E_XDP_TX_EMPTY_DS_COUNT + 1 /* SG DS */)
 
+int mlx5e_xdp_max_mtu(struct mlx5e_params *params);
 bool mlx5e_xdp_handle(struct mlx5e_rq *rq, struct mlx5e_dma_info *di,
 		      void *va, u16 *rx_headroom, u32 *len);
 bool mlx5e_poll_xdpsq_cq(struct mlx5e_cq *cq, struct mlx5e_rq *rq);

commit 407e17b1a69a51ba9a512a04342da56c1f931df4
Author: Saeed Mahameed <saeedm@mellanox.com>
Date:   Mon Feb 11 16:27:02 2019 -0800

    net/mlx5e: XDP, fix redirect resources availability check
    
    Currently mlx5 driver creates xdp redirect hw queues unconditionally on
    netdevice open, This is great until someone starts redirecting XDP traffic
    via ndo_xdp_xmit on mlx5 device and changes the device configuration at
    the same time, this might cause crashes, since the other device's napi
    is not aware of the mlx5 state change (resources un-availability).
    
    To fix this we must synchronize with other devices napi's on the system.
    Added a new flag under mlx5e_priv to determine XDP TX resources are
    available, set/clear it up when necessary and use synchronize_rcu()
    when the flag is turned off, so other napi's are in-sync with it, before
    we actually cleanup the hw resources.
    
    The flag is tested prior to committing to transmit on mlx5e_xdp_xmit, and
    it is sufficient to determine if it safe to transmit or not. The other
    two internal flags (MLX5E_STATE_OPENED and MLX5E_SQ_STATE_ENABLED) become
    unnecessary. Thus, they are removed from data path.
    
    Fixes: 58b99ee3e3eb ("net/mlx5e: Add support for XDP_REDIRECT in device-out side")
    Reported-by: Toke Høiland-Jørgensen <toke@redhat.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h b/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
index 3a67cb3cd179..ee27a7c8cd87 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
@@ -50,6 +50,23 @@ void mlx5e_xdp_rx_poll_complete(struct mlx5e_rq *rq);
 int mlx5e_xdp_xmit(struct net_device *dev, int n, struct xdp_frame **frames,
 		   u32 flags);
 
+static inline void mlx5e_xdp_tx_enable(struct mlx5e_priv *priv)
+{
+	set_bit(MLX5E_STATE_XDP_TX_ENABLED, &priv->state);
+}
+
+static inline void mlx5e_xdp_tx_disable(struct mlx5e_priv *priv)
+{
+	clear_bit(MLX5E_STATE_XDP_TX_ENABLED, &priv->state);
+	/* let other device's napi(s) see our new state */
+	synchronize_rcu();
+}
+
+static inline bool mlx5e_xdp_tx_is_enabled(struct mlx5e_priv *priv)
+{
+	return test_bit(MLX5E_STATE_XDP_TX_ENABLED, &priv->state);
+}
+
 static inline void mlx5e_xmit_xdp_doorbell(struct mlx5e_xdpsq *sq)
 {
 	if (sq->doorbell_cseg) {

commit 5e0d2eef771ee78b092bf93d040eac02a0965fea
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Wed Nov 21 14:08:06 2018 +0200

    net/mlx5e: XDP, Support Enhanced Multi-Packet TX WQE
    
    Add support for the HW feature of multi-packet WQE in XDP
    xmit flow.
    
    The conventional TX descriptor (WQE, Work Queue Element) serves
    a single packet. Our HW has support for multi-packet WQE (MPWQE)
    in which a single descriptor serves multiple TX packets.
    
    This reduces both the PCI overhead and the CPU cycles wasted on
    writing them.
    
    In this patch we add support for the HW feature, which is supported
    starting from ConnectX-5.
    
    Performance:
    Tested packet rate for UDP 64Byte multi-stream over ConnectX-5 NICs.
    CPU: Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz
    
    XDP_TX:
    We see a huge gain on single port ConnectX-5, and reach the 100 Mpps
    milestone.
    * Single-port HCA:
            Before:   70 Mpps
            After:   100 Mpps (+42.8%)
    
    * Dual-port HCA:
            Before: 51.7 Mpps
            After:  57.3 Mpps (+10.8%)
    
    * In both cases we tested traffic on one port and for now On Dual-port HCAs
      we see only small gain, we are working to overcome this bottleneck, but
      for the moment only with experimental firmware on dual port HCAs we can
      reach the wanted numbers as seen on Single-port HCAs.
    
    XDP_REDIRECT:
    Redirect from (A) ConnectX-5 to (B) ConnectX-5.
    Due to a setup limitation, (A) and (B) are on different NUMA nodes,
    so absolute performance numbers are not optimal.
    Note:
      Below is the transmit rate of (B), not the redirect rate of (A)
      which is in some cases higher.
    
    * (B) is single-port:
            Before:   77 Mpps
            After:    90 Mpps (+16.8%)
    
    * (B) is dual-port:
            Before:  61 Mpps
            After:   72 Mpps (+18%)
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h b/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
index fd689ed506af..3a67cb3cd179 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
@@ -37,15 +37,16 @@
 #define MLX5E_XDP_MAX_MTU ((int)(PAGE_SIZE - \
 				 MLX5_SKB_FRAG_SZ(XDP_PACKET_HEADROOM)))
 #define MLX5E_XDP_MIN_INLINE (ETH_HLEN + VLAN_HLEN)
-#define MLX5E_XDP_TX_DS_COUNT \
-	((sizeof(struct mlx5e_tx_wqe) / MLX5_SEND_WQE_DS) + 1 /* SG DS */)
+#define MLX5E_XDP_TX_EMPTY_DS_COUNT \
+	(sizeof(struct mlx5e_tx_wqe) / MLX5_SEND_WQE_DS)
+#define MLX5E_XDP_TX_DS_COUNT (MLX5E_XDP_TX_EMPTY_DS_COUNT + 1 /* SG DS */)
 
 bool mlx5e_xdp_handle(struct mlx5e_rq *rq, struct mlx5e_dma_info *di,
 		      void *va, u16 *rx_headroom, u32 *len);
 bool mlx5e_poll_xdpsq_cq(struct mlx5e_cq *cq, struct mlx5e_rq *rq);
 void mlx5e_free_xdpsq_descs(struct mlx5e_xdpsq *sq, struct mlx5e_rq *rq);
+void mlx5e_set_xmit_fp(struct mlx5e_xdpsq *sq, bool is_mpw);
 void mlx5e_xdp_rx_poll_complete(struct mlx5e_rq *rq);
-bool mlx5e_xmit_xdp_frame(struct mlx5e_xdpsq *sq, struct mlx5e_xdp_info *xdpi);
 int mlx5e_xdp_xmit(struct net_device *dev, int n, struct xdp_frame **frames,
 		   u32 flags);
 
@@ -57,6 +58,28 @@ static inline void mlx5e_xmit_xdp_doorbell(struct mlx5e_xdpsq *sq)
 	}
 }
 
+static inline void
+mlx5e_xdp_mpwqe_add_dseg(struct mlx5e_xdpsq *sq, dma_addr_t dma_addr, u16 dma_len)
+{
+	struct mlx5e_xdp_mpwqe *session = &sq->mpwqe;
+	struct mlx5_wqe_data_seg *dseg =
+		(struct mlx5_wqe_data_seg *)session->wqe + session->ds_count++;
+
+	dseg->addr       = cpu_to_be64(dma_addr);
+	dseg->byte_count = cpu_to_be32(dma_len);
+	dseg->lkey       = sq->mkey_be;
+}
+
+static inline void mlx5e_xdpsq_fetch_wqe(struct mlx5e_xdpsq *sq,
+					 struct mlx5e_tx_wqe **wqe)
+{
+	struct mlx5_wq_cyc *wq = &sq->wq;
+	u16 pi = mlx5_wq_cyc_ctr2ix(wq, sq->pc);
+
+	*wqe = mlx5_wq_cyc_get_wqe(wq, pi);
+	memset(*wqe, 0, sizeof(**wqe));
+}
+
 static inline void
 mlx5e_xdpi_fifo_push(struct mlx5e_xdp_info_fifo *fifo,
 		     struct mlx5e_xdp_info *xi)

commit fea28dd6a281045e18c1412ab5bba54436c11088
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Sun Oct 14 14:37:48 2018 +0300

    net/mlx5e: XDP, Maintain a FIFO structure for xdp_info instances
    
    This provides infrastructure to have multiple xdp_info instances
    for the same consumer index.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h b/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
index e2faf869e77c..fd689ed506af 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
@@ -57,4 +57,19 @@ static inline void mlx5e_xmit_xdp_doorbell(struct mlx5e_xdpsq *sq)
 	}
 }
 
+static inline void
+mlx5e_xdpi_fifo_push(struct mlx5e_xdp_info_fifo *fifo,
+		     struct mlx5e_xdp_info *xi)
+{
+	u32 i = (*fifo->pc)++ & fifo->mask;
+
+	fifo->xi[i] = *xi;
+}
+
+static inline struct mlx5e_xdp_info
+mlx5e_xdpi_fifo_pop(struct mlx5e_xdp_info_fifo *fifo)
+{
+	return fifo->xi[(*fifo->cc)++ & fifo->mask];
+}
+
 #endif

commit b8180392edd97cd5bfdf12270315a72fe601cf7e
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Wed Nov 21 14:06:02 2018 +0200

    net/mlx5e: XDP, Replace boolean doorbell indication with segment pointer
    
    Instead of calculating the control segment to be used upon an
    XDP xmit doorbell, save it in SQ structure.
    Nullify when no pending doorbell.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h b/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
index 37fcb17e7f27..e2faf869e77c 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
@@ -51,13 +51,10 @@ int mlx5e_xdp_xmit(struct net_device *dev, int n, struct xdp_frame **frames,
 
 static inline void mlx5e_xmit_xdp_doorbell(struct mlx5e_xdpsq *sq)
 {
-	struct mlx5_wq_cyc *wq = &sq->wq;
-	struct mlx5e_tx_wqe *wqe;
-	u16 pi = mlx5_wq_cyc_ctr2ix(wq, sq->pc - 1); /* last pi */
-
-	wqe  = mlx5_wq_cyc_get_wqe(wq, pi);
-
-	mlx5e_notify_hw(wq, sq->pc, sq->uar_map, &wqe->ctrl);
+	if (sq->doorbell_cseg) {
+		mlx5e_notify_hw(&sq->wq, sq->pc, sq->uar_map, sq->doorbell_cseg);
+		sq->doorbell_cseg = NULL;
+	}
 }
 
 #endif

commit feb2ff9d74f76a174e284255b7e537b6c090c13c
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Wed Nov 21 14:04:45 2018 +0200

    net/mlx5e: XDP, Change the XDP SQ redirect indication
    
    Do not maintain an SQ state bit to indicate whether an
    XDP SQ serves redirect operations.
    
    Instead, rely on the fact that such an XDP SQ doesn't reside
    in an RQ instance, while the others do.
    This info is not known to the XDP SQ functions themselves,
    and they rely on their callers to distinguish between the cases.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h b/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
index eecc0392fcff..37fcb17e7f27 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
@@ -42,8 +42,8 @@
 
 bool mlx5e_xdp_handle(struct mlx5e_rq *rq, struct mlx5e_dma_info *di,
 		      void *va, u16 *rx_headroom, u32 *len);
-bool mlx5e_poll_xdpsq_cq(struct mlx5e_cq *cq);
-void mlx5e_free_xdpsq_descs(struct mlx5e_xdpsq *sq);
+bool mlx5e_poll_xdpsq_cq(struct mlx5e_cq *cq, struct mlx5e_rq *rq);
+void mlx5e_free_xdpsq_descs(struct mlx5e_xdpsq *sq, struct mlx5e_rq *rq);
 void mlx5e_xdp_rx_poll_complete(struct mlx5e_rq *rq);
 bool mlx5e_xmit_xdp_frame(struct mlx5e_xdpsq *sq, struct mlx5e_xdp_info *xdpi);
 int mlx5e_xdp_xmit(struct net_device *dev, int n, struct xdp_frame **frames,

commit 4fb2f516186ea25dea326f97880431e14a5b9e9d
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Wed Nov 21 13:56:17 2018 +0200

    net/mlx5e: XDP, Precede XDP-related operations in RQ poll by a loaded program check
    
    At the end of the RQ polling loop, some XDP-related operations
    might be required. Before checking them one by one, check if
    an XDP program is even loaded.
    Combine all the checks and operations in a single function in xdp files.
    
    This saves unnecessary checks for non-XDP flows.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h b/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
index 6dfab045925f..eecc0392fcff 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
@@ -44,7 +44,7 @@ bool mlx5e_xdp_handle(struct mlx5e_rq *rq, struct mlx5e_dma_info *di,
 		      void *va, u16 *rx_headroom, u32 *len);
 bool mlx5e_poll_xdpsq_cq(struct mlx5e_cq *cq);
 void mlx5e_free_xdpsq_descs(struct mlx5e_xdpsq *sq);
-
+void mlx5e_xdp_rx_poll_complete(struct mlx5e_rq *rq);
 bool mlx5e_xmit_xdp_frame(struct mlx5e_xdpsq *sq, struct mlx5e_xdp_info *xdpi);
 int mlx5e_xdp_xmit(struct net_device *dev, int n, struct xdp_frame **frames,
 		   u32 flags);

commit 58b99ee3e3ebecfaccc5641a4014d92a818494a5
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Tue May 22 16:48:48 2018 +0300

    net/mlx5e: Add support for XDP_REDIRECT in device-out side
    
    Add implementation for the ndo_xdp_xmit callback.
    
    Dedicate a new set of XDP-SQ instances to satisfy the XDP_REDIRECT
    requests.  These instances are totally separated from the existing
    XDP-SQ objects that satisfy local XDP_TX actions.
    
    Performance tests:
    
    xdp_redirect_map from ConnectX-5 to ConnectX-5.
    CPU: Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz
    Packet-rate of 64B packets.
    
    Single queue: 7 Mpps.
    Multi queue: 55 Mpps.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Eugenia Emantayev <eugenia@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h b/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
index 81739aad0188..6dfab045925f 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
@@ -46,6 +46,8 @@ bool mlx5e_poll_xdpsq_cq(struct mlx5e_cq *cq);
 void mlx5e_free_xdpsq_descs(struct mlx5e_xdpsq *sq);
 
 bool mlx5e_xmit_xdp_frame(struct mlx5e_xdpsq *sq, struct mlx5e_xdp_info *xdpi);
+int mlx5e_xdp_xmit(struct net_device *dev, int n, struct xdp_frame **frames,
+		   u32 flags);
 
 static inline void mlx5e_xmit_xdp_doorbell(struct mlx5e_xdpsq *sq)
 {

commit c94e4f117e473dec11c7b9395b4d88cae2ba27c9
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Sun Jul 15 10:34:39 2018 +0300

    net/mlx5e: Make XDP xmit functions more generic
    
    Convert the XDP xmit functions to use the generic xdp_frame API
    in XDP_TX flow.
    Same functions will be used later in this series to transmit
    the XDP redirect-out packets as well.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Eugenia Emantayev <eugenia@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h b/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
index a8a856a82c63..81739aad0188 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
@@ -45,8 +45,7 @@ bool mlx5e_xdp_handle(struct mlx5e_rq *rq, struct mlx5e_dma_info *di,
 bool mlx5e_poll_xdpsq_cq(struct mlx5e_cq *cq);
 void mlx5e_free_xdpsq_descs(struct mlx5e_xdpsq *sq);
 
-bool mlx5e_xmit_xdp_frame(struct mlx5e_rq *rq, struct mlx5e_dma_info *di,
-			  const struct xdp_buff *xdp);
+bool mlx5e_xmit_xdp_frame(struct mlx5e_xdpsq *sq, struct mlx5e_xdp_info *xdpi);
 
 static inline void mlx5e_xmit_xdp_doorbell(struct mlx5e_xdpsq *sq)
 {

commit 159d21313423b5ffe301834273cba79e915c65ee
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Sun Jul 15 10:28:44 2018 +0300

    net/mlx5e: Move XDP related code into new XDP files
    
    Take XDP code out of the general EN header and RX file into
    new XDP files.
    
    Currently, XDP-SQ resides only within an RQ and used from a
    single flow (XDP_TX) triggered upon RX completions.
    In a downstream patch, additional type of XDP-SQ instances will be
    presented and used for the XDP_REDIRECT flow, totally unrelated to
    the RX context.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h b/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
new file mode 100644
index 000000000000..a8a856a82c63
--- /dev/null
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en/xdp.h
@@ -0,0 +1,62 @@
+/*
+ * Copyright (c) 2018, Mellanox Technologies. All rights reserved.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the
+ * OpenIB.org BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ */
+#ifndef __MLX5_EN_XDP_H__
+#define __MLX5_EN_XDP_H__
+
+#include "en.h"
+
+#define MLX5E_XDP_MAX_MTU ((int)(PAGE_SIZE - \
+				 MLX5_SKB_FRAG_SZ(XDP_PACKET_HEADROOM)))
+#define MLX5E_XDP_MIN_INLINE (ETH_HLEN + VLAN_HLEN)
+#define MLX5E_XDP_TX_DS_COUNT \
+	((sizeof(struct mlx5e_tx_wqe) / MLX5_SEND_WQE_DS) + 1 /* SG DS */)
+
+bool mlx5e_xdp_handle(struct mlx5e_rq *rq, struct mlx5e_dma_info *di,
+		      void *va, u16 *rx_headroom, u32 *len);
+bool mlx5e_poll_xdpsq_cq(struct mlx5e_cq *cq);
+void mlx5e_free_xdpsq_descs(struct mlx5e_xdpsq *sq);
+
+bool mlx5e_xmit_xdp_frame(struct mlx5e_rq *rq, struct mlx5e_dma_info *di,
+			  const struct xdp_buff *xdp);
+
+static inline void mlx5e_xmit_xdp_doorbell(struct mlx5e_xdpsq *sq)
+{
+	struct mlx5_wq_cyc *wq = &sq->wq;
+	struct mlx5e_tx_wqe *wqe;
+	u16 pi = mlx5_wq_cyc_ctr2ix(wq, sq->pc - 1); /* last pi */
+
+	wqe  = mlx5_wq_cyc_get_wqe(wq, pi);
+
+	mlx5e_notify_hw(wq, sq->pc, sq->uar_map, &wqe->ctrl);
+}
+
+#endif
