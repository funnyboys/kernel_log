commit 714c88a38bce3477392944d6c0d0bc724abaff09
Author: Maxim Mikityanskiy <maximmi@mellanox.com>
Date:   Wed Jan 29 14:35:09 2020 +0200

    net/mlx5e: Split TX acceleration offloads into two phases
    
    After previous modifications, the offloads are no longer called one by
    one, the pi is calculated and the wqe is cleared on between of TLS and
    IPSEC offloads, which doesn't quite fit mlx5e_accel_handle_tx's purpose.
    
    This patch splits mlx5e_accel_handle_tx into two functions that
    correspond to two logical phases of running offloads:
    
    1. Before fetching a WQE. Here runs the code that can post WQEs on its
    own, before the main WQE is fetched. It's the main part of TLS offload.
    
    2. After fetching a WQE. Here runs the code that updates the WQE's
    fields, but can't post other WQEs any more. It's a minor part of TLS
    offload that sets the tisn field in the cseg, and eseg-based offloads
    (currently IPSEC, and later patches will move GENEVE and checksum
    offloads there, too).
    
    It allows to make mlx5e_xmit take care of all actions needed to transmit
    a packet in the right order, improve the structure of the code and
    reduce unnecessary operations. The structure will be further improved in
    the following patches (all eseg-based offloads will be moved to a single
    place, and reserving space for the main WQE will happen between phase 1
    and phase 2 of offloads to eliminate unneeded data movements).
    
    Signed-off-by: Maxim Mikityanskiy <maximmi@mellanox.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en_accel/en_accel.h b/drivers/net/ethernet/mellanox/mlx5/core/en_accel/en_accel.h
index d286fb09955c..fac145dcf2ce 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_accel/en_accel.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_accel/en_accel.h
@@ -102,35 +102,44 @@ mlx5e_udp_gso_handle_tx_skb(struct sk_buff *skb)
 	udp_hdr(skb)->len = htons(payload_len);
 }
 
-static inline bool mlx5e_accel_handle_tx(struct sk_buff *skb,
-					 struct mlx5e_txqsq *sq,
-					 struct net_device *dev,
-					 struct mlx5e_tx_wqe **wqe,
-					 u16 *pi)
-{
+struct mlx5e_accel_tx_state {
 #ifdef CONFIG_MLX5_EN_TLS
-	u32 tls_tisn = 0;
+	struct mlx5e_accel_tx_tls_state tls;
 #endif
+};
 
+static inline bool mlx5e_accel_tx_begin(struct net_device *dev,
+					struct mlx5e_txqsq *sq,
+					struct sk_buff *skb,
+					struct mlx5e_accel_tx_state *state)
+{
 	if (skb_is_gso(skb) && skb_shinfo(skb)->gso_type & SKB_GSO_UDP_L4)
 		mlx5e_udp_gso_handle_tx_skb(skb);
 
 #ifdef CONFIG_MLX5_EN_TLS
 	if (test_bit(MLX5E_SQ_STATE_TLS, &sq->state)) {
 		/* May send SKBs and WQEs. */
-		if (unlikely(!mlx5e_tls_handle_tx_skb(dev, sq, skb, &tls_tisn)))
+		if (unlikely(!mlx5e_tls_handle_tx_skb(dev, sq, skb, &state->tls)))
 			return false;
 	}
+#endif
 
-	*pi = mlx5_wq_cyc_ctr2ix(&sq->wq, sq->pc);
-	*wqe = MLX5E_TX_FETCH_WQE(sq, *pi);
+	return true;
+}
 
-	(*wqe)->ctrl.tisn = cpu_to_be32(tls_tisn << 8);
+static inline bool mlx5e_accel_tx_finish(struct mlx5e_priv *priv,
+					 struct mlx5e_txqsq *sq,
+					 struct sk_buff *skb,
+					 struct mlx5e_tx_wqe *wqe,
+					 struct mlx5e_accel_tx_state *state)
+{
+#ifdef CONFIG_MLX5_EN_TLS
+	mlx5e_tls_handle_tx_wqe(sq, &wqe->ctrl, &state->tls);
 #endif
 
 #ifdef CONFIG_MLX5_EN_IPSEC
 	if (test_bit(MLX5E_SQ_STATE_IPSEC, &sq->state)) {
-		if (unlikely(!mlx5e_ipsec_handle_tx_skb(dev, &(*wqe)->eth, skb)))
+		if (unlikely(!mlx5e_ipsec_handle_tx_skb(priv, &wqe->eth, skb)))
 			return false;
 	}
 #endif

commit 5546100038eeff96fd0361f9b820405c016f8578
Author: Maxim Mikityanskiy <maximmi@mellanox.com>
Date:   Mon Dec 9 12:30:42 2019 +0200

    net/mlx5e: Update UDP fields of the SKB for GSO first
    
    mlx5e_udp_gso_handle_tx_skb updates the length field in the UDP header
    in case of GSO. It doesn't interfere with other offloads, so do it first
    to simplify further restructuring of the code. This way we'll make all
    independent modifications to the SKB before starting to work with WQEs.
    
    Signed-off-by: Maxim Mikityanskiy <maximmi@mellanox.com>
    Reviewed-by: Raed Salem <raeds@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en_accel/en_accel.h b/drivers/net/ethernet/mellanox/mlx5/core/en_accel/en_accel.h
index 66bfab021d6b..d286fb09955c 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_accel/en_accel.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_accel/en_accel.h
@@ -110,7 +110,12 @@ static inline bool mlx5e_accel_handle_tx(struct sk_buff *skb,
 {
 #ifdef CONFIG_MLX5_EN_TLS
 	u32 tls_tisn = 0;
+#endif
+
+	if (skb_is_gso(skb) && skb_shinfo(skb)->gso_type & SKB_GSO_UDP_L4)
+		mlx5e_udp_gso_handle_tx_skb(skb);
 
+#ifdef CONFIG_MLX5_EN_TLS
 	if (test_bit(MLX5E_SQ_STATE_TLS, &sq->state)) {
 		/* May send SKBs and WQEs. */
 		if (unlikely(!mlx5e_tls_handle_tx_skb(dev, sq, skb, &tls_tisn)))
@@ -130,9 +135,6 @@ static inline bool mlx5e_accel_handle_tx(struct sk_buff *skb,
 	}
 #endif
 
-	if (skb_is_gso(skb) && skb_shinfo(skb)->gso_type & SKB_GSO_UDP_L4)
-		mlx5e_udp_gso_handle_tx_skb(skb);
-
 	return true;
 }
 

commit 2eeb6e384102e1124d0a5633803dda0cdbcac471
Author: Maxim Mikityanskiy <maximmi@mellanox.com>
Date:   Fri Dec 6 16:42:09 2019 +0200

    net/mlx5e: Make TLS offload independent of wqe and pi
    
    TLS offload may write a 32-bit field (tisn) to the cseg of the WQE. To
    do that, it receives pi and wqe pointers. As TLS offload may also send
    additional WQEs, it has to update pi and wqe, and in many cases it even
    doesn't use pi calculated before and wqe zeroed before and does it
    itself. Also, mlx5e_sq_xmit has to copy the whole cseg if it goes to the
    mlx5e_fill_sq_frag_edge flow. This all is not efficient.
    
    It's more efficient to do the following:
    
    1. Just return tisn from TLS offload and make the caller fill it in a
    more appropriate place.
    
    2. Calculate pi and clear wqe after calling TLS offload.
    
    3. If TLS offload has to send WQEs, calculate pi and clear wqe just
    before that. It's already done in all places anyway, so this commit
    allows to remove some redundant memsets and calls.
    
    Copying of cseg will be eliminated in one of the following commits, and
    all other stuff is done here.
    
    Signed-off-by: Maxim Mikityanskiy <maximmi@mellanox.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en_accel/en_accel.h b/drivers/net/ethernet/mellanox/mlx5/core/en_accel/en_accel.h
index c658c8556863..66bfab021d6b 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_accel/en_accel.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_accel/en_accel.h
@@ -109,10 +109,18 @@ static inline bool mlx5e_accel_handle_tx(struct sk_buff *skb,
 					 u16 *pi)
 {
 #ifdef CONFIG_MLX5_EN_TLS
+	u32 tls_tisn = 0;
+
 	if (test_bit(MLX5E_SQ_STATE_TLS, &sq->state)) {
-		if (unlikely(!mlx5e_tls_handle_tx_skb(dev, sq, skb, wqe, pi)))
+		/* May send SKBs and WQEs. */
+		if (unlikely(!mlx5e_tls_handle_tx_skb(dev, sq, skb, &tls_tisn)))
 			return false;
 	}
+
+	*pi = mlx5_wq_cyc_ctr2ix(&sq->wq, sq->pc);
+	*wqe = MLX5E_TX_FETCH_WQE(sq, *pi);
+
+	(*wqe)->ctrl.tisn = cpu_to_be32(tls_tisn << 8);
 #endif
 
 #ifdef CONFIG_MLX5_EN_IPSEC

commit 0bdb078c74854c48bffa323899f2e0c5ca924e72
Author: Maxim Mikityanskiy <maximmi@mellanox.com>
Date:   Fri Dec 6 16:14:57 2019 +0200

    net/mlx5e: Pass only eseg to IPSEC offload
    
    IPSEC offload needs to modify the eseg of the WQE that is being filled,
    but it receives a pointer to the whole WQE. To make the contract
    stricter, pass only the pointer to the eseg of that WQE. This commit is
    preparation for the following refactoring of offloads in the TX path and
    for the MPWQE support.
    
    Signed-off-by: Maxim Mikityanskiy <maximmi@mellanox.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en_accel/en_accel.h b/drivers/net/ethernet/mellanox/mlx5/core/en_accel/en_accel.h
index 6249998444c0..c658c8556863 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_accel/en_accel.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_accel/en_accel.h
@@ -117,7 +117,7 @@ static inline bool mlx5e_accel_handle_tx(struct sk_buff *skb,
 
 #ifdef CONFIG_MLX5_EN_IPSEC
 	if (test_bit(MLX5E_SQ_STATE_IPSEC, &sq->state)) {
-		if (unlikely(!mlx5e_ipsec_handle_tx_skb(dev, *wqe, skb)))
+		if (unlikely(!mlx5e_ipsec_handle_tx_skb(dev, &(*wqe)->eth, skb)))
 			return false;
 	}
 #endif

commit f02bac9ad6415e40bf32bf84ce5832698ebe5d15
Author: Maxim Mikityanskiy <maximmi@mellanox.com>
Date:   Thu Dec 5 18:07:25 2019 +0200

    net/mlx5e: Return bool from TLS and IPSEC offloads
    
    TLS and IPSEC offloads currently return struct sk_buff *, but the value
    is either NULL or the same skb that was passed as a parameter. Return
    bool instead to provide stronger guarantees to the calling code (it
    won't need to support handling a different SKB that could be potentially
    returned before this change) and to simplify restructuring this code in
    the following commits.
    
    Signed-off-by: Maxim Mikityanskiy <maximmi@mellanox.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en_accel/en_accel.h b/drivers/net/ethernet/mellanox/mlx5/core/en_accel/en_accel.h
index a6f65d4b2f36..6249998444c0 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_accel/en_accel.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_accel/en_accel.h
@@ -102,33 +102,30 @@ mlx5e_udp_gso_handle_tx_skb(struct sk_buff *skb)
 	udp_hdr(skb)->len = htons(payload_len);
 }
 
-static inline struct sk_buff *
-mlx5e_accel_handle_tx(struct sk_buff *skb,
-		      struct mlx5e_txqsq *sq,
-		      struct net_device *dev,
-		      struct mlx5e_tx_wqe **wqe,
-		      u16 *pi)
+static inline bool mlx5e_accel_handle_tx(struct sk_buff *skb,
+					 struct mlx5e_txqsq *sq,
+					 struct net_device *dev,
+					 struct mlx5e_tx_wqe **wqe,
+					 u16 *pi)
 {
 #ifdef CONFIG_MLX5_EN_TLS
 	if (test_bit(MLX5E_SQ_STATE_TLS, &sq->state)) {
-		skb = mlx5e_tls_handle_tx_skb(dev, sq, skb, wqe, pi);
-		if (unlikely(!skb))
-			return NULL;
+		if (unlikely(!mlx5e_tls_handle_tx_skb(dev, sq, skb, wqe, pi)))
+			return false;
 	}
 #endif
 
 #ifdef CONFIG_MLX5_EN_IPSEC
 	if (test_bit(MLX5E_SQ_STATE_IPSEC, &sq->state)) {
-		skb = mlx5e_ipsec_handle_tx_skb(dev, *wqe, skb);
-		if (unlikely(!skb))
-			return NULL;
+		if (unlikely(!mlx5e_ipsec_handle_tx_skb(dev, *wqe, skb)))
+			return false;
 	}
 #endif
 
 	if (skb_is_gso(skb) && skb_shinfo(skb)->gso_type & SKB_GSO_UDP_L4)
 		mlx5e_udp_gso_handle_tx_skb(skb);
 
-	return skb;
+	return true;
 }
 
 #endif /* __MLX5E_EN_ACCEL_H__ */

commit 9425c595bd513948537ef355c07a65595dd2c771
Author: Raed Salem <raeds@mellanox.com>
Date:   Mon Jan 20 15:03:00 2020 +0200

    net/mlx5e: en_accel, Add missing net/geneve.h include
    
    The cited commit relies on include <net/geneve.h> being included
    implicitly prior to include "en_accel/en_accel.h".
    This mandates that all files that needs to include en_accel.h
    to redantantly include net/geneve.h.
    
    Include net/geneve.h explicitly at "en_accel/en_accel.h" to avoid
    undesired constrain as above.
    
    Fixes: e3cfc7e6b7bd ("net/mlx5e: TX, Add geneve tunnel stateless offload support")
    Signed-off-by: Raed Salem <raeds@mellanox.com>
    Reviewed-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en_accel/en_accel.h b/drivers/net/ethernet/mellanox/mlx5/core/en_accel/en_accel.h
index 3022463f2284..a6f65d4b2f36 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_accel/en_accel.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_accel/en_accel.h
@@ -42,6 +42,8 @@
 #include "en/txrx.h"
 
 #if IS_ENABLED(CONFIG_GENEVE)
+#include <net/geneve.h>
+
 static inline bool mlx5_geneve_tx_allowed(struct mlx5_core_dev *mdev)
 {
 	return mlx5_tx_swp_supported(mdev);

commit 542578c6793698a98cd0b7c77a96b8c6bfdfcb88
Author: Tariq Toukan <tariqt@mellanox.com>
Date:   Fri Jul 5 18:30:15 2019 +0300

    net/mlx5e: Move helper functions to a new txrx datapath header
    
    Take datapath helper functions to a new header file en/txrx.h.
    
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Eran Ben Elisha <eranbe@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en_accel/en_accel.h b/drivers/net/ethernet/mellanox/mlx5/core/en_accel/en_accel.h
index 6da7c88742dc..3022463f2284 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_accel/en_accel.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_accel/en_accel.h
@@ -39,6 +39,7 @@
 #include "en_accel/ipsec_rxtx.h"
 #include "en_accel/tls_rxtx.h"
 #include "en.h"
+#include "en/txrx.h"
 
 #if IS_ENABLED(CONFIG_GENEVE)
 static inline bool mlx5_geneve_tx_allowed(struct mlx5_core_dev *mdev)

commit e3cfc7e6b7bd59e4ce244deedbad4d1b7a609415
Author: Moshe Shemesh <moshe@mellanox.com>
Date:   Thu Mar 21 15:51:38 2019 -0700

    net/mlx5e: TX, Add geneve tunnel stateless offload support
    
    Currently support only default geneve udp port (6081).
    For the tx side, the HW is assisted by SW parsing, which sets the
    headers offset to offload tunneled LSO and csum. Note that for udp
    tunnels, we don't use special rx offloads, as rss on the outer headers
    is enough, we support checksum complete and GRO takes care of
    aggregation.
    
    Geneve TSO BW and CPU load results (tested using iperf single tcp
    stream).
    In this patch we add TSO support over Geneve, so the "before" result
    doesn't actually get to using the TSO HW offload even when turned on.
    Tested on ConnectX-5, Intel(R) Xeon(R) CPU E5-2660 v2 @2.20GHz.
    
     __________________________________
    | Before         | After           |
    |________________|_________________|
    | 12.6 Gbits/sec | 21.7 Gbits/sec  |
    | 100% CPU load  | 61.5% CPU load  |
    |________________|_________________|
    
    Signed-off-by: Moshe Shemesh <moshe@mellanox.com>
    Acked-by: Or Gerlitz <ogerlitz@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en_accel/en_accel.h b/drivers/net/ethernet/mellanox/mlx5/core/en_accel/en_accel.h
index 1dd225380a66..6da7c88742dc 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_accel/en_accel.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_accel/en_accel.h
@@ -40,6 +40,57 @@
 #include "en_accel/tls_rxtx.h"
 #include "en.h"
 
+#if IS_ENABLED(CONFIG_GENEVE)
+static inline bool mlx5_geneve_tx_allowed(struct mlx5_core_dev *mdev)
+{
+	return mlx5_tx_swp_supported(mdev);
+}
+
+static inline void
+mlx5e_tx_tunnel_accel(struct sk_buff *skb, struct mlx5_wqe_eth_seg *eseg)
+{
+	struct mlx5e_swp_spec swp_spec = {};
+	unsigned int offset = 0;
+	__be16 l3_proto;
+	u8 l4_proto;
+
+	l3_proto = vlan_get_protocol(skb);
+	switch (l3_proto) {
+	case htons(ETH_P_IP):
+		l4_proto = ip_hdr(skb)->protocol;
+		break;
+	case htons(ETH_P_IPV6):
+		l4_proto = ipv6_find_hdr(skb, &offset, -1, NULL, NULL);
+		break;
+	default:
+		return;
+	}
+
+	if (l4_proto != IPPROTO_UDP ||
+	    udp_hdr(skb)->dest != cpu_to_be16(GENEVE_UDP_PORT))
+		return;
+	swp_spec.l3_proto = l3_proto;
+	swp_spec.l4_proto = l4_proto;
+	swp_spec.is_tun = true;
+	if (inner_ip_hdr(skb)->version == 6) {
+		swp_spec.tun_l3_proto = htons(ETH_P_IPV6);
+		swp_spec.tun_l4_proto = inner_ipv6_hdr(skb)->nexthdr;
+	} else {
+		swp_spec.tun_l3_proto = htons(ETH_P_IP);
+		swp_spec.tun_l4_proto = inner_ip_hdr(skb)->protocol;
+	}
+
+	mlx5e_set_eseg_swp(skb, eseg, &swp_spec);
+}
+
+#else
+static inline bool mlx5_geneve_tx_allowed(struct mlx5_core_dev *mdev)
+{
+	return false;
+}
+
+#endif /* CONFIG_GENEVE */
+
 static inline void
 mlx5e_udp_gso_handle_tx_skb(struct sk_buff *skb)
 {

commit 3f44899ef2ce0c9da49feb0d6f08098a08cb96ae
Author: Boris Pismenny <borisp@mellanox.com>
Date:   Sat Jun 30 22:14:27 2018 +0300

    net/mlx5e: Use PARTIAL_GSO for UDP segmentation
    
    This patch removes the splitting of UDP_GSO_L4 packets in the driver,
    and exposes UDP_GSO_L4 as a PARTIAL_GSO feature. Thus, the network stack
    is not responsible for splitting the packet into two.
    
    Signed-off-by: Boris Pismenny <borisp@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en_accel/en_accel.h b/drivers/net/ethernet/mellanox/mlx5/core/en_accel/en_accel.h
index 39a5d13ba459..1dd225380a66 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_accel/en_accel.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_accel/en_accel.h
@@ -38,14 +38,22 @@
 #include <linux/netdevice.h>
 #include "en_accel/ipsec_rxtx.h"
 #include "en_accel/tls_rxtx.h"
-#include "en_accel/rxtx.h"
 #include "en.h"
 
-static inline struct sk_buff *mlx5e_accel_handle_tx(struct sk_buff *skb,
-						    struct mlx5e_txqsq *sq,
-						    struct net_device *dev,
-						    struct mlx5e_tx_wqe **wqe,
-						    u16 *pi)
+static inline void
+mlx5e_udp_gso_handle_tx_skb(struct sk_buff *skb)
+{
+	int payload_len = skb_shinfo(skb)->gso_size + sizeof(struct udphdr);
+
+	udp_hdr(skb)->len = htons(payload_len);
+}
+
+static inline struct sk_buff *
+mlx5e_accel_handle_tx(struct sk_buff *skb,
+		      struct mlx5e_txqsq *sq,
+		      struct net_device *dev,
+		      struct mlx5e_tx_wqe **wqe,
+		      u16 *pi)
 {
 #ifdef CONFIG_MLX5_EN_TLS
 	if (test_bit(MLX5E_SQ_STATE_TLS, &sq->state)) {
@@ -63,11 +71,8 @@ static inline struct sk_buff *mlx5e_accel_handle_tx(struct sk_buff *skb,
 	}
 #endif
 
-	if (skb_shinfo(skb)->gso_type & SKB_GSO_UDP_L4) {
-		skb = mlx5e_udp_gso_handle_tx_skb(dev, sq, skb, wqe, pi);
-		if (unlikely(!skb))
-			return NULL;
-	}
+	if (skb_is_gso(skb) && skb_shinfo(skb)->gso_type & SKB_GSO_UDP_L4)
+		mlx5e_udp_gso_handle_tx_skb(skb);
 
 	return skb;
 }

commit 689adf0d4892680f5998ea424e0ace560b492dc2
Author: Boris Pismenny <borisp@mellanox.com>
Date:   Thu May 31 15:29:42 2018 +0300

    net/mlx5e: Add UDP GSO support
    
    This patch enables UDP GSO support. We enable this by using two WQEs
    the first is a UDP LSO WQE for all segments with equal length, and the
    second is for the last segment in case it has different length.
    Due to HW limitation, before sending, we must adjust the packet length fields.
    
    We measure performance between two Intel(R) Xeon(R) CPU E5-2643 v2 @3.50GHz
    machines connected back-to-back with Connectx4-Lx (40Gbps) NICs.
    We compare single stream UDP, UDP GSO and UDP GSO with offload.
    Performance:
                    | MSS (bytes)   | Throughput (Gbps)     | CPU utilization (%)
    UDP GSO offload | 1472          | 35.6                  | 8%
    UDP GSO         | 1472          | 25.5                  | 17%
    UDP             | 1472          | 10.2                  | 17%
    UDP GSO offload | 1024          | 35.6                  | 8%
    UDP GSO         | 1024          | 19.2                  | 17%
    UDP             | 1024          | 5.7                   | 17%
    UDP GSO offload | 512           | 33.8                  | 16%
    UDP GSO         | 512           | 10.4                  | 17%
    UDP             | 512           | 3.5                   | 17%
    
    Signed-off-by: Boris Pismenny <borisp@mellanox.com>
    Signed-off-by: Yossi Kuperman <yossiku@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en_accel/en_accel.h b/drivers/net/ethernet/mellanox/mlx5/core/en_accel/en_accel.h
index f20074dbef32..39a5d13ba459 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_accel/en_accel.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_accel/en_accel.h
@@ -34,12 +34,11 @@
 #ifndef __MLX5E_EN_ACCEL_H__
 #define __MLX5E_EN_ACCEL_H__
 
-#ifdef CONFIG_MLX5_ACCEL
-
 #include <linux/skbuff.h>
 #include <linux/netdevice.h>
 #include "en_accel/ipsec_rxtx.h"
 #include "en_accel/tls_rxtx.h"
+#include "en_accel/rxtx.h"
 #include "en.h"
 
 static inline struct sk_buff *mlx5e_accel_handle_tx(struct sk_buff *skb,
@@ -64,9 +63,13 @@ static inline struct sk_buff *mlx5e_accel_handle_tx(struct sk_buff *skb,
 	}
 #endif
 
+	if (skb_shinfo(skb)->gso_type & SKB_GSO_UDP_L4) {
+		skb = mlx5e_udp_gso_handle_tx_skb(dev, sq, skb, wqe, pi);
+		if (unlikely(!skb))
+			return NULL;
+	}
+
 	return skb;
 }
 
-#endif /* CONFIG_MLX5_ACCEL */
-
 #endif /* __MLX5E_EN_ACCEL_H__ */

commit bfbe2057531026ac20bba9f45d3a21a98f3c592a
Author: Gal Pressman <galp@mellanox.com>
Date:   Tue Jan 23 12:28:12 2018 +0200

    net/mlx5e: Use test bit in en accel xmit flow
    
    Replace (mask & bit) check with test_bit.
    
    Signed-off-by: Gal Pressman <galp@mellanox.com>
    Signed-off-by: Tariq Toukan <tariqt@mellanox.com>
    Signed-off-by: Saeed Mahameed <saeedm@mellanox.com>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en_accel/en_accel.h b/drivers/net/ethernet/mellanox/mlx5/core/en_accel/en_accel.h
index 68fcb40a2847..f20074dbef32 100644
--- a/drivers/net/ethernet/mellanox/mlx5/core/en_accel/en_accel.h
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_accel/en_accel.h
@@ -49,7 +49,7 @@ static inline struct sk_buff *mlx5e_accel_handle_tx(struct sk_buff *skb,
 						    u16 *pi)
 {
 #ifdef CONFIG_MLX5_EN_TLS
-	if (sq->state & BIT(MLX5E_SQ_STATE_TLS)) {
+	if (test_bit(MLX5E_SQ_STATE_TLS, &sq->state)) {
 		skb = mlx5e_tls_handle_tx_skb(dev, sq, skb, wqe, pi);
 		if (unlikely(!skb))
 			return NULL;
@@ -57,7 +57,7 @@ static inline struct sk_buff *mlx5e_accel_handle_tx(struct sk_buff *skb,
 #endif
 
 #ifdef CONFIG_MLX5_EN_IPSEC
-	if (sq->state & BIT(MLX5E_SQ_STATE_IPSEC)) {
+	if (test_bit(MLX5E_SQ_STATE_IPSEC, &sq->state)) {
 		skb = mlx5e_ipsec_handle_tx_skb(dev, *wqe, skb);
 		if (unlikely(!skb))
 			return NULL;

commit bf23974104fa7aa2f3cbd2f2295f866a71875efd
Author: Ilya Lesokhin <ilyal@mellanox.com>
Date:   Mon Apr 30 10:16:20 2018 +0300

    net/mlx5e: TLS, Add Innova TLS TX offload data path
    
    Implement the TLS tx offload data path according to the
    requirements of the TLS generic NIC offload infrastructure.
    
    Special metadata ethertype is used to pass information to
    the hardware.
    
    Signed-off-by: Ilya Lesokhin <ilyal@mellanox.com>
    Signed-off-by: Boris Pismenny <borisp@mellanox.com>
    Acked-by: Saeed Mahameed <saeedm@mellanox.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/ethernet/mellanox/mlx5/core/en_accel/en_accel.h b/drivers/net/ethernet/mellanox/mlx5/core/en_accel/en_accel.h
new file mode 100644
index 000000000000..68fcb40a2847
--- /dev/null
+++ b/drivers/net/ethernet/mellanox/mlx5/core/en_accel/en_accel.h
@@ -0,0 +1,72 @@
+/*
+ * Copyright (c) 2018 Mellanox Technologies. All rights reserved.
+ *
+ * This software is available to you under a choice of one of two
+ * licenses.  You may choose to be licensed under the terms of the GNU
+ * General Public License (GPL) Version 2, available from the file
+ * COPYING in the main directory of this source tree, or the
+ * OpenIB.org BSD license below:
+ *
+ *     Redistribution and use in source and binary forms, with or
+ *     without modification, are permitted provided that the following
+ *     conditions are met:
+ *
+ *      - Redistributions of source code must retain the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer.
+ *
+ *      - Redistributions in binary form must reproduce the above
+ *        copyright notice, this list of conditions and the following
+ *        disclaimer in the documentation and/or other materials
+ *        provided with the distribution.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+ * EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+ * MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+ * NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS
+ * BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN
+ * ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+ * CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+ * SOFTWARE.
+ *
+ */
+
+#ifndef __MLX5E_EN_ACCEL_H__
+#define __MLX5E_EN_ACCEL_H__
+
+#ifdef CONFIG_MLX5_ACCEL
+
+#include <linux/skbuff.h>
+#include <linux/netdevice.h>
+#include "en_accel/ipsec_rxtx.h"
+#include "en_accel/tls_rxtx.h"
+#include "en.h"
+
+static inline struct sk_buff *mlx5e_accel_handle_tx(struct sk_buff *skb,
+						    struct mlx5e_txqsq *sq,
+						    struct net_device *dev,
+						    struct mlx5e_tx_wqe **wqe,
+						    u16 *pi)
+{
+#ifdef CONFIG_MLX5_EN_TLS
+	if (sq->state & BIT(MLX5E_SQ_STATE_TLS)) {
+		skb = mlx5e_tls_handle_tx_skb(dev, sq, skb, wqe, pi);
+		if (unlikely(!skb))
+			return NULL;
+	}
+#endif
+
+#ifdef CONFIG_MLX5_EN_IPSEC
+	if (sq->state & BIT(MLX5E_SQ_STATE_IPSEC)) {
+		skb = mlx5e_ipsec_handle_tx_skb(dev, *wqe, skb);
+		if (unlikely(!skb))
+			return NULL;
+	}
+#endif
+
+	return skb;
+}
+
+#endif /* CONFIG_MLX5_ACCEL */
+
+#endif /* __MLX5E_EN_ACCEL_H__ */
