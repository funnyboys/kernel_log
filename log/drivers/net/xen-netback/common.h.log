commit 6dc400af216a79c10cb082f25a7337bcbf532045
Author: Dongli Zhang <dongli.zhang@oracle.com>
Date:   Fri Apr 12 14:53:24 2019 +0800

    xen-netback: add reference from xenvif to backend_info to facilitate coredump analysis
    
    During coredump analysis, it is not easy to obtain the address of
    backend_info in xen-netback.
    
    So far there are two ways to obtain backend_info:
    
    1. Do what xenbus_device_find() does for vmcore to find the xenbus_device
    and then derive it from dev_get_drvdata().
    
    2. Extract backend_info from callstack of xenwatch (e.g., netback_remove()
    or frontend_changed()).
    
    This patch adds a reference from xenvif to backend_info so that it would be
    much more easier to obtain backend_info during coredump analysis.
    
    Signed-off-by: Dongli Zhang <dongli.zhang@oracle.com>
    Acked-by: Wei Liu <wei.liu2@citrix.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index 936c0b3e0ba2..05847eb91a1b 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -248,6 +248,22 @@ struct xenvif_hash {
 	struct xenvif_hash_cache cache;
 };
 
+struct backend_info {
+	struct xenbus_device *dev;
+	struct xenvif *vif;
+
+	/* This is the state that will be reflected in xenstore when any
+	 * active hotplug script completes.
+	 */
+	enum xenbus_state state;
+
+	enum xenbus_state frontend_state;
+	struct xenbus_watch hotplug_status_watch;
+	u8 have_hotplug_status_watch:1;
+
+	const char *hotplug_script;
+};
+
 struct xenvif {
 	/* Unique identifier for this interface. */
 	domid_t          domid;
@@ -283,6 +299,8 @@ struct xenvif {
 	struct xenbus_watch credit_watch;
 	struct xenbus_watch mcast_ctrl_watch;
 
+	struct backend_info *be;
+
 	spinlock_t lock;
 
 #ifdef CONFIG_DEBUG_FS

commit 22f9cde3401077ea450b69bf9b0bba373e12e454
Author: Jan Beulich <JBeulich@suse.com>
Date:   Tue Sep 25 02:13:01 2018 -0600

    xen-netback: validate queue numbers in xenvif_set_hash_mapping()
    
    Checking them before the grant copy means nothing as to the validity of
    the incoming request. As we shouldn't make the new data live before
    having validated it, introduce a second instance of the mapping array.
    
    Signed-off-by: Jan Beulich <jbeulich@suse.com>
    Reviewed-by: Paul Durrant <paul.durrant@citrix.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index a46a1e94505d..936c0b3e0ba2 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -241,8 +241,9 @@ struct xenvif_hash_cache {
 struct xenvif_hash {
 	unsigned int alg;
 	u32 flags;
+	bool mapping_sel;
 	u8 key[XEN_NETBK_MAX_HASH_KEY_SIZE];
-	u32 mapping[XEN_NETBK_MAX_HASH_MAPPING_SIZE];
+	u32 mapping[2][XEN_NETBK_MAX_HASH_MAPPING_SIZE];
 	unsigned int size;
 	struct xenvif_hash_cache cache;
 };

commit cac6a8f9017b532b39ecb9a9b040eadf26895035
Author: Kees Cook <keescook@chromium.org>
Date:   Mon Oct 16 17:29:38 2017 -0700

    net/xen-netback: Convert timers to use timer_setup()
    
    In preparation for unconditionally passing the struct timer_list pointer to
    all timer callbacks, switch to using the new timer_setup() and from_timer()
    to pass the timer pointer explicitly.
    
    Cc: Wei Liu <wei.liu2@citrix.com>
    Cc: Paul Durrant <paul.durrant@citrix.com>
    Cc: xen-devel@lists.xenproject.org
    Cc: netdev@vger.kernel.org
    Signed-off-by: Kees Cook <keescook@chromium.org>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index 5b1d2e8402d9..a46a1e94505d 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -307,7 +307,7 @@ static inline struct xenbus_device *xenvif_to_xenbus_device(struct xenvif *vif)
 	return to_xenbus_device(vif->dev->dev.parent);
 }
 
-void xenvif_tx_credit_callback(unsigned long data);
+void xenvif_tx_credit_callback(struct timer_list *t);
 
 struct xenvif *xenvif_alloc(struct device *parent,
 			    domid_t domid,

commit dfa523ae9f2542bee4cddaea37b3be3e157f6e6b
Author: Wei Liu <wei.liu2@citrix.com>
Date:   Wed Jun 21 10:21:22 2017 +0100

    xen-netback: correctly schedule rate-limited queues
    
    Add a flag to indicate if a queue is rate-limited. Test the flag in
    NAPI poll handler and avoid rescheduling the queue if true, otherwise
    we risk locking up the host. The rescheduling will be done in the
    timer callback function.
    
    Reported-by: Jean-Louis Dupond <jean-louis@dupond.be>
    Signed-off-by: Wei Liu <wei.liu2@citrix.com>
    Tested-by: Jean-Louis Dupond <jean-louis@dupond.be>
    Reviewed-by: Paul Durrant <paul.durrant@citrix.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index 530586be05b4..5b1d2e8402d9 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -199,6 +199,7 @@ struct xenvif_queue { /* Per-queue data for xenvif */
 	unsigned long   remaining_credit;
 	struct timer_list credit_timeout;
 	u64 credit_window_start;
+	bool rate_limited;
 
 	/* Statistics */
 	struct xenvif_stats stats;

commit ebf692f85ff78092cd238166d8d7ec51419f9c02
Author: Mart van Santen <mart@greenhost.nl>
Date:   Fri Feb 10 12:02:18 2017 +0000

    xen-netback: vif counters from int/long to u64
    
    This patch fixes an issue where the type of counters in the queue(s)
    and interface are not in sync (queue counters are int, interface
    counters are long), causing incorrect reporting of tx/rx values
    of the vif interface and unclear counter overflows.
    This patch sets both counters to the u64 type.
    
    Signed-off-by: Mart van Santen <mart@greenhost.nl>
    Reviewed-by: Paul Durrant <paul.durrant@citrix.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index 3ce1f7da8647..530586be05b4 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -113,10 +113,10 @@ struct xenvif_stats {
 	 * A subset of struct net_device_stats that contains only the
 	 * fields that are updated in netback.c for each queue.
 	 */
-	unsigned int rx_bytes;
-	unsigned int rx_packets;
-	unsigned int tx_bytes;
-	unsigned int tx_packets;
+	u64 rx_bytes;
+	u64 rx_packets;
+	u64 tx_bytes;
+	u64 tx_packets;
 
 	/* Additional stats used by xenvif */
 	unsigned long rx_gso_checksum_fixup;

commit a9339b8e138d81b6ee928d0de3372c551cbd3d34
Author: Paul Durrant <Paul.Durrant@citrix.com>
Date:   Mon Oct 10 09:30:53 2016 +0100

    xen-netback: (re-)create a debugfs node for hash information
    
    It is useful to be able to see the hash configuration when running tests.
    This patch adds a debugfs node for that purpose.
    
    The original version of this patch (commit c0c64c152389) was reverted due
    to build failures caused by a conflict with commit 0364a8824c02
    ("xen-netback: switch to threaded irq for control ring"). This new version
    of the patch is nearly identical to the original, the only difference
    being that creation of the debugfs node is predicated on 'ctrl_irq' being
    non-zero rather then the now non-existent 'ctrl_task'.
    
    Signed-off-by: Paul Durrant <paul.durrant@citrix.com>
    Cc: Wei Liu <wei.liu2@citrix.com>
    Cc: David S. Miller <davem@davemloft.net>
    Acked-by: Wei Liu <wei.liu2@citrix.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index cf68149cbb55..3ce1f7da8647 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -407,4 +407,8 @@ u32 xenvif_set_hash_mapping(struct xenvif *vif, u32 gref, u32 len,
 
 void xenvif_set_skb_hash(struct xenvif *vif, struct sk_buff *skb);
 
+#ifdef CONFIG_DEBUG_FS
+void xenvif_dump_hash_info(struct xenvif *vif, struct seq_file *m);
+#endif
+
 #endif /* __XEN_NETBACK__COMMON_H__ */

commit a37f12298c251a48bc74d4012e07bf0d78175f46
Author: David Vrabel <david.vrabel@citrix.com>
Date:   Tue Oct 4 10:29:17 2016 +0100

    xen-netback: batch copies for multiple to-guest rx packets
    
    Instead of flushing the copy ops when an packet is complete, complete
    packets when their copy ops are done.  This improves performance by
    reducing the number of grant copy hypercalls.
    
    Latency is still limited by the relatively small size of the copy
    batch.
    
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>
    [re-based]
    Signed-off-by: Paul Durrant <paul.durrant@citrix.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index 7d12a388afc6..cf68149cbb55 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -132,6 +132,7 @@ struct xenvif_copy_state {
 	struct gnttab_copy op[COPY_BATCH_SIZE];
 	RING_IDX idx[COPY_BATCH_SIZE];
 	unsigned int num;
+	struct sk_buff_head *completed;
 };
 
 struct xenvif_queue { /* Per-queue data for xenvif */

commit eb1723a29b9a75dd787510a39096a68dba6cc200
Author: David Vrabel <david.vrabel@citrix.com>
Date:   Tue Oct 4 10:29:14 2016 +0100

    xen-netback: refactor guest rx
    
    Refactor the to-guest (rx) path to:
    
    1. Push responses for completed skbs earlier, reducing latency.
    
    2. Reduce the per-queue memory overhead by greatly reducing the
       maximum number of grant copy ops in each hypercall (from 4352 to
       64).  Each struct xenvif_queue is now only 44 kB instead of 220 kB.
    
    3. Make the code more maintainable.
    
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>
    [re-based]
    Signed-off-by: Paul Durrant <paul.durrant@citrix.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index 0ba59106b1a5..7d12a388afc6 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -91,13 +91,6 @@ struct xenvif_rx_meta {
  */
 #define MAX_XEN_SKB_FRAGS (65536 / XEN_PAGE_SIZE + 1)
 
-/* It's possible for an skb to have a maximal number of frags
- * but still be less than MAX_BUFFER_OFFSET in size. Thus the
- * worst-case number of copy operations is MAX_XEN_SKB_FRAGS per
- * ring slot.
- */
-#define MAX_GRANT_COPY_OPS (MAX_XEN_SKB_FRAGS * XEN_NETIF_RX_RING_SIZE)
-
 #define NETBACK_INVALID_HANDLE -1
 
 /* To avoid confusion, we define XEN_NETBK_LEGACY_SLOTS_MAX indicating
@@ -133,6 +126,14 @@ struct xenvif_stats {
 	unsigned long tx_frag_overflow;
 };
 
+#define COPY_BATCH_SIZE 64
+
+struct xenvif_copy_state {
+	struct gnttab_copy op[COPY_BATCH_SIZE];
+	RING_IDX idx[COPY_BATCH_SIZE];
+	unsigned int num;
+};
+
 struct xenvif_queue { /* Per-queue data for xenvif */
 	unsigned int id; /* Queue ID, 0-based */
 	char name[QUEUE_NAME_SIZE]; /* DEVNAME-qN */
@@ -189,12 +190,7 @@ struct xenvif_queue { /* Per-queue data for xenvif */
 	unsigned long last_rx_time;
 	bool stalled;
 
-	struct gnttab_copy grant_copy_op[MAX_GRANT_COPY_OPS];
-
-	/* We create one meta structure per ring request we consume, so
-	 * the maximum number is the same as the ring size.
-	 */
-	struct xenvif_rx_meta meta[XEN_NETIF_RX_RING_SIZE];
+	struct xenvif_copy_state rx_copy;
 
 	/* Transmit shaping: allow 'credit_bytes' every 'credit_usec'. */
 	unsigned long   credit_bytes;
@@ -358,6 +354,7 @@ int xenvif_dealloc_kthread(void *data);
 
 irqreturn_t xenvif_ctrl_irq_fn(int irq, void *data);
 
+void xenvif_rx_action(struct xenvif_queue *queue);
 void xenvif_rx_queue_tail(struct xenvif_queue *queue, struct sk_buff *skb);
 
 void xenvif_carrier_on(struct xenvif *vif);

commit fedbc8c132bcf836358103195d8b6df6c03d9daf
Author: Paul Durrant <Paul.Durrant@citrix.com>
Date:   Tue Oct 4 10:29:13 2016 +0100

    xen-netback: retire guest rx side prefix GSO feature
    
    As far as I am aware only very old Windows network frontends make use of
    this style of passing GSO packets from backend to frontend. These
    frontends can easily be replaced by the freely available Xen Project
    Windows PV network frontend, which uses the 'default' mechanism for
    passing GSO packets, which is also used by all Linux frontends.
    
    NOTE: Removal of this feature will not cause breakage in old Windows
          frontends. They simply will no longer receive GSO packets - the
          packets instead being fragmented in the backend.
    
    Signed-off-by: Paul Durrant <paul.durrant@citrix.com>
    Reviewed-by: David Vrabel <david.vrabel@citrix.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index b38fb2cf3364..0ba59106b1a5 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -260,7 +260,6 @@ struct xenvif {
 
 	/* Frontend feature information. */
 	int gso_mask;
-	int gso_prefix_mask;
 
 	u8 can_sg:1;
 	u8 ip_csum:1;

commit fb2a3d5c7c85cb6e8bc88192be919b4ef8d6e630
Author: David S. Miller <davem@davemloft.net>
Date:   Fri Sep 23 09:09:31 2016 -0400

    Revert "xen-netback: create a debugfs node for hash information"
    
    This reverts commit c0c64c152389ad73306b9b0796357210ec6d32ee.
    
    There is no vif->ctrl_task member, so this change broke
    the build.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index ff94c513fe20..b38fb2cf3364 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -410,8 +410,4 @@ u32 xenvif_set_hash_mapping(struct xenvif *vif, u32 gref, u32 len,
 
 void xenvif_set_skb_hash(struct xenvif *vif, struct sk_buff *skb);
 
-#ifdef CONFIG_DEBUG_FS
-void xenvif_dump_hash_info(struct xenvif *vif, struct seq_file *m);
-#endif
-
 #endif /* __XEN_NETBACK__COMMON_H__ */

commit 0364a8824c020f12e2d5e9fad963685b58f7574e
Author: Juergen Gross <jgross@suse.com>
Date:   Thu Sep 22 11:06:25 2016 +0200

    xen-netback: switch to threaded irq for control ring
    
    Instead of open coding it use the threaded irq mechanism in
    xen-netback.
    
    Signed-off-by: Juergen Gross <jgross@suse.com>
    Acked-by: Wei Liu <wei.liu2@citrix.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index 3a562683603c..ff94c513fe20 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -292,8 +292,6 @@ struct xenvif {
 #endif
 
 	struct xen_netif_ctrl_back_ring ctrl;
-	struct task_struct *ctrl_task;
-	wait_queue_head_t ctrl_wq;
 	unsigned int ctrl_irq;
 
 	/* Miscellaneous private stuff. */
@@ -359,7 +357,7 @@ void xenvif_kick_thread(struct xenvif_queue *queue);
 
 int xenvif_dealloc_kthread(void *data);
 
-int xenvif_ctrl_kthread(void *data);
+irqreturn_t xenvif_ctrl_irq_fn(int irq, void *data);
 
 void xenvif_rx_queue_tail(struct xenvif_queue *queue, struct sk_buff *skb);
 

commit c0c64c152389ad73306b9b0796357210ec6d32ee
Author: Paul Durrant <Paul.Durrant@citrix.com>
Date:   Wed Aug 17 16:13:29 2016 +0100

    xen-netback: create a debugfs node for hash information
    
    It is useful to be able to see the hash configuration when running tests.
    This patch adds a debugfs node for that purpose.
    
    Signed-off-by: Paul Durrant <paul.durrant@citrix.com>
    Cc: Wei Liu <wei.liu2@citrix.com>
    Acked-by: Wei Liu <wei.liu2@citrix.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index 84d6cbdd11b2..3a562683603c 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -412,4 +412,8 @@ u32 xenvif_set_hash_mapping(struct xenvif *vif, u32 gref, u32 len,
 
 void xenvif_set_skb_hash(struct xenvif *vif, struct sk_buff *skb);
 
+#ifdef CONFIG_DEBUG_FS
+void xenvif_dump_hash_info(struct xenvif *vif, struct seq_file *m);
+#endif
+
 #endif /* __XEN_NETBACK__COMMON_H__ */

commit 40d8abdee806d496a60ee607a6d01b1cd7fabaf0
Author: Paul Durrant <Paul.Durrant@citrix.com>
Date:   Fri May 13 09:37:27 2016 +0100

    xen-netback: add control protocol implementation
    
    My recent patch to include/xen/interface/io/netif.h defines a new shared
    ring (in addition to the rx and tx rings) for passing control messages
    from a VM frontend driver to a backend driver.
    
    A previous patch added the necessary boilerplate for mapping the control
    ring from the frontend, should it be created. This patch adds
    implementations for each of the defined protocol messages.
    
    Signed-off-by: Paul Durrant <paul.durrant@citrix.com>
    Cc: Wei Liu <wei.liu2@citrix.com>
    Acked-by: Wei Liu <wei.liu2@citrix.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index 093a12abf71f..84d6cbdd11b2 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -220,6 +220,35 @@ struct xenvif_mcast_addr {
 
 #define XEN_NETBK_MCAST_MAX 64
 
+#define XEN_NETBK_MAX_HASH_KEY_SIZE 40
+#define XEN_NETBK_MAX_HASH_MAPPING_SIZE 128
+#define XEN_NETBK_HASH_TAG_SIZE 40
+
+struct xenvif_hash_cache_entry {
+	struct list_head link;
+	struct rcu_head rcu;
+	u8 tag[XEN_NETBK_HASH_TAG_SIZE];
+	unsigned int len;
+	u32 val;
+	int seq;
+};
+
+struct xenvif_hash_cache {
+	spinlock_t lock;
+	struct list_head list;
+	unsigned int count;
+	atomic_t seq;
+};
+
+struct xenvif_hash {
+	unsigned int alg;
+	u32 flags;
+	u8 key[XEN_NETBK_MAX_HASH_KEY_SIZE];
+	u32 mapping[XEN_NETBK_MAX_HASH_MAPPING_SIZE];
+	unsigned int size;
+	struct xenvif_hash_cache cache;
+};
+
 struct xenvif {
 	/* Unique identifier for this interface. */
 	domid_t          domid;
@@ -251,6 +280,8 @@ struct xenvif {
 	unsigned int num_queues; /* active queues, resource allocated */
 	unsigned int stalled_queues;
 
+	struct xenvif_hash hash;
+
 	struct xenbus_watch credit_watch;
 	struct xenbus_watch mcast_ctrl_watch;
 
@@ -353,6 +384,7 @@ extern bool separate_tx_rx_irq;
 extern unsigned int rx_drain_timeout_msecs;
 extern unsigned int rx_stall_timeout_msecs;
 extern unsigned int xenvif_max_queues;
+extern unsigned int xenvif_hash_cache_size;
 
 #ifdef CONFIG_DEBUG_FS
 extern struct dentry *xen_netback_dbg_root;
@@ -366,4 +398,18 @@ void xenvif_skb_zerocopy_complete(struct xenvif_queue *queue);
 bool xenvif_mcast_match(struct xenvif *vif, const u8 *addr);
 void xenvif_mcast_addr_list_free(struct xenvif *vif);
 
+/* Hash */
+void xenvif_init_hash(struct xenvif *vif);
+void xenvif_deinit_hash(struct xenvif *vif);
+
+u32 xenvif_set_hash_alg(struct xenvif *vif, u32 alg);
+u32 xenvif_get_hash_flags(struct xenvif *vif, u32 *flags);
+u32 xenvif_set_hash_flags(struct xenvif *vif, u32 flags);
+u32 xenvif_set_hash_key(struct xenvif *vif, u32 gref, u32 len);
+u32 xenvif_set_hash_mapping_size(struct xenvif *vif, u32 size);
+u32 xenvif_set_hash_mapping(struct xenvif *vif, u32 gref, u32 len,
+			    u32 off);
+
+void xenvif_set_skb_hash(struct xenvif *vif, struct sk_buff *skb);
+
 #endif /* __XEN_NETBACK__COMMON_H__ */

commit 4e15ee2cb46fed730fe6f0195a86d44e5aeef129
Author: Paul Durrant <Paul.Durrant@citrix.com>
Date:   Fri May 13 09:37:26 2016 +0100

    xen-netback: add control ring boilerplate
    
    My recent patch to include/xen/interface/io/netif.h defines a new shared
    ring (in addition to the rx and tx rings) for passing control messages
    from a VM frontend driver to a backend driver.
    
    This patch adds the necessary code to xen-netback to map this new shared
    ring, should it be created by a frontend, but does not add implementations
    for any of the defined protocol messages. These are added in a subsequent
    patch for clarity.
    
    Signed-off-by: Paul Durrant <paul.durrant@citrix.com>
    Acked-by: Wei Liu <wei.liu2@citrix.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index f44b38846420..093a12abf71f 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -260,6 +260,11 @@ struct xenvif {
 	struct dentry *xenvif_dbg_root;
 #endif
 
+	struct xen_netif_ctrl_back_ring ctrl;
+	struct task_struct *ctrl_task;
+	wait_queue_head_t ctrl_wq;
+	unsigned int ctrl_irq;
+
 	/* Miscellaneous private stuff. */
 	struct net_device *dev;
 };
@@ -285,10 +290,15 @@ struct xenvif *xenvif_alloc(struct device *parent,
 int xenvif_init_queue(struct xenvif_queue *queue);
 void xenvif_deinit_queue(struct xenvif_queue *queue);
 
-int xenvif_connect(struct xenvif_queue *queue, unsigned long tx_ring_ref,
-		   unsigned long rx_ring_ref, unsigned int tx_evtchn,
-		   unsigned int rx_evtchn);
-void xenvif_disconnect(struct xenvif *vif);
+int xenvif_connect_data(struct xenvif_queue *queue,
+			unsigned long tx_ring_ref,
+			unsigned long rx_ring_ref,
+			unsigned int tx_evtchn,
+			unsigned int rx_evtchn);
+void xenvif_disconnect_data(struct xenvif *vif);
+int xenvif_connect_ctrl(struct xenvif *vif, grant_ref_t ring_ref,
+			unsigned int evtchn);
+void xenvif_disconnect_ctrl(struct xenvif *vif);
 void xenvif_free(struct xenvif *vif);
 
 int xenvif_xenbus_init(void);
@@ -300,10 +310,10 @@ int xenvif_queue_stopped(struct xenvif_queue *queue);
 void xenvif_wake_queue(struct xenvif_queue *queue);
 
 /* (Un)Map communication rings. */
-void xenvif_unmap_frontend_rings(struct xenvif_queue *queue);
-int xenvif_map_frontend_rings(struct xenvif_queue *queue,
-			      grant_ref_t tx_ring_ref,
-			      grant_ref_t rx_ring_ref);
+void xenvif_unmap_frontend_data_rings(struct xenvif_queue *queue);
+int xenvif_map_frontend_data_rings(struct xenvif_queue *queue,
+				   grant_ref_t tx_ring_ref,
+				   grant_ref_t rx_ring_ref);
 
 /* Check for SKBs from frontend and schedule backend processing */
 void xenvif_napi_schedule_or_enable_events(struct xenvif_queue *queue);
@@ -318,6 +328,8 @@ void xenvif_kick_thread(struct xenvif_queue *queue);
 
 int xenvif_dealloc_kthread(void *data);
 
+int xenvif_ctrl_kthread(void *data);
+
 void xenvif_rx_queue_tail(struct xenvif_queue *queue, struct sk_buff *skb);
 
 void xenvif_carrier_on(struct xenvif *vif);

commit 562abd39a1902745bdcab266c7824cd6c5bc34d3
Author: Paul Durrant <Paul.Durrant@citrix.com>
Date:   Thu Mar 10 12:30:27 2016 +0000

    xen-netback: support multiple extra info fragments passed from frontend
    
    The code does not currently support a frontend passing multiple extra info
    fragments to the backend in a tx request. The xenvif_get_extras() function
    handles multiple extra_info fragments but make_tx_response() assumes there
    is only ever a single extra info fragment.
    
    This patch modifies xenvif_get_extras() to pass back a count of extra
    info fragments, which is then passed to make_tx_response() (after
    possibly being stashed in pending_tx_info for deferred responses).
    
    Signed-off-by: Paul Durrant <paul.durrant@citrix.com>
    Cc: Wei Liu <wei.liu2@citrix.com>
    Acked-by: Wei Liu <wei.liu2@citrix.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index 112825200d41..f44b38846420 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -52,6 +52,7 @@ typedef unsigned int pending_ring_idx_t;
 
 struct pending_tx_info {
 	struct xen_netif_tx_request req; /* tx request */
+	unsigned int extra_count;
 	/* Callback data for released SKBs. The callback is always
 	 * xenvif_zerocopy_callback, desc contains the pending_idx, which is
 	 * also an index in pending_tx_info array. It is initialized in

commit 22fae97d863679994b951799dd4bbe7afd95897b
Author: Paul Durrant <Paul.Durrant@citrix.com>
Date:   Tue Feb 2 11:55:05 2016 +0000

    xen-netback: implement dynamic multicast control
    
    My recent patch to the Xen Project documents a protocol for 'dynamic
    multicast control' in netif.h. This extends the previous multicast control
    protocol to not require a shared ring reconnection to turn the feature off.
    Instead the backend watches the "request-multicast-control" key in xenstore
    and turns the feature off if the key value is written to zero.
    
    This patch adds support for dynamic multicast control in xen-netback.
    
    Signed-off-by: Paul Durrant <paul.durrant@citrix.com>
    Cc: Ian Campbell <ian.campbell@citrix.com>
    Cc: Wei Liu <wei.liu2@citrix.com>
    Acked-by: Wei Liu <wei.liu2@citrix.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index 0333ab0fd926..112825200d41 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -251,6 +251,7 @@ struct xenvif {
 	unsigned int stalled_queues;
 
 	struct xenbus_watch credit_watch;
+	struct xenbus_watch mcast_ctrl_watch;
 
 	spinlock_t lock;
 

commit d0089e8a0e4c9723d85b01713671358e3d6960df
Author: Julien Grall <julien.grall@citrix.com>
Date:   Tue May 5 13:15:29 2015 +0100

    net/xen-netback: Make it running on 64KB page granularity
    
    The PV network protocol is using 4KB page granularity. The goal of this
    patch is to allow a Linux using 64KB page granularity working as a
    network backend on a non-modified Xen.
    
    It's only necessary to adapt the ring size and break skb data in small
    chunk of 4KB. The rest of the code is relying on the grant table code.
    
    Signed-off-by: Julien Grall <julien.grall@citrix.com>
    Reviewed-by: Wei Liu <wei.liu2@citrix.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index a7bf74727116..0333ab0fd926 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -44,6 +44,7 @@
 #include <xen/interface/grant_table.h>
 #include <xen/grant_table.h>
 #include <xen/xenbus.h>
+#include <xen/page.h>
 #include <linux/debugfs.h>
 
 typedef unsigned int pending_ring_idx_t;
@@ -64,8 +65,8 @@ struct pending_tx_info {
 	struct ubuf_info callback_struct;
 };
 
-#define XEN_NETIF_TX_RING_SIZE __CONST_RING_SIZE(xen_netif_tx, PAGE_SIZE)
-#define XEN_NETIF_RX_RING_SIZE __CONST_RING_SIZE(xen_netif_rx, PAGE_SIZE)
+#define XEN_NETIF_TX_RING_SIZE __CONST_RING_SIZE(xen_netif_tx, XEN_PAGE_SIZE)
+#define XEN_NETIF_RX_RING_SIZE __CONST_RING_SIZE(xen_netif_rx, XEN_PAGE_SIZE)
 
 struct xenvif_rx_meta {
 	int id;
@@ -80,16 +81,21 @@ struct xenvif_rx_meta {
 /* Discriminate from any valid pending_idx value. */
 #define INVALID_PENDING_IDX 0xFFFF
 
-#define MAX_BUFFER_OFFSET PAGE_SIZE
+#define MAX_BUFFER_OFFSET XEN_PAGE_SIZE
 
 #define MAX_PENDING_REQS XEN_NETIF_TX_RING_SIZE
 
+/* The maximum number of frags is derived from the size of a grant (same
+ * as a Xen page size for now).
+ */
+#define MAX_XEN_SKB_FRAGS (65536 / XEN_PAGE_SIZE + 1)
+
 /* It's possible for an skb to have a maximal number of frags
  * but still be less than MAX_BUFFER_OFFSET in size. Thus the
- * worst-case number of copy operations is MAX_SKB_FRAGS per
+ * worst-case number of copy operations is MAX_XEN_SKB_FRAGS per
  * ring slot.
  */
-#define MAX_GRANT_COPY_OPS (MAX_SKB_FRAGS * XEN_NETIF_RX_RING_SIZE)
+#define MAX_GRANT_COPY_OPS (MAX_XEN_SKB_FRAGS * XEN_NETIF_RX_RING_SIZE)
 
 #define NETBACK_INVALID_HANDLE -1
 

commit 1d5d48523900a4b0f25d6b52f1a93c84bd671186
Author: David Vrabel <david.vrabel@citrix.com>
Date:   Tue Sep 8 14:25:14 2015 +0100

    xen-netback: require fewer guest Rx slots when not using GSO
    
    Commit f48da8b14d04ca87ffcffe68829afd45f926ec6a (xen-netback: fix
    unlimited guest Rx internal queue and carrier flapping) introduced a
    regression.
    
    The PV frontend in IPXE only places 4 requests on the guest Rx ring.
    Since netback required at least (MAX_SKB_FRAGS + 1) slots, IPXE could
    not receive any packets.
    
    a) If GSO is not enabled on the VIF, fewer guest Rx slots are required
       for the largest possible packet.  Calculate the required slots
       based on the maximum GSO size or the MTU.
    
       This calculation of the number of required slots relies on
       1650d5455bd2 (xen-netback: always fully coalesce guest Rx packets)
       which present in 4.0-rc1 and later.
    
    b) Reduce the Rx stall detection to checking for at least one
       available Rx request.  This is fine since we're predominately
       concerned with detecting interfaces which are down and thus have
       zero available Rx requests.
    
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>
    Reviewed-by: Wei Liu <wei.liu2@citrix.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index 6dc76c1e807b..a7bf74727116 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -200,11 +200,6 @@ struct xenvif_queue { /* Per-queue data for xenvif */
 	struct xenvif_stats stats;
 };
 
-/* Maximum number of Rx slots a to-guest packet may use, including the
- * slot needed for GSO meta-data.
- */
-#define XEN_NETBK_RX_SLOTS_MAX (MAX_SKB_FRAGS + 1)
-
 enum state_bit_shift {
 	/* This bit marks that the vif is connected */
 	VIF_STATUS_CONNECTED,
@@ -317,11 +312,6 @@ int xenvif_dealloc_kthread(void *data);
 
 void xenvif_rx_queue_tail(struct xenvif_queue *queue, struct sk_buff *skb);
 
-/* Determine whether the needed number of slots (req) are available,
- * and set req_event if not.
- */
-bool xenvif_rx_ring_slots_available(struct xenvif_queue *queue, int needed);
-
 void xenvif_carrier_on(struct xenvif *vif);
 
 /* Callback from stack when TX packet can be released */

commit 210c34dcd8d912dcc740f1f17625a7293af5cb56
Author: Paul Durrant <Paul.Durrant@citrix.com>
Date:   Wed Sep 2 17:58:36 2015 +0100

    xen-netback: add support for multicast control
    
    Xen's PV network protocol includes messages to add/remove ethernet
    multicast addresses to/from a filter list in the backend. This allows
    the frontend to request the backend only forward multicast packets
    which are of interest thus preventing unnecessary noise on the shared
    ring.
    
    The canonical netif header in git://xenbits.xen.org/xen.git specifies
    the message format (two more XEN_NETIF_EXTRA_TYPEs) so the minimal
    necessary changes have been pulled into include/xen/interface/io/netif.h.
    
    To prevent the frontend from extending the multicast filter list
    arbitrarily a limit (XEN_NETBK_MCAST_MAX) has been set to 64 entries.
    This limit is not specified by the protocol and so may change in future.
    If the limit is reached then the next XEN_NETIF_EXTRA_TYPE_MCAST_ADD
    sent by the frontend will be failed with NETIF_RSP_ERROR.
    
    Signed-off-by: Paul Durrant <paul.durrant@citrix.com>
    Cc: Ian Campbell <ian.campbell@citrix.com>
    Cc: Wei Liu <wei.liu2@citrix.com>
    Acked-by: Wei Liu <wei.liu2@citrix.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index c6cb85a85c89..6dc76c1e807b 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -210,12 +210,22 @@ enum state_bit_shift {
 	VIF_STATUS_CONNECTED,
 };
 
+struct xenvif_mcast_addr {
+	struct list_head entry;
+	struct rcu_head rcu;
+	u8 addr[6];
+};
+
+#define XEN_NETBK_MCAST_MAX 64
+
 struct xenvif {
 	/* Unique identifier for this interface. */
 	domid_t          domid;
 	unsigned int     handle;
 
 	u8               fe_dev_addr[6];
+	struct list_head fe_mcast_addr;
+	unsigned int     fe_mcast_count;
 
 	/* Frontend feature information. */
 	int gso_mask;
@@ -224,6 +234,7 @@ struct xenvif {
 	u8 can_sg:1;
 	u8 ip_csum:1;
 	u8 ipv6_csum:1;
+	u8 multicast_control:1;
 
 	/* Is this interface disabled? True when backend discovers
 	 * frontend is rogue.
@@ -341,4 +352,8 @@ void xenvif_skb_zerocopy_prepare(struct xenvif_queue *queue,
 				 struct sk_buff *skb);
 void xenvif_skb_zerocopy_complete(struct xenvif_queue *queue);
 
+/* Multicast control */
+bool xenvif_mcast_match(struct xenvif *vif, const u8 *addr);
+void xenvif_mcast_addr_list_free(struct xenvif *vif);
+
 #endif /* __XEN_NETBACK__COMMON_H__ */

commit 6ab13b27699e5a71cca20d301c3c424653bd0841
Author: Li, Liang Z <liang.z.li@intel.com>
Date:   Mon Jul 6 08:42:56 2015 +0800

    xen-netback: remove duplicated function definition
    
    There are two duplicated xenvif_zerocopy_callback() definitions.
    Remove one of them.
    
    Signed-off-by: Liang Li <liang.z.li@intel.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index 8a495b318b6f..c6cb85a85c89 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -325,9 +325,6 @@ static inline pending_ring_idx_t nr_pending_reqs(struct xenvif_queue *queue)
 		queue->pending_prod + queue->pending_cons;
 }
 
-/* Callback from stack when TX packet can be released */
-void xenvif_zerocopy_callback(struct ubuf_info *ubuf, bool zerocopy_success);
-
 irqreturn_t xenvif_interrupt(int irq, void *dev_id);
 
 extern bool separate_tx_rx_irq;

commit edafc132baac4f5331b7bffd99a7af371776f2b5
Author: Palik, Imre <imrep@amazon.de>
Date:   Thu Mar 19 11:05:42 2015 +0100

    xen-netback: making the bandwidth limiter runtime settable
    
    With the current netback, the bandwidth limiter's parameters are only
    settable during vif setup time.  This patch register a watch on them, and
    thus makes them runtime changeable.
    
    When the watch fires, the timer is reset.  The timer's mutex is used for
    fencing the change.
    
    Cc: Anthony Liguori <aliguori@amazon.com>
    Signed-off-by: Imre Palik <imrep@amazon.de>
    Acked-by: Wei Liu <wei.liu2@citrix.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index 589fa256256b..8a495b318b6f 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -238,6 +238,8 @@ struct xenvif {
 	unsigned int num_queues; /* active queues, resource allocated */
 	unsigned int stalled_queues;
 
+	struct xenbus_watch credit_watch;
+
 	spinlock_t lock;
 
 #ifdef CONFIG_DEBUG_FS
@@ -260,6 +262,8 @@ static inline struct xenbus_device *xenvif_to_xenbus_device(struct xenvif *vif)
 	return to_xenbus_device(vif->dev->dev.parent);
 }
 
+void xenvif_tx_credit_callback(unsigned long data);
+
 struct xenvif *xenvif_alloc(struct device *parent,
 			    domid_t domid,
 			    unsigned int handle);

commit 1650d5455bd2dc6b5ee134bd6fc1a3236c266b5b
Author: David Vrabel <david.vrabel@citrix.com>
Date:   Tue Jan 20 14:49:52 2015 +0000

    xen-netback: always fully coalesce guest Rx packets
    
    Always fully coalesce guest Rx packets into the minimum number of ring
    slots.  Reducing the number of slots per packet has significant
    performance benefits when receiving off-host traffic.
    
    Results from XenServer's performance benchmarks:
    
                             Baseline    Full coalesce
    Interhost VM receive      7.2 Gb/s   11 Gb/s
    Interhost aggregate      24 Gb/s     24 Gb/s
    Intrahost single stream  14 Gb/s     14 Gb/s
    Intrahost aggregate      34 Gb/s     34 Gb/s
    
    However, this can increase the number of grant ops per packet which
    decreases performance of backend (dom0) to VM traffic (by ~10%)
    /unless/ grant copy has been optimized for adjacent ops with the same
    source or destination (see "grant-table: defer releasing pages
    acquired in a grant copy"[1] expected in Xen 4.6).
    
    [1] http://lists.xen.org/archives/html/xen-devel/2015-01/msg01118.html
    
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>
    Acked-by: Ian Campbell <ian.campbell@citrix.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index 5f1fda44882b..589fa256256b 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -251,7 +251,6 @@ struct xenvif {
 struct xenvif_rx_cb {
 	unsigned long expires;
 	int meta_slots_used;
-	bool full_coalesce;
 };
 
 #define XENVIF_RX_CB(skb) ((struct xenvif_rx_cb *)(skb)->cb)

commit 26c0e102585d5a4d311f5d6eb7f524d288e7f6b7
Author: David Vrabel <david.vrabel@citrix.com>
Date:   Thu Dec 18 11:13:06 2014 +0000

    xen-netback: support frontends without feature-rx-notify again
    
    Commit bc96f648df1bbc2729abbb84513cf4f64273a1f1 (xen-netback: make
    feature-rx-notify mandatory) incorrectly assumed that there were no
    frontends in use that did not support this feature.  But the frontend
    driver in MiniOS does not and since this is used by (qemu) stubdoms,
    these stopped working.
    
    Netback sort of works as-is in this mode except:
    
    - If there are no Rx requests and the internal Rx queue fills, only
      the drain timeout will wake the thread.  The default drain timeout
      of 10 s would give unacceptable pauses.
    
    - If an Rx stall was detected and the internal Rx queue is drained,
      then the Rx thread would never wake.
    
    Handle these two cases (when feature-rx-notify is disabled) by:
    
    - Reducing the drain timeout to 30 ms.
    
    - Disabling Rx stall detection.
    
    Reported-by: John <jw@nuclearfallout.net>
    Tested-by: John <jw@nuclearfallout.net>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>
    Reviewed-by: Wei Liu <wei.liu2@citrix.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index 083ecc93fe5e..5f1fda44882b 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -230,6 +230,8 @@ struct xenvif {
 	 */
 	bool disabled;
 	unsigned long status;
+	unsigned long drain_timeout;
+	unsigned long stall_timeout;
 
 	/* Queues */
 	struct xenvif_queue *queues;
@@ -328,7 +330,7 @@ irqreturn_t xenvif_interrupt(int irq, void *dev_id);
 extern bool separate_tx_rx_irq;
 
 extern unsigned int rx_drain_timeout_msecs;
-extern unsigned int rx_drain_timeout_jiffies;
+extern unsigned int rx_stall_timeout_msecs;
 extern unsigned int xenvif_max_queues;
 
 #ifdef CONFIG_DEBUG_FS

commit ecf08d2dbb96d5a4b4bcc53a39e8d29cc8fef02e
Author: David Vrabel <david.vrabel@citrix.com>
Date:   Wed Oct 22 14:08:55 2014 +0100

    xen-netback: reintroduce guest Rx stall detection
    
    If a frontend not receiving packets it is useful to detect this and
    turn off the carrier so packets are dropped early instead of being
    queued and drained when they expire.
    
    A to-guest queue is stalled if it doesn't have enough free slots for a
    an extended period of time (default 60 s).
    
    If at least one queue is stalled, the carrier is turned off (in the
    expectation that the other queues will soon stall as well).  The
    carrier is only turned on once all queues are ready.
    
    When the frontend connects, all the queues start in the stalled state
    and only become ready once the frontend queues enough Rx requests.
    
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>
    Reviewed-by: Wei Liu <wei.liu2@citrix.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index c2642402b7a1..083ecc93fe5e 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -179,6 +179,8 @@ struct xenvif_queue { /* Per-queue data for xenvif */
 
 	unsigned int rx_queue_max;
 	unsigned int rx_queue_len;
+	unsigned long last_rx_time;
+	bool stalled;
 
 	struct gnttab_copy grant_copy_op[MAX_GRANT_COPY_OPS];
 
@@ -232,6 +234,9 @@ struct xenvif {
 	/* Queues */
 	struct xenvif_queue *queues;
 	unsigned int num_queues; /* active queues, resource allocated */
+	unsigned int stalled_queues;
+
+	spinlock_t lock;
 
 #ifdef CONFIG_DEBUG_FS
 	struct dentry *xenvif_dbg_root;

commit f48da8b14d04ca87ffcffe68829afd45f926ec6a
Author: David Vrabel <david.vrabel@citrix.com>
Date:   Wed Oct 22 14:08:54 2014 +0100

    xen-netback: fix unlimited guest Rx internal queue and carrier flapping
    
    Netback needs to discard old to-guest skb's (guest Rx queue drain) and
    it needs detect guest Rx stalls (to disable the carrier so packets are
    discarded earlier), but the current implementation is very broken.
    
    1. The check in hard_start_xmit of the slot availability did not
       consider the number of packets that were already in the guest Rx
       queue.  This could allow the queue to grow without bound.
    
       The guest stops consuming packets and the ring was allowed to fill
       leaving S slot free.  Netback queues a packet requiring more than S
       slots (ensuring that the ring stays with S slots free).  Netback
       queue indefinately packets provided that then require S or fewer
       slots.
    
    2. The Rx stall detection is not triggered in this case since the
       (host) Tx queue is not stopped.
    
    3. If the Tx queue is stopped and a guest Rx interrupt occurs, netback
       will consider this an Rx purge event which may result in it taking
       the carrier down unnecessarily.  It also considers a queue with
       only 1 slot free as unstalled (even though the next packet might
       not fit in this).
    
    The internal guest Rx queue is limited by a byte length (to 512 Kib,
    enough for half the ring).  The (host) Tx queue is stopped and started
    based on this limit.  This sets an upper bound on the amount of memory
    used by packets on the internal queue.
    
    This allows the estimatation of the number of slots for an skb to be
    removed (it wasn't a very good estimate anyway).  Instead, the guest
    Rx thread just waits for enough free slots for a maximum sized packet.
    
    skbs queued on the internal queue have an 'expires' time (set to the
    current time plus the drain timeout).  The guest Rx thread will detect
    when the skb at the head of the queue has expired and discard expired
    skbs.  This sets a clear upper bound on the length of time an skb can
    be queued for.  For a guest being destroyed the maximum time needed to
    wait for all the packets it sent to be dropped is still the drain
    timeout (10 s) since it will not be sending new packets.
    
    Rx stall detection is reintroduced in a later commit.
    
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>
    Reviewed-by: Wei Liu <wei.liu2@citrix.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index 93ca77c129c3..c2642402b7a1 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -176,10 +176,9 @@ struct xenvif_queue { /* Per-queue data for xenvif */
 	char rx_irq_name[IRQ_NAME_SIZE]; /* DEVNAME-qN-rx */
 	struct xen_netif_rx_back_ring rx;
 	struct sk_buff_head rx_queue;
-	RING_IDX rx_last_skb_slots;
-	unsigned long status;
 
-	struct timer_list rx_stalled;
+	unsigned int rx_queue_max;
+	unsigned int rx_queue_len;
 
 	struct gnttab_copy grant_copy_op[MAX_GRANT_COPY_OPS];
 
@@ -199,18 +198,14 @@ struct xenvif_queue { /* Per-queue data for xenvif */
 	struct xenvif_stats stats;
 };
 
+/* Maximum number of Rx slots a to-guest packet may use, including the
+ * slot needed for GSO meta-data.
+ */
+#define XEN_NETBK_RX_SLOTS_MAX (MAX_SKB_FRAGS + 1)
+
 enum state_bit_shift {
 	/* This bit marks that the vif is connected */
 	VIF_STATUS_CONNECTED,
-	/* This bit signals the RX thread that queuing was stopped (in
-	 * start_xmit), and either the timer fired or an RX interrupt came
-	 */
-	QUEUE_STATUS_RX_PURGE_EVENT,
-	/* This bit tells the interrupt handler that this queue was the reason
-	 * for the carrier off, so it should kick the thread. Only queues which
-	 * brought it down can turn on the carrier.
-	 */
-	QUEUE_STATUS_RX_STALLED
 };
 
 struct xenvif {
@@ -246,6 +241,14 @@ struct xenvif {
 	struct net_device *dev;
 };
 
+struct xenvif_rx_cb {
+	unsigned long expires;
+	int meta_slots_used;
+	bool full_coalesce;
+};
+
+#define XENVIF_RX_CB(skb) ((struct xenvif_rx_cb *)(skb)->cb)
+
 static inline struct xenbus_device *xenvif_to_xenbus_device(struct xenvif *vif)
 {
 	return to_xenbus_device(vif->dev->dev.parent);
@@ -291,6 +294,8 @@ void xenvif_kick_thread(struct xenvif_queue *queue);
 
 int xenvif_dealloc_kthread(void *data);
 
+void xenvif_rx_queue_tail(struct xenvif_queue *queue, struct sk_buff *skb);
+
 /* Determine whether the needed number of slots (req) are available,
  * and set req_event if not.
  */

commit bc96f648df1bbc2729abbb84513cf4f64273a1f1
Author: David Vrabel <david.vrabel@citrix.com>
Date:   Wed Oct 22 14:08:53 2014 +0100

    xen-netback: make feature-rx-notify mandatory
    
    Frontends that do not provide feature-rx-notify may stall because
    netback depends on the notification from frontend to wake the guest Rx
    thread (even if can_queue is false).
    
    This could be fixed but feature-rx-notify was introduced in 2006 and I
    am not aware of any frontends that do not implement this.
    
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>
    Acked-by: Wei Liu <wei.liu2@citrix.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index d4eb8d2e9cb7..93ca77c129c3 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -228,9 +228,6 @@ struct xenvif {
 	u8 ip_csum:1;
 	u8 ipv6_csum:1;
 
-	/* Internal feature information. */
-	u8 can_queue:1;	    /* can queue packets for receiver? */
-
 	/* Is this interface disabled? True when backend discovers
 	 * frontend is rogue.
 	 */
@@ -272,8 +269,6 @@ void xenvif_xenbus_fini(void);
 
 int xenvif_schedulable(struct xenvif *vif);
 
-int xenvif_must_stop_queue(struct xenvif_queue *queue);
-
 int xenvif_queue_stopped(struct xenvif_queue *queue);
 void xenvif_wake_queue(struct xenvif_queue *queue);
 

commit a64bd934528e26e8956112e43a279fba2ee0634e
Author: Wei Liu <wei.liu2@citrix.com>
Date:   Tue Aug 12 11:48:07 2014 +0100

    xen-netback: don't stop dealloc kthread too early
    
    Reference count the number of packets in host stack, so that we don't
    stop the deallocation thread too early. If not, we can end up with
    xenvif_free permanently waiting for deallocation thread to unmap grefs.
    
    Reported-by: Thomas Leonard <talex5@gmail.com>
    Signed-off-by: Wei Liu <wei.liu2@citrix.com>
    Cc: Ian Campbell <ian.campbell@citrix.com>
    Cc: Zoltan Kiss <zoltan.kiss@citrix.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index ef3026f46a37..d4eb8d2e9cb7 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -165,6 +165,7 @@ struct xenvif_queue { /* Per-queue data for xenvif */
 	u16 dealloc_ring[MAX_PENDING_REQS];
 	struct task_struct *dealloc_task;
 	wait_queue_head_t dealloc_wq;
+	atomic_t inflight_packets;
 
 	/* Use kthread for guest RX */
 	struct task_struct *task;
@@ -329,4 +330,8 @@ extern unsigned int xenvif_max_queues;
 extern struct dentry *xen_netback_dbg_root;
 #endif
 
+void xenvif_skb_zerocopy_prepare(struct xenvif_queue *queue,
+				 struct sk_buff *skb);
+void xenvif_skb_zerocopy_complete(struct xenvif_queue *queue);
+
 #endif /* __XEN_NETBACK__COMMON_H__ */

commit f34a4cf9c9b4fd35ba7f9a596cedb011879a1a4d
Author: Zoltan Kiss <zoltan.kiss@citrix.com>
Date:   Mon Aug 4 16:20:58 2014 +0100

    xen-netback: Turn off the carrier if the guest is not able to receive
    
    Currently when the guest is not able to receive more packets, qdisc layer starts
    a timer, and when it goes off, qdisc is started again to deliver a packet again.
    This is a very slow way to drain the queues, consumes unnecessary resources and
    slows down other guests shutdown.
    This patch change the behaviour by turning the carrier off when that timer
    fires, so all the packets are freed up which were stucked waiting for that vif.
    Instead of the rx_queue_purge bool it uses the VIF_STATUS_RX_PURGE_EVENT bit to
    signal the thread that either the timeout happened or an RX interrupt arrived,
    so the thread can check what it should do. It also disables NAPI, so the guest
    can't transmit, but leaves the interrupts on, so it can resurrect.
    Only the queues which brought down the interface can enable it again, the bit
    QUEUE_STATUS_RX_STALLED makes sure of that.
    
    Signed-off-by: Zoltan Kiss <zoltan.kiss@citrix.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>
    Cc: netdev@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Cc: xen-devel@lists.xenproject.org
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index 4a92fc19f410..ef3026f46a37 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -176,9 +176,9 @@ struct xenvif_queue { /* Per-queue data for xenvif */
 	struct xen_netif_rx_back_ring rx;
 	struct sk_buff_head rx_queue;
 	RING_IDX rx_last_skb_slots;
-	bool rx_queue_purge;
+	unsigned long status;
 
-	struct timer_list wake_queue;
+	struct timer_list rx_stalled;
 
 	struct gnttab_copy grant_copy_op[MAX_GRANT_COPY_OPS];
 
@@ -200,7 +200,16 @@ struct xenvif_queue { /* Per-queue data for xenvif */
 
 enum state_bit_shift {
 	/* This bit marks that the vif is connected */
-	VIF_STATUS_CONNECTED
+	VIF_STATUS_CONNECTED,
+	/* This bit signals the RX thread that queuing was stopped (in
+	 * start_xmit), and either the timer fired or an RX interrupt came
+	 */
+	QUEUE_STATUS_RX_PURGE_EVENT,
+	/* This bit tells the interrupt handler that this queue was the reason
+	 * for the carrier off, so it should kick the thread. Only queues which
+	 * brought it down can turn on the carrier.
+	 */
+	QUEUE_STATUS_RX_STALLED
 };
 
 struct xenvif {

commit 3d1af1df9762e56e563e8fd088a1b4ce2bcfaf8b
Author: Zoltan Kiss <zoltan.kiss@citrix.com>
Date:   Mon Aug 4 16:20:57 2014 +0100

    xen-netback: Using a new state bit instead of carrier
    
    This patch introduces a new state bit VIF_STATUS_CONNECTED to track whether the
    vif is in a connected state. Using carrier will not work with the next patch
    in this series, which aims to turn the carrier temporarily off if the guest
    doesn't seem to be able to receive packets.
    
    Signed-off-by: Zoltan Kiss <zoltan.kiss@citrix.com>
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>
    Cc: netdev@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Cc: xen-devel@lists.xenproject.org
    
    v2:
    - rename the bitshift type to "enum state_bit_shift" here, not in the next patch
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index 28c98229e95f..4a92fc19f410 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -198,6 +198,11 @@ struct xenvif_queue { /* Per-queue data for xenvif */
 	struct xenvif_stats stats;
 };
 
+enum state_bit_shift {
+	/* This bit marks that the vif is connected */
+	VIF_STATUS_CONNECTED
+};
+
 struct xenvif {
 	/* Unique identifier for this interface. */
 	domid_t          domid;
@@ -220,6 +225,7 @@ struct xenvif {
 	 * frontend is rogue.
 	 */
 	bool disabled;
+	unsigned long status;
 
 	/* Queues */
 	struct xenvif_queue *queues;

commit f51de24356e49e4dcb5095e87717065580912120
Author: Zoltan Kiss <zoltan.kiss@citrix.com>
Date:   Tue Jul 8 19:49:14 2014 +0100

    xen-netback: Adding debugfs "io_ring_qX" files
    
    This patch adds debugfs capabilities to netback. There used to be a similar
    patch floating around for classic kernel, but it used procfs. It is based on a
    very similar blkback patch.
    It creates xen-netback/[vifname]/io_ring_q[queueno] files, reading them output
    various ring variables etc. Writing "kick" into it imitates an interrupt
    happened, it can be useful to check whether the ring is just stalled due to a
    missed interrupt.
    
    Signed-off-by: Zoltan Kiss <zoltan.kiss@citrix.com>
    Cc: netdev@vger.kernel.org
    Cc: linux-kernel@vger.kernel.org
    Cc: xen-devel@lists.xenproject.org
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index 2532ce85d718..28c98229e95f 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -44,6 +44,7 @@
 #include <xen/interface/grant_table.h>
 #include <xen/grant_table.h>
 #include <xen/xenbus.h>
+#include <linux/debugfs.h>
 
 typedef unsigned int pending_ring_idx_t;
 #define INVALID_PENDING_RING_IDX (~0U)
@@ -224,6 +225,10 @@ struct xenvif {
 	struct xenvif_queue *queues;
 	unsigned int num_queues; /* active queues, resource allocated */
 
+#ifdef CONFIG_DEBUG_FS
+	struct dentry *xenvif_dbg_root;
+#endif
+
 	/* Miscellaneous private stuff. */
 	struct net_device *dev;
 };
@@ -297,10 +302,16 @@ static inline pending_ring_idx_t nr_pending_reqs(struct xenvif_queue *queue)
 /* Callback from stack when TX packet can be released */
 void xenvif_zerocopy_callback(struct ubuf_info *ubuf, bool zerocopy_success);
 
+irqreturn_t xenvif_interrupt(int irq, void *dev_id);
+
 extern bool separate_tx_rx_irq;
 
 extern unsigned int rx_drain_timeout_msecs;
 extern unsigned int rx_drain_timeout_jiffies;
 extern unsigned int xenvif_max_queues;
 
+#ifdef CONFIG_DEBUG_FS
+extern struct dentry *xen_netback_dbg_root;
+#endif
+
 #endif /* __XEN_NETBACK__COMMON_H__ */

commit f7b50c4e7ced702d80d3b873d81a2cdafb580f13
Author: Wei Liu <wei.liu2@citrix.com>
Date:   Mon Jun 23 10:50:17 2014 +0100

    xen-netback: bookkeep number of active queues in our own module
    
    The original code uses netdev->real_num_tx_queues to bookkeep number of
    queues and invokes netif_set_real_num_tx_queues to set the number of
    queues. However, netif_set_real_num_tx_queues doesn't allow
    real_num_tx_queues to be smaller than 1, which means setting the number
    to 0 will not work and real_num_tx_queues is untouched.
    
    This is bogus when xenvif_free is invoked before any number of queues is
    allocated. That function needs to iterate through all queues to free
    resources. Using the wrong number of queues results in NULL pointer
    dereference.
    
    So we bookkeep the number of queues in xen-netback to solve this
    problem. This fixes a regression introduced by multiqueue patchset in
    3.16-rc1.
    
    There's another bug in original code that the real number of RX queues
    is never set. In current Xen multiqueue design, the number of TX queues
    and RX queues are in fact the same. We need to set the numbers of TX and
    RX queues to the same value.
    
    Also remove xenvif_select_queue and leave queue selection to core
    driver, as suggested by David Miller.
    
    Reported-by: Boris Ostrovsky <boris.ostrovsky@oracle.com>
    Signed-off-by: Wei Liu <wei.liu2@citrix.com>
    CC: Ian Campbell <ian.campbell@citrix.com>
    CC: Paul Durrant <paul.durrant@citrix.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index 4dd7c4a1923b..2532ce85d718 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -222,6 +222,7 @@ struct xenvif {
 
 	/* Queues */
 	struct xenvif_queue *queues;
+	unsigned int num_queues; /* active queues, resource allocated */
 
 	/* Miscellaneous private stuff. */
 	struct net_device *dev;

commit 8d3d53b3e43363e79ab9a9ecc149b06c1314b25d
Author: Andrew J. Bennieston <andrew.bennieston@citrix.com>
Date:   Wed Jun 4 10:30:43 2014 +0100

    xen-netback: Add support for multiple queues
    
    Builds on the refactoring of the previous patch to implement multiple
    queues between xen-netfront and xen-netback.
    
    Writes the maximum supported number of queues into XenStore, and reads
    the values written by the frontend to determine how many queues to use.
    
    Ring references and event channels are read from XenStore on a per-queue
    basis and rings are connected accordingly.
    
    Also adds code to handle the cleanup of any already initialised queues
    if the initialisation of a subsequent queue fails.
    
    Signed-off-by: Andrew J. Bennieston <andrew.bennieston@citrix.com>
    Acked-by: Wei Liu <wei.liu2@citrix.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index b6885cfcc7df..4dd7c4a1923b 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -237,6 +237,7 @@ struct xenvif *xenvif_alloc(struct device *parent,
 			    unsigned int handle);
 
 int xenvif_init_queue(struct xenvif_queue *queue);
+void xenvif_deinit_queue(struct xenvif_queue *queue);
 
 int xenvif_connect(struct xenvif_queue *queue, unsigned long tx_ring_ref,
 		   unsigned long rx_ring_ref, unsigned int tx_evtchn,
@@ -299,5 +300,6 @@ extern bool separate_tx_rx_irq;
 
 extern unsigned int rx_drain_timeout_msecs;
 extern unsigned int rx_drain_timeout_jiffies;
+extern unsigned int xenvif_max_queues;
 
 #endif /* __XEN_NETBACK__COMMON_H__ */

commit e9ce7cb6b107407e4798e8905b18ad8b642766f6
Author: Wei Liu <wei.liu2@citrix.com>
Date:   Wed Jun 4 10:30:42 2014 +0100

    xen-netback: Factor queue-specific data into queue struct
    
    In preparation for multi-queue support in xen-netback, move the
    queue-specific data from struct xenvif into struct xenvif_queue, and
    update the rest of the code to use this.
    
    Also adds loops over queues where appropriate, even though only one is
    configured at this point, and uses alloc_netdev_mq() and the
    corresponding multi-queue netif wake/start/stop functions in preparation
    for multiple active queues.
    
    Finally, implements a trivial queue selection function suitable for
    ndo_select_queue, which simply returns 0 for a single queue and uses
    skb_get_hash() to compute the queue index otherwise.
    
    Signed-off-by: Andrew J. Bennieston <andrew.bennieston@citrix.com>
    Signed-off-by: Wei Liu <wei.liu2@citrix.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index 2c283d693330..b6885cfcc7df 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -99,22 +99,43 @@ struct xenvif_rx_meta {
  */
 #define XEN_NETBK_LEGACY_SLOTS_MAX XEN_NETIF_NR_SLOTS_MIN
 
-struct xenvif {
-	/* Unique identifier for this interface. */
-	domid_t          domid;
-	unsigned int     handle;
+/* Queue name is interface name with "-qNNN" appended */
+#define QUEUE_NAME_SIZE (IFNAMSIZ + 5)
 
-	/* Is this interface disabled? True when backend discovers
-	 * frontend is rogue.
+/* IRQ name is queue name with "-tx" or "-rx" appended */
+#define IRQ_NAME_SIZE (QUEUE_NAME_SIZE + 3)
+
+struct xenvif;
+
+struct xenvif_stats {
+	/* Stats fields to be updated per-queue.
+	 * A subset of struct net_device_stats that contains only the
+	 * fields that are updated in netback.c for each queue.
 	 */
-	bool disabled;
+	unsigned int rx_bytes;
+	unsigned int rx_packets;
+	unsigned int tx_bytes;
+	unsigned int tx_packets;
+
+	/* Additional stats used by xenvif */
+	unsigned long rx_gso_checksum_fixup;
+	unsigned long tx_zerocopy_sent;
+	unsigned long tx_zerocopy_success;
+	unsigned long tx_zerocopy_fail;
+	unsigned long tx_frag_overflow;
+};
+
+struct xenvif_queue { /* Per-queue data for xenvif */
+	unsigned int id; /* Queue ID, 0-based */
+	char name[QUEUE_NAME_SIZE]; /* DEVNAME-qN */
+	struct xenvif *vif; /* Parent VIF */
 
 	/* Use NAPI for guest TX */
 	struct napi_struct napi;
 	/* When feature-split-event-channels = 0, tx_irq = rx_irq. */
 	unsigned int tx_irq;
 	/* Only used when feature-split-event-channels = 1 */
-	char tx_irq_name[IFNAMSIZ+4]; /* DEVNAME-tx */
+	char tx_irq_name[IRQ_NAME_SIZE]; /* DEVNAME-qN-tx */
 	struct xen_netif_tx_back_ring tx;
 	struct sk_buff_head tx_queue;
 	struct page *mmap_pages[MAX_PENDING_REQS];
@@ -150,7 +171,7 @@ struct xenvif {
 	/* When feature-split-event-channels = 0, tx_irq = rx_irq. */
 	unsigned int rx_irq;
 	/* Only used when feature-split-event-channels = 1 */
-	char rx_irq_name[IFNAMSIZ+4]; /* DEVNAME-rx */
+	char rx_irq_name[IRQ_NAME_SIZE]; /* DEVNAME-qN-rx */
 	struct xen_netif_rx_back_ring rx;
 	struct sk_buff_head rx_queue;
 	RING_IDX rx_last_skb_slots;
@@ -165,6 +186,22 @@ struct xenvif {
 	 */
 	struct xenvif_rx_meta meta[XEN_NETIF_RX_RING_SIZE];
 
+	/* Transmit shaping: allow 'credit_bytes' every 'credit_usec'. */
+	unsigned long   credit_bytes;
+	unsigned long   credit_usec;
+	unsigned long   remaining_credit;
+	struct timer_list credit_timeout;
+	u64 credit_window_start;
+
+	/* Statistics */
+	struct xenvif_stats stats;
+};
+
+struct xenvif {
+	/* Unique identifier for this interface. */
+	domid_t          domid;
+	unsigned int     handle;
+
 	u8               fe_dev_addr[6];
 
 	/* Frontend feature information. */
@@ -178,19 +215,13 @@ struct xenvif {
 	/* Internal feature information. */
 	u8 can_queue:1;	    /* can queue packets for receiver? */
 
-	/* Transmit shaping: allow 'credit_bytes' every 'credit_usec'. */
-	unsigned long   credit_bytes;
-	unsigned long   credit_usec;
-	unsigned long   remaining_credit;
-	struct timer_list credit_timeout;
-	u64 credit_window_start;
+	/* Is this interface disabled? True when backend discovers
+	 * frontend is rogue.
+	 */
+	bool disabled;
 
-	/* Statistics */
-	unsigned long rx_gso_checksum_fixup;
-	unsigned long tx_zerocopy_sent;
-	unsigned long tx_zerocopy_success;
-	unsigned long tx_zerocopy_fail;
-	unsigned long tx_frag_overflow;
+	/* Queues */
+	struct xenvif_queue *queues;
 
 	/* Miscellaneous private stuff. */
 	struct net_device *dev;
@@ -205,7 +236,9 @@ struct xenvif *xenvif_alloc(struct device *parent,
 			    domid_t domid,
 			    unsigned int handle);
 
-int xenvif_connect(struct xenvif *vif, unsigned long tx_ring_ref,
+int xenvif_init_queue(struct xenvif_queue *queue);
+
+int xenvif_connect(struct xenvif_queue *queue, unsigned long tx_ring_ref,
 		   unsigned long rx_ring_ref, unsigned int tx_evtchn,
 		   unsigned int rx_evtchn);
 void xenvif_disconnect(struct xenvif *vif);
@@ -216,44 +249,47 @@ void xenvif_xenbus_fini(void);
 
 int xenvif_schedulable(struct xenvif *vif);
 
-int xenvif_must_stop_queue(struct xenvif *vif);
+int xenvif_must_stop_queue(struct xenvif_queue *queue);
+
+int xenvif_queue_stopped(struct xenvif_queue *queue);
+void xenvif_wake_queue(struct xenvif_queue *queue);
 
 /* (Un)Map communication rings. */
-void xenvif_unmap_frontend_rings(struct xenvif *vif);
-int xenvif_map_frontend_rings(struct xenvif *vif,
+void xenvif_unmap_frontend_rings(struct xenvif_queue *queue);
+int xenvif_map_frontend_rings(struct xenvif_queue *queue,
 			      grant_ref_t tx_ring_ref,
 			      grant_ref_t rx_ring_ref);
 
 /* Check for SKBs from frontend and schedule backend processing */
-void xenvif_napi_schedule_or_enable_events(struct xenvif *vif);
+void xenvif_napi_schedule_or_enable_events(struct xenvif_queue *queue);
 
 /* Prevent the device from generating any further traffic. */
 void xenvif_carrier_off(struct xenvif *vif);
 
-int xenvif_tx_action(struct xenvif *vif, int budget);
+int xenvif_tx_action(struct xenvif_queue *queue, int budget);
 
 int xenvif_kthread_guest_rx(void *data);
-void xenvif_kick_thread(struct xenvif *vif);
+void xenvif_kick_thread(struct xenvif_queue *queue);
 
 int xenvif_dealloc_kthread(void *data);
 
 /* Determine whether the needed number of slots (req) are available,
  * and set req_event if not.
  */
-bool xenvif_rx_ring_slots_available(struct xenvif *vif, int needed);
+bool xenvif_rx_ring_slots_available(struct xenvif_queue *queue, int needed);
 
-void xenvif_stop_queue(struct xenvif *vif);
+void xenvif_carrier_on(struct xenvif *vif);
 
 /* Callback from stack when TX packet can be released */
 void xenvif_zerocopy_callback(struct ubuf_info *ubuf, bool zerocopy_success);
 
 /* Unmap a pending page and release it back to the guest */
-void xenvif_idx_unmap(struct xenvif *vif, u16 pending_idx);
+void xenvif_idx_unmap(struct xenvif_queue *queue, u16 pending_idx);
 
-static inline pending_ring_idx_t nr_pending_reqs(struct xenvif *vif)
+static inline pending_ring_idx_t nr_pending_reqs(struct xenvif_queue *queue)
 {
 	return MAX_PENDING_REQS -
-		vif->pending_prod + vif->pending_cons;
+		queue->pending_prod + queue->pending_cons;
 }
 
 /* Callback from stack when TX packet can be released */

commit a55d9766cecf2b1b9af4fcf93b2d41b71e599c76
Author: Andrew J. Bennieston <andrew.bennieston@citrix.com>
Date:   Wed Jun 4 10:30:41 2014 +0100

    xen-netback: Move grant_copy_op array back into struct xenvif.
    
    This array was allocated separately in commit ac3d5ac2 ("xen-netback:
    fix guest-receive-side array sizes") due to it being very large, and a
    struct xenvif is allocated as the netdev_priv part of a struct
    net_device, i.e. via kmalloc() but falling back to vmalloc() if the
    initial alloc. fails.
    
    In preparation for the multi-queue patches, where this array becomes
    part of struct xenvif_queue and is always allocated through vzalloc(),
    move this back into the struct xenvif.
    
    Signed-off-by: Andrew J. Bennieston <andrew.bennieston@citrix.com>
    Acked-by: Wei Liu <wei.liu2@citrix.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index 0d4a285cbd7e..2c283d693330 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -158,8 +158,7 @@ struct xenvif {
 
 	struct timer_list wake_queue;
 
-	/* This array is allocated seperately as it is large */
-	struct gnttab_copy *grant_copy_op;
+	struct gnttab_copy grant_copy_op[MAX_GRANT_COPY_OPS];
 
 	/* We create one meta structure per ring request we consume, so
 	 * the maximum number is the same as the ring size.

commit 0d08fceb2e21c30ca3e1e462e678723f806acf18
Author: David Vrabel <david.vrabel@citrix.com>
Date:   Fri May 16 12:26:04 2014 +0100

    xen-netback: fix race between napi_complete() and interrupt handler
    
    When the NAPI budget was not all used, xenvif_poll() would call
    napi_complete() /after/ enabling the interrupt.  This resulted in a
    race between the napi_complete() and the napi_schedule() in the
    interrupt handler.  The use of local_irq_save/restore() avoided by
    race iff the handler is running on the same CPU but not if it was
    running on a different CPU.
    
    Fix this properly by calling napi_complete() before reenabling
    interrupts (in the xenvif_napi_schedule_or_enable_irq() call).
    
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>
    Acked-by: Wei Liu <wei.liu2@citrix.com>
    Acked-by: Ian Campbell <ian.campbell@citrix.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index 630a3fcf65bc..0d4a285cbd7e 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -226,7 +226,7 @@ int xenvif_map_frontend_rings(struct xenvif *vif,
 			      grant_ref_t rx_ring_ref);
 
 /* Check for SKBs from frontend and schedule backend processing */
-void xenvif_check_rx_xenvif(struct xenvif *vif);
+void xenvif_napi_schedule_or_enable_events(struct xenvif *vif);
 
 /* Prevent the device from generating any further traffic. */
 void xenvif_carrier_off(struct xenvif *vif);

commit bdab82759b8e3620096d6db46dc1cac38a52d779
Author: Zoltan Kiss <zoltan.kiss@citrix.com>
Date:   Wed Apr 2 18:04:58 2014 +0100

    xen-netback: Grant copy the header instead of map and memcpy
    
    An old inefficiency of the TX path that we are grant mapping the first slot,
    and then copy the header part to the linear area. Instead, doing a grant copy
    for that header straight on is more reasonable. Especially because there are
    ongoing efforts to make Xen avoiding TLB flush after unmap when the page were
    not touched in Dom0. In the original way the memcpy ruined that.
    The key changes:
    - the vif has a tx_copy_ops array again
    - xenvif_tx_build_gops sets up the grant copy operations
    - we don't have to figure out whether the header and first frag are on the same
      grant mapped page or not
    Note, we only grant copy PKT_PROT_LEN bytes from the first slot, the rest (if
    any) will be on the first frag, which is grant mapped. If the first slot is
    smaller than PKT_PROT_LEN, then we grant copy that, and later __pskb_pull_tail
    will pull more from the frags (if any)
    
    Signed-off-by: Zoltan Kiss <zoltan.kiss@citrix.com>
    Reviewed-by: Paul Durrant <paul.durrant@citrix.com>
    Acked-by: Ian Campbell <ian.campbell@citrix.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index 89d1d0556b6e..630a3fcf65bc 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -124,6 +124,7 @@ struct xenvif {
 	struct pending_tx_info pending_tx_info[MAX_PENDING_REQS];
 	grant_handle_t grant_tx_handle[MAX_PENDING_REQS];
 
+	struct gnttab_copy tx_copy_ops[MAX_PENDING_REQS];
 	struct gnttab_map_grant_ref tx_map_ops[MAX_PENDING_REQS];
 	struct gnttab_unmap_grant_ref tx_unmap_ops[MAX_PENDING_REQS];
 	/* passed to gnttab_[un]map_refs with pages under (un)mapping */

commit e9d8b2c2968499c1f96563e6522c56958d5a1d0d
Author: Wei Liu <wei.liu2@citrix.com>
Date:   Tue Apr 1 12:46:12 2014 +0100

    xen-netback: disable rogue vif in kthread context
    
    When netback discovers frontend is sending malformed packet it will
    disables the interface which serves that frontend.
    
    However disabling a network interface involving taking a mutex which
    cannot be done in softirq context, so we need to defer this process to
    kthread context.
    
    This patch does the following:
    1. introduce a flag to indicate the interface is disabled.
    2. check that flag in TX path, don't do any work if it's true.
    3. check that flag in RX path, turn off that interface if it's true.
    
    The reason to disable it in RX path is because RX uses kthread. After
    this change the behavior of netback is still consistent -- it won't do
    any TX work for a rogue frontend, and the interface will be eventually
    turned off.
    
    Also change a "continue" to "break" after xenvif_fatal_tx_err, as it
    doesn't make sense to continue processing packets if frontend is rogue.
    
    This is a fix for XSA-90.
    
    Reported-by: Trk Edwin <edwin@etorok.net>
    Signed-off-by: Wei Liu <wei.liu2@citrix.com>
    Cc: Ian Campbell <ian.campbell@citrix.com>
    Reviewed-by: David Vrabel <david.vrabel@citrix.com>
    Acked-by: Ian Campbell <ian.campbell@citrix.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index 89b2d429c440..89d1d0556b6e 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -104,6 +104,11 @@ struct xenvif {
 	domid_t          domid;
 	unsigned int     handle;
 
+	/* Is this interface disabled? True when backend discovers
+	 * frontend is rogue.
+	 */
+	bool disabled;
+
 	/* Use NAPI for guest TX */
 	struct napi_struct napi;
 	/* When feature-split-event-channels = 0, tx_irq = rx_irq. */

commit 869b9b19b3affd81cee853d33c0b124797f3c387
Author: Zoltan Kiss <zoltan.kiss@citrix.com>
Date:   Mon Mar 24 23:59:49 2014 +0000

    xen-netback: Stop using xenvif_tx_pending_slots_available
    
    Since the early days TX stops if there isn't enough free pending slots to
    consume a maximum sized (slot-wise) packet. Probably the reason for that is to
    avoid the case when we don't have enough free pending slot in the ring to finish
    the packet. But if we make sure that the pending ring has the same size as the
    shared ring, that shouldn't really happen. The frontend can only post packets
    which fit the to the free space of the shared ring. If it doesn't, the frontend
    has to stop, as it can only increase the req_prod when the whole packet fits
    onto the ring.
    This patch avoid using this checking, makes sure the 2 ring has the same size,
    and remove a checking from the callback. As now we don't stop the NAPI instance
    on this condition, we don't have to wake it up if we free pending slots up.
    
    Signed-off-by: Zoltan Kiss <zoltan.kiss@citrix.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index 0355f8767e3b..89b2d429c440 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -81,7 +81,7 @@ struct xenvif_rx_meta {
 
 #define MAX_BUFFER_OFFSET PAGE_SIZE
 
-#define MAX_PENDING_REQS 256
+#define MAX_PENDING_REQS XEN_NETIF_TX_RING_SIZE
 
 /* It's possible for an skb to have a maximal number of frags
  * but still be less than MAX_BUFFER_OFFSET in size. Thus the
@@ -251,12 +251,6 @@ static inline pending_ring_idx_t nr_pending_reqs(struct xenvif *vif)
 		vif->pending_prod + vif->pending_cons;
 }
 
-static inline bool xenvif_tx_pending_slots_available(struct xenvif *vif)
-{
-	return nr_pending_reqs(vif) + XEN_NETBK_LEGACY_SLOTS_MAX
-		< MAX_PENDING_REQS;
-}
-
 /* Callback from stack when TX packet can be released */
 void xenvif_zerocopy_callback(struct ubuf_info *ubuf, bool zerocopy_success);
 

commit 397dfd9f93ccfe71660eafbaac651a96195c24ed
Author: Zoltan Kiss <zoltan.kiss@citrix.com>
Date:   Fri Mar 21 17:23:04 2014 +0000

    Revert "xen-netback: Aggregate TX unmap operations"
    
    This reverts commit e9275f5e2df1b2098a8cc405d87b88b9affd73e6. This commit is the
    last in the netback grant mapping series, and it tries to do more aggressive
    aggreagtion of unmap operations. However practical use showed almost no
    positive effect, whilst with certain frontends it causes significant performance
    regression.
    
    Signed-off-by: Zoltan Kiss <zoltan.kiss@citrix.com>
    Acked-by: Ian Campbell <ian.campbell@citrix.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index bef37be402b8..0355f8767e3b 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -137,8 +137,6 @@ struct xenvif {
 	u16 dealloc_ring[MAX_PENDING_REQS];
 	struct task_struct *dealloc_task;
 	wait_queue_head_t dealloc_wq;
-	struct timer_list dealloc_delay;
-	bool dealloc_delay_timed_out;
 
 	/* Use kthread for guest RX */
 	struct task_struct *task;

commit e9275f5e2df1b2098a8cc405d87b88b9affd73e6
Author: Zoltan Kiss <zoltan.kiss@citrix.com>
Date:   Thu Mar 6 21:48:31 2014 +0000

    xen-netback: Aggregate TX unmap operations
    
    Unmapping causes TLB flushing, therefore we should make it in the largest
    possible batches. However we shouldn't starve the guest for too long. So if
    the guest has space for at least two big packets and we don't have at least a
    quarter ring to unmap, delay it for at most 1 milisec.
    
    Signed-off-by: Zoltan Kiss <zoltan.kiss@citrix.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index 0355f8767e3b..bef37be402b8 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -137,6 +137,8 @@ struct xenvif {
 	u16 dealloc_ring[MAX_PENDING_REQS];
 	struct task_struct *dealloc_task;
 	wait_queue_head_t dealloc_wq;
+	struct timer_list dealloc_delay;
+	bool dealloc_delay_timed_out;
 
 	/* Use kthread for guest RX */
 	struct task_struct *task;

commit 093507885ae5dc0288af07fbb922d2f85b3a88a6
Author: Zoltan Kiss <zoltan.kiss@citrix.com>
Date:   Thu Mar 6 21:48:30 2014 +0000

    xen-netback: Timeout packets in RX path
    
    A malicious or buggy guest can leave its queue filled indefinitely, in which
    case qdisc start to queue packets for that VIF. If those packets came from an
    another guest, it can block its slots and prevent shutdown. To avoid that, we
    make sure the queue is drained in every 10 seconds.
    The QDisc queue in worst case takes 3 round to flush usually.
    
    Signed-off-by: Zoltan Kiss <zoltan.kiss@citrix.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index f2f8a02afc36..0355f8767e3b 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -148,6 +148,9 @@ struct xenvif {
 	struct xen_netif_rx_back_ring rx;
 	struct sk_buff_head rx_queue;
 	RING_IDX rx_last_skb_slots;
+	bool rx_queue_purge;
+
+	struct timer_list wake_queue;
 
 	/* This array is allocated seperately as it is large */
 	struct gnttab_copy *grant_copy_op;
@@ -259,4 +262,7 @@ void xenvif_zerocopy_callback(struct ubuf_info *ubuf, bool zerocopy_success);
 
 extern bool separate_tx_rx_irq;
 
+extern unsigned int rx_drain_timeout_msecs;
+extern unsigned int rx_drain_timeout_jiffies;
+
 #endif /* __XEN_NETBACK__COMMON_H__ */

commit e3377f36ca20a034dce56335dc9b89f41094d845
Author: Zoltan Kiss <zoltan.kiss@citrix.com>
Date:   Thu Mar 6 21:48:29 2014 +0000

    xen-netback: Handle guests with too many frags
    
    Xen network protocol had implicit dependency on MAX_SKB_FRAGS. Netback has to
    handle guests sending up to XEN_NETBK_LEGACY_SLOTS_MAX slots. To achieve that:
    - create a new skb
    - map the leftover slots to its frags (no linear buffer here!)
    - chain it to the previous through skb_shinfo(skb)->frag_list
    - map them
    - copy and coalesce the frags into a brand new one and send it to the stack
    - unmap the 2 old skb's pages
    
    It's also introduces new stat counters, which help determine how often the guest
    sends a packet with more than MAX_SKB_FRAGS frags.
    
    NOTE: if bisect brought you here, you should apply the series up until
    "xen-netback: Timeout packets in RX path", otherwise malicious guests can block
    other guests by not releasing their sent packets.
    
    Signed-off-by: Zoltan Kiss <zoltan.kiss@citrix.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index 683d30160a7c..f2f8a02afc36 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -182,6 +182,7 @@ struct xenvif {
 	unsigned long tx_zerocopy_sent;
 	unsigned long tx_zerocopy_success;
 	unsigned long tx_zerocopy_fail;
+	unsigned long tx_frag_overflow;
 
 	/* Miscellaneous private stuff. */
 	struct net_device *dev;

commit 1bb332af4cd889e4b64dacbf4a793ceb3a70445d
Author: Zoltan Kiss <zoltan.kiss@citrix.com>
Date:   Thu Mar 6 21:48:28 2014 +0000

    xen-netback: Add stat counters for zerocopy
    
    These counters help determine how often the buffers had to be copied. Also
    they help find out if packets are leaked, as if "sent != success + fail",
    there are probably packets never freed up properly.
    
    NOTE: if bisect brought you here, you should apply the series up until
    "xen-netback: Timeout packets in RX path", otherwise Windows guests can't work
    properly and malicious guests can block other guests by not releasing their sent
    packets.
    
    Signed-off-by: Zoltan Kiss <zoltan.kiss@citrix.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index 49109afa2253..683d30160a7c 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -179,6 +179,9 @@ struct xenvif {
 
 	/* Statistics */
 	unsigned long rx_gso_checksum_fixup;
+	unsigned long tx_zerocopy_sent;
+	unsigned long tx_zerocopy_success;
+	unsigned long tx_zerocopy_fail;
 
 	/* Miscellaneous private stuff. */
 	struct net_device *dev;

commit 62bad3199a4c20505fc36c169deef20b25e17c5f
Author: Zoltan Kiss <zoltan.kiss@citrix.com>
Date:   Thu Mar 6 21:48:27 2014 +0000

    xen-netback: Remove old TX grant copy definitons and fix indentations
    
    These became obsolete with grant mapping. I've left intentionally the
    indentations in this way, to improve readability of previous patches.
    
    NOTE: if bisect brought you here, you should apply the series up until
    "xen-netback: Timeout packets in RX path", otherwise Windows guests can't work
    properly and malicious guests can block other guests by not releasing their sent
    packets.
    
    Signed-off-by: Zoltan Kiss <zoltan.kiss@citrix.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index 5a991266a394..49109afa2253 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -48,37 +48,8 @@
 typedef unsigned int pending_ring_idx_t;
 #define INVALID_PENDING_RING_IDX (~0U)
 
-/* For the head field in pending_tx_info: it is used to indicate
- * whether this tx info is the head of one or more coalesced requests.
- *
- * When head != INVALID_PENDING_RING_IDX, it means the start of a new
- * tx requests queue and the end of previous queue.
- *
- * An example sequence of head fields (I = INVALID_PENDING_RING_IDX):
- *
- * ...|0 I I I|5 I|9 I I I|...
- * -->|<-INUSE----------------
- *
- * After consuming the first slot(s) we have:
- *
- * ...|V V V V|5 I|9 I I I|...
- * -----FREE->|<-INUSE--------
- *
- * where V stands for "valid pending ring index". Any number other
- * than INVALID_PENDING_RING_IDX is OK. These entries are considered
- * free and can contain any number other than
- * INVALID_PENDING_RING_IDX. In practice we use 0.
- *
- * The in use non-INVALID_PENDING_RING_IDX (say 0, 5 and 9 in the
- * above example) number is the index into pending_tx_info and
- * mmap_pages arrays.
- */
 struct pending_tx_info {
-	struct xen_netif_tx_request req; /* coalesced tx request */
-	pending_ring_idx_t head; /* head != INVALID_PENDING_RING_IDX
-				  * if it is head of one or more tx
-				  * reqs
-				  */
+	struct xen_netif_tx_request req; /* tx request */
 	/* Callback data for released SKBs. The callback is always
 	 * xenvif_zerocopy_callback, desc contains the pending_idx, which is
 	 * also an index in pending_tx_info array. It is initialized in
@@ -148,11 +119,6 @@ struct xenvif {
 	struct pending_tx_info pending_tx_info[MAX_PENDING_REQS];
 	grant_handle_t grant_tx_handle[MAX_PENDING_REQS];
 
-	/* Coalescing tx requests before copying makes number of grant
-	 * copy ops greater or equal to number of slots required. In
-	 * worst case a tx request consumes 2 gnttab_copy.
-	 */
-	struct gnttab_copy tx_copy_ops[2*MAX_PENDING_REQS];
 	struct gnttab_map_grant_ref tx_map_ops[MAX_PENDING_REQS];
 	struct gnttab_unmap_grant_ref tx_unmap_ops[MAX_PENDING_REQS];
 	/* passed to gnttab_[un]map_refs with pages under (un)mapping */

commit f53c3fe8dad725b014e9c7682720d8e3e2a8a5b3
Author: Zoltan Kiss <zoltan.kiss@citrix.com>
Date:   Thu Mar 6 21:48:26 2014 +0000

    xen-netback: Introduce TX grant mapping
    
    This patch introduces grant mapping on netback TX path. It replaces grant copy
    operations, ditching grant copy coalescing along the way. Another solution for
    copy coalescing is introduced in "xen-netback: Handle guests with too many
    frags", older guests and Windows can broke before that patch applies.
    There is a callback (xenvif_zerocopy_callback) from core stack to release the
    slots back to the guests when kfree_skb or skb_orphan_frags called. It feeds a
    separate dealloc thread, as scheduling NAPI instance from there is inefficient,
    therefore we can't do dealloc from the instance.
    
    Signed-off-by: Zoltan Kiss <zoltan.kiss@citrix.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index 8f264df8818a..5a991266a394 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -79,6 +79,17 @@ struct pending_tx_info {
 				  * if it is head of one or more tx
 				  * reqs
 				  */
+	/* Callback data for released SKBs. The callback is always
+	 * xenvif_zerocopy_callback, desc contains the pending_idx, which is
+	 * also an index in pending_tx_info array. It is initialized in
+	 * xenvif_alloc and it never changes.
+	 * skb_shinfo(skb)->destructor_arg points to the first mapped slot's
+	 * callback_struct in this array of struct pending_tx_info's, then ctx
+	 * to the next, or NULL if there is no more slot for this skb.
+	 * ubuf_to_vif is a helper which finds the struct xenvif from a pointer
+	 * to this field.
+	 */
+	struct ubuf_info callback_struct;
 };
 
 #define XEN_NETIF_TX_RING_SIZE __CONST_RING_SIZE(xen_netif_tx, PAGE_SIZE)
@@ -135,13 +146,31 @@ struct xenvif {
 	pending_ring_idx_t pending_cons;
 	u16 pending_ring[MAX_PENDING_REQS];
 	struct pending_tx_info pending_tx_info[MAX_PENDING_REQS];
+	grant_handle_t grant_tx_handle[MAX_PENDING_REQS];
 
 	/* Coalescing tx requests before copying makes number of grant
 	 * copy ops greater or equal to number of slots required. In
 	 * worst case a tx request consumes 2 gnttab_copy.
 	 */
 	struct gnttab_copy tx_copy_ops[2*MAX_PENDING_REQS];
-
+	struct gnttab_map_grant_ref tx_map_ops[MAX_PENDING_REQS];
+	struct gnttab_unmap_grant_ref tx_unmap_ops[MAX_PENDING_REQS];
+	/* passed to gnttab_[un]map_refs with pages under (un)mapping */
+	struct page *pages_to_map[MAX_PENDING_REQS];
+	struct page *pages_to_unmap[MAX_PENDING_REQS];
+
+	/* This prevents zerocopy callbacks  to race over dealloc_ring */
+	spinlock_t callback_lock;
+	/* This prevents dealloc thread and NAPI instance to race over response
+	 * creation and pending_ring in xenvif_idx_release. In xenvif_tx_err
+	 * it only protect response creation
+	 */
+	spinlock_t response_lock;
+	pending_ring_idx_t dealloc_prod;
+	pending_ring_idx_t dealloc_cons;
+	u16 dealloc_ring[MAX_PENDING_REQS];
+	struct task_struct *dealloc_task;
+	wait_queue_head_t dealloc_wq;
 
 	/* Use kthread for guest RX */
 	struct task_struct *task;
@@ -228,6 +257,8 @@ int xenvif_tx_action(struct xenvif *vif, int budget);
 int xenvif_kthread_guest_rx(void *data);
 void xenvif_kick_thread(struct xenvif *vif);
 
+int xenvif_dealloc_kthread(void *data);
+
 /* Determine whether the needed number of slots (req) are available,
  * and set req_event if not.
  */
@@ -235,6 +266,12 @@ bool xenvif_rx_ring_slots_available(struct xenvif *vif, int needed);
 
 void xenvif_stop_queue(struct xenvif *vif);
 
+/* Callback from stack when TX packet can be released */
+void xenvif_zerocopy_callback(struct ubuf_info *ubuf, bool zerocopy_success);
+
+/* Unmap a pending page and release it back to the guest */
+void xenvif_idx_unmap(struct xenvif *vif, u16 pending_idx);
+
 static inline pending_ring_idx_t nr_pending_reqs(struct xenvif *vif)
 {
 	return MAX_PENDING_REQS -

commit 3e2234b3149f66bc4be2343a3a0f637d922e4a36
Author: Zoltan Kiss <zoltan.kiss@citrix.com>
Date:   Thu Mar 6 21:48:25 2014 +0000

    xen-netback: Handle foreign mapped pages on the guest RX path
    
    RX path need to know if the SKB fragments are stored on pages from another
    domain.
    Logically this patch should be after introducing the grant mapping itself, as
    it makes sense only after that. But to keep bisectability, I moved it here. It
    shouldn't change any functionality here. xenvif_zerocopy_callback and
    ubuf_to_vif are just stubs here, they will be introduced properly later on.
    
    Signed-off-by: Zoltan Kiss <zoltan.kiss@citrix.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index 9d3584545e5d..8f264df8818a 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -247,6 +247,9 @@ static inline bool xenvif_tx_pending_slots_available(struct xenvif *vif)
 		< MAX_PENDING_REQS;
 }
 
+/* Callback from stack when TX packet can be released */
+void xenvif_zerocopy_callback(struct ubuf_info *ubuf, bool zerocopy_success);
+
 extern bool separate_tx_rx_irq;
 
 #endif /* __XEN_NETBACK__COMMON_H__ */

commit 121fa4b77775549c3c5eb41eb335d7dcbb801f90
Author: Zoltan Kiss <zoltan.kiss@citrix.com>
Date:   Thu Mar 6 21:48:24 2014 +0000

    xen-netback: Minor refactoring of netback code
    
    This patch contains a few bits of refactoring before introducing the grant
    mapping changes:
    - introducing xenvif_tx_pending_slots_available(), as this is used several
      times, and will be used more often
    - rename the thread to vifX.Y-guest-rx, to signify it does RX work from the
      guest point of view
    
    Signed-off-by: Zoltan Kiss <zoltan.kiss@citrix.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index ae413a2cbee7..9d3584545e5d 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -108,6 +108,15 @@ struct xenvif_rx_meta {
  */
 #define MAX_GRANT_COPY_OPS (MAX_SKB_FRAGS * XEN_NETIF_RX_RING_SIZE)
 
+#define NETBACK_INVALID_HANDLE -1
+
+/* To avoid confusion, we define XEN_NETBK_LEGACY_SLOTS_MAX indicating
+ * the maximum slots a valid packet can use. Now this value is defined
+ * to be XEN_NETIF_NR_SLOTS_MIN, which is supposed to be supported by
+ * all backend.
+ */
+#define XEN_NETBK_LEGACY_SLOTS_MAX XEN_NETIF_NR_SLOTS_MIN
+
 struct xenvif {
 	/* Unique identifier for this interface. */
 	domid_t          domid;
@@ -216,7 +225,7 @@ void xenvif_carrier_off(struct xenvif *vif);
 
 int xenvif_tx_action(struct xenvif *vif, int budget);
 
-int xenvif_kthread(void *data);
+int xenvif_kthread_guest_rx(void *data);
 void xenvif_kick_thread(struct xenvif *vif);
 
 /* Determine whether the needed number of slots (req) are available,
@@ -226,6 +235,18 @@ bool xenvif_rx_ring_slots_available(struct xenvif *vif, int needed);
 
 void xenvif_stop_queue(struct xenvif *vif);
 
+static inline pending_ring_idx_t nr_pending_reqs(struct xenvif *vif)
+{
+	return MAX_PENDING_REQS -
+		vif->pending_prod + vif->pending_cons;
+}
+
+static inline bool xenvif_tx_pending_slots_available(struct xenvif *vif)
+{
+	return nr_pending_reqs(vif) + XEN_NETBK_LEGACY_SLOTS_MAX
+		< MAX_PENDING_REQS;
+}
+
 extern bool separate_tx_rx_irq;
 
 #endif /* __XEN_NETBACK__COMMON_H__ */

commit 9ab9831b4c59d379a17f92a6157c7e921811dea2
Author: Zoltan Kiss <zoltan.kiss@citrix.com>
Date:   Tue Feb 4 19:54:37 2014 +0000

    xen-netback: Fix Rx stall due to race condition
    
    The recent patch to fix receive side flow control
    (11b57f90257c1d6a91cee720151b69e0c2020cf6: xen-netback: stop vif thread
    spinning if frontend is unresponsive) solved the spinning thread problem,
    however caused an another one. The receive side can stall, if:
    - [THREAD] xenvif_rx_action sets rx_queue_stopped to true
    - [INTERRUPT] interrupt happens, and sets rx_event to true
    - [THREAD] then xenvif_kthread sets rx_event to false
    - [THREAD] rx_work_todo doesn't return true anymore
    
    Also, if interrupt sent but there is still no room in the ring, it take quite a
    long time until xenvif_rx_action realize it. This patch ditch that two variable,
    and rework rx_work_todo. If the thread finds it can't fit more skb's into the
    ring, it saves the last slot estimation into rx_last_skb_slots, otherwise it's
    kept as 0. Then rx_work_todo will check if:
    - there is something to send to the ring (like before)
    - there is space for the topmost packet in the queue
    
    I think that's more natural and optimal thing to test than two bool which are
    set somewhere else.
    
    Signed-off-by: Zoltan Kiss <zoltan.kiss@citrix.com>
    Reviewed-by: Paul Durrant <paul.durrant@citrix.com>
    Acked-by: Wei Liu <wei.liu2@citrix.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index 4c76bcb9a879..ae413a2cbee7 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -143,11 +143,7 @@ struct xenvif {
 	char rx_irq_name[IFNAMSIZ+4]; /* DEVNAME-rx */
 	struct xen_netif_rx_back_ring rx;
 	struct sk_buff_head rx_queue;
-	bool rx_queue_stopped;
-	/* Set when the RX interrupt is triggered by the frontend.
-	 * The worker thread may need to wake the queue.
-	 */
-	bool rx_event;
+	RING_IDX rx_last_skb_slots;
 
 	/* This array is allocated seperately as it is large */
 	struct gnttab_copy *grant_copy_op;

commit 11b57f90257c1d6a91cee720151b69e0c2020cf6
Author: Paul Durrant <Paul.Durrant@citrix.com>
Date:   Wed Jan 8 12:41:58 2014 +0000

    xen-netback: stop vif thread spinning if frontend is unresponsive
    
    The recent patch to improve guest receive side flow control (ca2f09f2) had a
    slight flaw in the wait condition for the vif thread in that any remaining
    skbs in the guest receive side netback internal queue would prevent the
    thread from sleeping. An unresponsive frontend can lead to a permanently
    non-empty internal queue and thus the thread will spin. In this case the
    thread should really sleep until the frontend becomes responsive again.
    
    This patch adds an extra flag to the vif which is set if the shared ring
    is full and cleared when skbs are drained into the shared ring. Thus,
    if the thread runs, finds the shared ring full and can make no progress the
    flag remains set. If the flag remains set then the thread will sleep,
    regardless of a non-empty queue, until the next event from the frontend.
    
    Signed-off-by: Paul Durrant <paul.durrant@citrix.com>
    Cc: Wei Liu <wei.liu2@citrix.com>
    Cc: Ian Campbell <ian.campbell@citrix.com>
    Cc: David Vrabel <david.vrabel@citrix.com>
    Acked-by: Wei Liu <wei.liu2@citrix.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index c955fc39d69a..4c76bcb9a879 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -143,6 +143,7 @@ struct xenvif {
 	char rx_irq_name[IFNAMSIZ+4]; /* DEVNAME-rx */
 	struct xen_netif_rx_back_ring rx;
 	struct sk_buff_head rx_queue;
+	bool rx_queue_stopped;
 	/* Set when the RX interrupt is triggered by the frontend.
 	 * The worker thread may need to wake the queue.
 	 */

commit 56a4342dfe3145cd66f766adccb28fd9b571606d
Merge: 805c1f4aedab fe0d692bbc64
Author: David S. Miller <davem@davemloft.net>
Date:   Mon Jan 6 17:37:45 2014 -0500

    Merge branch 'master' of git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Conflicts:
            drivers/net/ethernet/qlogic/qlcnic/qlcnic_sriov_pf.c
            net/ipv6/ip6_tunnel.c
            net/ipv6/ip6_vti.c
    
    ipv6 tunnel statistic bug fixes conflicting with consolidation into
    generic sw per-cpu net stats.
    
    qlogic conflict between queue counting bug fix and the addition
    of multiple MAC address support.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit ac3d5ac277352fe6e27809286768e9f1f8aa388d
Author: Paul Durrant <Paul.Durrant@citrix.com>
Date:   Mon Dec 23 09:27:17 2013 +0000

    xen-netback: fix guest-receive-side array sizes
    
    The sizes chosen for the metadata and grant_copy_op arrays on the guest
    receive size are wrong;
    
    - The meta array is needlessly twice the ring size, when we only ever
      consume a single array element per RX ring slot
    - The grant_copy_op array is way too small. It's sized based on a bogus
      assumption: that at most two copy ops will be used per ring slot. This
      may have been true at some point in the past but it's clear from looking
      at start_new_rx_buffer() that a new ring slot is only consumed if a frag
      would overflow the current slot (plus some other conditions) so the actual
      limit is MAX_SKB_FRAGS grant_copy_ops per ring slot.
    
    This patch fixes those two sizing issues and, because grant_copy_ops grows
    so much, it pulls it out into a separate chunk of vmalloc()ed memory.
    
    Signed-off-by: Paul Durrant <paul.durrant@citrix.com>
    Acked-by: Wei Liu <wei.liu2@citrix.com>
    Cc: Ian Campbell <ian.campbell@citrix.com>
    Cc: David Vrabel <david.vrabel@citrix.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index 08ae01b41c83..c47794b9d42f 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -101,6 +101,13 @@ struct xenvif_rx_meta {
 
 #define MAX_PENDING_REQS 256
 
+/* It's possible for an skb to have a maximal number of frags
+ * but still be less than MAX_BUFFER_OFFSET in size. Thus the
+ * worst-case number of copy operations is MAX_SKB_FRAGS per
+ * ring slot.
+ */
+#define MAX_GRANT_COPY_OPS (MAX_SKB_FRAGS * XEN_NETIF_RX_RING_SIZE)
+
 struct xenvif {
 	/* Unique identifier for this interface. */
 	domid_t          domid;
@@ -143,13 +150,13 @@ struct xenvif {
 	 */
 	RING_IDX rx_req_cons_peek;
 
-	/* Given MAX_BUFFER_OFFSET of 4096 the worst case is that each
-	 * head/fragment page uses 2 copy operations because it
-	 * straddles two buffers in the frontend.
-	 */
-	struct gnttab_copy grant_copy_op[2*XEN_NETIF_RX_RING_SIZE];
-	struct xenvif_rx_meta meta[2*XEN_NETIF_RX_RING_SIZE];
+	/* This array is allocated seperately as it is large */
+	struct gnttab_copy *grant_copy_op;
 
+	/* We create one meta structure per ring request we consume, so
+	 * the maximum number is the same as the ring size.
+	 */
+	struct xenvif_rx_meta meta[XEN_NETIF_RX_RING_SIZE];
 
 	u8               fe_dev_addr[6];
 

commit ca2f09f2b2c6c25047cfc545d057c4edfcfe561c
Author: Paul Durrant <Paul.Durrant@citrix.com>
Date:   Fri Dec 6 16:36:07 2013 +0000

    xen-netback: improve guest-receive-side flow control
    
    The way that flow control works without this patch is that, in start_xmit()
    the code uses xenvif_count_skb_slots() to predict how many slots
    xenvif_gop_skb() will consume and then adds this to a 'req_cons_peek'
    counter which it then uses to determine if the shared ring has that amount
    of space available by checking whether 'req_prod' has passed that value.
    If the ring doesn't have space the tx queue is stopped.
    xenvif_gop_skb() will then consume slots and update 'req_cons' and issue
    responses, updating 'rsp_prod' as it goes. The frontend will consume those
    responses and post new requests, by updating req_prod. So, req_prod chases
    req_cons which chases rsp_prod, and can never exceed that value. Thus if
    xenvif_count_skb_slots() ever returns a number of slots greater than
    xenvif_gop_skb() uses, req_cons_peek will get to a value that req_prod cannot
    possibly achieve (since it's limited by the 'real' req_cons) and, if this
    happens enough times, req_cons_peek gets more than a ring size ahead of
    req_cons and the tx queue then remains stopped forever waiting for an
    unachievable amount of space to become available in the ring.
    
    Having two routines trying to calculate the same value is always going to be
    fragile, so this patch does away with that. All we essentially need to do is
    make sure that we have 'enough stuff' on our internal queue without letting
    it build up uncontrollably. So start_xmit() makes a cheap optimistic check
    of how much space is needed for an skb and only turns the queue off if that
    is unachievable. net_rx_action() is the place where we could do with an
    accurate predicition but, since that has proven tricky to calculate, a cheap
    worse-case (but not too bad) estimate is all we really need since the only
    thing we *must* prevent is xenvif_gop_skb() consuming more slots than are
    available.
    
    Without this patch I can trivially stall netback permanently by just doing
    a large guest to guest file copy between two Windows Server 2008R2 VMs on a
    single host.
    
    Patch tested with frontends in:
    - Windows Server 2008R2
    - CentOS 6.0
    - Debian Squeeze
    - Debian Wheezy
    - SLES11
    
    Signed-off-by: Paul Durrant <paul.durrant@citrix.com>
    Cc: Wei Liu <wei.liu2@citrix.com>
    Cc: Ian Campbell <ian.campbell@citrix.com>
    Cc: David Vrabel <david.vrabel@citrix.com>
    Cc: Annie Li <annie.li@oracle.com>
    Cc: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Acked-by: Wei Liu <wei.liu2@citrix.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index 08ae01b41c83..ba30a6d9fefa 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -136,12 +136,10 @@ struct xenvif {
 	char rx_irq_name[IFNAMSIZ+4]; /* DEVNAME-rx */
 	struct xen_netif_rx_back_ring rx;
 	struct sk_buff_head rx_queue;
-
-	/* Allow xenvif_start_xmit() to peek ahead in the rx request
-	 * ring.  This is a prediction of what rx_req_cons will be
-	 * once all queued skbs are put on the ring.
+	/* Set when the RX interrupt is triggered by the frontend.
+	 * The worker thread may need to wake the queue.
 	 */
-	RING_IDX rx_req_cons_peek;
+	bool rx_event;
 
 	/* Given MAX_BUFFER_OFFSET of 4096 the worst case is that each
 	 * head/fragment page uses 2 copy operations because it
@@ -198,8 +196,6 @@ void xenvif_xenbus_fini(void);
 
 int xenvif_schedulable(struct xenvif *vif);
 
-int xenvif_rx_ring_full(struct xenvif *vif);
-
 int xenvif_must_stop_queue(struct xenvif *vif);
 
 /* (Un)Map communication rings. */
@@ -211,21 +207,20 @@ int xenvif_map_frontend_rings(struct xenvif *vif,
 /* Check for SKBs from frontend and schedule backend processing */
 void xenvif_check_rx_xenvif(struct xenvif *vif);
 
-/* Queue an SKB for transmission to the frontend */
-void xenvif_queue_tx_skb(struct xenvif *vif, struct sk_buff *skb);
-/* Notify xenvif that ring now has space to send an skb to the frontend */
-void xenvif_notify_tx_completion(struct xenvif *vif);
-
 /* Prevent the device from generating any further traffic. */
 void xenvif_carrier_off(struct xenvif *vif);
 
-/* Returns number of ring slots required to send an skb to the frontend */
-unsigned int xenvif_count_skb_slots(struct xenvif *vif, struct sk_buff *skb);
-
 int xenvif_tx_action(struct xenvif *vif, int budget);
-void xenvif_rx_action(struct xenvif *vif);
 
 int xenvif_kthread(void *data);
+void xenvif_kick_thread(struct xenvif *vif);
+
+/* Determine whether the needed number of slots (req) are available,
+ * and set req_event if not.
+ */
+bool xenvif_rx_ring_slots_available(struct xenvif *vif, int needed);
+
+void xenvif_stop_queue(struct xenvif *vif);
 
 extern bool separate_tx_rx_irq;
 

commit 394efd19d5fcae936261bd48e5b33b21897aacf8
Merge: f421436a591d be408cd3e1fe
Author: David S. Miller <davem@davemloft.net>
Date:   Mon Nov 4 13:48:30 2013 -0500

    Merge git://git.kernel.org/pub/scm/linux/kernel/git/davem/net
    
    Conflicts:
            drivers/net/ethernet/emulex/benet/be.h
            drivers/net/netconsole.c
            net/bridge/br_private.h
    
    Three mostly trivial conflicts.
    
    The net/bridge/br_private.h conflict was a function signature (argument
    addition) change overlapping with the extern removals from Joe Perches.
    
    In drivers/net/netconsole.c we had one change adjusting a printk message
    whilst another changed "printk(KERN_INFO" into "pr_info(".
    
    Lastly, the emulex change was a new inline function addition overlapping
    with Joe Perches's extern removals.
    
    Signed-off-by: David S. Miller <davem@davemloft.net>

commit 059dfa6a93b779516321e5112db9d7621b1367ba
Author: Wei Liu <wei.liu2@citrix.com>
Date:   Mon Oct 28 12:07:57 2013 +0000

    xen-netback: use jiffies_64 value to calculate credit timeout
    
    time_after_eq() only works if the delta is < MAX_ULONG/2.
    
    For a 32bit Dom0, if netfront sends packets at a very low rate, the time
    between subsequent calls to tx_credit_exceeded() may exceed MAX_ULONG/2
    and the test for timer_after_eq() will be incorrect. Credit will not be
    replenished and the guest may become unable to send packets (e.g., if
    prior to the long gap, all credit was exhausted).
    
    Use jiffies_64 variant to mitigate this problem for 32bit Dom0.
    
    Suggested-by: Jan Beulich <jbeulich@suse.com>
    Signed-off-by: Wei Liu <wei.liu2@citrix.com>
    Reviewed-by: David Vrabel <david.vrabel@citrix.com>
    Cc: Ian Campbell <ian.campbell@citrix.com>
    Cc: Jason Luan <jianhai.luan@oracle.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index 5715318d6bab..400fea1de080 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -163,6 +163,7 @@ struct xenvif {
 	unsigned long   credit_usec;
 	unsigned long   remaining_credit;
 	struct timer_list credit_timeout;
+	u64 credit_window_start;
 
 	/* Statistics */
 	unsigned long rx_gso_checksum_fixup;

commit 82cada22a0bbec6a7afb573ef5fb6c512aaa2739
Author: Paul Durrant <Paul.Durrant@citrix.com>
Date:   Wed Oct 16 17:50:32 2013 +0100

    xen-netback: enable IPv6 TCP GSO to the guest
    
    This patch adds code to handle SKB_GSO_TCPV6 skbs and construct appropriate
    extra or prefix segments to pass the large packet to the frontend. New
    xenstore flags, feature-gso-tcpv6 and feature-gso-tcpv6-prefix, are sampled
    to determine if the frontend is capable of handling such packets.
    
    Signed-off-by: Paul Durrant <paul.durrant@citrix.com>
    Cc: Wei Liu <wei.liu2@citrix.com>
    Cc: David Vrabel <david.vrabel@citrix.com>
    Cc: Ian Campbell <ian.campbell@citrix.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index b4a9a3c844b2..55b8dec86233 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -87,9 +87,13 @@ struct pending_tx_info {
 struct xenvif_rx_meta {
 	int id;
 	int size;
+	int gso_type;
 	int gso_size;
 };
 
+#define GSO_BIT(type) \
+	(1 << XEN_NETIF_GSO_TYPE_ ## type)
+
 /* Discriminate from any valid pending_idx value. */
 #define INVALID_PENDING_IDX 0xFFFF
 
@@ -150,9 +154,10 @@ struct xenvif {
 	u8               fe_dev_addr[6];
 
 	/* Frontend feature information. */
+	int gso_mask;
+	int gso_prefix_mask;
+
 	u8 can_sg:1;
-	u8 gso:1;
-	u8 gso_prefix:1;
 	u8 ip_csum:1;
 	u8 ipv6_csum:1;
 

commit 146c8a77d27bcbd7722120f70f51e3b287205d0a
Author: Paul Durrant <Paul.Durrant@citrix.com>
Date:   Wed Oct 16 17:50:28 2013 +0100

    xen-netback: add support for IPv6 checksum offload to guest
    
    Check xenstore flag feature-ipv6-csum-offload to determine if a
    guest is happy to accept IPv6 packets with only partial checksum.
    
    Signed-off-by: Paul Durrant <paul.durrant@citrix.com>
    Cc: Wei Liu <wei.liu2@citrix.com>
    Cc: David Vrabel <david.vrabel@citrix.com>
    Cc: Ian Campbell <ian.campbell@citrix.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index 5715318d6bab..b4a9a3c844b2 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -153,7 +153,8 @@ struct xenvif {
 	u8 can_sg:1;
 	u8 gso:1;
 	u8 gso_prefix:1;
-	u8 csum:1;
+	u8 ip_csum:1;
+	u8 ipv6_csum:1;
 
 	/* Internal feature information. */
 	u8 can_queue:1;	    /* can queue packets for receiver? */

commit 279f438e36c0a70b23b86d2090aeec50155034a9
Author: Paul Durrant <Paul.Durrant@citrix.com>
Date:   Tue Sep 17 17:46:08 2013 +0100

    xen-netback: Don't destroy the netdev until the vif is shut down
    
    Without this patch, if a frontend cycles through states Closing
    and Closed (which Windows frontends need to do) then the netdev
    will be destroyed and requires re-invocation of hotplug scripts
    to restore state before the frontend can move to Connected. Thus
    when udev is not in use the backend gets stuck in InitWait.
    
    With this patch, the netdev is left alone whilst the backend is
    still online and is only de-registered and freed just prior to
    destroying the vif (which is also nicely symmetrical with the
    netdev allocation and registration being done during probe) so
    no re-invocation of hotplug scripts is required.
    
    Signed-off-by: Paul Durrant <paul.durrant@citrix.com>
    Cc: David Vrabel <david.vrabel@citrix.com>
    Cc: Wei Liu <wei.liu2@citrix.com>
    Cc: Ian Campbell <ian.campbell@citrix.com>
    Acked-by: Wei Liu <wei.liu2@citrix.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index a1977430ddfb..5715318d6bab 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -184,6 +184,7 @@ int xenvif_connect(struct xenvif *vif, unsigned long tx_ring_ref,
 		   unsigned long rx_ring_ref, unsigned int tx_evtchn,
 		   unsigned int rx_evtchn);
 void xenvif_disconnect(struct xenvif *vif);
+void xenvif_free(struct xenvif *vif);
 
 int xenvif_xenbus_init(void);
 void xenvif_xenbus_fini(void);

commit 7376419a4697657b2e0ab904a592aacc2e485bf1
Author: Wei Liu <wei.liu2@citrix.com>
Date:   Mon Aug 26 12:59:39 2013 +0100

    xen-netback: rename functions
    
    As we move to 1:1 model and melt xen_netbk and xenvif together, it would
    be better to use single prefix for all functions in xen-netback.
    
    Signed-off-by: Wei Liu <wei.liu2@citrix.com>
    Acked-by: Ian Campbell <ian.campbell@citrix.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index 9c1f15872e17..a1977430ddfb 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -190,21 +190,21 @@ void xenvif_xenbus_fini(void);
 
 int xenvif_schedulable(struct xenvif *vif);
 
-int xen_netbk_rx_ring_full(struct xenvif *vif);
+int xenvif_rx_ring_full(struct xenvif *vif);
 
-int xen_netbk_must_stop_queue(struct xenvif *vif);
+int xenvif_must_stop_queue(struct xenvif *vif);
 
 /* (Un)Map communication rings. */
-void xen_netbk_unmap_frontend_rings(struct xenvif *vif);
-int xen_netbk_map_frontend_rings(struct xenvif *vif,
-				 grant_ref_t tx_ring_ref,
-				 grant_ref_t rx_ring_ref);
+void xenvif_unmap_frontend_rings(struct xenvif *vif);
+int xenvif_map_frontend_rings(struct xenvif *vif,
+			      grant_ref_t tx_ring_ref,
+			      grant_ref_t rx_ring_ref);
 
 /* Check for SKBs from frontend and schedule backend processing */
-void xen_netbk_check_rx_xenvif(struct xenvif *vif);
+void xenvif_check_rx_xenvif(struct xenvif *vif);
 
 /* Queue an SKB for transmission to the frontend */
-void xen_netbk_queue_tx_skb(struct xenvif *vif, struct sk_buff *skb);
+void xenvif_queue_tx_skb(struct xenvif *vif, struct sk_buff *skb);
 /* Notify xenvif that ring now has space to send an skb to the frontend */
 void xenvif_notify_tx_completion(struct xenvif *vif);
 
@@ -212,12 +212,12 @@ void xenvif_notify_tx_completion(struct xenvif *vif);
 void xenvif_carrier_off(struct xenvif *vif);
 
 /* Returns number of ring slots required to send an skb to the frontend */
-unsigned int xen_netbk_count_skb_slots(struct xenvif *vif, struct sk_buff *skb);
+unsigned int xenvif_count_skb_slots(struct xenvif *vif, struct sk_buff *skb);
 
-int xen_netbk_tx_action(struct xenvif *vif, int budget);
-void xen_netbk_rx_action(struct xenvif *vif);
+int xenvif_tx_action(struct xenvif *vif, int budget);
+void xenvif_rx_action(struct xenvif *vif);
 
-int xen_netbk_kthread(void *data);
+int xenvif_kthread(void *data);
 
 extern bool separate_tx_rx_irq;
 

commit b3f980bd827e6e81a050c518d60ed7811a83061d
Author: Wei Liu <wei.liu2@citrix.com>
Date:   Mon Aug 26 12:59:38 2013 +0100

    xen-netback: switch to NAPI + kthread 1:1 model
    
    This patch implements 1:1 model netback. NAPI and kthread are utilized
    to do the weight-lifting job:
    
    - NAPI is used for guest side TX (host side RX)
    - kthread is used for guest side RX (host side TX)
    
    Xenvif and xen_netbk are made into one structure to reduce code size.
    
    This model provides better scheduling fairness among vifs. It is also
    prerequisite for implementing multiqueue for Xen netback.
    
    Signed-off-by: Wei Liu <wei.liu2@citrix.com>
    Acked-by: Ian Campbell <ian.campbell@citrix.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index 8a4d77ee9c5b..9c1f15872e17 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -45,31 +45,109 @@
 #include <xen/grant_table.h>
 #include <xen/xenbus.h>
 
-struct xen_netbk;
+typedef unsigned int pending_ring_idx_t;
+#define INVALID_PENDING_RING_IDX (~0U)
+
+/* For the head field in pending_tx_info: it is used to indicate
+ * whether this tx info is the head of one or more coalesced requests.
+ *
+ * When head != INVALID_PENDING_RING_IDX, it means the start of a new
+ * tx requests queue and the end of previous queue.
+ *
+ * An example sequence of head fields (I = INVALID_PENDING_RING_IDX):
+ *
+ * ...|0 I I I|5 I|9 I I I|...
+ * -->|<-INUSE----------------
+ *
+ * After consuming the first slot(s) we have:
+ *
+ * ...|V V V V|5 I|9 I I I|...
+ * -----FREE->|<-INUSE--------
+ *
+ * where V stands for "valid pending ring index". Any number other
+ * than INVALID_PENDING_RING_IDX is OK. These entries are considered
+ * free and can contain any number other than
+ * INVALID_PENDING_RING_IDX. In practice we use 0.
+ *
+ * The in use non-INVALID_PENDING_RING_IDX (say 0, 5 and 9 in the
+ * above example) number is the index into pending_tx_info and
+ * mmap_pages arrays.
+ */
+struct pending_tx_info {
+	struct xen_netif_tx_request req; /* coalesced tx request */
+	pending_ring_idx_t head; /* head != INVALID_PENDING_RING_IDX
+				  * if it is head of one or more tx
+				  * reqs
+				  */
+};
+
+#define XEN_NETIF_TX_RING_SIZE __CONST_RING_SIZE(xen_netif_tx, PAGE_SIZE)
+#define XEN_NETIF_RX_RING_SIZE __CONST_RING_SIZE(xen_netif_rx, PAGE_SIZE)
+
+struct xenvif_rx_meta {
+	int id;
+	int size;
+	int gso_size;
+};
+
+/* Discriminate from any valid pending_idx value. */
+#define INVALID_PENDING_IDX 0xFFFF
+
+#define MAX_BUFFER_OFFSET PAGE_SIZE
+
+#define MAX_PENDING_REQS 256
 
 struct xenvif {
 	/* Unique identifier for this interface. */
 	domid_t          domid;
 	unsigned int     handle;
 
-	/* Reference to netback processing backend. */
-	struct xen_netbk *netbk;
+	/* Use NAPI for guest TX */
+	struct napi_struct napi;
+	/* When feature-split-event-channels = 0, tx_irq = rx_irq. */
+	unsigned int tx_irq;
+	/* Only used when feature-split-event-channels = 1 */
+	char tx_irq_name[IFNAMSIZ+4]; /* DEVNAME-tx */
+	struct xen_netif_tx_back_ring tx;
+	struct sk_buff_head tx_queue;
+	struct page *mmap_pages[MAX_PENDING_REQS];
+	pending_ring_idx_t pending_prod;
+	pending_ring_idx_t pending_cons;
+	u16 pending_ring[MAX_PENDING_REQS];
+	struct pending_tx_info pending_tx_info[MAX_PENDING_REQS];
+
+	/* Coalescing tx requests before copying makes number of grant
+	 * copy ops greater or equal to number of slots required. In
+	 * worst case a tx request consumes 2 gnttab_copy.
+	 */
+	struct gnttab_copy tx_copy_ops[2*MAX_PENDING_REQS];
 
-	u8               fe_dev_addr[6];
 
+	/* Use kthread for guest RX */
+	struct task_struct *task;
+	wait_queue_head_t wq;
 	/* When feature-split-event-channels = 0, tx_irq = rx_irq. */
-	unsigned int tx_irq;
 	unsigned int rx_irq;
 	/* Only used when feature-split-event-channels = 1 */
-	char tx_irq_name[IFNAMSIZ+4]; /* DEVNAME-tx */
 	char rx_irq_name[IFNAMSIZ+4]; /* DEVNAME-rx */
+	struct xen_netif_rx_back_ring rx;
+	struct sk_buff_head rx_queue;
 
-	/* List of frontends to notify after a batch of frames sent. */
-	struct list_head notify_list;
+	/* Allow xenvif_start_xmit() to peek ahead in the rx request
+	 * ring.  This is a prediction of what rx_req_cons will be
+	 * once all queued skbs are put on the ring.
+	 */
+	RING_IDX rx_req_cons_peek;
+
+	/* Given MAX_BUFFER_OFFSET of 4096 the worst case is that each
+	 * head/fragment page uses 2 copy operations because it
+	 * straddles two buffers in the frontend.
+	 */
+	struct gnttab_copy grant_copy_op[2*XEN_NETIF_RX_RING_SIZE];
+	struct xenvif_rx_meta meta[2*XEN_NETIF_RX_RING_SIZE];
 
-	/* The shared rings and indexes. */
-	struct xen_netif_tx_back_ring tx;
-	struct xen_netif_rx_back_ring rx;
+
+	u8               fe_dev_addr[6];
 
 	/* Frontend feature information. */
 	u8 can_sg:1;
@@ -80,13 +158,6 @@ struct xenvif {
 	/* Internal feature information. */
 	u8 can_queue:1;	    /* can queue packets for receiver? */
 
-	/*
-	 * Allow xenvif_start_xmit() to peek ahead in the rx request
-	 * ring.  This is a prediction of what rx_req_cons will be
-	 * once all queued skbs are put on the ring.
-	 */
-	RING_IDX rx_req_cons_peek;
-
 	/* Transmit shaping: allow 'credit_bytes' every 'credit_usec'. */
 	unsigned long   credit_bytes;
 	unsigned long   credit_usec;
@@ -97,11 +168,7 @@ struct xenvif {
 	unsigned long rx_gso_checksum_fixup;
 
 	/* Miscellaneous private stuff. */
-	struct list_head schedule_list;
-	atomic_t         refcnt;
 	struct net_device *dev;
-
-	wait_queue_head_t waiting_to_free;
 };
 
 static inline struct xenbus_device *xenvif_to_xenbus_device(struct xenvif *vif)
@@ -109,9 +176,6 @@ static inline struct xenbus_device *xenvif_to_xenbus_device(struct xenvif *vif)
 	return to_xenbus_device(vif->dev->dev.parent);
 }
 
-#define XEN_NETIF_TX_RING_SIZE __CONST_RING_SIZE(xen_netif_tx, PAGE_SIZE)
-#define XEN_NETIF_RX_RING_SIZE __CONST_RING_SIZE(xen_netif_rx, PAGE_SIZE)
-
 struct xenvif *xenvif_alloc(struct device *parent,
 			    domid_t domid,
 			    unsigned int handle);
@@ -121,9 +185,6 @@ int xenvif_connect(struct xenvif *vif, unsigned long tx_ring_ref,
 		   unsigned int rx_evtchn);
 void xenvif_disconnect(struct xenvif *vif);
 
-void xenvif_get(struct xenvif *vif);
-void xenvif_put(struct xenvif *vif);
-
 int xenvif_xenbus_init(void);
 void xenvif_xenbus_fini(void);
 
@@ -139,18 +200,8 @@ int xen_netbk_map_frontend_rings(struct xenvif *vif,
 				 grant_ref_t tx_ring_ref,
 				 grant_ref_t rx_ring_ref);
 
-/* (De)Register a xenvif with the netback backend. */
-void xen_netbk_add_xenvif(struct xenvif *vif);
-void xen_netbk_remove_xenvif(struct xenvif *vif);
-
-/* (De)Schedule backend processing for a xenvif */
-void xen_netbk_schedule_xenvif(struct xenvif *vif);
-void xen_netbk_deschedule_xenvif(struct xenvif *vif);
-
 /* Check for SKBs from frontend and schedule backend processing */
 void xen_netbk_check_rx_xenvif(struct xenvif *vif);
-/* Receive an SKB from the frontend */
-void xenvif_receive_skb(struct xenvif *vif, struct sk_buff *skb);
 
 /* Queue an SKB for transmission to the frontend */
 void xen_netbk_queue_tx_skb(struct xenvif *vif, struct sk_buff *skb);
@@ -163,6 +214,11 @@ void xenvif_carrier_off(struct xenvif *vif);
 /* Returns number of ring slots required to send an skb to the frontend */
 unsigned int xen_netbk_count_skb_slots(struct xenvif *vif, struct sk_buff *skb);
 
+int xen_netbk_tx_action(struct xenvif *vif, int budget);
+void xen_netbk_rx_action(struct xenvif *vif);
+
+int xen_netbk_kthread(void *data);
+
 extern bool separate_tx_rx_irq;
 
 #endif /* __XEN_NETBACK__COMMON_H__ */

commit e1f00a69ec26e3eb9847c61c665b8fb3f0c6b477
Author: Wei Liu <wei.liu2@citrix.com>
Date:   Wed May 22 06:34:45 2013 +0000

    xen-netback: split event channels support for Xen backend driver
    
    Netback and netfront only use one event channel to do TX / RX notification,
    which may cause unnecessary wake-up of processing routines. This patch adds a
    new feature called feature-split-event-channels to netback, enabling it to
    handle TX and RX events separately.
    
    Netback will use tx_irq to notify guest for TX completion, rx_irq for RX
    notification.
    
    If frontend doesn't support this feature, tx_irq equals to rx_irq.
    
    Signed-off-by: Wei Liu <wei.liu2@citrix.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index 6102a6c59f4e..8a4d77ee9c5b 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -57,8 +57,12 @@ struct xenvif {
 
 	u8               fe_dev_addr[6];
 
-	/* Physical parameters of the comms window. */
-	unsigned int     irq;
+	/* When feature-split-event-channels = 0, tx_irq = rx_irq. */
+	unsigned int tx_irq;
+	unsigned int rx_irq;
+	/* Only used when feature-split-event-channels = 1 */
+	char tx_irq_name[IFNAMSIZ+4]; /* DEVNAME-tx */
+	char rx_irq_name[IFNAMSIZ+4]; /* DEVNAME-rx */
 
 	/* List of frontends to notify after a batch of frames sent. */
 	struct list_head notify_list;
@@ -113,7 +117,8 @@ struct xenvif *xenvif_alloc(struct device *parent,
 			    unsigned int handle);
 
 int xenvif_connect(struct xenvif *vif, unsigned long tx_ring_ref,
-		   unsigned long rx_ring_ref, unsigned int evtchn);
+		   unsigned long rx_ring_ref, unsigned int tx_evtchn,
+		   unsigned int rx_evtchn);
 void xenvif_disconnect(struct xenvif *vif);
 
 void xenvif_get(struct xenvif *vif);
@@ -158,4 +163,6 @@ void xenvif_carrier_off(struct xenvif *vif);
 /* Returns number of ring slots required to send an skb to the frontend */
 unsigned int xen_netbk_count_skb_slots(struct xenvif *vif, struct sk_buff *skb);
 
+extern bool separate_tx_rx_irq;
+
 #endif /* __XEN_NETBACK__COMMON_H__ */

commit b103f358d9f6f58658f1a6dc08912ab921dd86f1
Author: Wei Liu <wei.liu2@citrix.com>
Date:   Thu May 16 23:26:11 2013 +0000

    xen-netback: enable user to unload netback module
    
    This patch enables user to unload netback module, which is useful when user
    wants to upgrade to a newer netback module without rebooting the host.
    
    Netfront cannot handle netback removal event. As we cannot fix all possible
    frontends we add module get / put along with vif get / put to avoid
    mis-unloading of netback. To unload netback module, user needs to shutdown all
    VMs or migrate them to another host or unplug all vifs before hand.
    
    Signed-off-by: Wei Liu <wei.liu2@citrix.com>
    Acked-by: Ian Campbell <ian.campbell@citrix.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index 9d7f1723dd8f..6102a6c59f4e 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -120,6 +120,7 @@ void xenvif_get(struct xenvif *vif);
 void xenvif_put(struct xenvif *vif);
 
 int xenvif_xenbus_init(void);
+void xenvif_xenbus_fini(void);
 
 int xenvif_schedulable(struct xenvif *vif);
 

commit 48856286b64e4b66ec62b94e504d0b29c1ade664
Author: Ian Campbell <Ian.Campbell@citrix.com>
Date:   Wed Feb 6 23:41:35 2013 +0000

    xen/netback: shutdown the ring if it contains garbage.
    
    A buggy or malicious frontend should not be able to confuse netback.
    If we spot anything which is not as it should be then shutdown the
    device and don't try to continue with the ring in a potentially
    hostile state. Well behaved and non-hostile frontends will not be
    penalised.
    
    As well as making the existing checks for such errors fatal also add a
    new check that ensures that there isn't an insane number of requests
    on the ring (i.e. more than would fit in the ring). If the ring
    contains garbage then previously is was possible to loop over this
    insane number, getting an error each time and therefore not generating
    any more pending requests and therefore not exiting the loop in
    xen_netbk_tx_build_gops for an externded period.
    
    Also turn various netdev_dbg calls which no precipitate a fatal error
    into netdev_err, they are rate limited because the device is shutdown
    afterwards.
    
    This fixes at least one known DoS/softlockup of the backend domain.
    
    Signed-off-by: Ian Campbell <ian.campbell@citrix.com>
    Reviewed-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>
    Acked-by: Jan Beulich <JBeulich@suse.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index 94b79c3338c4..9d7f1723dd8f 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -151,6 +151,9 @@ void xen_netbk_queue_tx_skb(struct xenvif *vif, struct sk_buff *skb);
 /* Notify xenvif that ring now has space to send an skb to the frontend */
 void xenvif_notify_tx_completion(struct xenvif *vif);
 
+/* Prevent the device from generating any further traffic. */
+void xenvif_carrier_off(struct xenvif *vif);
+
 /* Returns number of ring slots required to send an skb to the frontend */
 unsigned int xen_netbk_count_skb_slots(struct xenvif *vif, struct sk_buff *skb);
 

commit c9d6369978411f690513994e6e53e2e6410874a4
Author: David Vrabel <david.vrabel@citrix.com>
Date:   Thu Sep 29 16:53:31 2011 +0100

    net: xen-netback: use API provided by xenbus module to map rings
    
    The xenbus module provides xenbus_map_ring_valloc() and
    xenbus_map_ring_vfree().  Use these to map the Tx and Rx ring pages
    granted by the frontend.
    
    Signed-off-by: David Vrabel <david.vrabel@citrix.com>
    Acked-by: David S. Miller <davem@davemloft.net>
    Acked-by: Ian Campbell <ian.campbell@citrix.com>
    Signed-off-by: Konrad Rzeszutek Wilk <konrad.wilk@oracle.com>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index 161f207786a4..94b79c3338c4 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -58,10 +58,6 @@ struct xenvif {
 	u8               fe_dev_addr[6];
 
 	/* Physical parameters of the comms window. */
-	grant_handle_t   tx_shmem_handle;
-	grant_ref_t      tx_shmem_ref;
-	grant_handle_t   rx_shmem_handle;
-	grant_ref_t      rx_shmem_ref;
 	unsigned int     irq;
 
 	/* List of frontends to notify after a batch of frames sent. */
@@ -70,8 +66,6 @@ struct xenvif {
 	/* The shared rings and indexes. */
 	struct xen_netif_tx_back_ring tx;
 	struct xen_netif_rx_back_ring rx;
-	struct vm_struct *tx_comms_area;
-	struct vm_struct *rx_comms_area;
 
 	/* Frontend feature information. */
 	u8 can_sg:1;
@@ -106,6 +100,11 @@ struct xenvif {
 	wait_queue_head_t waiting_to_free;
 };
 
+static inline struct xenbus_device *xenvif_to_xenbus_device(struct xenvif *vif)
+{
+	return to_xenbus_device(vif->dev->dev.parent);
+}
+
 #define XEN_NETIF_TX_RING_SIZE __CONST_RING_SIZE(xen_netif_tx, PAGE_SIZE)
 #define XEN_NETIF_RX_RING_SIZE __CONST_RING_SIZE(xen_netif_rx, PAGE_SIZE)
 

commit 11e73de7ccc1c9c61c65f914a214cb6467966a51
Author: Ian Campbell <Ian.Campbell@citrix.com>
Date:   Mon May 16 23:59:48 2011 +0000

    xen: netback: use __CONST_RING_SIZE not __RING_SIZE
    
    The later causes warnings with gcc 4.5+. __CONST_RING_SIZE was introduced in
    667c78afaec0 to fix this but as netback wasn't upstream at the time it did not
    benefit, hence:
    
      CC      drivers/net/xen-netback/netback.o
    drivers/net/xen-netback/netback.c:110:37: warning: variably modified 'grant_copy_op' at file scope [enabled by default]
    drivers/net/xen-netback/netback.c:111:30: warning: variably modified 'meta' at file scope [enabled by default]
    drivers/net/xen-netback/netback.c: In function 'xen_netbk_rx_action':
    drivers/net/xen-netback/netback.c:584:6: warning: variable 'irq' set but not used [-Wunused-but-set-variable]
    
    Thanks to Witold Baryluk for pointing this out.
    
    Signed-off-by: Ian Campbell <ian.campbell@citrix.com>
    Cc: Witold Baryluk <baryluk@smp.if.uj.edu.pl>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index 8753e6ddff8f..161f207786a4 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -106,8 +106,8 @@ struct xenvif {
 	wait_queue_head_t waiting_to_free;
 };
 
-#define XEN_NETIF_TX_RING_SIZE __RING_SIZE((struct xen_netif_tx_sring *)0, PAGE_SIZE)
-#define XEN_NETIF_RX_RING_SIZE __RING_SIZE((struct xen_netif_rx_sring *)0, PAGE_SIZE)
+#define XEN_NETIF_TX_RING_SIZE __CONST_RING_SIZE(xen_netif_tx, PAGE_SIZE)
+#define XEN_NETIF_RX_RING_SIZE __CONST_RING_SIZE(xen_netif_rx, PAGE_SIZE)
 
 struct xenvif *xenvif_alloc(struct device *parent,
 			    domid_t domid,

commit 47103041e91794acdbc6165da0ae288d844c820b
Author: Micha Mirosaw <mirq-linux@rere.qmqm.pl>
Date:   Tue Apr 19 03:35:06 2011 +0000

    net: xen-netback: convert to hw_features
    
    Signed-off-by: Micha Mirosaw <mirq-linux@rere.qmqm.pl>
    Acked-by: Ian Campbell <ian.campbell@citrix.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
index 5d7bbf2b2ee7..8753e6ddff8f 100644
--- a/drivers/net/xen-netback/common.h
+++ b/drivers/net/xen-netback/common.h
@@ -73,9 +73,6 @@ struct xenvif {
 	struct vm_struct *tx_comms_area;
 	struct vm_struct *rx_comms_area;
 
-	/* Flags that must not be set in dev->features */
-	u32 features_disabled;
-
 	/* Frontend feature information. */
 	u8 can_sg:1;
 	u8 gso:1;

commit f942dc2552b8bfdee607be867b12a8971bb9cd85
Author: Ian Campbell <Ian.Campbell@citrix.com>
Date:   Tue Mar 15 00:06:18 2011 +0000

    xen network backend driver
    
    netback is the host side counterpart to the frontend driver in
    drivers/net/xen-netfront.c. The PV protocol is also implemented by
    frontend drivers in other OSes too, such as the BSDs and even Windows.
    
    The patch is based on the driver from the xen.git pvops kernel tree but
    has been put through the checkpatch.pl wringer plus several manual
    cleanup passes and review iterations. The driver has been moved from
    drivers/xen/netback to drivers/net/xen-netback.
    
    One major change from xen.git is that the guest transmit path (i.e. what
    looks like receive to netback) has been significantly reworked to remove
    the dependency on the out of tree PageForeign page flag (a core kernel
    patch which enables a per page destructor callback on the final
    put_page). This page flag was used in order to implement a grant map
    based transmit path (where guest pages are mapped directly into SKB
    frags). Instead this version of netback uses grant copy operations into
    regular memory belonging to the backend domain. Reinstating the grant
    map functionality is something which I would like to revisit in the
    future.
    
    Note that this driver depends on 2e820f58f7ad "xen/irq: implement
    bind_interdomain_evtchn_to_irqhandler for backend drivers" which is in
    linux next via the "xen-two" tree and is intended for the 2.6.39 merge
    window:
            git://git.kernel.org/pub/scm/linux/kernel/git/konrad/xen.git stable/backends
    this branch has only that single commit since 2.6.38-rc2 and is safe for
    cross merging into the net branch.
    
    Signed-off-by: Ian Campbell <ian.campbell@citrix.com>
    Reviewed-by: Ben Hutchings <bhutchings@solarflare.com>
    Signed-off-by: David S. Miller <davem@davemloft.net>

diff --git a/drivers/net/xen-netback/common.h b/drivers/net/xen-netback/common.h
new file mode 100644
index 000000000000..5d7bbf2b2ee7
--- /dev/null
+++ b/drivers/net/xen-netback/common.h
@@ -0,0 +1,161 @@
+/*
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License version 2
+ * as published by the Free Software Foundation; or, when distributed
+ * separately from the Linux kernel or incorporated into other
+ * software packages, subject to the following license:
+ *
+ * Permission is hereby granted, free of charge, to any person obtaining a copy
+ * of this source file (the "Software"), to deal in the Software without
+ * restriction, including without limitation the rights to use, copy, modify,
+ * merge, publish, distribute, sublicense, and/or sell copies of the Software,
+ * and to permit persons to whom the Software is furnished to do so, subject to
+ * the following conditions:
+ *
+ * The above copyright notice and this permission notice shall be included in
+ * all copies or substantial portions of the Software.
+ *
+ * THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+ * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+ * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+ * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+ * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING
+ * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS
+ * IN THE SOFTWARE.
+ */
+
+#ifndef __XEN_NETBACK__COMMON_H__
+#define __XEN_NETBACK__COMMON_H__
+
+#define pr_fmt(fmt) KBUILD_MODNAME ":%s: " fmt, __func__
+
+#include <linux/module.h>
+#include <linux/interrupt.h>
+#include <linux/slab.h>
+#include <linux/ip.h>
+#include <linux/in.h>
+#include <linux/io.h>
+#include <linux/netdevice.h>
+#include <linux/etherdevice.h>
+#include <linux/wait.h>
+#include <linux/sched.h>
+
+#include <xen/interface/io/netif.h>
+#include <xen/interface/grant_table.h>
+#include <xen/grant_table.h>
+#include <xen/xenbus.h>
+
+struct xen_netbk;
+
+struct xenvif {
+	/* Unique identifier for this interface. */
+	domid_t          domid;
+	unsigned int     handle;
+
+	/* Reference to netback processing backend. */
+	struct xen_netbk *netbk;
+
+	u8               fe_dev_addr[6];
+
+	/* Physical parameters of the comms window. */
+	grant_handle_t   tx_shmem_handle;
+	grant_ref_t      tx_shmem_ref;
+	grant_handle_t   rx_shmem_handle;
+	grant_ref_t      rx_shmem_ref;
+	unsigned int     irq;
+
+	/* List of frontends to notify after a batch of frames sent. */
+	struct list_head notify_list;
+
+	/* The shared rings and indexes. */
+	struct xen_netif_tx_back_ring tx;
+	struct xen_netif_rx_back_ring rx;
+	struct vm_struct *tx_comms_area;
+	struct vm_struct *rx_comms_area;
+
+	/* Flags that must not be set in dev->features */
+	u32 features_disabled;
+
+	/* Frontend feature information. */
+	u8 can_sg:1;
+	u8 gso:1;
+	u8 gso_prefix:1;
+	u8 csum:1;
+
+	/* Internal feature information. */
+	u8 can_queue:1;	    /* can queue packets for receiver? */
+
+	/*
+	 * Allow xenvif_start_xmit() to peek ahead in the rx request
+	 * ring.  This is a prediction of what rx_req_cons will be
+	 * once all queued skbs are put on the ring.
+	 */
+	RING_IDX rx_req_cons_peek;
+
+	/* Transmit shaping: allow 'credit_bytes' every 'credit_usec'. */
+	unsigned long   credit_bytes;
+	unsigned long   credit_usec;
+	unsigned long   remaining_credit;
+	struct timer_list credit_timeout;
+
+	/* Statistics */
+	unsigned long rx_gso_checksum_fixup;
+
+	/* Miscellaneous private stuff. */
+	struct list_head schedule_list;
+	atomic_t         refcnt;
+	struct net_device *dev;
+
+	wait_queue_head_t waiting_to_free;
+};
+
+#define XEN_NETIF_TX_RING_SIZE __RING_SIZE((struct xen_netif_tx_sring *)0, PAGE_SIZE)
+#define XEN_NETIF_RX_RING_SIZE __RING_SIZE((struct xen_netif_rx_sring *)0, PAGE_SIZE)
+
+struct xenvif *xenvif_alloc(struct device *parent,
+			    domid_t domid,
+			    unsigned int handle);
+
+int xenvif_connect(struct xenvif *vif, unsigned long tx_ring_ref,
+		   unsigned long rx_ring_ref, unsigned int evtchn);
+void xenvif_disconnect(struct xenvif *vif);
+
+void xenvif_get(struct xenvif *vif);
+void xenvif_put(struct xenvif *vif);
+
+int xenvif_xenbus_init(void);
+
+int xenvif_schedulable(struct xenvif *vif);
+
+int xen_netbk_rx_ring_full(struct xenvif *vif);
+
+int xen_netbk_must_stop_queue(struct xenvif *vif);
+
+/* (Un)Map communication rings. */
+void xen_netbk_unmap_frontend_rings(struct xenvif *vif);
+int xen_netbk_map_frontend_rings(struct xenvif *vif,
+				 grant_ref_t tx_ring_ref,
+				 grant_ref_t rx_ring_ref);
+
+/* (De)Register a xenvif with the netback backend. */
+void xen_netbk_add_xenvif(struct xenvif *vif);
+void xen_netbk_remove_xenvif(struct xenvif *vif);
+
+/* (De)Schedule backend processing for a xenvif */
+void xen_netbk_schedule_xenvif(struct xenvif *vif);
+void xen_netbk_deschedule_xenvif(struct xenvif *vif);
+
+/* Check for SKBs from frontend and schedule backend processing */
+void xen_netbk_check_rx_xenvif(struct xenvif *vif);
+/* Receive an SKB from the frontend */
+void xenvif_receive_skb(struct xenvif *vif, struct sk_buff *skb);
+
+/* Queue an SKB for transmission to the frontend */
+void xen_netbk_queue_tx_skb(struct xenvif *vif, struct sk_buff *skb);
+/* Notify xenvif that ring now has space to send an skb to the frontend */
+void xenvif_notify_tx_completion(struct xenvif *vif);
+
+/* Returns number of ring slots required to send an skb to the frontend */
+unsigned int xen_netbk_count_skb_slots(struct xenvif *vif, struct sk_buff *skb);
+
+#endif /* __XEN_NETBACK__COMMON_H__ */
