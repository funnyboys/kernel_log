commit 338f6dac8585beaf4d913de8847e430808fb7596
Author: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
Date:   Tue Mar 24 09:18:21 2020 +0530

    libnvdimm: Update persistence domain value for of_pmem and papr_scm device
    
    Currently, kernel shows the below values
            "persistence_domain":"cpu_cache"
            "persistence_domain":"memory_controller"
            "persistence_domain":"unknown"
    
    "cpu_cache" indicates no extra instructions is needed to ensure the persistence
    of data in the pmem media on power failure.
    
    "memory_controller" indicates cpu cache flush instructions are required to flush
    the data. Platform provides mechanisms to automatically flush outstanding
    write data from memory controler to pmem on system power loss.
    
    Based on the above use memory_controller for non volatile regions on ppc64.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Link: https://lore.kernel.org/r/20200324034821.60869-1-aneesh.kumar@linux.ibm.com
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/drivers/nvdimm/of_pmem.c b/drivers/nvdimm/of_pmem.c
index 8224d1431ea9..6826a274a1f1 100644
--- a/drivers/nvdimm/of_pmem.c
+++ b/drivers/nvdimm/of_pmem.c
@@ -62,8 +62,10 @@ static int of_pmem_region_probe(struct platform_device *pdev)
 
 		if (is_volatile)
 			region = nvdimm_volatile_region_create(bus, &ndr_desc);
-		else
+		else {
+			set_bit(ND_REGION_PERSIST_MEMCTRL, &ndr_desc.flags);
 			region = nvdimm_pmem_region_create(bus, &ndr_desc);
+		}
 
 		if (!region)
 			dev_warn(&pdev->dev, "Unable to register region %pR from %pOF\n",

commit e755799aefa9385469bec49b2c2ccf1aaa33829a
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Tue Nov 12 17:08:56 2019 -0800

    libnvdimm: Move nvdimm_bus_attribute_group to device_type
    
    A 'struct device_type' instance can carry default attributes for the
    device. Use this facility to remove the export of
    nvdimm_bus_attribute_group and put the responsibility on the core rather
    than leaf implementations to define this attribute.
    
    Cc: Ira Weiny <ira.weiny@intel.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: "Oliver O'Halloran" <oohall@gmail.com>
    Cc: Vishal Verma <vishal.l.verma@intel.com>
    Cc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>
    Reviewed-by: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Link: https://lore.kernel.org/r/157309903815.1582359.6418211876315050283.stgit@dwillia2-desk3.amr.corp.intel.com

diff --git a/drivers/nvdimm/of_pmem.c b/drivers/nvdimm/of_pmem.c
index c0b5ac36df9d..8224d1431ea9 100644
--- a/drivers/nvdimm/of_pmem.c
+++ b/drivers/nvdimm/of_pmem.c
@@ -9,11 +9,6 @@
 #include <linux/ioport.h>
 #include <linux/slab.h>
 
-static const struct attribute_group *bus_attr_groups[] = {
-	&nvdimm_bus_attribute_group,
-	NULL,
-};
-
 struct of_pmem_private {
 	struct nvdimm_bus_descriptor bus_desc;
 	struct nvdimm_bus *bus;
@@ -35,7 +30,6 @@ static int of_pmem_region_probe(struct platform_device *pdev)
 	if (!priv)
 		return -ENOMEM;
 
-	priv->bus_desc.attr_groups = bus_attr_groups;
 	priv->bus_desc.provider_name = kstrdup(pdev->name, GFP_KERNEL);
 	priv->bus_desc.module = THIS_MODULE;
 	priv->bus_desc.of_node = np;

commit 7c4fc8cde1641e3213eb1dafc6854331e9e0828c
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Tue Nov 12 17:07:16 2019 -0800

    libnvdimm: Move nd_region_attribute_group to device_type
    
    A 'struct device_type' instance can carry default attributes for the
    device. Use this facility to remove the export of
    nd_region_attribute_group and put the responsibility on the core rather
    than leaf implementations to define this attribute.
    
    Cc: Ira Weiny <ira.weiny@intel.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: "Oliver O'Halloran" <oohall@gmail.com>
    Cc: Vishal Verma <vishal.l.verma@intel.com>
    Cc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>
    Reviewed-by: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Link: https://lore.kernel.org/r/157309902169.1582359.16828508538444551337.stgit@dwillia2-desk3.amr.corp.intel.com

diff --git a/drivers/nvdimm/of_pmem.c b/drivers/nvdimm/of_pmem.c
index 41348fa6b74c..c0b5ac36df9d 100644
--- a/drivers/nvdimm/of_pmem.c
+++ b/drivers/nvdimm/of_pmem.c
@@ -9,11 +9,6 @@
 #include <linux/ioport.h>
 #include <linux/slab.h>
 
-static const struct attribute_group *region_attr_groups[] = {
-	&nd_region_attribute_group,
-	NULL,
-};
-
 static const struct attribute_group *bus_attr_groups[] = {
 	&nvdimm_bus_attribute_group,
 	NULL,
@@ -65,7 +60,6 @@ static int of_pmem_region_probe(struct platform_device *pdev)
 		 * structures so passing a stack pointer is fine.
 		 */
 		memset(&ndr_desc, 0, sizeof(ndr_desc));
-		ndr_desc.attr_groups = region_attr_groups;
 		ndr_desc.numa_node = dev_to_node(&pdev->dev);
 		ndr_desc.target_node = ndr_desc.numa_node;
 		ndr_desc.res = &pdev->resource[i];

commit adbb68293fc5950a46e3e22f9dc9c619661194ae
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Tue Nov 12 17:00:24 2019 -0800

    libnvdimm: Move nd_device_attribute_group to device_type
    
    A 'struct device_type' instance can carry default attributes for the
    device. Use this facility to remove the export of
    nd_device_attribute_group and put the responsibility on the core rather
    than leaf implementations to define this attribute.
    
    For regions this creates a new nd_region_attribute_groups[] added to the
    per-region device-type instances.
    
    Cc: Ira Weiny <ira.weiny@intel.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: "Oliver O'Halloran" <oohall@gmail.com>
    Cc: Vishal Verma <vishal.l.verma@intel.com>
    Cc: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Reviewed-by: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Link: https://lore.kernel.org/r/157309901138.1582359.12909354140826530394.stgit@dwillia2-desk3.amr.corp.intel.com
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/drivers/nvdimm/of_pmem.c b/drivers/nvdimm/of_pmem.c
index 97187d6c0bdb..41348fa6b74c 100644
--- a/drivers/nvdimm/of_pmem.c
+++ b/drivers/nvdimm/of_pmem.c
@@ -11,7 +11,6 @@
 
 static const struct attribute_group *region_attr_groups[] = {
 	&nd_region_attribute_group,
-	&nd_device_attribute_group,
 	NULL,
 };
 

commit 49bddc73d15c25a68e4294d76fc74519fda984cd
Author: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
Date:   Wed Aug 7 09:30:29 2019 +0530

    libnvdimm/of_pmem: Provide a unique name for bus provider
    
    ndctl binaries, v66 and older, mistakenly require the ndbus to have
    unique names. If not while enumerating the bus in userspace it drops bus
    with similar names.  This results in us not listing devices beneath the
    bus.
    
    Signed-off-by: Aneesh Kumar K.V <aneesh.kumar@linux.ibm.com>
    Tested-by: Vaibhav Jain <vaibhav@linux.ibm.com>
    Link: https://lore.kernel.org/r/20190807040029.11344-1-aneesh.kumar@linux.ibm.com
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/drivers/nvdimm/of_pmem.c b/drivers/nvdimm/of_pmem.c
index a0c8dcfa0bf9..97187d6c0bdb 100644
--- a/drivers/nvdimm/of_pmem.c
+++ b/drivers/nvdimm/of_pmem.c
@@ -42,7 +42,7 @@ static int of_pmem_region_probe(struct platform_device *pdev)
 		return -ENOMEM;
 
 	priv->bus_desc.attr_groups = bus_attr_groups;
-	priv->bus_desc.provider_name = "of_pmem";
+	priv->bus_desc.provider_name = kstrdup(pdev->name, GFP_KERNEL);
 	priv->bus_desc.module = THIS_MODULE;
 	priv->bus_desc.of_node = np;
 

commit f67e3fb4891287b8248ebb3320f794b9f5e782d4
Merge: 477558d7e8d8 c221c0b0308f
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Sat Mar 16 13:05:32 2019 -0700

    Merge tag 'devdax-for-5.1' of git://git.kernel.org/pub/scm/linux/kernel/git/nvdimm/nvdimm
    
    Pull device-dax updates from Dan Williams:
     "New device-dax infrastructure to allow persistent memory and other
      "reserved" / performance differentiated memories, to be assigned to
      the core-mm as "System RAM".
    
      Some users want to use persistent memory as additional volatile
      memory. They are willing to cope with potential performance
      differences, for example between DRAM and 3D Xpoint, and want to use
      typical Linux memory management apis rather than a userspace memory
      allocator layered over an mmap() of a dax file. The administration
      model is to decide how much Persistent Memory (pmem) to use as System
      RAM, create a device-dax-mode namespace of that size, and then assign
      it to the core-mm. The rationale for device-dax is that it is a
      generic memory-mapping driver that can be layered over any "special
      purpose" memory, not just pmem. On subsequent boots udev rules can be
      used to restore the memory assignment.
    
      One implication of using pmem as RAM is that mlock() no longer keeps
      data off persistent media. For this reason it is recommended to enable
      NVDIMM Security (previously merged for 5.0) to encrypt pmem contents
      at rest. We considered making this recommendation an actively enforced
      requirement, but in the end decided to leave it as a distribution /
      administrator policy to allow for emulation and test environments that
      lack security capable NVDIMMs.
    
      Summary:
    
       - Replace the /sys/class/dax device model with /sys/bus/dax, and
         include a compat driver so distributions can opt-in to the new ABI.
    
       - Allow for an alternative driver for the device-dax address-range
    
       - Introduce the 'kmem' driver to hotplug / assign a device-dax
         address-range to the core-mm.
    
       - Arrange for the device-dax target-node to be onlined so that the
         newly added memory range can be uniquely referenced by numa apis"
    
    NOTE! I'm not entirely happy with the whole "PMEM as RAM" model because
    we currently have special - and very annoying rules in the kernel about
    accessing PMEM only with the "MC safe" accessors, because machine checks
    inside the regular repeat string copy functions can be fatal in some
    (not described) circumstances.
    
    And apparently the PMEM modules can cause that a lot more than regular
    RAM.  The argument is that this happens because PMEM doesn't necessarily
    get scrubbed at boot like RAM does, but that is planned to be added for
    the user space tooling.
    
    Quoting Dan from another email:
     "The exposure can be reduced in the volatile-RAM case by scanning for
      and clearing errors before it is onlined as RAM. The userspace tooling
      for that can be in place before v5.1-final. There's also runtime
      notifications of errors via acpi_nfit_uc_error_notify() from
      background scrubbers on the DIMM devices. With that mechanism the
      kernel could proactively clear newly discovered poison in the volatile
      case, but that would be additional development more suitable for v5.2.
    
      I understand the concern, and the need to highlight this issue by
      tapping the brakes on feature development, but I don't see PMEM as RAM
      making the situation worse when the exposure is also there via DAX in
      the PMEM case. Volatile-RAM is arguably a safer use case since it's
      possible to repair pages where the persistent case needs active
      application coordination"
    
    * tag 'devdax-for-5.1' of git://git.kernel.org/pub/scm/linux/kernel/git/nvdimm/nvdimm:
      device-dax: "Hotplug" persistent memory for use like normal RAM
      mm/resource: Let walk_system_ram_range() search child resources
      mm/memory-hotplug: Allow memory resources to be children
      mm/resource: Move HMM pr_debug() deeper into resource code
      mm/resource: Return real error codes from walk failures
      device-dax: Add a 'modalias' attribute to DAX 'bus' devices
      device-dax: Add a 'target_node' attribute
      device-dax: Auto-bind device after successful new_id
      acpi/nfit, device-dax: Identify differentiated memory with a unique numa-node
      device-dax: Add /sys/class/dax backwards compatibility
      device-dax: Add support for a dax override driver
      device-dax: Move resource pinning+mapping into the common driver
      device-dax: Introduce bus + driver model
      device-dax: Start defining a dax bus model
      device-dax: Remove multi-resource infrastructure
      device-dax: Kill dax_region base
      device-dax: Kill dax_region ida

commit 316720b9c2341307b9a17103cdafa1ca9b2fb872
Author: YueHaibing <yuehaibing@huawei.com>
Date:   Tue Feb 26 01:42:53 2019 +0000

    libnvdimm/of_pmem: Fix platform_no_drv_owner.cocci warnings
    
    Remove .owner field if calls are used which set it automatically
    Generated by: scripts/coccinelle/api/platform_no_drv_owner.cocci
    
    Signed-off-by: YueHaibing <yuehaibing@huawei.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/drivers/nvdimm/of_pmem.c b/drivers/nvdimm/of_pmem.c
index 0a701837dfc0..11b9821eba85 100644
--- a/drivers/nvdimm/of_pmem.c
+++ b/drivers/nvdimm/of_pmem.c
@@ -108,7 +108,6 @@ static struct platform_driver of_pmem_region_driver = {
 	.remove = of_pmem_region_remove,
 	.driver = {
 		.name = "of_pmem",
-		.owner = THIS_MODULE,
 		.of_match_table = of_pmem_region_match,
 	},
 };

commit 8fc5c73554db0ac18c0c6ac5b2099ab917f83bdf
Author: Dan Williams <dan.j.williams@intel.com>
Date:   Fri Nov 9 12:43:07 2018 -0800

    acpi/nfit, device-dax: Identify differentiated memory with a unique numa-node
    
    Persistent memory, as described by the ACPI NFIT (NVDIMM Firmware
    Interface Table), is the first known instance of a memory range
    described by a unique "target" proximity domain. Where "initiator" and
    "target" proximity domains is an approach that the ACPI HMAT
    (Heterogeneous Memory Attributes Table) uses to described the unique
    performance properties of a memory range relative to a given initiator
    (e.g. CPU or DMA device).
    
    Currently the numa-node for a /dev/pmemX block-device or /dev/daxX.Y
    char-device follows the traditional notion of 'numa-node' where the
    attribute conveys the closest online numa-node. That numa-node attribute
    is useful for cpu-binding and memory-binding processes *near* the
    device. However, when the memory range backing a 'pmem', or 'dax' device
    is onlined (memory hot-add) the memory-only-numa-node representing that
    address needs to be differentiated from the set of online nodes. In
    other words, the numa-node association of the device depends on whether
    you can bind processes *near* the cpu-numa-node in the offline
    device-case, or bind process *on* the memory-range directly after the
    backing address range is onlined.
    
    Allow for the case that platform firmware describes persistent memory
    with a unique proximity domain, i.e. when it is distinct from the
    proximity of DRAM and CPUs that are on the same socket. Plumb the Linux
    numa-node translation of that proximity through the libnvdimm region
    device to namespaces that are in device-dax mode. With this in place the
    proposed kmem driver [1] can optionally discover a unique numa-node
    number for the address range as it transitions the memory from an
    offline state managed by a device-driver to an online memory range
    managed by the core-mm.
    
    [1]: https://lore.kernel.org/lkml/20181022201317.8558C1D8@viggo.jf.intel.com
    
    Reported-by: Fan Du <fan.du@intel.com>
    Cc: Michael Ellerman <mpe@ellerman.id.au>
    Cc: "Oliver O'Halloran" <oohall@gmail.com>
    Cc: Dave Hansen <dave.hansen@linux.intel.com>
    Cc: Jérôme Glisse <jglisse@redhat.com>
    Reviewed-by: Yang Shi <yang.shi@linux.alibaba.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/drivers/nvdimm/of_pmem.c b/drivers/nvdimm/of_pmem.c
index 0a701837dfc0..ecaaa27438e2 100644
--- a/drivers/nvdimm/of_pmem.c
+++ b/drivers/nvdimm/of_pmem.c
@@ -68,6 +68,7 @@ static int of_pmem_region_probe(struct platform_device *pdev)
 		memset(&ndr_desc, 0, sizeof(ndr_desc));
 		ndr_desc.attr_groups = region_attr_groups;
 		ndr_desc.numa_node = dev_to_node(&pdev->dev);
+		ndr_desc.target_node = ndr_desc.numa_node;
 		ndr_desc.res = &pdev->resource[i];
 		ndr_desc.of_node = np;
 		set_bit(ND_REGION_PAGEMAP, &ndr_desc.flags);

commit df3f126482dba8e00cdbfc8fc44a05a5a35b1704
Author: Rob Herring <robh@kernel.org>
Date:   Mon Apr 16 11:58:16 2018 -0500

    libnvdimm, of_pmem: use dev_to_node() instead of of_node_to_nid()
    
    Remove the direct dependency on of_node_to_nid() by using dev_to_node()
    instead. Any DT platform device will have its NUMA node id set when the
    device is created.
    
    With this, commit 291717b6fbdb ("libnvdimm, of_pmem: workaround OF_NUMA=n
    build error") can be reverted.
    
    Fixes: 717197608952 ("libnvdimm: Add device-tree based driver")
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Oliver O'Halloran <oohall@gmail.com>
    Cc: linux-nvdimm@lists.01.org
    Signed-off-by: Rob Herring <robh@kernel.org>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/drivers/nvdimm/of_pmem.c b/drivers/nvdimm/of_pmem.c
index 85013bad35de..0a701837dfc0 100644
--- a/drivers/nvdimm/of_pmem.c
+++ b/drivers/nvdimm/of_pmem.c
@@ -67,7 +67,7 @@ static int of_pmem_region_probe(struct platform_device *pdev)
 		 */
 		memset(&ndr_desc, 0, sizeof(ndr_desc));
 		ndr_desc.attr_groups = region_attr_groups;
-		ndr_desc.numa_node = of_node_to_nid(np);
+		ndr_desc.numa_node = dev_to_node(&pdev->dev);
 		ndr_desc.res = &pdev->resource[i];
 		ndr_desc.of_node = np;
 		set_bit(ND_REGION_PAGEMAP, &ndr_desc.flags);

commit 7171976089528cb3d057a6fb288e7f8f89ab7f68
Author: Oliver O'Halloran <oohall@gmail.com>
Date:   Fri Apr 6 15:21:14 2018 +1000

    libnvdimm: Add device-tree based driver
    
    This patch adds peliminary device-tree bindings for persistent memory
    regions. The driver registers a libnvdimm bus for each pmem-region
    node and each address range under the node is converted to a region
    within that bus.
    
    Signed-off-by: Oliver O'Halloran <oohall@gmail.com>
    Signed-off-by: Dan Williams <dan.j.williams@intel.com>

diff --git a/drivers/nvdimm/of_pmem.c b/drivers/nvdimm/of_pmem.c
new file mode 100644
index 000000000000..85013bad35de
--- /dev/null
+++ b/drivers/nvdimm/of_pmem.c
@@ -0,0 +1,119 @@
+// SPDX-License-Identifier: GPL-2.0+
+
+#define pr_fmt(fmt) "of_pmem: " fmt
+
+#include <linux/of_platform.h>
+#include <linux/of_address.h>
+#include <linux/libnvdimm.h>
+#include <linux/module.h>
+#include <linux/ioport.h>
+#include <linux/slab.h>
+
+static const struct attribute_group *region_attr_groups[] = {
+	&nd_region_attribute_group,
+	&nd_device_attribute_group,
+	NULL,
+};
+
+static const struct attribute_group *bus_attr_groups[] = {
+	&nvdimm_bus_attribute_group,
+	NULL,
+};
+
+struct of_pmem_private {
+	struct nvdimm_bus_descriptor bus_desc;
+	struct nvdimm_bus *bus;
+};
+
+static int of_pmem_region_probe(struct platform_device *pdev)
+{
+	struct of_pmem_private *priv;
+	struct device_node *np;
+	struct nvdimm_bus *bus;
+	bool is_volatile;
+	int i;
+
+	np = dev_of_node(&pdev->dev);
+	if (!np)
+		return -ENXIO;
+
+	priv = kzalloc(sizeof(*priv), GFP_KERNEL);
+	if (!priv)
+		return -ENOMEM;
+
+	priv->bus_desc.attr_groups = bus_attr_groups;
+	priv->bus_desc.provider_name = "of_pmem";
+	priv->bus_desc.module = THIS_MODULE;
+	priv->bus_desc.of_node = np;
+
+	priv->bus = bus = nvdimm_bus_register(&pdev->dev, &priv->bus_desc);
+	if (!bus) {
+		kfree(priv);
+		return -ENODEV;
+	}
+	platform_set_drvdata(pdev, priv);
+
+	is_volatile = !!of_find_property(np, "volatile", NULL);
+	dev_dbg(&pdev->dev, "Registering %s regions from %pOF\n",
+			is_volatile ? "volatile" : "non-volatile",  np);
+
+	for (i = 0; i < pdev->num_resources; i++) {
+		struct nd_region_desc ndr_desc;
+		struct nd_region *region;
+
+		/*
+		 * NB: libnvdimm copies the data from ndr_desc into it's own
+		 * structures so passing a stack pointer is fine.
+		 */
+		memset(&ndr_desc, 0, sizeof(ndr_desc));
+		ndr_desc.attr_groups = region_attr_groups;
+		ndr_desc.numa_node = of_node_to_nid(np);
+		ndr_desc.res = &pdev->resource[i];
+		ndr_desc.of_node = np;
+		set_bit(ND_REGION_PAGEMAP, &ndr_desc.flags);
+
+		if (is_volatile)
+			region = nvdimm_volatile_region_create(bus, &ndr_desc);
+		else
+			region = nvdimm_pmem_region_create(bus, &ndr_desc);
+
+		if (!region)
+			dev_warn(&pdev->dev, "Unable to register region %pR from %pOF\n",
+					ndr_desc.res, np);
+		else
+			dev_dbg(&pdev->dev, "Registered region %pR from %pOF\n",
+					ndr_desc.res, np);
+	}
+
+	return 0;
+}
+
+static int of_pmem_region_remove(struct platform_device *pdev)
+{
+	struct of_pmem_private *priv = platform_get_drvdata(pdev);
+
+	nvdimm_bus_unregister(priv->bus);
+	kfree(priv);
+
+	return 0;
+}
+
+static const struct of_device_id of_pmem_region_match[] = {
+	{ .compatible = "pmem-region" },
+	{ },
+};
+
+static struct platform_driver of_pmem_region_driver = {
+	.probe = of_pmem_region_probe,
+	.remove = of_pmem_region_remove,
+	.driver = {
+		.name = "of_pmem",
+		.owner = THIS_MODULE,
+		.of_match_table = of_pmem_region_match,
+	},
+};
+
+module_platform_driver(of_pmem_region_driver);
+MODULE_DEVICE_TABLE(of, of_pmem_region_match);
+MODULE_LICENSE("GPL");
+MODULE_AUTHOR("IBM Corporation");
