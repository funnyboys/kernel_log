commit d2a8cfc6f320263b90ca523590a339661d0f4fae
Author: Douglas Anderson <dianders@chromium.org>
Date:   Mon May 4 10:50:19 2020 -0700

    soc: qcom: rpmh-rsc: Remove the pm_lock
    
    It has been postulated that the pm_lock is bad for performance because
    a CPU currently running rpmh_flush() could block other CPUs from
    coming out of idle.  Similarly CPUs coming out of / going into idle
    all need to contend with each other for the spinlock just to update
    the variable tracking who's in PM.
    
    Let's optimize this a bit.  Specifically:
    
    - Use a count rather than a bitmask.  This is faster to access and
      also means we can use the atomic_inc_return() function to really
      detect who the last one to enter PM was.
    - Accept that it's OK if we race and are doing the flush (because we
      think we're last) while another CPU is coming out of idle.  As long
      as we block that CPU if/when it tries to do an active-only transfer
      we're OK.
    
    Signed-off-by: Douglas Anderson <dianders@chromium.org>
    Reviewed-by: Stephen Boyd <swboyd@chromium.org>
    Link: https://lore.kernel.org/r/20200504104917.v6.5.I295cb72bc5334a2af80313cbe97cb5c9dcb1442c@changeid
    Signed-off-by: Bjorn Andersson <bjorn.andersson@linaro.org>

diff --git a/drivers/soc/qcom/rpmh-internal.h b/drivers/soc/qcom/rpmh-internal.h
index 1f2857b3f38e..ef60e790a750 100644
--- a/drivers/soc/qcom/rpmh-internal.h
+++ b/drivers/soc/qcom/rpmh-internal.h
@@ -95,7 +95,7 @@ struct rpmh_ctrlr {
  * @num_tcs:            Number of TCSes in this DRV.
  * @rsc_pm:             CPU PM notifier for controller.
  *                      Used when solver mode is not present.
- * @cpus_entered_pm:    CPU mask for cpus in idle power collapse.
+ * @cpus_in_pm:         Number of CPUs not in idle power collapse.
  *                      Used when solver mode is not present.
  * @tcs:                TCS groups.
  * @tcs_in_use:         S/W state of the TCS; only set for ACTIVE_ONLY
@@ -103,9 +103,9 @@ struct rpmh_ctrlr {
  *                      it was borrowed for an active_only transfer.  You
  *                      must hold the lock in this struct (AKA drv->lock) in
  *                      order to update this.
- * @lock:               Synchronize state of the controller.
- * @pm_lock:            Synchronize during PM notifications.
- *                      Used when solver mode is not present.
+ * @lock:               Synchronize state of the controller.  If RPMH's cache
+ *                      lock will also be held, the order is: drv->lock then
+ *                      cache_lock.
  * @client:             Handle to the DRV's client.
  */
 struct rsc_drv {
@@ -114,11 +114,10 @@ struct rsc_drv {
 	int id;
 	int num_tcs;
 	struct notifier_block rsc_pm;
-	struct cpumask cpus_entered_pm;
+	atomic_t cpus_in_pm;
 	struct tcs_group tcs[TCS_TYPE_NR];
 	DECLARE_BITMAP(tcs_in_use, MAX_TCS_NR);
 	spinlock_t lock;
-	spinlock_t pm_lock;
 	struct rpmh_ctrlr client;
 };
 

commit 555701a45f146673c8961f084b6880c637d41129
Author: Douglas Anderson <dianders@chromium.org>
Date:   Mon May 4 10:50:18 2020 -0700

    soc: qcom: rpmh-rsc: Simplify locking by eliminating the per-TCS lock
    
    The rpmh-rsc code had both a driver-level lock (sometimes referred to
    in comments as drv->lock) and a lock per-TCS.  The idea was supposed
    to be that there would be times where you could get by with just
    locking a TCS lock and therefor other RPMH users wouldn't be blocked.
    
    The above didn't work out so well.
    
    Looking at tcs_write() the bigger drv->lock was held for most of the
    function anyway.  Only the __tcs_buffer_write() and
    __tcs_set_trigger() calls were called without holding the drv->lock.
    It actually turns out that in tcs_write() we don't need to hold the
    drv->lock for those function calls anyway even if the per-TCS lock
    isn't there anymore.  From the newly added comments in the code, this
    is because:
    - We marked "tcs_in_use" under lock.
    - Once "tcs_in_use" has been marked nobody else could be writing
      to these registers until the interrupt goes off.
    - The interrupt can't go off until we trigger w/ the last line
      of __tcs_set_trigger().
    Thus, from a tcs_write() point of view, the per-TCS lock was useless.
    
    Looking at rpmh_rsc_write_ctrl_data(), only the per-TCS lock was held.
    It turns out, though, that this function already needs to be called
    with the equivalent of the drv->lock held anyway (we either need to
    hold drv->lock as we will in a future patch or we need to know no
    other CPUs could be running as happens today).  Specifically
    rpmh_rsc_write_ctrl_data() might be writing to a TCS that has been
    borrowed for writing an active transation but it never checks this.
    
    Let's eliminate this extra overhead and avoid possible AB BA locking
    headaches.
    
    Suggested-by: Maulik Shah <mkshah@codeaurora.org>
    Signed-off-by: Douglas Anderson <dianders@chromium.org>
    Reviewed-by: Stephen Boyd <swboyd@chromium.org>
    Link: https://lore.kernel.org/r/20200504104917.v6.4.Ib8dccfdb10bf6b1fb1d600ca1c21d9c0db1ef746@changeid
    Signed-off-by: Bjorn Andersson <bjorn.andersson@linaro.org>

diff --git a/drivers/soc/qcom/rpmh-internal.h b/drivers/soc/qcom/rpmh-internal.h
index dba8510c0669..1f2857b3f38e 100644
--- a/drivers/soc/qcom/rpmh-internal.h
+++ b/drivers/soc/qcom/rpmh-internal.h
@@ -28,7 +28,6 @@ struct rsc_drv;
  * @offset:    Start of the TCS group relative to the TCSes in the RSC.
  * @num_tcs:   Number of TCSes in this type.
  * @ncpt:      Number of commands in each TCS.
- * @lock:      Lock for synchronizing this TCS writes.
  * @req:       Requests that are sent from the TCS; only used for ACTIVE_ONLY
  *             transfers (could be on a wake/sleep TCS if we are borrowing for
  *             an ACTIVE_ONLY transfer).
@@ -48,7 +47,6 @@ struct tcs_group {
 	u32 offset;
 	int num_tcs;
 	int ncpt;
-	spinlock_t lock;
 	const struct tcs_request *req[MAX_TCS_PER_TYPE];
 	DECLARE_BITMAP(slots, MAX_TCS_SLOTS);
 };
@@ -103,14 +101,9 @@ struct rpmh_ctrlr {
  * @tcs_in_use:         S/W state of the TCS; only set for ACTIVE_ONLY
  *                      transfers, but might show a sleep/wake TCS in use if
  *                      it was borrowed for an active_only transfer.  You
- *                      must hold both the lock in this struct and the
- *                      tcs_lock for the TCS in order to mark a TCS as
- *                      in-use, but you only need the lock in this structure
- *                      (aka the drv->lock) to mark one freed.
- * @lock:               Synchronize state of the controller.  If you will be
- *                      grabbing this lock and a tcs_lock at the same time,
- *                      grab the tcs_lock first so we always have a
- *                      consistent lock ordering.
+ *                      must hold the lock in this struct (AKA drv->lock) in
+ *                      order to update this.
+ * @lock:               Synchronize state of the controller.
  * @pm_lock:            Synchronize during PM notifications.
  *                      Used when solver mode is not present.
  * @client:             Handle to the DRV's client.

commit 881808d0bbf336d333981ad86bde62ef2165e437
Author: Douglas Anderson <dianders@chromium.org>
Date:   Mon Apr 13 10:04:14 2020 -0700

    soc: qcom: rpmh-rsc: Caller handles tcs_invalidate() exclusivity
    
    Auditing tcs_invalidate() made me worried.  Specifically I saw that it
    used spin_lock(), not spin_lock_irqsave().  That always worries me
    unless I can trace for sure that I'm in the interrupt handler or that
    someone else already disabled interrupts.
    
    Looking more at it, there is actually no reason for these locks
    anyway.  Specifically the only reason you'd ever call
    rpmh_rsc_invalidate() is if you cared that the sleep/wake TCSes were
    empty.  That means that they need to continue to be empty even after
    rpmh_rsc_invalidate() returns.  The only way that can happen is if the
    caller already has done something to keep all other RPMH users out.
    It should be noted that even though the caller is only worried about
    making sleep/wake TCSes empty, they also need to worry about stopping
    active-only transfers if they need to handle the case where
    active-only transfers might borrow the wake TCS.
    
    At the moment rpmh_rsc_invalidate() is only called in PM code from the
    last CPU.  If that later changes the caller will still need to solve
    the above problems themselves, so these locks will never be useful.
    
    Continuing to audit tcs_invalidate(), I found a bug.  The function
    didn't properly check for a borrowed TCS if we hadn't recently written
    anything into the TCS.  Specifically, if we've never written to the
    WAKE_TCS (or we've flushed it recently) then tcs->slots is empty.
    We'll early-out and we'll never call tcs_is_free().
    
    I thought about fixing this bug by either deleting the early check for
    bitmap_empty() or possibly only doing it if we knew we weren't on a
    TCS that could be borrowed.  However, I think it's better to just
    delete the checks.
    
    As argued above it's up to the caller to make sure that all other
    users of RPMH are quiet before tcs_invalidate() is called.  Since
    callers need to handle the zero-active-TCS case anyway that means they
    need to make sure that the active-only transfers are quiet before
    calling too.  The one way tcs_invalidate() gets called today is
    through rpmh_rsc_cpu_pm_callback() which calls
    rpmh_rsc_ctrlr_is_busy() to handle this.  When we have another path to
    get to tcs_invalidate() it will also need to come up with something
    similar and it won't need this extra check either.  If we later find
    some code path that actually needs this check back in (and somehow
    manages to be race free) we can always add it back in.
    
    Signed-off-by: Douglas Anderson <dianders@chromium.org>
    Reviewed-by: Maulik Shah <mkshah@codeaurora.org>
    Reviewed-by: Stephen Boyd <swboyd@chromium.org>
    Tested-by: Maulik Shah <mkshah@codeaurora.org>
    Link: https://lore.kernel.org/r/20200413100321.v4.9.I07c1f70e0e8f2dc0004bd38970b4e258acdc773e@changeid
    Signed-off-by: Bjorn Andersson <bjorn.andersson@linaro.org>

diff --git a/drivers/soc/qcom/rpmh-internal.h b/drivers/soc/qcom/rpmh-internal.h
index f06350cbc9a2..dba8510c0669 100644
--- a/drivers/soc/qcom/rpmh-internal.h
+++ b/drivers/soc/qcom/rpmh-internal.h
@@ -132,7 +132,7 @@ struct rsc_drv {
 int rpmh_rsc_send_data(struct rsc_drv *drv, const struct tcs_request *msg);
 int rpmh_rsc_write_ctrl_data(struct rsc_drv *drv,
 			     const struct tcs_request *msg);
-int rpmh_rsc_invalidate(struct rsc_drv *drv);
+void rpmh_rsc_invalidate(struct rsc_drv *drv);
 
 void rpmh_tx_done(const struct tcs_request *msg, int r);
 int rpmh_flush(struct rpmh_ctrlr *ctrlr);

commit e40b0c1628f27986dd90f94c43464df5aa8968cf
Author: Douglas Anderson <dianders@chromium.org>
Date:   Mon Apr 13 10:04:11 2020 -0700

    soc: qcom: rpmh-rsc: A lot of comments
    
    I've been pouring through the rpmh-rsc code and trying to understand
    it.  Document everything to the best of my ability.  All documentation
    here is strictly from code analysis--no actual knowledge of the
    hardware was used.  If something is wrong in here I either
    misunderstood the code, had a typo, or the code has a bug in it
    leading to my incorrect understanding.
    
    In a few places here I have documented things that don't make tons of
    sense.  A future patch will try to address this.  While this means I'm
    adding comments / todos and then later fixing them in the series, it
    seemed more urgent to get things documented first so that people could
    understand the later patches.
    
    Any comments I adjusted I also tried to make match kernel-doc better.
    Specifically:
    - kernel-doc says do not leave a blank line between the function
      description and the arguments
    - kernel-doc examples always have things starting w/ a capital and
      ending with a period.
    
    This should be a no-op.  It's just comment changes.
    
    Signed-off-by: Douglas Anderson <dianders@chromium.org>
    Reviewed-by: Maulik Shah <mkshah@codeaurora.org>
    Reviewed-by: Stephen Boyd <swboyd@chromium.org>
    Link: https://lore.kernel.org/r/20200413100321.v4.6.I52653eb85d7dc8981ee0dafcd0b6cc0f273e9425@changeid
    Signed-off-by: Bjorn Andersson <bjorn.andersson@linaro.org>

diff --git a/drivers/soc/qcom/rpmh-internal.h b/drivers/soc/qcom/rpmh-internal.h
index 6a6d776ccca9..f06350cbc9a2 100644
--- a/drivers/soc/qcom/rpmh-internal.h
+++ b/drivers/soc/qcom/rpmh-internal.h
@@ -22,15 +22,24 @@ struct rsc_drv;
  * struct tcs_group: group of Trigger Command Sets (TCS) to send state requests
  * to the controller
  *
- * @drv:       the controller
- * @type:      type of the TCS in this group - active, sleep, wake
- * @mask:      mask of the TCSes relative to all the TCSes in the RSC
- * @offset:    start of the TCS group relative to the TCSes in the RSC
- * @num_tcs:   number of TCSes in this type
- * @ncpt:      number of commands in each TCS
- * @lock:      lock for synchronizing this TCS writes
- * @req:       requests that are sent from the TCS
- * @slots:     indicates which of @cmd_addr are occupied
+ * @drv:       The controller.
+ * @type:      Type of the TCS in this group - active, sleep, wake.
+ * @mask:      Mask of the TCSes relative to all the TCSes in the RSC.
+ * @offset:    Start of the TCS group relative to the TCSes in the RSC.
+ * @num_tcs:   Number of TCSes in this type.
+ * @ncpt:      Number of commands in each TCS.
+ * @lock:      Lock for synchronizing this TCS writes.
+ * @req:       Requests that are sent from the TCS; only used for ACTIVE_ONLY
+ *             transfers (could be on a wake/sleep TCS if we are borrowing for
+ *             an ACTIVE_ONLY transfer).
+ *             Start: grab drv->lock, set req, set tcs_in_use, drop drv->lock,
+ *                    trigger
+ *             End: get irq, access req,
+ *                  grab drv->lock, clear tcs_in_use, drop drv->lock
+ * @slots:     Indicates which of @cmd_addr are occupied; only used for
+ *             SLEEP / WAKE TCSs.  Things are tightly packed in the
+ *             case that (ncpt < MAX_CMDS_PER_TCS).  That is if ncpt = 2 and
+ *             MAX_CMDS_PER_TCS = 16 then bit[2] = the first bit in 2nd TCS.
  */
 struct tcs_group {
 	struct rsc_drv *drv;
@@ -82,19 +91,28 @@ struct rpmh_ctrlr {
  * struct rsc_drv: the Direct Resource Voter (DRV) of the
  * Resource State Coordinator controller (RSC)
  *
- * @name:               Controller identifier
- * @tcs_base:           Start address of the TCS registers in this controller
- * @id:                 Instance id in the controller (Direct Resource Voter)
- * @num_tcs:            Number of TCSes in this DRV
- * @rsc_pm:             CPU PM notifier for controller
- *                      Used when solver mode is not present
- * @cpus_entered_pm:    CPU mask for cpus in idle power collapse
- *                      Used when solver mode is not present
- * @tcs:                TCS groups
- * @tcs_in_use:         S/W state of the TCS
- * @lock:               Synchronize state of the controller
- * @pm_lock:            Synchronize during PM notifications
- *                      Used when solver mode is not present
+ * @name:               Controller identifier.
+ * @tcs_base:           Start address of the TCS registers in this controller.
+ * @id:                 Instance id in the controller (Direct Resource Voter).
+ * @num_tcs:            Number of TCSes in this DRV.
+ * @rsc_pm:             CPU PM notifier for controller.
+ *                      Used when solver mode is not present.
+ * @cpus_entered_pm:    CPU mask for cpus in idle power collapse.
+ *                      Used when solver mode is not present.
+ * @tcs:                TCS groups.
+ * @tcs_in_use:         S/W state of the TCS; only set for ACTIVE_ONLY
+ *                      transfers, but might show a sleep/wake TCS in use if
+ *                      it was borrowed for an active_only transfer.  You
+ *                      must hold both the lock in this struct and the
+ *                      tcs_lock for the TCS in order to mark a TCS as
+ *                      in-use, but you only need the lock in this structure
+ *                      (aka the drv->lock) to mark one freed.
+ * @lock:               Synchronize state of the controller.  If you will be
+ *                      grabbing this lock and a tcs_lock at the same time,
+ *                      grab the tcs_lock first so we always have a
+ *                      consistent lock ordering.
+ * @pm_lock:            Synchronize during PM notifications.
+ *                      Used when solver mode is not present.
  * @client:             Handle to the DRV's client.
  */
 struct rsc_drv {

commit 1bc92a933f19e2415353a4892f7df4617f691b6c
Author: Douglas Anderson <dianders@chromium.org>
Date:   Mon Apr 13 10:04:10 2020 -0700

    soc: qcom: rpmh-rsc: Kill cmd_cache and find_match() with fire
    
    The "cmd_cache" in RPMH wasn't terribly sensible.  Specifically:
    
    - The current code doesn't really detect "conflicts" properly any case
      where the sequence being checked has more than one entry.  One
      simple way to see this in the current code is that if cmd[0].addr
      isn't found then cmd[1].addr is never checked.
    - The code attempted to use the "cmd_cache" to update an existing
      message in a sleep/wake TCS with new data.  The goal appeared to be
      to update part of a TCS while leaving the rest of the TCS alone.  We
      never actually do this.  We always fully invalidate and re-write
      everything.
    - If/when we try to optimize things to not fully invalidate / re-write
      every time we update the TCSes we'll need to think it through very
      carefully.  Specifically requirement of find_match() that the new
      sequence of addrs must match exactly the old sequence of addrs seems
      inflexible.  It's also not documented in rpmh_write() and
      rpmh_write_batch().  In any case, if we do decide to require updates
      to keep the exact same sequence and length then presumably the API
      and data structures should be updated to understand groups more
      properly.  The current algorithm doesn't really keep track of the
      length of the old sequence and there are several boundary-condition
      bugs because of that.  Said another way: if we decide to do
      something like this in the future we should start from scratch and
      thus find_match() isn't useful to keep around.
    
    This patch isn't quite a no-op.  Specifically:
    
    - It should be a slight performance boost of not searching through so
      many arrays.
    - The old code would have done something useful in one case: it would
      allow someone calling rpmh_write() to override the data that came
      from rpmh_write_batch().  I don't believe that actually happens in
      reality.
    
    Signed-off-by: Douglas Anderson <dianders@chromium.org>
    Reviewed-by: Maulik Shah <mkshah@codeaurora.org>
    Reviewed-by: Stephen Boyd <swboyd@chromium.org>
    Tested-by: Maulik Shah <mkshah@codeaurora.org>
    Link: https://lore.kernel.org/r/20200413100321.v4.5.I6d3d0a3ec810dc72ff1df3cbf97deefdcdeb8eef@changeid
    Signed-off-by: Bjorn Andersson <bjorn.andersson@linaro.org>

diff --git a/drivers/soc/qcom/rpmh-internal.h b/drivers/soc/qcom/rpmh-internal.h
index e9a90cb7773e..6a6d776ccca9 100644
--- a/drivers/soc/qcom/rpmh-internal.h
+++ b/drivers/soc/qcom/rpmh-internal.h
@@ -30,7 +30,6 @@ struct rsc_drv;
  * @ncpt:      number of commands in each TCS
  * @lock:      lock for synchronizing this TCS writes
  * @req:       requests that are sent from the TCS
- * @cmd_cache: flattened cache of cmds in sleep/wake TCS
  * @slots:     indicates which of @cmd_addr are occupied
  */
 struct tcs_group {
@@ -42,7 +41,6 @@ struct tcs_group {
 	int ncpt;
 	spinlock_t lock;
 	const struct tcs_request *req[MAX_TCS_PER_TYPE];
-	u32 *cmd_cache;
 	DECLARE_BITMAP(slots, MAX_TCS_SLOTS);
 };
 

commit 985427f997b6a31155cce841eb395d43c64771c5
Author: Maulik Shah <mkshah@codeaurora.org>
Date:   Sun Apr 12 20:20:02 2020 +0530

    soc: qcom: rpmh: Invoke rpmh_flush() for dirty caches
    
    Add changes to invoke rpmh flush() from CPU PM notification.
    This is done when the last the cpu is entering deep CPU idle
    states and controller is not busy.
    
    Controllers that have 'HW solver' mode like display RSC do not need
    to register for CPU PM notification. They may be in autonomous mode
    executing low power mode and do not require rpmh_flush() to happen
    from CPU PM notification.
    
    Signed-off-by: Maulik Shah <mkshah@codeaurora.org>
    Reviewed-by: Douglas Anderson <dianders@chromium.org>
    Reviewed-by: Stephen Boyd <swboyd@chromium.org>
    Link: https://lore.kernel.org/r/1586703004-13674-5-git-send-email-mkshah@codeaurora.org
    Signed-off-by: Bjorn Andersson <bjorn.andersson@linaro.org>

diff --git a/drivers/soc/qcom/rpmh-internal.h b/drivers/soc/qcom/rpmh-internal.h
index 6eec32b97f83..e9a90cb7773e 100644
--- a/drivers/soc/qcom/rpmh-internal.h
+++ b/drivers/soc/qcom/rpmh-internal.h
@@ -84,23 +84,32 @@ struct rpmh_ctrlr {
  * struct rsc_drv: the Direct Resource Voter (DRV) of the
  * Resource State Coordinator controller (RSC)
  *
- * @name:       controller identifier
- * @tcs_base:   start address of the TCS registers in this controller
- * @id:         instance id in the controller (Direct Resource Voter)
- * @num_tcs:    number of TCSes in this DRV
- * @tcs:        TCS groups
- * @tcs_in_use: s/w state of the TCS
- * @lock:       synchronize state of the controller
- * @client:     handle to the DRV's client.
+ * @name:               Controller identifier
+ * @tcs_base:           Start address of the TCS registers in this controller
+ * @id:                 Instance id in the controller (Direct Resource Voter)
+ * @num_tcs:            Number of TCSes in this DRV
+ * @rsc_pm:             CPU PM notifier for controller
+ *                      Used when solver mode is not present
+ * @cpus_entered_pm:    CPU mask for cpus in idle power collapse
+ *                      Used when solver mode is not present
+ * @tcs:                TCS groups
+ * @tcs_in_use:         S/W state of the TCS
+ * @lock:               Synchronize state of the controller
+ * @pm_lock:            Synchronize during PM notifications
+ *                      Used when solver mode is not present
+ * @client:             Handle to the DRV's client.
  */
 struct rsc_drv {
 	const char *name;
 	void __iomem *tcs_base;
 	int id;
 	int num_tcs;
+	struct notifier_block rsc_pm;
+	struct cpumask cpus_entered_pm;
 	struct tcs_group tcs[TCS_TYPE_NR];
 	DECLARE_BITMAP(tcs_in_use, MAX_TCS_NR);
 	spinlock_t lock;
+	spinlock_t pm_lock;
 	struct rpmh_ctrlr client;
 };
 

commit d5e205079c34aa1f33157627814f707d6057727a
Author: Maulik Shah <mkshah@codeaurora.org>
Date:   Mon Feb 3 19:05:35 2020 +0530

    drivers: qcom: rpmh: remove rpmh_flush export
    
    rpmh_flush() was exported with the idea that an external entity
    operation during CPU idle would know when to flush the sleep and wake
    TCS. Since, this is not the case when defining a power domain for the
    RSC. Remove the function export and instead allow the function to be
    called internally.
    
    Signed-off-by: Maulik Shah <mkshah@codeaurora.org>
    Reviewed-by: Stephen Boyd <swboyd@chromium.org>
    Link: https://lore.kernel.org/r/1580736940-6985-3-git-send-email-mkshah@codeaurora.org
    Signed-off-by: Bjorn Andersson <bjorn.andersson@linaro.org>

diff --git a/drivers/soc/qcom/rpmh-internal.h b/drivers/soc/qcom/rpmh-internal.h
index a7bbbb67991c..6eec32b97f83 100644
--- a/drivers/soc/qcom/rpmh-internal.h
+++ b/drivers/soc/qcom/rpmh-internal.h
@@ -110,5 +110,6 @@ int rpmh_rsc_write_ctrl_data(struct rsc_drv *drv,
 int rpmh_rsc_invalidate(struct rsc_drv *drv);
 
 void rpmh_tx_done(const struct tcs_request *msg, int r);
+int rpmh_flush(struct rpmh_ctrlr *ctrlr);
 
 #endif /* __RPM_INTERNAL_H__ */

commit c8790cb6da58d3fa09dfa707aa486fe6769c23bc
Author: Lina Iyer <ilina@codeaurora.org>
Date:   Wed Jun 20 18:57:06 2018 +0530

    drivers: qcom: rpmh: add support for batch RPMH request
    
    Platform drivers need make a lot of resource state requests at the same
    time, say, at the start or end of an usecase. It can be quite
    inefficient to send each request separately. Instead they can give the
    RPMH library a batch of requests to be sent and wait on the whole
    transaction to be complete.
    
    rpmh_write_batch() is a blocking call that can be used to send multiple
    RPMH command sets. Each RPMH command set is set asynchronously and the
    API blocks until all the command sets are complete and receive their
    tx_done callbacks.
    
    Signed-off-by: Lina Iyer <ilina@codeaurora.org>
    Signed-off-by: Raju P.L.S.S.S.N <rplsssn@codeaurora.org>
    Reviewed-by: Matthias Kaehlcke <mka@chromium.org>
    Signed-off-by: Andy Gross <andy.gross@linaro.org>

diff --git a/drivers/soc/qcom/rpmh-internal.h b/drivers/soc/qcom/rpmh-internal.h
index 6a8a4b7aeead..a7bbbb67991c 100644
--- a/drivers/soc/qcom/rpmh-internal.h
+++ b/drivers/soc/qcom/rpmh-internal.h
@@ -71,11 +71,13 @@ struct rpmh_request {
  * @cache: the list of cached requests
  * @cache_lock: synchronize access to the cache data
  * @dirty: was the cache updated since flush
+ * @batch_cache: Cache sleep and wake requests sent as batch
  */
 struct rpmh_ctrlr {
 	struct list_head cache;
 	spinlock_t cache_lock;
 	bool dirty;
+	struct list_head batch_cache;
 };
 
 /**

commit 564b5e24ccd4c840a7f84dfd952e5715dd9b3966
Author: Lina Iyer <ilina@codeaurora.org>
Date:   Wed Jun 20 18:57:05 2018 +0530

    drivers: qcom: rpmh: allow requests to be sent asynchronously
    
    Platform drivers that want to send a request but do not want to block
    until the RPMH request completes have now a new API -
    rpmh_write_async().
    
    The API allocates memory and send the requests and returns the control
    back to the platform driver. The tx_done callback from the controller is
    handled in the context of the controller's thread and frees the
    allocated memory. This API allows RPMH requests from atomic contexts as
    well.
    
    Signed-off-by: Lina Iyer <ilina@codeaurora.org>
    Signed-off-by: Raju P.L.S.S.S.N <rplsssn@codeaurora.org>
    Signed-off-by: Andy Gross <andy.gross@linaro.org>

diff --git a/drivers/soc/qcom/rpmh-internal.h b/drivers/soc/qcom/rpmh-internal.h
index 101145c9db1c..6a8a4b7aeead 100644
--- a/drivers/soc/qcom/rpmh-internal.h
+++ b/drivers/soc/qcom/rpmh-internal.h
@@ -54,6 +54,7 @@ struct tcs_group {
  * @completion: triggered when request is done
  * @dev: the device making the request
  * @err: err return from the controller
+ * @needs_free: check to free dynamically allocated request object
  */
 struct rpmh_request {
 	struct tcs_request msg;
@@ -61,6 +62,7 @@ struct rpmh_request {
 	struct completion *completion;
 	const struct device *dev;
 	int err;
+	bool needs_free;
 };
 
 /**

commit 600513dfeef33cb05c694d1b13d319b9e8cde536
Author: Lina Iyer <ilina@codeaurora.org>
Date:   Wed Jun 20 18:57:04 2018 +0530

    drivers: qcom: rpmh: cache sleep/wake state requests
    
    Active state requests are sent immediately to the RSC controller, while
    sleep and wake state requests are cached in this driver to avoid taxing
    the RSC controller repeatedly. The cached values will be sent to the
    controller when the rpmh_flush() is called.
    
    Generally, flushing is a system PM activity and may be called from the
    system PM drivers when the system is entering suspend or deeper sleep
    modes during cpuidle.
    
    Also allow invalidating the cached requests, so they may be re-populated
    again.
    
    Signed-off-by: Lina Iyer <ilina@codeaurora.org>
    [rplsssn: remove unneeded semicolon, address line over 80chars error]
    Signed-off-by: Raju P.L.S.S.S.N <rplsssn@codeaurora.org>
    Reviewed-by: Evan Green <evgreen@chromium.org>
    Reviewed-by: Matthias Kaehlcke <mka@chromium.org>
    Signed-off-by: Andy Gross <andy.gross@linaro.org>

diff --git a/drivers/soc/qcom/rpmh-internal.h b/drivers/soc/qcom/rpmh-internal.h
index af4da1cb6558..101145c9db1c 100644
--- a/drivers/soc/qcom/rpmh-internal.h
+++ b/drivers/soc/qcom/rpmh-internal.h
@@ -66,10 +66,14 @@ struct rpmh_request {
 /**
  * struct rpmh_ctrlr: our representation of the controller
  *
- * @drv: the controller instance
+ * @cache: the list of cached requests
+ * @cache_lock: synchronize access to the cache data
+ * @dirty: was the cache updated since flush
  */
 struct rpmh_ctrlr {
-	struct rsc_drv *drv;
+	struct list_head cache;
+	spinlock_t cache_lock;
+	bool dirty;
 };
 
 /**

commit fa460e453a83af0054dc3d3d210a6617db333a20
Author: Lina Iyer <ilina@codeaurora.org>
Date:   Wed Jun 20 18:57:02 2018 +0530

    drivers: qcom: rpmh-rsc: write sleep/wake requests to TCS
    
    Sleep and wake requests are sent when the application processor
    subsystem of the SoC is entering deep sleep states like in suspend.
    These requests help lower the system power requirements when the
    resources are not in use.
    
    Sleep and wake requests are written to the TCS slots but are not
    triggered at the time of writing. The TCS are triggered by the firmware
    after the last of the CPUs has executed its WFI. Since these requests
    may come in different batches of requests, it is the job of this
    controller driver to find and arrange the requests into the available
    TCSes.
    
    Signed-off-by: Lina Iyer <ilina@codeaurora.org>
    Signed-off-by: Raju P.L.S.S.S.N <rplsssn@codeaurora.org>
    Reviewed-by: Evan Green <evgreen@chromium.org>
    Reviewed-by: Matthias Kaehlcke <mka@chromium.org>
    Signed-off-by: Andy Gross <andy.gross@linaro.org>

diff --git a/drivers/soc/qcom/rpmh-internal.h b/drivers/soc/qcom/rpmh-internal.h
index 1e687f719301..af4da1cb6558 100644
--- a/drivers/soc/qcom/rpmh-internal.h
+++ b/drivers/soc/qcom/rpmh-internal.h
@@ -14,6 +14,7 @@
 #define MAX_CMDS_PER_TCS		16
 #define MAX_TCS_PER_TYPE		3
 #define MAX_TCS_NR			(MAX_TCS_PER_TYPE * TCS_TYPE_NR)
+#define MAX_TCS_SLOTS			(MAX_CMDS_PER_TCS * MAX_TCS_PER_TYPE)
 
 struct rsc_drv;
 
@@ -29,6 +30,8 @@ struct rsc_drv;
  * @ncpt:      number of commands in each TCS
  * @lock:      lock for synchronizing this TCS writes
  * @req:       requests that are sent from the TCS
+ * @cmd_cache: flattened cache of cmds in sleep/wake TCS
+ * @slots:     indicates which of @cmd_addr are occupied
  */
 struct tcs_group {
 	struct rsc_drv *drv;
@@ -39,6 +42,8 @@ struct tcs_group {
 	int ncpt;
 	spinlock_t lock;
 	const struct tcs_request *req[MAX_TCS_PER_TYPE];
+	u32 *cmd_cache;
+	DECLARE_BITMAP(slots, MAX_TCS_SLOTS);
 };
 
 /**
@@ -92,6 +97,9 @@ struct rsc_drv {
 };
 
 int rpmh_rsc_send_data(struct rsc_drv *drv, const struct tcs_request *msg);
+int rpmh_rsc_write_ctrl_data(struct rsc_drv *drv,
+			     const struct tcs_request *msg);
+int rpmh_rsc_invalidate(struct rsc_drv *drv);
 
 void rpmh_tx_done(const struct tcs_request *msg, int r);
 

commit c1038456b02b86cc4445441a8d33c5aca0ac103e
Author: Lina Iyer <ilina@codeaurora.org>
Date:   Wed Jun 20 18:57:01 2018 +0530

    drivers: qcom: rpmh: add RPMH helper functions
    
    Sending RPMH requests and waiting for response from the controller
    through a callback is common functionality across all platform drivers.
    To simplify drivers, add a library functions to create RPMH client and
    send resource state requests.
    
    rpmh_write() is a synchronous blocking call that can be used to send
    active state requests.
    
    Signed-off-by: Lina Iyer <ilina@codeaurora.org>
    Signed-off-by: Raju P.L.S.S.S.N <rplsssn@codeaurora.org>
    Signed-off-by: Andy Gross <andy.gross@linaro.org>

diff --git a/drivers/soc/qcom/rpmh-internal.h b/drivers/soc/qcom/rpmh-internal.h
index cc29176f1303..1e687f719301 100644
--- a/drivers/soc/qcom/rpmh-internal.h
+++ b/drivers/soc/qcom/rpmh-internal.h
@@ -41,6 +41,32 @@ struct tcs_group {
 	const struct tcs_request *req[MAX_TCS_PER_TYPE];
 };
 
+/**
+ * struct rpmh_request: the message to be sent to rpmh-rsc
+ *
+ * @msg: the request
+ * @cmd: the payload that will be part of the @msg
+ * @completion: triggered when request is done
+ * @dev: the device making the request
+ * @err: err return from the controller
+ */
+struct rpmh_request {
+	struct tcs_request msg;
+	struct tcs_cmd cmd[MAX_RPMH_PAYLOAD];
+	struct completion *completion;
+	const struct device *dev;
+	int err;
+};
+
+/**
+ * struct rpmh_ctrlr: our representation of the controller
+ *
+ * @drv: the controller instance
+ */
+struct rpmh_ctrlr {
+	struct rsc_drv *drv;
+};
+
 /**
  * struct rsc_drv: the Direct Resource Voter (DRV) of the
  * Resource State Coordinator controller (RSC)
@@ -52,6 +78,7 @@ struct tcs_group {
  * @tcs:        TCS groups
  * @tcs_in_use: s/w state of the TCS
  * @lock:       synchronize state of the controller
+ * @client:     handle to the DRV's client.
  */
 struct rsc_drv {
 	const char *name;
@@ -61,9 +88,11 @@ struct rsc_drv {
 	struct tcs_group tcs[TCS_TYPE_NR];
 	DECLARE_BITMAP(tcs_in_use, MAX_TCS_NR);
 	spinlock_t lock;
+	struct rpmh_ctrlr client;
 };
 
-
 int rpmh_rsc_send_data(struct rsc_drv *drv, const struct tcs_request *msg);
 
+void rpmh_tx_done(const struct tcs_request *msg, int r);
+
 #endif /* __RPM_INTERNAL_H__ */

commit 658628e7ef78e875cfe13064387c1a7a287d6338
Author: Lina Iyer <ilina@codeaurora.org>
Date:   Wed Jun 20 18:56:58 2018 +0530

    drivers: qcom: rpmh-rsc: add RPMH controller for QCOM SoCs
    
    Add controller driver for QCOM SoCs that have hardware based shared
    resource management. The hardware IP known as RSC (Resource State
    Coordinator) houses multiple Direct Resource Voter (DRV) for different
    execution levels. A DRV is a unique voter on the state of a shared
    resource. A Trigger Control Set (TCS) is a bunch of slots that can house
    multiple resource state requests, that when triggered will issue those
    requests through an internal bus to the Resource Power Manager Hardened
    (RPMH) blocks. These hardware blocks are capable of adjusting clocks,
    voltages, etc. The resource state request from a DRV are aggregated
    along with state requests from other processors in the SoC and the
    aggregate value is applied on the resource.
    
    Some important aspects of the RPMH communication -
    - Requests are <addr, value> with some header information
    - Multiple requests (upto 16) may be sent through a TCS, at a time
    - Requests in a TCS are sent in sequence
    - Requests may be fire-n-forget or completion (response expected)
    - Multiple TCS from the same DRV may be triggered simultaneously
    - Cannot send a request if another request for the same addr is in
      progress from the same DRV
    - When all the requests from a TCS are complete, an IRQ is raised
    - The IRQ handler needs to clear the TCS before it is available for
      reuse
    - TCS configuration is specific to a DRV
    - Platform drivers may use DRV from different RSCs to make requests
    
    Resource state requests made when CPUs are active are called 'active'
    state requests. Requests made when all the CPUs are powered down (idle
    state) are called 'sleep' state requests. They are matched by a
    corresponding 'wake' state requests which puts the resources back in to
    previously requested active state before resuming any CPU. TCSes are
    dedicated for each type of requests. Active mode TCSes (AMC) are used to
    send requests immediately to the resource, while control TCS are used to
    provide specific information to the controller. Sleep and Wake TCS send
    sleep and wake requests, after and before the system halt respectively.
    
    Signed-off-by: Lina Iyer <ilina@codeaurora.org>
    Signed-off-by: Raju P.L.S.S.S.N <rplsssn@codeaurora.org>
    Signed-off-by: Andy Gross <andy.gross@linaro.org>

diff --git a/drivers/soc/qcom/rpmh-internal.h b/drivers/soc/qcom/rpmh-internal.h
new file mode 100644
index 000000000000..cc29176f1303
--- /dev/null
+++ b/drivers/soc/qcom/rpmh-internal.h
@@ -0,0 +1,69 @@
+/* SPDX-License-Identifier: GPL-2.0 */
+/*
+ * Copyright (c) 2016-2018, The Linux Foundation. All rights reserved.
+ */
+
+
+#ifndef __RPM_INTERNAL_H__
+#define __RPM_INTERNAL_H__
+
+#include <linux/bitmap.h>
+#include <soc/qcom/tcs.h>
+
+#define TCS_TYPE_NR			4
+#define MAX_CMDS_PER_TCS		16
+#define MAX_TCS_PER_TYPE		3
+#define MAX_TCS_NR			(MAX_TCS_PER_TYPE * TCS_TYPE_NR)
+
+struct rsc_drv;
+
+/**
+ * struct tcs_group: group of Trigger Command Sets (TCS) to send state requests
+ * to the controller
+ *
+ * @drv:       the controller
+ * @type:      type of the TCS in this group - active, sleep, wake
+ * @mask:      mask of the TCSes relative to all the TCSes in the RSC
+ * @offset:    start of the TCS group relative to the TCSes in the RSC
+ * @num_tcs:   number of TCSes in this type
+ * @ncpt:      number of commands in each TCS
+ * @lock:      lock for synchronizing this TCS writes
+ * @req:       requests that are sent from the TCS
+ */
+struct tcs_group {
+	struct rsc_drv *drv;
+	int type;
+	u32 mask;
+	u32 offset;
+	int num_tcs;
+	int ncpt;
+	spinlock_t lock;
+	const struct tcs_request *req[MAX_TCS_PER_TYPE];
+};
+
+/**
+ * struct rsc_drv: the Direct Resource Voter (DRV) of the
+ * Resource State Coordinator controller (RSC)
+ *
+ * @name:       controller identifier
+ * @tcs_base:   start address of the TCS registers in this controller
+ * @id:         instance id in the controller (Direct Resource Voter)
+ * @num_tcs:    number of TCSes in this DRV
+ * @tcs:        TCS groups
+ * @tcs_in_use: s/w state of the TCS
+ * @lock:       synchronize state of the controller
+ */
+struct rsc_drv {
+	const char *name;
+	void __iomem *tcs_base;
+	int id;
+	int num_tcs;
+	struct tcs_group tcs[TCS_TYPE_NR];
+	DECLARE_BITMAP(tcs_in_use, MAX_TCS_NR);
+	spinlock_t lock;
+};
+
+
+int rpmh_rsc_send_data(struct rsc_drv *drv, const struct tcs_request *msg);
+
+#endif /* __RPM_INTERNAL_H__ */
