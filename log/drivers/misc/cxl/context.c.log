commit e10e02464396e1a322338d43c5a931ebda1544ea
Author: Julia Lawall <Julia.Lawall@inria.fr>
Date:   Sun Dec 29 16:42:55 2019 +0100

    misc: cxl: use mmgrab
    
    Mmgrab was introduced in commit f1f1007644ff ("mm: add new mmgrab()
    helper") and most of the kernel was updated to use it. Update a
    remaining file.
    
    The semantic patch that makes this change is as follows:
    (http://coccinelle.lip6.fr/)
    
    <smpl>
    @@ expression e; @@
    - atomic_inc(&e->mm_count);
    + mmgrab(e);
    </smpl>
    
    Signed-off-by: Julia Lawall <Julia.Lawall@inria.fr>
    Acked-by: Andrew Donnellan <ajd@linux.ibm.com>
    Link: https://lore.kernel.org/r/1577634178-22530-2-git-send-email-Julia.Lawall@inria.fr
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/drivers/misc/cxl/context.c b/drivers/misc/cxl/context.c
index aed9c445d1e2..fb2eff69e449 100644
--- a/drivers/misc/cxl/context.c
+++ b/drivers/misc/cxl/context.c
@@ -352,7 +352,7 @@ void cxl_context_free(struct cxl_context *ctx)
 void cxl_context_mm_count_get(struct cxl_context *ctx)
 {
 	if (ctx->mm)
-		atomic_inc(&ctx->mm->mm_count);
+		mmgrab(ctx->mm);
 }
 
 void cxl_context_mm_count_put(struct cxl_context *ctx)

commit 2874c5fd284268364ece81a7bd936f3c8168e567
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon May 27 08:55:01 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 152
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license as published by
      the free software foundation either version 2 of the license or at
      your option any later version
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-or-later
    
    has been chosen to replace the boilerplate/reference in 3029 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190527070032.746973796@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/drivers/misc/cxl/context.c b/drivers/misc/cxl/context.c
index 5fe529b43ebe..aed9c445d1e2 100644
--- a/drivers/misc/cxl/context.c
+++ b/drivers/misc/cxl/context.c
@@ -1,10 +1,6 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
 /*
  * Copyright 2014 IBM Corp.
- *
- * This program is free software; you can redistribute it and/or
- * modify it under the terms of the GNU General Public License
- * as published by the Free Software Foundation; either version
- * 2 of the License, or (at your option) any later version.
  */
 
 #include <linux/module.h>

commit f3988ca4c74e136e49487b51231d324d0c923495
Author: Frederic Barrat <fbarrat@linux.ibm.com>
Date:   Thu Jun 28 12:05:09 2018 +0200

    cxl: Remove abandonned capi support for the Mellanox CX4, final cleanup
    
    Remove a few XSL/CX4 oddities which are no longer needed. A simple
    revert of the initial commits was not possible (or not worth it) due
    to the history of the code.
    
    Signed-off-by: Frederic Barrat <fbarrat@linux.ibm.com>
    Acked-by: Andrew Donnellan <andrew.donnellan@au1.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/drivers/misc/cxl/context.c b/drivers/misc/cxl/context.c
index 0355d42d367f..5fe529b43ebe 100644
--- a/drivers/misc/cxl/context.c
+++ b/drivers/misc/cxl/context.c
@@ -95,7 +95,7 @@ int cxl_context_init(struct cxl_context *ctx, struct cxl_afu *afu, bool master)
 	 */
 	mutex_lock(&afu->contexts_lock);
 	idr_preload(GFP_KERNEL);
-	i = idr_alloc(&ctx->afu->contexts_idr, ctx, ctx->afu->adapter->min_pe,
+	i = idr_alloc(&ctx->afu->contexts_idr, ctx, 0,
 		      ctx->afu->num_procs, GFP_NOWAIT);
 	idr_preload_end();
 	mutex_unlock(&afu->contexts_lock);

commit 17d29039388807305ab02a4d6eae7cbe09f81f90
Author: Alastair D'Silva <alastair@d-silva.org>
Date:   Thu Jun 28 12:05:02 2018 +0200

    Revert "cxl: Add preliminary workaround for CX4 interrupt limitation"
    
    Remove abandonned capi support for the Mellanox CX4.
    
    This reverts commit cbce0917e2e47d4bf5aa3b5fd6b1247f33e1a126.
    
    Signed-off-by: Alastair D'Silva <alastair@d-silva.org>
    Acked-by: Andrew Donnellan <andrew.donnellan@au1.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/drivers/misc/cxl/context.c b/drivers/misc/cxl/context.c
index c6ec872800a2..0355d42d367f 100644
--- a/drivers/misc/cxl/context.c
+++ b/drivers/misc/cxl/context.c
@@ -74,7 +74,6 @@ int cxl_context_init(struct cxl_context *ctx, struct cxl_afu *afu, bool master)
 	ctx->pending_afu_err = false;
 
 	INIT_LIST_HEAD(&ctx->irq_names);
-	INIT_LIST_HEAD(&ctx->extra_irq_contexts);
 
 	/*
 	 * When we have to destroy all contexts in cxl_context_detach_all() we

commit a5a0b45eda75c6f12cf871ac6c365007a1f330ff
Author: Souptick Joarder <jrdr.linux@gmail.com>
Date:   Tue Apr 17 20:23:54 2018 +0530

    misc: cxl: Change return type to vm_fault_t
    
    Use new return type vm_fault_t for fault handler. For
    now, this is just documenting that the function returns
    a VM_FAULT value rather than an errno. Once all instances
    are converted, vm_fault_t will become a distinct type.
    
    Reference id -> 1c8f422059ae ("mm: change return type to
    vm_fault_t")
    
    previously cxl_mmap_fault returns VM_FAULT_NOPAGE as
    default value irrespective of vm_insert_pfn() return
    value. This bug is fixed with new vmf_insert_pfn()
    which will return VM_FAULT_ type based on err.
    
    Signed-off-by: Souptick Joarder <jrdr.linux@gmail.com>
    Acked-by: Andrew Donnellan <andrew.donnellan@au1.ibm.com>
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/drivers/misc/cxl/context.c b/drivers/misc/cxl/context.c
index 7ff315ad3692..c6ec872800a2 100644
--- a/drivers/misc/cxl/context.c
+++ b/drivers/misc/cxl/context.c
@@ -128,11 +128,12 @@ void cxl_context_set_mapping(struct cxl_context *ctx,
 	mutex_unlock(&ctx->mapping_lock);
 }
 
-static int cxl_mmap_fault(struct vm_fault *vmf)
+static vm_fault_t cxl_mmap_fault(struct vm_fault *vmf)
 {
 	struct vm_area_struct *vma = vmf->vma;
 	struct cxl_context *ctx = vma->vm_file->private_data;
 	u64 area, offset;
+	vm_fault_t ret;
 
 	offset = vmf->pgoff << PAGE_SHIFT;
 
@@ -169,11 +170,11 @@ static int cxl_mmap_fault(struct vm_fault *vmf)
 		return VM_FAULT_SIGBUS;
 	}
 
-	vm_insert_pfn(vma, vmf->address, (area + offset) >> PAGE_SHIFT);
+	ret = vmf_insert_pfn(vma, vmf->address, (area + offset) >> PAGE_SHIFT);
 
 	mutex_unlock(&ctx->status_mutex);
 
-	return VM_FAULT_NOPAGE;
+	return ret;
 }
 
 static const struct vm_operations_struct cxl_mmap_vmops = {

commit b1db551324f72fa14ad82ca31237a7ed418104df
Author: Christophe Lombard <clombard@linux.vnet.ibm.com>
Date:   Thu Jan 11 09:55:25 2018 +0100

    cxl: Add support for ASB_Notify on POWER9
    
    The POWER9 core supports a new feature: ASB_Notify which requires the
    support of the Special Purpose Register: TIDR.
    
    The ASB_Notify command, generated by the AFU, will attempt to
    wake-up the host thread identified by the particular LPID:PID:TID.
    
    This patch assign a unique TIDR (thread id) for the current thread which
    will be used in the process element entry.
    
    Signed-off-by: Christophe Lombard <clombard@linux.vnet.ibm.com>
    Reviewed-by: Philippe Bergheaud <felix@linux.vnet.ibm.com>
    Acked-by: Frederic Barrat <fbarrat@linux.vnet.ibm.com>
    Reviewed-by: Vaibhav Jain <vaibhav@linux.vnet.ibm.com>
    Acked-by: Andrew Donnellan <andrew.donnellan@au1.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/drivers/misc/cxl/context.c b/drivers/misc/cxl/context.c
index 12a41b2753f0..7ff315ad3692 100644
--- a/drivers/misc/cxl/context.c
+++ b/drivers/misc/cxl/context.c
@@ -45,6 +45,8 @@ int cxl_context_init(struct cxl_context *ctx, struct cxl_afu *afu, bool master)
 	ctx->pid = NULL; /* Set in start work ioctl */
 	mutex_init(&ctx->mapping_lock);
 	ctx->mapping = NULL;
+	ctx->tidr = 0;
+	ctx->assign_tidr = false;
 
 	if (cxl_is_power8()) {
 		spin_lock_init(&ctx->sste_lock);

commit 03b8abedf4f4965e7e9e0d4f92877c42c07ce19f
Author: Frederic Barrat <fbarrat@linux.vnet.ibm.com>
Date:   Sun Sep 3 20:15:13 2017 +0200

    cxl: Enable global TLBIs for cxl contexts
    
    The PSL and nMMU need to see all TLB invalidations for the memory
    contexts used on the adapter. For the hash memory model, it is done by
    making all TLBIs global as soon as the cxl driver is in use. For
    radix, we need something similar, but we can refine and only convert
    to global the invalidations for contexts actually used by the device.
    
    The new mm_context_add_copro() API increments the 'active_cpus' count
    for the contexts attached to the cxl adapter. As soon as there's more
    than 1 active cpu, the TLBIs for the context become global. Active cpu
    count must be decremented when detaching to restore locality if
    possible and to avoid overflowing the counter.
    
    The hash memory model support is somewhat limited, as we can't
    decrement the active cpus count when mm_context_remove_copro() is
    called, because we can't flush the TLB for a mm on hash. So TLBIs
    remain global on hash.
    
    Signed-off-by: Frederic Barrat <fbarrat@linux.vnet.ibm.com>
    Fixes: f24be42aab37 ("cxl: Add psl9 specific code")
    Tested-by: Alistair Popple <alistair@popple.id.au>
    [mpe: Fold in updated comment on the barrier from Fred]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/drivers/misc/cxl/context.c b/drivers/misc/cxl/context.c
index 8c32040b9c09..12a41b2753f0 100644
--- a/drivers/misc/cxl/context.c
+++ b/drivers/misc/cxl/context.c
@@ -18,6 +18,7 @@
 #include <linux/slab.h>
 #include <linux/idr.h>
 #include <linux/sched/mm.h>
+#include <linux/mmu_context.h>
 #include <asm/cputable.h>
 #include <asm/current.h>
 #include <asm/copro.h>
@@ -267,6 +268,8 @@ int __detach_context(struct cxl_context *ctx)
 
 	/* Decrease the mm count on the context */
 	cxl_context_mm_count_put(ctx);
+	if (ctx->mm)
+		mm_context_remove_copro(ctx->mm);
 	ctx->mm = NULL;
 
 	return 0;

commit 797625deaedd9a0621376817db2813244b3246e3
Author: Christophe Lombard <clombard@linux.vnet.ibm.com>
Date:   Tue Jun 13 17:41:05 2017 +0200

    cxl: Fixes for Coherent Accelerator Interface Architecture 2.0
    
    A previous set of patches "cxl: Add support for Coherent Accelerator
    Interface Architecture 2.0" has introduced a new support for the CAPI
    cards. These patches have been tested on Simulation environment and
    quite a bit of them have been tested on real hardware.
    
    This patch brings new fixes after a series of tests carried out on new
    equipment:
      - Add POWER9 definition.
      - Re-enable any masked interrupts when the AFU is not activated
        after resetting the AFU.
      - Remove the api cxl_is_psl8/9 which is no longer useful.
      - Do not dump CAPI1 registers.
      - Rewrite cxl_is_page_fault() function.
      - Do not register slb callack on P9.
    
    Fixes: f24be42aab37 ("cxl: Add psl9 specific code")
    Signed-off-by: Christophe Lombard <clombard@linux.vnet.ibm.com>
    Acked-by: Frederic Barrat <fbarrat@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/drivers/misc/cxl/context.c b/drivers/misc/cxl/context.c
index 4472ce11f98d..8c32040b9c09 100644
--- a/drivers/misc/cxl/context.c
+++ b/drivers/misc/cxl/context.c
@@ -45,7 +45,7 @@ int cxl_context_init(struct cxl_context *ctx, struct cxl_afu *afu, bool master)
 	mutex_init(&ctx->mapping_lock);
 	ctx->mapping = NULL;
 
-	if (cxl_is_psl8(afu)) {
+	if (cxl_is_power8()) {
 		spin_lock_init(&ctx->sste_lock);
 
 		/*
@@ -189,7 +189,7 @@ int cxl_context_iomap(struct cxl_context *ctx, struct vm_area_struct *vma)
 		if (start + len > ctx->afu->adapter->ps_size)
 			return -EINVAL;
 
-		if (cxl_is_psl9(ctx->afu)) {
+		if (cxl_is_power9()) {
 			/*
 			 * Make sure there is a valid problem state
 			 * area space for this AFU.
@@ -324,7 +324,7 @@ static void reclaim_ctx(struct rcu_head *rcu)
 {
 	struct cxl_context *ctx = container_of(rcu, struct cxl_context, rcu);
 
-	if (cxl_is_psl8(ctx->afu))
+	if (cxl_is_power8())
 		free_page((u64)ctx->sstp);
 	if (ctx->ff_page)
 		__free_page(ctx->ff_page);

commit f24be42aab37c6d07c05126673138e06223a6399
Author: Christophe Lombard <clombard@linux.vnet.ibm.com>
Date:   Wed Apr 12 16:34:07 2017 +0200

    cxl: Add psl9 specific code
    
    The new Coherent Accelerator Interface Architecture, level 2, for the
    IBM POWER9 brings new content and features:
    - POWER9 Service Layer
    - Registers
    - Radix mode
    - Process element entry
    - Dedicated-Shared Process Programming Model
    - Translation Fault Handling
    - CAPP
    - Memory Context ID
        If a valid mm_struct is found the memory context id is used for each
        transaction associated with the process handle. The PSL uses the
        context ID to find the corresponding process element.
    
    Signed-off-by: Christophe Lombard <clombard@linux.vnet.ibm.com>
    Acked-by: Frederic Barrat <fbarrat@linux.vnet.ibm.com>
    [mpe: Fixup comment formatting, unsplit long strings]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/drivers/misc/cxl/context.c b/drivers/misc/cxl/context.c
index ac2531e4ad32..4472ce11f98d 100644
--- a/drivers/misc/cxl/context.c
+++ b/drivers/misc/cxl/context.c
@@ -188,13 +188,26 @@ int cxl_context_iomap(struct cxl_context *ctx, struct vm_area_struct *vma)
 	if (ctx->afu->current_mode == CXL_MODE_DEDICATED) {
 		if (start + len > ctx->afu->adapter->ps_size)
 			return -EINVAL;
+
+		if (cxl_is_psl9(ctx->afu)) {
+			/*
+			 * Make sure there is a valid problem state
+			 * area space for this AFU.
+			 */
+			if (ctx->master && !ctx->afu->psa) {
+				pr_devel("AFU doesn't support mmio space\n");
+				return -EINVAL;
+			}
+
+			/* Can't mmap until the AFU is enabled */
+			if (!ctx->afu->enabled)
+				return -EBUSY;
+		}
 	} else {
 		if (start + len > ctx->psn_size)
 			return -EINVAL;
-	}
 
-	if (ctx->afu->current_mode != CXL_MODE_DEDICATED) {
-		/* make sure there is a valid per process space for this AFU */
+		/* Make sure there is a valid per process space for this AFU */
 		if ((ctx->master && !ctx->afu->psa) || (!ctx->afu->pp_psa)) {
 			pr_devel("AFU doesn't support mmio space\n");
 			return -EINVAL;

commit abd1d99bb3da42d6c7341c14986f5b8f4dcc6bd5
Author: Christophe Lombard <clombard@linux.vnet.ibm.com>
Date:   Fri Apr 7 16:11:58 2017 +0200

    cxl: Isolate few psl8 specific calls
    
    Point out the specific Coherent Accelerator Interface Architecture,
    level 1, registers.
    Code and functions specific to PSL8 (CAIA1) must be framed.
    
    Signed-off-by: Christophe Lombard <clombard@linux.vnet.ibm.com>
    Acked-by: Frederic Barrat <fbarrat@linux.vnet.ibm.com>
    [mpe: Don't split long strings, it makes them hard to grep for]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/drivers/misc/cxl/context.c b/drivers/misc/cxl/context.c
index 2e935eadee80..ac2531e4ad32 100644
--- a/drivers/misc/cxl/context.c
+++ b/drivers/misc/cxl/context.c
@@ -39,23 +39,26 @@ int cxl_context_init(struct cxl_context *ctx, struct cxl_afu *afu, bool master)
 {
 	int i;
 
-	spin_lock_init(&ctx->sste_lock);
 	ctx->afu = afu;
 	ctx->master = master;
 	ctx->pid = NULL; /* Set in start work ioctl */
 	mutex_init(&ctx->mapping_lock);
 	ctx->mapping = NULL;
 
-	/*
-	 * Allocate the segment table before we put it in the IDR so that we
-	 * can always access it when dereferenced from IDR. For the same
-	 * reason, the segment table is only destroyed after the context is
-	 * removed from the IDR.  Access to this in the IOCTL is protected by
-	 * Linux filesytem symantics (can't IOCTL until open is complete).
-	 */
-	i = cxl_alloc_sst(ctx);
-	if (i)
-		return i;
+	if (cxl_is_psl8(afu)) {
+		spin_lock_init(&ctx->sste_lock);
+
+		/*
+		 * Allocate the segment table before we put it in the IDR so that we
+		 * can always access it when dereferenced from IDR. For the same
+		 * reason, the segment table is only destroyed after the context is
+		 * removed from the IDR.  Access to this in the IOCTL is protected by
+		 * Linux filesytem symantics (can't IOCTL until open is complete).
+		 */
+		i = cxl_alloc_sst(ctx);
+		if (i)
+			return i;
+	}
 
 	INIT_WORK(&ctx->fault_work, cxl_handle_fault);
 
@@ -308,7 +311,8 @@ static void reclaim_ctx(struct rcu_head *rcu)
 {
 	struct cxl_context *ctx = container_of(rcu, struct cxl_context, rcu);
 
-	free_page((u64)ctx->sstp);
+	if (cxl_is_psl8(ctx->afu))
+		free_page((u64)ctx->sstp);
 	if (ctx->ff_page)
 		__free_page(ctx->ff_page);
 	ctx->sstp = NULL;

commit 6dd2d23403396d8e6d153a6c9db56e1a1012bad8
Author: Christophe Lombard <clombard@linux.vnet.ibm.com>
Date:   Fri Apr 7 16:11:55 2017 +0200

    cxl: Keep track of mm struct associated with a context
    
    The mm_struct corresponding to the current task is acquired each time
    an interrupt is raised. So to simplify the code, we only get the
    mm_struct when attaching an AFU context to the process.
    The mm_count reference is increased to ensure that the mm_struct can't
    be freed. The mm_struct will be released when the context is detached.
    A reference on mm_users is not kept to avoid a circular dependency if
    the process mmaps its cxl mmio and forget to unmap before exiting.
    The field glpid (pid of the group leader associated with the pid), of
    the structure cxl_context, is removed because it's no longer useful.
    
    Signed-off-by: Christophe Lombard <clombard@linux.vnet.ibm.com>
    Reviewed-by: Andrew Donnellan <andrew.donnellan@au1.ibm.com>
    Acked-by: Frederic Barrat <fbarrat@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/drivers/misc/cxl/context.c b/drivers/misc/cxl/context.c
index 062bf6ca2625..2e935eadee80 100644
--- a/drivers/misc/cxl/context.c
+++ b/drivers/misc/cxl/context.c
@@ -17,6 +17,7 @@
 #include <linux/debugfs.h>
 #include <linux/slab.h>
 #include <linux/idr.h>
+#include <linux/sched/mm.h>
 #include <asm/cputable.h>
 #include <asm/current.h>
 #include <asm/copro.h>
@@ -41,7 +42,7 @@ int cxl_context_init(struct cxl_context *ctx, struct cxl_afu *afu, bool master)
 	spin_lock_init(&ctx->sste_lock);
 	ctx->afu = afu;
 	ctx->master = master;
-	ctx->pid = ctx->glpid = NULL; /* Set in start work ioctl */
+	ctx->pid = NULL; /* Set in start work ioctl */
 	mutex_init(&ctx->mapping_lock);
 	ctx->mapping = NULL;
 
@@ -242,12 +243,16 @@ int __detach_context(struct cxl_context *ctx)
 
 	/* release the reference to the group leader and mm handling pid */
 	put_pid(ctx->pid);
-	put_pid(ctx->glpid);
 
 	cxl_ctx_put();
 
 	/* Decrease the attached context count on the adapter */
 	cxl_adapter_context_put(ctx->afu->adapter);
+
+	/* Decrease the mm count on the context */
+	cxl_context_mm_count_put(ctx);
+	ctx->mm = NULL;
+
 	return 0;
 }
 
@@ -325,3 +330,15 @@ void cxl_context_free(struct cxl_context *ctx)
 	mutex_unlock(&ctx->afu->contexts_lock);
 	call_rcu(&ctx->rcu, reclaim_ctx);
 }
+
+void cxl_context_mm_count_get(struct cxl_context *ctx)
+{
+	if (ctx->mm)
+		atomic_inc(&ctx->mm->mm_count);
+}
+
+void cxl_context_mm_count_put(struct cxl_context *ctx)
+{
+	if (ctx->mm)
+		mmdrop(ctx->mm);
+}

commit 11bac80004499ea59f361ef2a5516c84b6eab675
Author: Dave Jiang <dave.jiang@intel.com>
Date:   Fri Feb 24 14:56:41 2017 -0800

    mm, fs: reduce fault, page_mkwrite, and pfn_mkwrite to take only vmf
    
    ->fault(), ->page_mkwrite(), and ->pfn_mkwrite() calls do not need to
    take a vma and vmf parameter when the vma already resides in vmf.
    
    Remove the vma parameter to simplify things.
    
    [arnd@arndb.de: fix ARM build]
      Link: http://lkml.kernel.org/r/20170125223558.1451224-1-arnd@arndb.de
    Link: http://lkml.kernel.org/r/148521301778.19116.10840599906674778980.stgit@djiang5-desk3.ch.intel.com
    Signed-off-by: Dave Jiang <dave.jiang@intel.com>
    Signed-off-by: Arnd Bergmann <arnd@arndb.de>
    Reviewed-by: Ross Zwisler <ross.zwisler@linux.intel.com>
    Cc: Theodore Ts'o <tytso@mit.edu>
    Cc: Darrick J. Wong <darrick.wong@oracle.com>
    Cc: Matthew Wilcox <mawilcox@microsoft.com>
    Cc: Dave Hansen <dave.hansen@intel.com>
    Cc: Christoph Hellwig <hch@lst.de>
    Cc: Jan Kara <jack@suse.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/drivers/misc/cxl/context.c b/drivers/misc/cxl/context.c
index 3907387b6d15..062bf6ca2625 100644
--- a/drivers/misc/cxl/context.c
+++ b/drivers/misc/cxl/context.c
@@ -121,8 +121,9 @@ void cxl_context_set_mapping(struct cxl_context *ctx,
 	mutex_unlock(&ctx->mapping_lock);
 }
 
-static int cxl_mmap_fault(struct vm_area_struct *vma, struct vm_fault *vmf)
+static int cxl_mmap_fault(struct vm_fault *vmf)
 {
+	struct vm_area_struct *vma = vmf->vma;
 	struct cxl_context *ctx = vma->vm_file->private_data;
 	u64 area, offset;
 

commit de399813b521ea7e38bbfb5e5b620b5e202e5783
Merge: 57ca04ab4401 c6f6634721c8
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri Dec 16 09:26:42 2016 -0800

    Merge tag 'powerpc-4.10-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc updates from Michael Ellerman:
     "Highlights include:
    
       - Support for the kexec_file_load() syscall, which is a prereq for
         secure and trusted boot.
    
       - Prevent kernel execution of userspace on P9 Radix (similar to
         SMEP/PXN).
    
       - Sort the exception tables at build time, to save time at boot, and
         store them as relative offsets to save space in the kernel image &
         memory.
    
       - Allow building the kernel with thin archives, which should allow us
         to build an allyesconfig once some other fixes land.
    
       - Build fixes to allow us to correctly rebuild when changing the
         kernel endian from big to little or vice versa.
    
       - Plumbing so that we can avoid doing a full mm TLB flush on P9
         Radix.
    
       - Initial stack protector support (-fstack-protector).
    
       - Support for dumping the radix (aka. Linux) and hash page tables via
         debugfs.
    
       - Fix an oops in cxl coredump generation when cxl_get_fd() is used.
    
       - Freescale updates from Scott: "Highlights include 8xx hugepage
         support, qbman fixes/cleanup, device tree updates, and some misc
         cleanup."
    
       - Many and varied fixes and minor enhancements as always.
    
      Thanks to:
        Alexey Kardashevskiy, Andrew Donnellan, Aneesh Kumar K.V, Anshuman
        Khandual, Anton Blanchard, Balbir Singh, Bartlomiej Zolnierkiewicz,
        Christophe Jaillet, Christophe Leroy, Denis Kirjanov, Elimar
        Riesebieter, Frederic Barrat, Gautham R. Shenoy, Geliang Tang, Geoff
        Levand, Jack Miller, Johan Hovold, Lars-Peter Clausen, Libin,
        Madhavan Srinivasan, Michael Neuling, Nathan Fontenot, Naveen N.
        Rao, Nicholas Piggin, Pan Xinhui, Peter Senna Tschudin, Rashmica
        Gupta, Rui Teng, Russell Currey, Scott Wood, Simon Guo, Suraj
        Jitindar Singh, Thiago Jung Bauermann, Tobias Klauser, Vaibhav Jain"
    
    [ And thanks to Michael, who took time off from a new baby to get this
      pull request done.   - Linus ]
    
    * tag 'powerpc-4.10-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (174 commits)
      powerpc/fsl/dts: add FMan node for t1042d4rdb
      powerpc/fsl/dts: add sg_2500_aqr105_phy4 alias on t1024rdb
      powerpc/fsl/dts: add QMan and BMan nodes on t1024
      powerpc/fsl/dts: add QMan and BMan nodes on t1023
      soc/fsl/qman: test: use DEFINE_SPINLOCK()
      powerpc/fsl-lbc: use DEFINE_SPINLOCK()
      powerpc/8xx: Implement support of hugepages
      powerpc: get hugetlbpage handling more generic
      powerpc: port 64 bits pgtable_cache to 32 bits
      powerpc/boot: Request no dynamic linker for boot wrapper
      soc/fsl/bman: Use resource_size instead of computation
      soc/fsl/qe: use builtin_platform_driver
      powerpc/fsl_pmc: use builtin_platform_driver
      powerpc/83xx/suspend: use builtin_platform_driver
      powerpc/ftrace: Fix the comments for ftrace_modify_code
      powerpc/perf: macros for power9 format encoding
      powerpc/perf: power9 raw event format encoding
      powerpc/perf: update attribute_group data structure
      powerpc/perf: factor out the event format field
      powerpc/mm/iommu, vfio/spapr: Put pages on VFIO container shutdown
      ...

commit 1a29d85eb0f19b7d8271923d8917d7b4f5540b3e
Author: Jan Kara <jack@suse.cz>
Date:   Wed Dec 14 15:07:01 2016 -0800

    mm: use vmf->address instead of of vmf->virtual_address
    
    Every single user of vmf->virtual_address typed that entry to unsigned
    long before doing anything with it so the type of virtual_address does
    not really provide us any additional safety.  Just use masked
    vmf->address which already has the appropriate type.
    
    Link: http://lkml.kernel.org/r/1479460644-25076-3-git-send-email-jack@suse.cz
    Signed-off-by: Jan Kara <jack@suse.cz>
    Acked-by: Kirill A. Shutemov <kirill.shutemov@linux.intel.com>
    Cc: Dan Williams <dan.j.williams@intel.com>
    Cc: Ross Zwisler <ross.zwisler@linux.intel.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/drivers/misc/cxl/context.c b/drivers/misc/cxl/context.c
index 5e506c19108a..5d36dcc7f47e 100644
--- a/drivers/misc/cxl/context.c
+++ b/drivers/misc/cxl/context.c
@@ -117,13 +117,12 @@ int cxl_context_init(struct cxl_context *ctx, struct cxl_afu *afu, bool master,
 static int cxl_mmap_fault(struct vm_area_struct *vma, struct vm_fault *vmf)
 {
 	struct cxl_context *ctx = vma->vm_file->private_data;
-	unsigned long address = (unsigned long)vmf->virtual_address;
 	u64 area, offset;
 
 	offset = vmf->pgoff << PAGE_SHIFT;
 
 	pr_devel("%s: pe: %i address: 0x%lx offset: 0x%llx\n",
-			__func__, ctx->pe, address, offset);
+			__func__, ctx->pe, vmf->address, offset);
 
 	if (ctx->afu->current_mode == CXL_MODE_DEDICATED) {
 		area = ctx->afu->psn_phys;
@@ -155,7 +154,7 @@ static int cxl_mmap_fault(struct vm_area_struct *vma, struct vm_fault *vmf)
 		return VM_FAULT_SIGBUS;
 	}
 
-	vm_insert_pfn(vma, address, (area + offset) >> PAGE_SHIFT);
+	vm_insert_pfn(vma, vmf->address, (area + offset) >> PAGE_SHIFT);
 
 	mutex_unlock(&ctx->status_mutex);
 

commit bdecf76e319a29735d828575f4a9269f0e17c547
Author: Frederic Barrat <fbarrat@linux.vnet.ibm.com>
Date:   Fri Nov 18 23:00:31 2016 +1100

    cxl: Fix coredump generation when cxl_get_fd() is used
    
    If a process dumps core while owning a cxl file descriptor obtained
    from an AFU driver (e.g. cxlflash) through the cxl_get_fd() API, the
    following error occurs:
    
      [  868.027591] Unable to handle kernel paging request for data at address ...
      [  868.027778] Faulting instruction address: 0xc00000000035edb0
      cpu 0x8c: Vector: 300 (Data Access) at [c000003c688275e0]
          pc: c00000000035edb0: elf_core_dump+0xd60/0x1300
          lr: c00000000035ed80: elf_core_dump+0xd30/0x1300
          sp: c000003c68827860
         msr: 9000000100009033
         dar: c
      dsisr: 40000000
       current = 0xc000003c68780000
       paca    = 0xc000000001b73200   softe: 0        irq_happened: 0x01
          pid   = 46725, comm = hxesurelock
      enter ? for help
      [c000003c68827a60] c00000000036948c do_coredump+0xcec/0x11e0
      [c000003c68827c20] c0000000000ce9e0 get_signal+0x540/0x7b0
      [c000003c68827d10] c000000000017354 do_signal+0x54/0x2b0
      [c000003c68827e00] c00000000001777c do_notify_resume+0xbc/0xd0
      [c000003c68827e30] c000000000009838 ret_from_except_lite+0x64/0x68
      --- Exception: 300 (Data Access) at 00003fff98ad2918
    
    The root cause is that the address_space structure for the file
    doesn't define a 'host' member.
    
    When cxl allocates a file descriptor, it's using the anonymous inode
    to back the file, but allocates a private address_space for each
    context. The private address_space allows to track memory allocation
    for each context. cxl doesn't define the 'host' member of the address
    space, i.e. the inode. We don't want to define it as the anonymous
    inode, since there's no longer a 1-to-1 relation between address_space
    and inode.
    
    To fix it, instead of using the anonymous inode, we introduce a simple
    pseudo filesystem so that cxl can allocate its own inodes. So we now
    have one inode for each file and address_space. The pseudo filesystem
    is only mounted on the first allocation of a file descriptor by
    cxl_get_fd().
    
    Tested with cxlflash.
    
    Signed-off-by: Frederic Barrat <fbarrat@linux.vnet.ibm.com>
    Reviewed-by: Matthew R. Ochs <mrochs@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/drivers/misc/cxl/context.c b/drivers/misc/cxl/context.c
index 5e506c19108a..ff5e7e8cb1d1 100644
--- a/drivers/misc/cxl/context.c
+++ b/drivers/misc/cxl/context.c
@@ -34,8 +34,7 @@ struct cxl_context *cxl_context_alloc(void)
 /*
  * Initialises a CXL context.
  */
-int cxl_context_init(struct cxl_context *ctx, struct cxl_afu *afu, bool master,
-		     struct address_space *mapping)
+int cxl_context_init(struct cxl_context *ctx, struct cxl_afu *afu, bool master)
 {
 	int i;
 
@@ -44,7 +43,7 @@ int cxl_context_init(struct cxl_context *ctx, struct cxl_afu *afu, bool master,
 	ctx->master = master;
 	ctx->pid = ctx->glpid = NULL; /* Set in start work ioctl */
 	mutex_init(&ctx->mapping_lock);
-	ctx->mapping = mapping;
+	ctx->mapping = NULL;
 
 	/*
 	 * Allocate the segment table before we put it in the IDR so that we
@@ -114,6 +113,14 @@ int cxl_context_init(struct cxl_context *ctx, struct cxl_afu *afu, bool master,
 	return 0;
 }
 
+void cxl_context_set_mapping(struct cxl_context *ctx,
+			struct address_space *mapping)
+{
+	mutex_lock(&ctx->mapping_lock);
+	ctx->mapping = mapping;
+	mutex_unlock(&ctx->mapping_lock);
+}
+
 static int cxl_mmap_fault(struct vm_area_struct *vma, struct vm_fault *vmf)
 {
 	struct cxl_context *ctx = vma->vm_file->private_data;
@@ -300,8 +307,6 @@ static void reclaim_ctx(struct rcu_head *rcu)
 	if (ctx->ff_page)
 		__free_page(ctx->ff_page);
 	ctx->sstp = NULL;
-	if (ctx->kernelapi)
-		kfree(ctx->mapping);
 
 	kfree(ctx->irq_bitmap);
 
@@ -313,6 +318,8 @@ static void reclaim_ctx(struct rcu_head *rcu)
 
 void cxl_context_free(struct cxl_context *ctx)
 {
+	if (ctx->kernelapi && ctx->mapping)
+		cxl_release_mapping(ctx);
 	mutex_lock(&ctx->afu->contexts_lock);
 	idr_remove(&ctx->afu->contexts_idr, ctx->pe);
 	mutex_unlock(&ctx->afu->contexts_lock);

commit 70b565bbdb911023373e035225ab10077e4ab937
Author: Vaibhav Jain <vaibhav@linux.vnet.ibm.com>
Date:   Fri Oct 14 15:08:36 2016 +0530

    cxl: Prevent adapter reset if an active context exists
    
    This patch prevents resetting the cxl adapter via sysfs in presence of
    one or more active cxl_context on it. This protects against an
    unrecoverable error caused by PSL owning a dirty cache line even after
    reset and host tries to touch the same cache line. In case a force reset
    of the card is required irrespective of any active contexts, the int
    value -1 can be stored in the 'reset' sysfs attribute of the card.
    
    The patch introduces a new atomic_t member named contexts_num inside
    struct cxl that holds the number of active context attached to the card
    , which is checked against '0' before proceeding with the reset. To
    prevent against a race condition where a context is activated just after
    reset check is performed, the contexts_num is atomically set to '-1'
    after reset-check to indicate that no more contexts can be activated on
    the card anymore.
    
    Before activating a context we atomically test if contexts_num is
    non-negative and if so, increment its value by one. In case the value of
    contexts_num is negative then it indicates that the card is about to be
    reset and context activation is error-ed out at that point.
    
    Fixes: 62fa19d4b4fd ("cxl: Add ability to reset the card")
    Cc: stable@vger.kernel.org # v4.0+
    Acked-by: Frederic Barrat <fbarrat@linux.vnet.ibm.com>
    Reviewed-by: Andrew Donnellan <andrew.donnellan@au1.ibm.com>
    Signed-off-by: Vaibhav Jain <vaibhav@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/drivers/misc/cxl/context.c b/drivers/misc/cxl/context.c
index c466ee2b0c97..5e506c19108a 100644
--- a/drivers/misc/cxl/context.c
+++ b/drivers/misc/cxl/context.c
@@ -238,6 +238,9 @@ int __detach_context(struct cxl_context *ctx)
 	put_pid(ctx->glpid);
 
 	cxl_ctx_put();
+
+	/* Decrease the attached context count on the adapter */
+	cxl_adapter_context_put(ctx->afu->adapter);
 	return 0;
 }
 

commit 164793379ad3b7ef5fc5a28260c111358892dff3
Author: Andrew Donnellan <andrew.donnellan@au1.ibm.com>
Date:   Thu Jul 28 15:39:41 2016 +1000

    cxl: Fix NULL dereference in cxl_context_init() on PowerVM guests
    
    Commit f67a6722d650 ("cxl: Workaround PE=0 hardware limitation in
    Mellanox CX4") added a "min_pe" field to struct cxl_service_layer_ops,
    to allow us to work around a Mellanox CX-4 hardware limitation.
    
    When allocating the PE number in cxl_context_init(), we read from
    ctx->afu->adapter->native->sl_ops->min_pe to get the minimum PE number.
    Unsurprisingly, in a PowerVM guest ctx->afu->adapter->native is NULL,
    and guests don't have a cxl_service_layer_ops struct anywhere.
    
    Move min_pe from struct cxl_service_layer_ops to struct cxl so it's
    accessible in both native and PowerVM environments. For the Mellanox
    CX-4, set the min_pe value in set_sl_ops().
    
    Fixes: f67a6722d650 ("cxl: Workaround PE=0 hardware limitation in Mellanox CX4")
    Reported-by: Frederic Barrat <fbarrat@linux.vnet.ibm.com>
    Signed-off-by: Andrew Donnellan <andrew.donnellan@au1.ibm.com>
    Acked-by: Ian Munsie <imunsie@au1.ibm.com>
    Reviewed-by: Frederic Barrat <fbarrat@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/drivers/misc/cxl/context.c b/drivers/misc/cxl/context.c
index bdee9a01ef35..c466ee2b0c97 100644
--- a/drivers/misc/cxl/context.c
+++ b/drivers/misc/cxl/context.c
@@ -90,8 +90,7 @@ int cxl_context_init(struct cxl_context *ctx, struct cxl_afu *afu, bool master,
 	 */
 	mutex_lock(&afu->contexts_lock);
 	idr_preload(GFP_KERNEL);
-	i = idr_alloc(&ctx->afu->contexts_idr, ctx,
-		      ctx->afu->adapter->native->sl_ops->min_pe,
+	i = idr_alloc(&ctx->afu->contexts_idr, ctx, ctx->afu->adapter->min_pe,
 		      ctx->afu->num_procs, GFP_NOWAIT);
 	idr_preload_end();
 	mutex_unlock(&afu->contexts_lock);

commit f67a6722d650b864b020b19b3926e7152b55f1ff
Author: Ian Munsie <imunsie@au1.ibm.com>
Date:   Thu Jul 14 07:17:11 2016 +1000

    cxl: Workaround PE=0 hardware limitation in Mellanox CX4
    
    The CX4 card cannot cope with a context with PE=0 due to a hardware
    limitation, resulting in:
    
    [   34.166577] command failed, status limits exceeded(0x8), syndrome 0x5a7939
    [   34.166580] mlx5_core 0000:01:00.1: Failed allocating uar, aborting
    
    Since the kernel API allocates a default context very early during
    device init that will almost certainly get Process Element ID 0 there is
    no easy way for us to extend the API to allow the Mellanox to inform us
    of this limitation ahead of time.
    
    Instead, work around the issue by extending the XSL structure to include
    a minimum PE to allocate. Although the bug is not in the XSL, it is the
    easiest place to work around this limitation given that the CX4 is
    currently the only card that uses an XSL.
    
    Signed-off-by: Ian Munsie <imunsie@au1.ibm.com>
    Reviewed-by: Andrew Donnellan <andrew.donnellan@au1.ibm.com>
    Reviewed-by: Frederic Barrat <fbarrat@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/drivers/misc/cxl/context.c b/drivers/misc/cxl/context.c
index 2616cddbbb33..bdee9a01ef35 100644
--- a/drivers/misc/cxl/context.c
+++ b/drivers/misc/cxl/context.c
@@ -90,7 +90,8 @@ int cxl_context_init(struct cxl_context *ctx, struct cxl_afu *afu, bool master,
 	 */
 	mutex_lock(&afu->contexts_lock);
 	idr_preload(GFP_KERNEL);
-	i = idr_alloc(&ctx->afu->contexts_idr, ctx, 0,
+	i = idr_alloc(&ctx->afu->contexts_idr, ctx,
+		      ctx->afu->adapter->native->sl_ops->min_pe,
 		      ctx->afu->num_procs, GFP_NOWAIT);
 	idr_preload_end();
 	mutex_unlock(&afu->contexts_lock);

commit cbce0917e2e47d4bf5aa3b5fd6b1247f33e1a126
Author: Ian Munsie <imunsie@au1.ibm.com>
Date:   Thu Jul 14 07:17:09 2016 +1000

    cxl: Add preliminary workaround for CX4 interrupt limitation
    
    The Mellanox CX4 has a hardware limitation where only 4 bits of the
    AFU interrupt number can be passed to the XSL when sending an interrupt,
    limiting it to only 15 interrupts per context (AFU interrupt number 0 is
    invalid).
    
    In order to overcome this, we will allocate additional contexts linked
    to the default context as extra address space for the extra interrupts -
    this will be implemented in the next patch.
    
    This patch adds the preliminary support to allow this, by way of adding
    a linked list in the context structure that we use to keep track of the
    contexts dedicated to interrupts, and an API to simultaneously iterate
    over the related context structures, AFU interrupt numbers and hardware
    interrupt numbers. The point of using a single API to iterate these is
    to hide some of the details of the iteration from external code, and to
    reduce the number of APIs that need to be exported via base.c to allow
    built in code to call.
    
    Signed-off-by: Ian Munsie <imunsie@au1.ibm.com>
    Reviewed-by: Frederic Barrat <fbarrat@linux.vnet.ibm.com>
    Reviewed-by: Andrew Donnellan <andrew.donnellan@au1.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/drivers/misc/cxl/context.c b/drivers/misc/cxl/context.c
index edbb99e93114..2616cddbbb33 100644
--- a/drivers/misc/cxl/context.c
+++ b/drivers/misc/cxl/context.c
@@ -68,6 +68,7 @@ int cxl_context_init(struct cxl_context *ctx, struct cxl_afu *afu, bool master,
 	ctx->pending_afu_err = false;
 
 	INIT_LIST_HEAD(&ctx->irq_names);
+	INIT_LIST_HEAD(&ctx->extra_irq_contexts);
 
 	/*
 	 * When we have to destroy all contexts in cxl_context_detach_all() we

commit f5c9df9a442f586b183947627210e167ded81d19
Author: Ian Munsie <imunsie@au1.ibm.com>
Date:   Thu Jun 30 04:55:17 2016 +1000

    cxl: Fix NULL pointer dereference on kernel contexts with no AFU interrupts
    
    If a kernel context is initialised and does not have any AFU interrupts
    allocated it will cause a NULL pointer dereference when the context is
    detached since the irq_names list will not have been initialised.
    
    Move the initialisation of the irq_names list into the cxl_context_init
    routine so that it will be valid for the entire lifetime of the context
    and will not cause a NULL pointer dereference.
    
    Signed-off-by: Ian Munsie <imunsie@au1.ibm.com>
    Reviewed-by: Andrew Donnellan <andrew.donnellan@au1.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/drivers/misc/cxl/context.c b/drivers/misc/cxl/context.c
index 26d206b1d08c..edbb99e93114 100644
--- a/drivers/misc/cxl/context.c
+++ b/drivers/misc/cxl/context.c
@@ -67,6 +67,8 @@ int cxl_context_init(struct cxl_context *ctx, struct cxl_afu *afu, bool master,
 	ctx->pending_fault = false;
 	ctx->pending_afu_err = false;
 
+	INIT_LIST_HEAD(&ctx->irq_names);
+
 	/*
 	 * When we have to destroy all contexts in cxl_context_detach_all() we
 	 * end up with afu_release_irqs() called from inside a

commit c04a5880299eab3da8c10547db96ea9cdffd44a6
Merge: a1c28b75a958 138a076496e6
Author: Linus Torvalds <torvalds@linux-foundation.org>
Date:   Fri May 20 10:12:41 2016 -0700

    Merge tag 'powerpc-4.7-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux
    
    Pull powerpc updates from Michael Ellerman:
     "Highlights:
       - Support for Power ISA 3.0 (Power9) Radix Tree MMU from Aneesh Kumar K.V
       - Live patching support for ppc64le (also merged via livepatching.git)
    
      Various cleanups & minor fixes from:
       - Aaro Koskinen, Alexey Kardashevskiy, Andrew Donnellan, Aneesh Kumar K.V,
         Chris Smart, Daniel Axtens, Frederic Barrat, Gavin Shan, Ian Munsie,
         Lennart Sorensen, Madhavan Srinivasan, Mahesh Salgaonkar, Markus Elfring,
         Michael Ellerman, Oliver O'Halloran, Paul Gortmaker, Paul Mackerras,
         Rashmica Gupta, Russell Currey, Suraj Jitindar Singh, Thiago Jung
         Bauermann, Valentin Rothberg, Vipin K Parashar.
    
      General:
       - Update LMB associativity index during DLPAR add/remove from Nathan
         Fontenot
       - Fix branching to OOL handlers in relocatable kernel from Hari Bathini
       - Add support for userspace Power9 copy/paste from Chris Smart
       - Always use STRICT_MM_TYPECHECKS from Michael Ellerman
       - Add mask of possible MMU features from Michael Ellerman
    
      PCI:
       - Enable pass through of NVLink to guests from Alexey Kardashevskiy
       - Cleanups in preparation for powernv PCI hotplug from Gavin Shan
       - Don't report error in eeh_pe_reset_and_recover() from Gavin Shan
       - Restore initial state in eeh_pe_reset_and_recover() from Gavin Shan
       - Revert "powerpc/eeh: Fix crash in eeh_add_device_early() on Cell"
         from Guilherme G Piccoli
       - Remove the dependency on EEH struct in DDW mechanism from Guilherme
         G Piccoli
    
      selftests:
       - Test cp_abort during context switch from Chris Smart
       - Add several tests for transactional memory support from Rashmica
         Gupta
    
      perf:
       - Add support for sampling interrupt register state from Anju T
       - Add support for unwinding perf-stackdump from Chandan Kumar
    
      cxl:
       - Configure the PSL for two CAPI ports on POWER8NVL from Philippe
         Bergheaud
       - Allow initialization on timebase sync failures from Frederic Barrat
       - Increase timeout for detection of AFU mmio hang from Frederic
         Barrat
       - Handle num_of_processes larger than can fit in the SPA from Ian
         Munsie
       - Ensure PSL interrupt is configured for contexts with no AFU IRQs
         from Ian Munsie
       - Add kernel API to allow a context to operate with relocate disabled
         from Ian Munsie
       - Check periodically the coherent platform function's state from
         Christophe Lombard
    
      Freescale:
       - Updates from Scott: "Contains 86xx fixes, minor device tree fixes,
         an erratum workaround, and a kconfig dependency fix."
    
    * tag 'powerpc-4.7-1' of git://git.kernel.org/pub/scm/linux/kernel/git/powerpc/linux: (192 commits)
      powerpc/86xx: Fix PCI interrupt map definition
      powerpc/86xx: Move pci1 definition to the include file
      powerpc/fsl: Fix build of the dtb embedded kernel images
      powerpc/fsl: Fix rcpm compatible string
      powerpc/fsl: Remove FSL_SOC dependency from FSL_LBC
      powerpc/fsl-pci: Add a workaround for PCI 5 errata
      powerpc/fsl: Fix SPI compatible on t208xrdb and t1040rdb
      powerpc/powernv/npu: Add PE to PHB's list
      powerpc/powernv: Fix insufficient memory allocation
      powerpc/iommu: Remove the dependency on EEH struct in DDW mechanism
      Revert "powerpc/eeh: Fix crash in eeh_add_device_early() on Cell"
      powerpc/eeh: Drop unnecessary label in eeh_pe_change_owner()
      powerpc/eeh: Ignore handlers in eeh_pe_reset_and_recover()
      powerpc/eeh: Restore initial state in eeh_pe_reset_and_recover()
      powerpc/eeh: Don't report error in eeh_pe_reset_and_recover()
      Revert "powerpc/powernv: Exclude root bus in pnv_pci_reset_secondary_bus()"
      powerpc/powernv/npu: Enable NVLink pass through
      powerpc/powernv/npu: Rework TCE Kill handling
      powerpc/powernv/npu: Add set/unset window helpers
      powerpc/powernv/ioda2: Export debug helper pe_level_printk()
      ...

commit 2bc79ffcbb817873cc43d63118008ab75181b73d
Author: Michael Neuling <mikey@neuling.org>
Date:   Fri Apr 22 14:57:49 2016 +1000

    cxl: Poll for outstanding IRQs when detaching a context
    
    When detaching contexts, we may still have interrupts in the system
    which are yet to be delivered to any CPU and be acked in the PSL.
    This can result in a subsequent unrelated process getting an spurious
    IRQ or an interrupt for a non-existent context.
    
    This polls the PSL to ensure that the PSL is clear of IRQs for the
    detached context, before removing the context from the idr.
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Tested-by: Andrew Donnellan <andrew.donnellan@au1.ibm.com>
    Acked-by: Ian Munsie <imunsie@au1.ibm.com>
    Tested-by: Vaibhav Jain <vaibhav@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/drivers/misc/cxl/context.c b/drivers/misc/cxl/context.c
index 10370f280500..7edea9c19199 100644
--- a/drivers/misc/cxl/context.c
+++ b/drivers/misc/cxl/context.c
@@ -223,6 +223,13 @@ int __detach_context(struct cxl_context *ctx)
 		cxl_ops->link_ok(ctx->afu->adapter, ctx->afu));
 	flush_work(&ctx->fault_work); /* Only needed for dedicated process */
 
+	/*
+	 * Wait until no further interrupts are presented by the PSL
+	 * for this context.
+	 */
+	if (cxl_ops->irq_wait)
+		cxl_ops->irq_wait(ctx);
+
 	/* release the reference to the group leader and mm handling pid */
 	put_pid(ctx->pid);
 	put_pid(ctx->glpid);

commit 1050e689a63baffdadcd33498c15d859922504c0
Author: Markus Elfring <elfring@users.sourceforge.net>
Date:   Fri Nov 6 11:00:23 2015 +0100

    cxl: Delete an unnecessary check before the function call "kfree"
    
    The kfree() function tests whether its argument is NULL and then
    returns immediately. Thus the test around the call is not needed.
    
    This issue was detected by using the Coccinelle software.
    
    Signed-off-by: Markus Elfring <elfring@users.sourceforge.net>
    Reviewed-by: Andrew Donnellan <andrew.donnellan@au1.ibm.com>
    Acked-by: Ian Munsie <imunsie@au1.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/drivers/misc/cxl/context.c b/drivers/misc/cxl/context.c
index 10370f280500..80203c993b5e 100644
--- a/drivers/misc/cxl/context.c
+++ b/drivers/misc/cxl/context.c
@@ -290,8 +290,7 @@ static void reclaim_ctx(struct rcu_head *rcu)
 	if (ctx->kernelapi)
 		kfree(ctx->mapping);
 
-	if (ctx->irq_bitmap)
-		kfree(ctx->irq_bitmap);
+	kfree(ctx->irq_bitmap);
 
 	/* Drop ref to the afu device taken during cxl_context_init */
 	cxl_afu_put(ctx->afu);

commit 0d400f77c19e8d2606f8194846bcf18ebdc9df2a
Author: Christophe Lombard <clombard@linux.vnet.ibm.com>
Date:   Fri Mar 4 12:26:41 2016 +0100

    cxl: Adapter failure handling
    
    Check the AFU state whenever an API is called. The hypervisor may
    issue a reset of the adapter when it detects a fault. When it happens,
    it launches an error recovery which will either move the AFU to a
    permanent failure state, or in the disabled state.
    If the AFU is found to be disabled, detach all existing contexts from
    it before issuing a AFU reset to re-enable it.
    
    Before detaching contexts, notify any kernel driver through the EEH
    callbacks of the AFU pci device.
    
    Co-authored-by: Frederic Barrat <fbarrat@linux.vnet.ibm.com>
    Signed-off-by: Frederic Barrat <fbarrat@linux.vnet.ibm.com>
    Signed-off-by: Christophe Lombard <clombard@linux.vnet.ibm.com>
    Reviewed-by: Manoj Kumar <manoj@linux.vnet.ibm.com>
    Acked-by: Ian Munsie <imunsie@au1.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/drivers/misc/cxl/context.c b/drivers/misc/cxl/context.c
index 180c85a32825..10370f280500 100644
--- a/drivers/misc/cxl/context.c
+++ b/drivers/misc/cxl/context.c
@@ -220,7 +220,7 @@ int __detach_context(struct cxl_context *ctx)
 	 * If detach fails when hw is down, we don't care.
 	 */
 	WARN_ON(cxl_ops->detach_process(ctx) &&
-		cxl_ops->link_ok(ctx->afu->adapter));
+		cxl_ops->link_ok(ctx->afu->adapter, ctx->afu));
 	flush_work(&ctx->fault_work); /* Only needed for dedicated process */
 
 	/* release the reference to the group leader and mm handling pid */

commit 14baf4d9c739e6e69150512d2eb23c71fffcc192
Author: Christophe Lombard <clombard@linux.vnet.ibm.com>
Date:   Fri Mar 4 12:26:36 2016 +0100

    cxl: Add guest-specific code
    
    The new of.c file contains code to parse the device tree to find out
    about cxl adapters and AFUs.
    
    guest.c implements the guest-specific callbacks for the backend API.
    
    The process element ID is not known until the context is attached, so
    we have to separate the context ID assigned by the cxl driver from the
    process element ID visible to the user applications. In bare-metal,
    the 2 IDs match.
    
    Co-authored-by: Frederic Barrat <fbarrat@linux.vnet.ibm.com>
    Signed-off-by: Frederic Barrat <fbarrat@linux.vnet.ibm.com>
    Signed-off-by: Christophe Lombard <clombard@linux.vnet.ibm.com>
    Reviewed-by: Manoj Kumar <manoj@linux.vnet.ibm.com>
    Acked-by: Ian Munsie <imunsie@au1.ibm.com>
    [mpe: Fix SMP=n build, fix PSERIES=n build, minor whitespace fixes]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/drivers/misc/cxl/context.c b/drivers/misc/cxl/context.c
index 200837f7612b..180c85a32825 100644
--- a/drivers/misc/cxl/context.c
+++ b/drivers/misc/cxl/context.c
@@ -95,8 +95,12 @@ int cxl_context_init(struct cxl_context *ctx, struct cxl_afu *afu, bool master,
 		return i;
 
 	ctx->pe = i;
-	if (cpu_has_feature(CPU_FTR_HVMODE))
+	if (cpu_has_feature(CPU_FTR_HVMODE)) {
 		ctx->elem = &ctx->afu->native->spa[i];
+		ctx->external_pe = ctx->pe;
+	} else {
+		ctx->external_pe = -1; /* assigned when attaching */
+	}
 	ctx->pe_inserted = false;
 
 	/*

commit cbffa3a5146a90f46806cef3a98b8be5833727e8
Author: Christophe Lombard <clombard@linux.vnet.ibm.com>
Date:   Fri Mar 4 12:26:35 2016 +0100

    cxl: Separate bare-metal fields in adapter and AFU data structures
    
    Introduce sub-structures containing the bare-metal specific fields in
    the structures describing the adapter (struct cxl) and AFU (struct
    cxl_afu).
    Update all their references.
    
    Co-authored-by: Frederic Barrat <fbarrat@linux.vnet.ibm.com>
    Signed-off-by: Frederic Barrat <fbarrat@linux.vnet.ibm.com>
    Signed-off-by: Christophe Lombard <clombard@linux.vnet.ibm.com>
    Reviewed-by: Manoj Kumar <manoj@linux.vnet.ibm.com>
    Acked-by: Ian Munsie <imunsie@au1.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/drivers/misc/cxl/context.c b/drivers/misc/cxl/context.c
index 46f98441d7a1..200837f7612b 100644
--- a/drivers/misc/cxl/context.c
+++ b/drivers/misc/cxl/context.c
@@ -96,7 +96,7 @@ int cxl_context_init(struct cxl_context *ctx, struct cxl_afu *afu, bool master,
 
 	ctx->pe = i;
 	if (cpu_has_feature(CPU_FTR_HVMODE))
-		ctx->elem = &ctx->afu->spa[i];
+		ctx->elem = &ctx->afu->native->spa[i];
 	ctx->pe_inserted = false;
 
 	/*

commit ea2d1f95efd7c100617235918bac370414aec1ad
Author: Frederic Barrat <fbarrat@linux.vnet.ibm.com>
Date:   Fri Mar 4 12:26:30 2016 +0100

    cxl: Isolate a few bare-metal-specific calls
    
    A few functions are mostly common between bare-metal and guest and
    just need minor tuning. To avoid crowding the backend API, introduce a
    few 'if' based on the CPU being in HV mode.
    
    Co-authored-by: Christophe Lombard <clombard@linux.vnet.ibm.com>
    Signed-off-by: Frederic Barrat <fbarrat@linux.vnet.ibm.com>
    Signed-off-by: Christophe Lombard <clombard@linux.vnet.ibm.com>
    Reviewed-by: Manoj Kumar <manoj@linux.vnet.ibm.com>
    Acked-by: Ian Munsie <imunsie@au1.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/drivers/misc/cxl/context.c b/drivers/misc/cxl/context.c
index aa65262d4a32..46f98441d7a1 100644
--- a/drivers/misc/cxl/context.c
+++ b/drivers/misc/cxl/context.c
@@ -95,7 +95,8 @@ int cxl_context_init(struct cxl_context *ctx, struct cxl_afu *afu, bool master,
 		return i;
 
 	ctx->pe = i;
-	ctx->elem = &ctx->afu->spa[i];
+	if (cpu_has_feature(CPU_FTR_HVMODE))
+		ctx->elem = &ctx->afu->spa[i];
 	ctx->pe_inserted = false;
 
 	/*

commit 5be587b1110132b4f05e0bc3515a145365e910fe
Author: Frederic Barrat <fbarrat@linux.vnet.ibm.com>
Date:   Fri Mar 4 12:26:28 2016 +0100

    cxl: Introduce implementation-specific API
    
    The backend API (in cxl.h) lists some low-level functions whose
    implementation is different on bare-metal and in a guest. Each
    environment implements its own functions, and the common code uses
    them through function pointers, defined in cxl_backend_ops
    
    Co-authored-by: Christophe Lombard <clombard@linux.vnet.ibm.com>
    Signed-off-by: Frederic Barrat <fbarrat@linux.vnet.ibm.com>
    Signed-off-by: Christophe Lombard <clombard@linux.vnet.ibm.com>
    Reviewed-by: Manoj Kumar <manoj@linux.vnet.ibm.com>
    Acked-by: Ian Munsie <imunsie@au1.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/drivers/misc/cxl/context.c b/drivers/misc/cxl/context.c
index 262b88eac414..aa65262d4a32 100644
--- a/drivers/misc/cxl/context.c
+++ b/drivers/misc/cxl/context.c
@@ -214,8 +214,8 @@ int __detach_context(struct cxl_context *ctx)
 	/* Only warn if we detached while the link was OK.
 	 * If detach fails when hw is down, we don't care.
 	 */
-	WARN_ON(cxl_detach_process(ctx) &&
-		cxl_adapter_link_ok(ctx->afu->adapter));
+	WARN_ON(cxl_ops->detach_process(ctx) &&
+		cxl_ops->link_ok(ctx->afu->adapter));
 	flush_work(&ctx->fault_work); /* Only needed for dedicated process */
 
 	/* release the reference to the group leader and mm handling pid */

commit 7b8ad495d59280b634a7b546f4cdf58cf4d65f61
Author: Vaibhav Jain <vaibhav@linux.vnet.ibm.com>
Date:   Tue Nov 24 16:26:18 2015 +0530

    cxl: Fix DSI misses when the context owning task exits
    
    Presently when a user-space process issues CXL_IOCTL_START_WORK ioctl we
    store the pid of the current task_struct and use it to get pointer to
    the mm_struct of the process, while processing page or segment faults
    from the capi card. However this causes issues when the thread that had
    originally issued the start-work ioctl exits in which case the stored
    pid is no more valid and the cxl driver is unable to handle faults as
    the mm_struct corresponding to process is no more accessible.
    
    This patch fixes this issue by using the mm_struct of the next alive
    task in the thread group. This is done by iterating over all the tasks
    in the thread group starting from thread group leader and calling
    get_task_mm on each one of them. When a valid mm_struct is obtained the
    pid of the associated task is stored in the context replacing the
    exiting one for handling future faults.
    
    The patch introduces a new function named get_mem_context that checks if
    the current task pointed to by ctx->pid is dead? If yes it performs the
    steps described above. Also a new variable cxl_context.glpid is
    introduced which stores the pid of the thread group leader associated
    with the context owning task.
    
    Reported-by: Matthew R. Ochs <mrochs@linux.vnet.ibm.com>
    Reported-by: Frank Haverkamp <HAVERKAM@de.ibm.com>
    Suggested-by: Ian Munsie <imunsie@au1.ibm.com>
    Signed-off-by: Vaibhav Jain <vaibhav@linux.vnet.ibm.com>
    Acked-by: Ian Munsie <imunsie@au1.ibm.com>
    Reviewed-by: Frederic Barrat <fbarrat@linux.vnet.ibm.com>
    Reviewed-by: Matthew R. Ochs <mrochs@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/drivers/misc/cxl/context.c b/drivers/misc/cxl/context.c
index 6dde7a9d6a7e..262b88eac414 100644
--- a/drivers/misc/cxl/context.c
+++ b/drivers/misc/cxl/context.c
@@ -42,7 +42,7 @@ int cxl_context_init(struct cxl_context *ctx, struct cxl_afu *afu, bool master,
 	spin_lock_init(&ctx->sste_lock);
 	ctx->afu = afu;
 	ctx->master = master;
-	ctx->pid = NULL; /* Set in start work ioctl */
+	ctx->pid = ctx->glpid = NULL; /* Set in start work ioctl */
 	mutex_init(&ctx->mapping_lock);
 	ctx->mapping = mapping;
 
@@ -217,7 +217,11 @@ int __detach_context(struct cxl_context *ctx)
 	WARN_ON(cxl_detach_process(ctx) &&
 		cxl_adapter_link_ok(ctx->afu->adapter));
 	flush_work(&ctx->fault_work); /* Only needed for dedicated process */
+
+	/* release the reference to the group leader and mm handling pid */
 	put_pid(ctx->pid);
+	put_pid(ctx->glpid);
+
 	cxl_ctx_put();
 	return 0;
 }

commit 1b5df59e50874b9034c0fa389cd52b65f1f93292
Author: Vaibhav Jain <vaibhav@linux.vnet.ibm.com>
Date:   Mon Nov 16 09:33:45 2015 +0530

    cxl: Fix possible idr warning when contexts are released
    
    An idr warning is reported when a context is release after the capi card
    is unbound from the cxl driver via sysfs. Below are the steps to
    reproduce:
    
    1. Create multiple afu contexts in an user-space application using libcxl.
    2. Unbind capi card from cxl using command of form
       echo <capi-card-pci-addr> > /sys/bus/pci/drivers/cxl-pci/unbind
    3. Exit/kill the application owning afu contexts.
    
    After above steps a warning message is usually seen in the kernel logs
    of the form "idr_remove called for id=<context-id> which is not
    allocated."
    
    This is caused by the function cxl_release_afu which destroys the
    contexts_idr table. So when a context is release no entry for context pe
    is found in the contexts_idr table and idr code prints this warning.
    
    This patch fixes this issue by increasing & decreasing the ref-count on
    the afu device when a context is initialized or when its freed
    respectively. This prevents the afu from being released until all the
    afu contexts have been released. The patch introduces two new functions
    namely cxl_afu_get/put that manage the ref-count on the afu device.
    
    Also the patch removes code inside cxl_dev_context_init that increases ref
    on the afu device as its guaranteed to be alive during this function.
    
    Reported-by: Ian Munsie <imunsie@au1.ibm.com>
    Signed-off-by: Vaibhav Jain <vaibhav@linux.vnet.ibm.com>
    Acked-by: Ian Munsie <imunsie@au1.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/drivers/misc/cxl/context.c b/drivers/misc/cxl/context.c
index 2faa1270d085..6dde7a9d6a7e 100644
--- a/drivers/misc/cxl/context.c
+++ b/drivers/misc/cxl/context.c
@@ -97,6 +97,12 @@ int cxl_context_init(struct cxl_context *ctx, struct cxl_afu *afu, bool master,
 	ctx->pe = i;
 	ctx->elem = &ctx->afu->spa[i];
 	ctx->pe_inserted = false;
+
+	/*
+	 * take a ref on the afu so that it stays alive at-least till
+	 * this context is reclaimed inside reclaim_ctx.
+	 */
+	cxl_afu_get(afu);
 	return 0;
 }
 
@@ -278,6 +284,9 @@ static void reclaim_ctx(struct rcu_head *rcu)
 	if (ctx->irq_bitmap)
 		kfree(ctx->irq_bitmap);
 
+	/* Drop ref to the afu device taken during cxl_context_init */
+	cxl_afu_put(ctx->afu);
+
 	kfree(ctx);
 }
 

commit 52adee580d3c71a0dfabc3168597421981d68b86
Author: Andrew Donnellan <andrew.donnellan@au1.ibm.com>
Date:   Wed Sep 30 11:58:06 2015 +1000

    cxl: fix leak of ctx->irq_bitmap when releasing context via kernel API
    
    At present, ctx->irq_bitmap is freed in afu_release_irqs(), which is called
    from afu_release() via cxl_context_detach().
    
    Move the freeing of ctx->irq_bitmap from afu_release_irqs() to
    reclaim_ctx() (called through cxl_context_free()) so it's freed when
    releasing a context via the kernel API (cxl_release_context()) or the
    userspace API (afu_release()).
    
    Reported-by: Matthew R. Ochs <mrochs@linux.vnet.ibm.com>
    Fixes: 6f7f0b3df6d4 ("cxl: Add AFU virtual PHB and kernel API")
    Signed-off-by: Andrew Donnellan <andrew.donnellan@au1.ibm.com>
    Acked-by: Ian Munsie <imunsie@au1.ibm.com>
    Reviewed-by: Matthew R. Ochs <mrochs@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/drivers/misc/cxl/context.c b/drivers/misc/cxl/context.c
index e762f85ee233..2faa1270d085 100644
--- a/drivers/misc/cxl/context.c
+++ b/drivers/misc/cxl/context.c
@@ -275,6 +275,9 @@ static void reclaim_ctx(struct rcu_head *rcu)
 	if (ctx->kernelapi)
 		kfree(ctx->mapping);
 
+	if (ctx->irq_bitmap)
+		kfree(ctx->irq_bitmap);
+
 	kfree(ctx);
 }
 

commit 55e07668fbba9466e6a9ef7650718356cda38406
Author: Ian Munsie <imunsie@au1.ibm.com>
Date:   Thu Aug 27 19:50:19 2015 +1000

    cxl: Fix force unmapping mmaps of contexts allocated through the kernel api
    
    The cxl user api uses the address_space associated with the file when we
    need to force unmap all cxl mmap regions (e.g. on eeh, driver detach,
    etc). Currently, contexts allocated through the kernel api do not do
    this and instead skip the mmap invalidation, potentially allowing them
    to poke at the hardware after such an event, which may cause all sorts
    of trouble.
    
    This patch allocates an address_space for cxl contexts allocated through
    the kernel api so that the same invalidate path will for these contexts
    as well. We don't use the anonymous inode's address_space, as doing so
    could invalidate any mmaps of completely unrelated drivers using
    anonymous file descriptors.
    
    This patch also introduces a kernelapi flag, so we know when freeing the
    context if the address_space was allocated by us and needs to be freed.
    
    Signed-off-by: Ian Munsie <imunsie@au1.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/drivers/misc/cxl/context.c b/drivers/misc/cxl/context.c
index 941fda04aa9a..e762f85ee233 100644
--- a/drivers/misc/cxl/context.c
+++ b/drivers/misc/cxl/context.c
@@ -272,6 +272,8 @@ static void reclaim_ctx(struct rcu_head *rcu)
 	if (ctx->ff_page)
 		__free_page(ctx->ff_page);
 	ctx->sstp = NULL;
+	if (ctx->kernelapi)
+		kfree(ctx->mapping);
 
 	kfree(ctx);
 }

commit d9232a3da8683cd9c9854a858bcca968fe5f3bca
Author: Ian Munsie <imunsie@au1.ibm.com>
Date:   Thu Jul 23 16:43:56 2015 +1000

    cxl: Add alternate MMIO error handling
    
    userspace programs using cxl currently have to use two strategies for
    dealing with MMIO errors simultaneously. They have to check every read
    for a return of all Fs in case the adapter has gone away and the kernel
    has not yet noticed, and they have to deal with SIGBUS in case the
    kernel has already noticed, invalidated the mapping and marked the
    context as failed.
    
    In order to simplify things, this patch adds an alternative approach
    where the kernel will return a page filled with Fs instead of delivering
    a SIGBUS. This allows userspace to only need to deal with one of these
    two error paths, and is intended for use in libraries that use cxl
    transparently and may not be able to safely install a signal handler.
    
    This approach will only work if certain constraints are met. Namely, if
    the application is both reading and writing to an address in the problem
    state area it cannot assume that a non-FF read is OK, as it may just be
    reading out a value it has previously written. Further - since only one
    page is used per context a write to a given offset would be visible when
    reading the same offset from a different page in the mapping (this only
    applies within a single context, not between contexts).
    
    An application could deal with this by e.g. making sure it also reads
    from a read-only offset after any reads to a read/write offset.
    
    Due to these constraints, this functionality must be explicitly
    requested by userspace when starting the context by passing in the
    CXL_START_WORK_ERR_FF flag.
    
    Signed-off-by: Ian Munsie <imunsie@au1.ibm.com>
    Acked-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/drivers/misc/cxl/context.c b/drivers/misc/cxl/context.c
index 615842115848..941fda04aa9a 100644
--- a/drivers/misc/cxl/context.c
+++ b/drivers/misc/cxl/context.c
@@ -126,6 +126,18 @@ static int cxl_mmap_fault(struct vm_area_struct *vma, struct vm_fault *vmf)
 	if (ctx->status != STARTED) {
 		mutex_unlock(&ctx->status_mutex);
 		pr_devel("%s: Context not started, failing problem state access\n", __func__);
+		if (ctx->mmio_err_ff) {
+			if (!ctx->ff_page) {
+				ctx->ff_page = alloc_page(GFP_USER);
+				if (!ctx->ff_page)
+					return VM_FAULT_OOM;
+				memset(page_address(ctx->ff_page), 0xff, PAGE_SIZE);
+			}
+			get_page(ctx->ff_page);
+			vmf->page = ctx->ff_page;
+			vma->vm_page_prot = pgprot_cached(vma->vm_page_prot);
+			return 0;
+		}
 		return VM_FAULT_SIGBUS;
 	}
 
@@ -257,6 +269,8 @@ static void reclaim_ctx(struct rcu_head *rcu)
 	struct cxl_context *ctx = container_of(rcu, struct cxl_context, rcu);
 
 	free_page((u64)ctx->sstp);
+	if (ctx->ff_page)
+		__free_page(ctx->ff_page);
 	ctx->sstp = NULL;
 
 	kfree(ctx);

commit 0b3f9c757cabad4b8101c5fcddddd029ed5506a6
Author: Daniel Axtens <dja@axtens.net>
Date:   Fri Aug 14 17:41:18 2015 +1000

    cxl: Drop commands if the PCI channel is not in normal state
    
    If the PCI channel has gone down, don't attempt to poke the hardware.
    
    We need to guard every time cxl_whatever_(read|write) is called. This
    is because a call to those functions will dereference an offset into an
    mmio register, and the mmio mappings get invalidated in the EEH
    teardown.
    
    Check in the read/write functions in the header.
    We give them the same semantics as usual PCI operations:
     - a write to a channel that is down is ignored.
     - a read from a channel that is down returns all fs.
    
    Also, we try to access the MMIO space of a vPHB device as part of the
    PCI disable path. Because that's a read that bypasses most of our usual
    checks, we handle it explicitly.
    
    As far as user visible warnings go:
     - Check link state in file ops, return -EIO if down.
     - Be reasonably quiet if there's an error in a teardown path,
       or when we already know the hardware is going down.
     - Throw a big WARN if someone tries to start a CXL operation
       while the card is down. This gives a useful stacktrace for
       debugging whatever is doing that.
    
    Signed-off-by: Daniel Axtens <dja@axtens.net>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/drivers/misc/cxl/context.c b/drivers/misc/cxl/context.c
index 1287148629c0..615842115848 100644
--- a/drivers/misc/cxl/context.c
+++ b/drivers/misc/cxl/context.c
@@ -193,7 +193,11 @@ int __detach_context(struct cxl_context *ctx)
 	if (status != STARTED)
 		return -EBUSY;
 
-	WARN_ON(cxl_detach_process(ctx));
+	/* Only warn if we detached while the link was OK.
+	 * If detach fails when hw is down, we don't care.
+	 */
+	WARN_ON(cxl_detach_process(ctx) &&
+		cxl_adapter_link_ok(ctx->afu->adapter));
 	flush_work(&ctx->fault_work); /* Only needed for dedicated process */
 	put_pid(ctx->pid);
 	cxl_ctx_put();

commit 10a5894f2dedd8a26b3132497445b314c0d952c4
Author: Ian Munsie <imunsie@au1.ibm.com>
Date:   Tue Jul 7 15:45:45 2015 +1000

    cxl: Fix off by one error allowing subsequent mmap page to be accessed
    
    It was discovered that if a process mmaped their problem state area they
    were able to access one page more than expected, potentially allowing
    them to access the problem state area of an unrelated process.
    
    This was due to a simple off by one error in the mmap fault handler
    introduced in 0712dc7e73e59d79bcead5d5520acf4e9e917e87 ("cxl: Fix issues
    when unmapping contexts"), which is fixed in this patch.
    
    Cc: stable@vger.kernel.org
    Fixes: 0712dc7e73e5 ("cxl: Fix issues when unmapping contexts")
    Signed-off-by: Ian Munsie <imunsie@au1.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/drivers/misc/cxl/context.c b/drivers/misc/cxl/context.c
index 6236d4a1f7ad..1287148629c0 100644
--- a/drivers/misc/cxl/context.c
+++ b/drivers/misc/cxl/context.c
@@ -113,11 +113,11 @@ static int cxl_mmap_fault(struct vm_area_struct *vma, struct vm_fault *vmf)
 
 	if (ctx->afu->current_mode == CXL_MODE_DEDICATED) {
 		area = ctx->afu->psn_phys;
-		if (offset > ctx->afu->adapter->ps_size)
+		if (offset >= ctx->afu->adapter->ps_size)
 			return VM_FAULT_SIGBUS;
 	} else {
 		area = ctx->psn_phys;
-		if (offset > ctx->psn_size)
+		if (offset >= ctx->psn_size)
 			return VM_FAULT_SIGBUS;
 	}
 

commit 5caaf5346892d1e7f0b8b7223062644f8538483f
Author: Ian Munsie <imunsie@au1.ibm.com>
Date:   Tue Jul 7 15:45:46 2015 +1000

    cxl: Fail mmap if requested mapping is larger than assigned problem state area
    
    This patch makes the mmap call fail outright if the requested region is
    larger than the problem state area assigned to the context so the error
    is reported immediately rather than waiting for an attempt to access an
    address out of bounds.
    
    Although we never expect users to map more than the assigned problem
    state area and are not aware of anyone doing this (other than for
    testing), this does have the potential to break users if someone has
    used a larger range regardless. I'm submitting it for consideration, but
    if this change is not considered acceptable the previous patch is
    sufficient to prevent access out of bounds without breaking anyone.
    
    Signed-off-by: Ian Munsie <imunsie@au1.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/drivers/misc/cxl/context.c b/drivers/misc/cxl/context.c
index 2a4c80ac322a..6236d4a1f7ad 100644
--- a/drivers/misc/cxl/context.c
+++ b/drivers/misc/cxl/context.c
@@ -145,8 +145,16 @@ static const struct vm_operations_struct cxl_mmap_vmops = {
  */
 int cxl_context_iomap(struct cxl_context *ctx, struct vm_area_struct *vma)
 {
+	u64 start = vma->vm_pgoff << PAGE_SHIFT;
 	u64 len = vma->vm_end - vma->vm_start;
-	len = min(len, ctx->psn_size);
+
+	if (ctx->afu->current_mode == CXL_MODE_DEDICATED) {
+		if (start + len > ctx->afu->adapter->ps_size)
+			return -EINVAL;
+	} else {
+		if (start + len > ctx->psn_size)
+			return -EINVAL;
+	}
 
 	if (ctx->afu->current_mode != CXL_MODE_DEDICATED) {
 		/* make sure there is a valid per process space for this AFU */

commit 7bb5d91a4dda92e28b2704a3e7ebe94260ccd6f1
Author: Michael Neuling <mikey@neuling.org>
Date:   Wed May 27 16:07:14 2015 +1000

    cxl: Rework context lifetimes
    
    This reworks contexts lifetimes a bit to enable the kernel API where we may
    want to reuse contexts. Here we will want to start and stop contexts without
    freeing them.
    
    Start context does the get pid & ctx so stop context will need to do the puts.
    Here we move put pid & ctx to the detach context path which will become part of
    the stop context path.
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Acked-by: Ian Munsie <imunsie@au1.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/drivers/misc/cxl/context.c b/drivers/misc/cxl/context.c
index 7d857b7d686d..2a4c80ac322a 100644
--- a/drivers/misc/cxl/context.c
+++ b/drivers/misc/cxl/context.c
@@ -186,6 +186,9 @@ int __detach_context(struct cxl_context *ctx)
 		return -EBUSY;
 
 	WARN_ON(cxl_detach_process(ctx));
+	flush_work(&ctx->fault_work); /* Only needed for dedicated process */
+	put_pid(ctx->pid);
+	cxl_ctx_put();
 	return 0;
 }
 
@@ -204,7 +207,6 @@ void cxl_context_detach(struct cxl_context *ctx)
 		return;
 
 	afu_release_irqs(ctx, ctx);
-	flush_work(&ctx->fault_work); /* Only needed for dedicated process */
 	wake_up_all(&ctx->wq);
 }
 
@@ -245,7 +247,6 @@ static void reclaim_ctx(struct rcu_head *rcu)
 	free_page((u64)ctx->sstp);
 	ctx->sstp = NULL;
 
-	put_pid(ctx->pid);
 	kfree(ctx);
 }
 

commit eda3693c842ed169af66af943554c648633769d0
Author: Michael Neuling <mikey@neuling.org>
Date:   Wed May 27 16:07:08 2015 +1000

    cxl: Rework detach context functions
    
    Rework __detach_context() and cxl_context_detach() so we can reuse them in the
    kernel API.
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Acked-by: Ian Munsie <imunsie@au1.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/drivers/misc/cxl/context.c b/drivers/misc/cxl/context.c
index 36bb8e4195d5..7d857b7d686d 100644
--- a/drivers/misc/cxl/context.c
+++ b/drivers/misc/cxl/context.c
@@ -174,7 +174,7 @@ int cxl_context_iomap(struct cxl_context *ctx, struct vm_area_struct *vma)
  * return until all outstanding interrupts for this context have completed. The
  * hardware should no longer access *ctx after this has returned.
  */
-static void __detach_context(struct cxl_context *ctx)
+int __detach_context(struct cxl_context *ctx)
 {
 	enum cxl_context_status status;
 
@@ -183,12 +183,10 @@ static void __detach_context(struct cxl_context *ctx)
 	ctx->status = CLOSED;
 	mutex_unlock(&ctx->status_mutex);
 	if (status != STARTED)
-		return;
+		return -EBUSY;
 
 	WARN_ON(cxl_detach_process(ctx));
-	afu_release_irqs(ctx, ctx);
-	flush_work(&ctx->fault_work); /* Only needed for dedicated process */
-	wake_up_all(&ctx->wq);
+	return 0;
 }
 
 /*
@@ -199,7 +197,15 @@ static void __detach_context(struct cxl_context *ctx)
  */
 void cxl_context_detach(struct cxl_context *ctx)
 {
-	__detach_context(ctx);
+	int rc;
+
+	rc = __detach_context(ctx);
+	if (rc)
+		return;
+
+	afu_release_irqs(ctx, ctx);
+	flush_work(&ctx->fault_work); /* Only needed for dedicated process */
+	wake_up_all(&ctx->wq);
 }
 
 /*
@@ -216,7 +222,7 @@ void cxl_context_detach_all(struct cxl_afu *afu)
 		 * Anything done in here needs to be setup before the IDR is
 		 * created and torn down after the IDR removed
 		 */
-		__detach_context(ctx);
+		cxl_context_detach(ctx);
 
 		/*
 		 * We are force detaching - remove any active PSA mappings so

commit 6428832a7bfae73345706d63a228a6ce60af0081
Author: Michael Neuling <mikey@neuling.org>
Date:   Wed May 27 16:07:07 2015 +1000

    cxl: Add cookie parameter to afu_release_irqs()
    
    Add cookie parameter to afu_release_irqs() so that we can pass in a different
    cookie than the context structure.  This will be useful for other kernel
    drivers that want to call this but get their own cookie back in the interrupt
    handler.
    
    Update all existing call sites.
    
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Acked-by: Ian Munsie <imunsie@au1.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/drivers/misc/cxl/context.c b/drivers/misc/cxl/context.c
index 78ce990d77ca..36bb8e4195d5 100644
--- a/drivers/misc/cxl/context.c
+++ b/drivers/misc/cxl/context.c
@@ -186,7 +186,7 @@ static void __detach_context(struct cxl_context *ctx)
 		return;
 
 	WARN_ON(cxl_detach_process(ctx));
-	afu_release_irqs(ctx);
+	afu_release_irqs(ctx, ctx);
 	flush_work(&ctx->fault_work); /* Only needed for dedicated process */
 	wake_up_all(&ctx->wq);
 }

commit 8ac75b96be71e20ec1785ca18170890c4dfffe87
Author: Ian Munsie <imunsie@au1.ibm.com>
Date:   Fri May 8 22:55:18 2015 +1000

    cxl: Use call_rcu to reduce latency when releasing the afu fd
    
    The afu fd release path was identified as a significant bottleneck in
    the overall performance of cxl. While an optimal AFU design would
    minimise the need to close & reopen the AFU fd, it is not always
    practical to avoid.
    
    The bottleneck seems to be down to the call to synchronize_rcu(), which
    will block until every other thread is guaranteed to be out of an RCU
    critical section. Replace it with call_rcu() to free the context
    structures later so we can return to the application sooner.
    
    This reduces the time spent in the fd release path from 13356 usec to
    13.3 usec - about a 100x speed up.
    
    Reported-by: Fei K Chen <uchen@cn.ibm.com>
    Signed-off-by: Ian Munsie <imunsie@au1.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/drivers/misc/cxl/context.c b/drivers/misc/cxl/context.c
index d1b55fe62817..78ce990d77ca 100644
--- a/drivers/misc/cxl/context.c
+++ b/drivers/misc/cxl/context.c
@@ -232,12 +232,9 @@ void cxl_context_detach_all(struct cxl_afu *afu)
 	mutex_unlock(&afu->contexts_lock);
 }
 
-void cxl_context_free(struct cxl_context *ctx)
+static void reclaim_ctx(struct rcu_head *rcu)
 {
-	mutex_lock(&ctx->afu->contexts_lock);
-	idr_remove(&ctx->afu->contexts_idr, ctx->pe);
-	mutex_unlock(&ctx->afu->contexts_lock);
-	synchronize_rcu();
+	struct cxl_context *ctx = container_of(rcu, struct cxl_context, rcu);
 
 	free_page((u64)ctx->sstp);
 	ctx->sstp = NULL;
@@ -245,3 +242,11 @@ void cxl_context_free(struct cxl_context *ctx)
 	put_pid(ctx->pid);
 	kfree(ctx);
 }
+
+void cxl_context_free(struct cxl_context *ctx)
+{
+	mutex_lock(&ctx->afu->contexts_lock);
+	idr_remove(&ctx->afu->contexts_idr, ctx->pe);
+	mutex_unlock(&ctx->afu->contexts_lock);
+	call_rcu(&ctx->rcu, reclaim_ctx);
+}

commit 0712dc7e73e59d79bcead5d5520acf4e9e917e87
Author: Ian Munsie <imunsie@au1.ibm.com>
Date:   Wed Jan 7 16:33:04 2015 +1100

    cxl: Fix issues when unmapping contexts
    
    An issue was introduced with "cxl: Unmap MMIO regions when detaching a
    context" (b123429e6a9e8d03aacf888d23262835f0081448) where closing a
    context normally could also unmap the problem state area of other
    contexts currently using the AFU.
    
    It was also discovered that after a context's MMIO space had been
    unmapped it would read 0s when accessing it, whereas the expected
    behaviour was for the access to fail altogether.
    
    In order to address these issues, this patch does two things:
    
    - Forced mmap unmapping is only done when we are forcefully detaching
      all contexts, and not in the normal detach path. Since the normal
      context close path is tied to the file release any mmaps must have
      already been released so we don't need to worry in that case.
    
    - The mmap path now uses a vm_operations_struct with a fault handler.
      The fault handler ensures that the context is in started state,
      otherwise it fails the access attempt with a SIGBUS.
    
    Fixes: b123429e6a9e ("cxl: Unmap MMIO regions when detaching a context")
    Signed-off-by: Ian Munsie <imunsie@au1.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/drivers/misc/cxl/context.c b/drivers/misc/cxl/context.c
index 51fd6b524371..d1b55fe62817 100644
--- a/drivers/misc/cxl/context.c
+++ b/drivers/misc/cxl/context.c
@@ -100,6 +100,46 @@ int cxl_context_init(struct cxl_context *ctx, struct cxl_afu *afu, bool master,
 	return 0;
 }
 
+static int cxl_mmap_fault(struct vm_area_struct *vma, struct vm_fault *vmf)
+{
+	struct cxl_context *ctx = vma->vm_file->private_data;
+	unsigned long address = (unsigned long)vmf->virtual_address;
+	u64 area, offset;
+
+	offset = vmf->pgoff << PAGE_SHIFT;
+
+	pr_devel("%s: pe: %i address: 0x%lx offset: 0x%llx\n",
+			__func__, ctx->pe, address, offset);
+
+	if (ctx->afu->current_mode == CXL_MODE_DEDICATED) {
+		area = ctx->afu->psn_phys;
+		if (offset > ctx->afu->adapter->ps_size)
+			return VM_FAULT_SIGBUS;
+	} else {
+		area = ctx->psn_phys;
+		if (offset > ctx->psn_size)
+			return VM_FAULT_SIGBUS;
+	}
+
+	mutex_lock(&ctx->status_mutex);
+
+	if (ctx->status != STARTED) {
+		mutex_unlock(&ctx->status_mutex);
+		pr_devel("%s: Context not started, failing problem state access\n", __func__);
+		return VM_FAULT_SIGBUS;
+	}
+
+	vm_insert_pfn(vma, address, (area + offset) >> PAGE_SHIFT);
+
+	mutex_unlock(&ctx->status_mutex);
+
+	return VM_FAULT_NOPAGE;
+}
+
+static const struct vm_operations_struct cxl_mmap_vmops = {
+	.fault = cxl_mmap_fault,
+};
+
 /*
  * Map a per-context mmio space into the given vma.
  */
@@ -108,26 +148,25 @@ int cxl_context_iomap(struct cxl_context *ctx, struct vm_area_struct *vma)
 	u64 len = vma->vm_end - vma->vm_start;
 	len = min(len, ctx->psn_size);
 
-	if (ctx->afu->current_mode == CXL_MODE_DEDICATED) {
-		vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
-		return vm_iomap_memory(vma, ctx->afu->psn_phys, ctx->afu->adapter->ps_size);
-	}
+	if (ctx->afu->current_mode != CXL_MODE_DEDICATED) {
+		/* make sure there is a valid per process space for this AFU */
+		if ((ctx->master && !ctx->afu->psa) || (!ctx->afu->pp_psa)) {
+			pr_devel("AFU doesn't support mmio space\n");
+			return -EINVAL;
+		}
 
-	/* make sure there is a valid per process space for this AFU */
-	if ((ctx->master && !ctx->afu->psa) || (!ctx->afu->pp_psa)) {
-		pr_devel("AFU doesn't support mmio space\n");
-		return -EINVAL;
+		/* Can't mmap until the AFU is enabled */
+		if (!ctx->afu->enabled)
+			return -EBUSY;
 	}
 
-	/* Can't mmap until the AFU is enabled */
-	if (!ctx->afu->enabled)
-		return -EBUSY;
-
 	pr_devel("%s: mmio physical: %llx pe: %i master:%i\n", __func__,
 		 ctx->psn_phys, ctx->pe , ctx->master);
 
+	vma->vm_flags |= VM_IO | VM_PFNMAP;
 	vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
-	return vm_iomap_memory(vma, ctx->psn_phys, len);
+	vma->vm_ops = &cxl_mmap_vmops;
+	return 0;
 }
 
 /*
@@ -150,12 +189,6 @@ static void __detach_context(struct cxl_context *ctx)
 	afu_release_irqs(ctx);
 	flush_work(&ctx->fault_work); /* Only needed for dedicated process */
 	wake_up_all(&ctx->wq);
-
-	/* Release Problem State Area mapping */
-	mutex_lock(&ctx->mapping_lock);
-	if (ctx->mapping)
-		unmap_mapping_range(ctx->mapping, 0, 0, 1);
-	mutex_unlock(&ctx->mapping_lock);
 }
 
 /*
@@ -184,6 +217,17 @@ void cxl_context_detach_all(struct cxl_afu *afu)
 		 * created and torn down after the IDR removed
 		 */
 		__detach_context(ctx);
+
+		/*
+		 * We are force detaching - remove any active PSA mappings so
+		 * userspace cannot interfere with the card if it comes back.
+		 * Easiest way to exercise this is to unbind and rebind the
+		 * driver via sysfs while it is in use.
+		 */
+		mutex_lock(&ctx->mapping_lock);
+		if (ctx->mapping)
+			unmap_mapping_range(ctx->mapping, 0, 0, 1);
+		mutex_unlock(&ctx->mapping_lock);
 	}
 	mutex_unlock(&afu->contexts_lock);
 }

commit b123429e6a9e8d03aacf888d23262835f0081448
Author: Ian Munsie <imunsie@au1.ibm.com>
Date:   Mon Dec 8 19:18:01 2014 +1100

    cxl: Unmap MMIO regions when detaching a context
    
    If we need to force detach a context (e.g. due to EEH or simply force
    unbinding the driver) we should prevent the userspace contexts from
    being able to access the Problem State Area MMIO region further, which
    they may have mapped with mmap().
    
    This patch unmaps any mapped MMIO regions when detaching a userspace
    context.
    
    Cc: stable@vger.kernel.org
    Signed-off-by: Ian Munsie <imunsie@au1.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/drivers/misc/cxl/context.c b/drivers/misc/cxl/context.c
index 4aa31a3fb448..51fd6b524371 100644
--- a/drivers/misc/cxl/context.c
+++ b/drivers/misc/cxl/context.c
@@ -34,7 +34,8 @@ struct cxl_context *cxl_context_alloc(void)
 /*
  * Initialises a CXL context.
  */
-int cxl_context_init(struct cxl_context *ctx, struct cxl_afu *afu, bool master)
+int cxl_context_init(struct cxl_context *ctx, struct cxl_afu *afu, bool master,
+		     struct address_space *mapping)
 {
 	int i;
 
@@ -42,6 +43,8 @@ int cxl_context_init(struct cxl_context *ctx, struct cxl_afu *afu, bool master)
 	ctx->afu = afu;
 	ctx->master = master;
 	ctx->pid = NULL; /* Set in start work ioctl */
+	mutex_init(&ctx->mapping_lock);
+	ctx->mapping = mapping;
 
 	/*
 	 * Allocate the segment table before we put it in the IDR so that we
@@ -147,6 +150,12 @@ static void __detach_context(struct cxl_context *ctx)
 	afu_release_irqs(ctx);
 	flush_work(&ctx->fault_work); /* Only needed for dedicated process */
 	wake_up_all(&ctx->wq);
+
+	/* Release Problem State Area mapping */
+	mutex_lock(&ctx->mapping_lock);
+	if (ctx->mapping)
+		unmap_mapping_range(ctx->mapping, 0, 0, 1);
+	mutex_unlock(&ctx->mapping_lock);
 }
 
 /*

commit ee41d11d53c8fc4968f0816504651541d606cf40
Author: Ian Munsie <imunsie@au1.ibm.com>
Date:   Mon Dec 8 19:17:55 2014 +1100

    cxl: Change contexts_lock to a mutex to fix sleep while atomic bug
    
    We had a known sleep while atomic bug if a CXL device was forcefully
    unbound while it was in use. This could occur as a result of EEH, or
    manually induced with something like this while the device was in use:
    
    echo 0000:01:00.0 > /sys/bus/pci/drivers/cxl-pci/unbind
    
    The issue was that in this code path we iterated over each context and
    forcefully detached it with the contexts_lock spin lock held, however
    the detach also needed to take the spu_mutex, and call schedule.
    
    This patch changes the contexts_lock to a mutex so that we are not in
    atomic context while doing the detach, thereby avoiding the sleep while
    atomic.
    
    Also delete the related TODO comment, which suggested an alternate
    solution which turned out to not be workable.
    
    Cc: stable@vger.kernel.org
    Signed-off-by: Ian Munsie <imunsie@au1.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/drivers/misc/cxl/context.c b/drivers/misc/cxl/context.c
index cca472109135..4aa31a3fb448 100644
--- a/drivers/misc/cxl/context.c
+++ b/drivers/misc/cxl/context.c
@@ -82,12 +82,12 @@ int cxl_context_init(struct cxl_context *ctx, struct cxl_afu *afu, bool master)
 	 * Allocating IDR! We better make sure everything's setup that
 	 * dereferences from it.
 	 */
+	mutex_lock(&afu->contexts_lock);
 	idr_preload(GFP_KERNEL);
-	spin_lock(&afu->contexts_lock);
 	i = idr_alloc(&ctx->afu->contexts_idr, ctx, 0,
 		      ctx->afu->num_procs, GFP_NOWAIT);
-	spin_unlock(&afu->contexts_lock);
 	idr_preload_end();
+	mutex_unlock(&afu->contexts_lock);
 	if (i < 0)
 		return i;
 
@@ -168,21 +168,22 @@ void cxl_context_detach_all(struct cxl_afu *afu)
 	struct cxl_context *ctx;
 	int tmp;
 
-	rcu_read_lock();
-	idr_for_each_entry(&afu->contexts_idr, ctx, tmp)
+	mutex_lock(&afu->contexts_lock);
+	idr_for_each_entry(&afu->contexts_idr, ctx, tmp) {
 		/*
 		 * Anything done in here needs to be setup before the IDR is
 		 * created and torn down after the IDR removed
 		 */
 		__detach_context(ctx);
-	rcu_read_unlock();
+	}
+	mutex_unlock(&afu->contexts_lock);
 }
 
 void cxl_context_free(struct cxl_context *ctx)
 {
-	spin_lock(&ctx->afu->contexts_lock);
+	mutex_lock(&ctx->afu->contexts_lock);
 	idr_remove(&ctx->afu->contexts_idr, ctx->pe);
-	spin_unlock(&ctx->afu->contexts_lock);
+	mutex_unlock(&ctx->afu->contexts_lock);
 	synchronize_rcu();
 
 	free_page((u64)ctx->sstp);

commit f204e0b8cedd7da1dfcfd05ed6b7692737e24029
Author: Ian Munsie <imunsie@au1.ibm.com>
Date:   Wed Oct 8 19:55:02 2014 +1100

    cxl: Driver code for powernv PCIe based cards for userspace access
    
    This is the core of the cxl driver.
    
    It adds support for using cxl cards in the powernv environment only (ie POWER8
    bare metal). It allows access to cxl accelerators by userspace using the
    /dev/cxl/afuM.N char devices.
    
    The kernel driver has no knowledge of the function implemented by the
    accelerator. It provides services to userspace via the /dev/cxl/afuM.N
    devices. When a program opens this device and runs the start work IOCTL, the
    accelerator will have coherent access to that processes memory using the same
    virtual addresses. That process may mmap the device to access any MMIO space
    the accelerator provides.  Also, reads on the device will allow interrupts to
    be received. These services are further documented in a later patch in
    Documentation/powerpc/cxl.txt.
    
    Documentation of the cxl hardware architecture and userspace API is provided in
    subsequent patches.
    
    Signed-off-by: Ian Munsie <imunsie@au1.ibm.com>
    Signed-off-by: Michael Neuling <mikey@neuling.org>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/drivers/misc/cxl/context.c b/drivers/misc/cxl/context.c
new file mode 100644
index 000000000000..cca472109135
--- /dev/null
+++ b/drivers/misc/cxl/context.c
@@ -0,0 +1,193 @@
+/*
+ * Copyright 2014 IBM Corp.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version
+ * 2 of the License, or (at your option) any later version.
+ */
+
+#include <linux/module.h>
+#include <linux/kernel.h>
+#include <linux/bitmap.h>
+#include <linux/sched.h>
+#include <linux/pid.h>
+#include <linux/fs.h>
+#include <linux/mm.h>
+#include <linux/debugfs.h>
+#include <linux/slab.h>
+#include <linux/idr.h>
+#include <asm/cputable.h>
+#include <asm/current.h>
+#include <asm/copro.h>
+
+#include "cxl.h"
+
+/*
+ * Allocates space for a CXL context.
+ */
+struct cxl_context *cxl_context_alloc(void)
+{
+	return kzalloc(sizeof(struct cxl_context), GFP_KERNEL);
+}
+
+/*
+ * Initialises a CXL context.
+ */
+int cxl_context_init(struct cxl_context *ctx, struct cxl_afu *afu, bool master)
+{
+	int i;
+
+	spin_lock_init(&ctx->sste_lock);
+	ctx->afu = afu;
+	ctx->master = master;
+	ctx->pid = NULL; /* Set in start work ioctl */
+
+	/*
+	 * Allocate the segment table before we put it in the IDR so that we
+	 * can always access it when dereferenced from IDR. For the same
+	 * reason, the segment table is only destroyed after the context is
+	 * removed from the IDR.  Access to this in the IOCTL is protected by
+	 * Linux filesytem symantics (can't IOCTL until open is complete).
+	 */
+	i = cxl_alloc_sst(ctx);
+	if (i)
+		return i;
+
+	INIT_WORK(&ctx->fault_work, cxl_handle_fault);
+
+	init_waitqueue_head(&ctx->wq);
+	spin_lock_init(&ctx->lock);
+
+	ctx->irq_bitmap = NULL;
+	ctx->pending_irq = false;
+	ctx->pending_fault = false;
+	ctx->pending_afu_err = false;
+
+	/*
+	 * When we have to destroy all contexts in cxl_context_detach_all() we
+	 * end up with afu_release_irqs() called from inside a
+	 * idr_for_each_entry(). Hence we need to make sure that anything
+	 * dereferenced from this IDR is ok before we allocate the IDR here.
+	 * This clears out the IRQ ranges to ensure this.
+	 */
+	for (i = 0; i < CXL_IRQ_RANGES; i++)
+		ctx->irqs.range[i] = 0;
+
+	mutex_init(&ctx->status_mutex);
+
+	ctx->status = OPENED;
+
+	/*
+	 * Allocating IDR! We better make sure everything's setup that
+	 * dereferences from it.
+	 */
+	idr_preload(GFP_KERNEL);
+	spin_lock(&afu->contexts_lock);
+	i = idr_alloc(&ctx->afu->contexts_idr, ctx, 0,
+		      ctx->afu->num_procs, GFP_NOWAIT);
+	spin_unlock(&afu->contexts_lock);
+	idr_preload_end();
+	if (i < 0)
+		return i;
+
+	ctx->pe = i;
+	ctx->elem = &ctx->afu->spa[i];
+	ctx->pe_inserted = false;
+	return 0;
+}
+
+/*
+ * Map a per-context mmio space into the given vma.
+ */
+int cxl_context_iomap(struct cxl_context *ctx, struct vm_area_struct *vma)
+{
+	u64 len = vma->vm_end - vma->vm_start;
+	len = min(len, ctx->psn_size);
+
+	if (ctx->afu->current_mode == CXL_MODE_DEDICATED) {
+		vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
+		return vm_iomap_memory(vma, ctx->afu->psn_phys, ctx->afu->adapter->ps_size);
+	}
+
+	/* make sure there is a valid per process space for this AFU */
+	if ((ctx->master && !ctx->afu->psa) || (!ctx->afu->pp_psa)) {
+		pr_devel("AFU doesn't support mmio space\n");
+		return -EINVAL;
+	}
+
+	/* Can't mmap until the AFU is enabled */
+	if (!ctx->afu->enabled)
+		return -EBUSY;
+
+	pr_devel("%s: mmio physical: %llx pe: %i master:%i\n", __func__,
+		 ctx->psn_phys, ctx->pe , ctx->master);
+
+	vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
+	return vm_iomap_memory(vma, ctx->psn_phys, len);
+}
+
+/*
+ * Detach a context from the hardware. This disables interrupts and doesn't
+ * return until all outstanding interrupts for this context have completed. The
+ * hardware should no longer access *ctx after this has returned.
+ */
+static void __detach_context(struct cxl_context *ctx)
+{
+	enum cxl_context_status status;
+
+	mutex_lock(&ctx->status_mutex);
+	status = ctx->status;
+	ctx->status = CLOSED;
+	mutex_unlock(&ctx->status_mutex);
+	if (status != STARTED)
+		return;
+
+	WARN_ON(cxl_detach_process(ctx));
+	afu_release_irqs(ctx);
+	flush_work(&ctx->fault_work); /* Only needed for dedicated process */
+	wake_up_all(&ctx->wq);
+}
+
+/*
+ * Detach the given context from the AFU. This doesn't actually
+ * free the context but it should stop the context running in hardware
+ * (ie. prevent this context from generating any further interrupts
+ * so that it can be freed).
+ */
+void cxl_context_detach(struct cxl_context *ctx)
+{
+	__detach_context(ctx);
+}
+
+/*
+ * Detach all contexts on the given AFU.
+ */
+void cxl_context_detach_all(struct cxl_afu *afu)
+{
+	struct cxl_context *ctx;
+	int tmp;
+
+	rcu_read_lock();
+	idr_for_each_entry(&afu->contexts_idr, ctx, tmp)
+		/*
+		 * Anything done in here needs to be setup before the IDR is
+		 * created and torn down after the IDR removed
+		 */
+		__detach_context(ctx);
+	rcu_read_unlock();
+}
+
+void cxl_context_free(struct cxl_context *ctx)
+{
+	spin_lock(&ctx->afu->contexts_lock);
+	idr_remove(&ctx->afu->contexts_idr, ctx->pe);
+	spin_unlock(&ctx->afu->contexts_lock);
+	synchronize_rcu();
+
+	free_page((u64)ctx->sstp);
+	ctx->sstp = NULL;
+
+	put_pid(ctx->pid);
+	kfree(ctx);
+}
