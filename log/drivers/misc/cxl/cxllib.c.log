commit c1e8d7c6a7a682e1405e3e242d32fc377fd196ff
Author: Michel Lespinasse <walken@google.com>
Date:   Mon Jun 8 21:33:54 2020 -0700

    mmap locking API: convert mmap_sem comments
    
    Convert comments that reference mmap_sem to reference mmap_lock instead.
    
    [akpm@linux-foundation.org: fix up linux-next leftovers]
    [akpm@linux-foundation.org: s/lockaphore/lock/, per Vlastimil]
    [akpm@linux-foundation.org: more linux-next fixups, per Michel]
    
    Signed-off-by: Michel Lespinasse <walken@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Vlastimil Babka <vbabka@suse.cz>
    Reviewed-by: Daniel Jordan <daniel.m.jordan@oracle.com>
    Cc: Davidlohr Bueso <dbueso@suse.de>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Jason Gunthorpe <jgg@ziepe.ca>
    Cc: Jerome Glisse <jglisse@redhat.com>
    Cc: John Hubbard <jhubbard@nvidia.com>
    Cc: Laurent Dufour <ldufour@linux.ibm.com>
    Cc: Liam Howlett <Liam.Howlett@oracle.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ying Han <yinghan@google.com>
    Link: http://lkml.kernel.org/r/20200520052908.204642-13-walken@google.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/drivers/misc/cxl/cxllib.c b/drivers/misc/cxl/cxllib.c
index a2c0fbb5eb28..2a1783f32254 100644
--- a/drivers/misc/cxl/cxllib.c
+++ b/drivers/misc/cxl/cxllib.c
@@ -245,9 +245,8 @@ int cxllib_handle_fault(struct mm_struct *mm, u64 addr, u64 size, u64 flags)
 	     dar += page_size) {
 		if (dar < vma_start || dar >= vma_end) {
 			/*
-			 * We don't hold the mm->mmap_sem semaphore
-			 * while iterating, since the semaphore is
-			 * required by one of the lower-level page
+			 * We don't hold mm->mmap_lock while iterating, since
+			 * the lock is required by one of the lower-level page
 			 * fault processing functions and it could
 			 * create a deadlock.
 			 *

commit d8ed45c5dcd455fc5848d47f86883a1b872ac0d0
Author: Michel Lespinasse <walken@google.com>
Date:   Mon Jun 8 21:33:25 2020 -0700

    mmap locking API: use coccinelle to convert mmap_sem rwsem call sites
    
    This change converts the existing mmap_sem rwsem calls to use the new mmap
    locking API instead.
    
    The change is generated using coccinelle with the following rule:
    
    // spatch --sp-file mmap_lock_api.cocci --in-place --include-headers --dir .
    
    @@
    expression mm;
    @@
    (
    -init_rwsem
    +mmap_init_lock
    |
    -down_write
    +mmap_write_lock
    |
    -down_write_killable
    +mmap_write_lock_killable
    |
    -down_write_trylock
    +mmap_write_trylock
    |
    -up_write
    +mmap_write_unlock
    |
    -downgrade_write
    +mmap_write_downgrade
    |
    -down_read
    +mmap_read_lock
    |
    -down_read_killable
    +mmap_read_lock_killable
    |
    -down_read_trylock
    +mmap_read_trylock
    |
    -up_read
    +mmap_read_unlock
    )
    -(&mm->mmap_sem)
    +(mm)
    
    Signed-off-by: Michel Lespinasse <walken@google.com>
    Signed-off-by: Andrew Morton <akpm@linux-foundation.org>
    Reviewed-by: Daniel Jordan <daniel.m.jordan@oracle.com>
    Reviewed-by: Laurent Dufour <ldufour@linux.ibm.com>
    Reviewed-by: Vlastimil Babka <vbabka@suse.cz>
    Cc: Davidlohr Bueso <dbueso@suse.de>
    Cc: David Rientjes <rientjes@google.com>
    Cc: Hugh Dickins <hughd@google.com>
    Cc: Jason Gunthorpe <jgg@ziepe.ca>
    Cc: Jerome Glisse <jglisse@redhat.com>
    Cc: John Hubbard <jhubbard@nvidia.com>
    Cc: Liam Howlett <Liam.Howlett@oracle.com>
    Cc: Matthew Wilcox <willy@infradead.org>
    Cc: Peter Zijlstra <peterz@infradead.org>
    Cc: Ying Han <yinghan@google.com>
    Link: http://lkml.kernel.org/r/20200520052908.204642-5-walken@google.com
    Signed-off-by: Linus Torvalds <torvalds@linux-foundation.org>

diff --git a/drivers/misc/cxl/cxllib.c b/drivers/misc/cxl/cxllib.c
index 258c43a95ac3..a2c0fbb5eb28 100644
--- a/drivers/misc/cxl/cxllib.c
+++ b/drivers/misc/cxl/cxllib.c
@@ -207,7 +207,7 @@ static int get_vma_info(struct mm_struct *mm, u64 addr,
 	struct vm_area_struct *vma = NULL;
 	int rc = 0;
 
-	down_read(&mm->mmap_sem);
+	mmap_read_lock(mm);
 
 	vma = find_vma(mm, addr);
 	if (!vma) {
@@ -218,7 +218,7 @@ static int get_vma_info(struct mm_struct *mm, u64 addr,
 	*vma_start = vma->vm_start;
 	*vma_end = vma->vm_end;
 out:
-	up_read(&mm->mmap_sem);
+	mmap_read_unlock(mm);
 	return rc;
 }
 

commit 2874c5fd284268364ece81a7bd936f3c8168e567
Author: Thomas Gleixner <tglx@linutronix.de>
Date:   Mon May 27 08:55:01 2019 +0200

    treewide: Replace GPLv2 boilerplate/reference with SPDX - rule 152
    
    Based on 1 normalized pattern(s):
    
      this program is free software you can redistribute it and or modify
      it under the terms of the gnu general public license as published by
      the free software foundation either version 2 of the license or at
      your option any later version
    
    extracted by the scancode license scanner the SPDX license identifier
    
      GPL-2.0-or-later
    
    has been chosen to replace the boilerplate/reference in 3029 file(s).
    
    Signed-off-by: Thomas Gleixner <tglx@linutronix.de>
    Reviewed-by: Allison Randal <allison@lohutok.net>
    Cc: linux-spdx@vger.kernel.org
    Link: https://lkml.kernel.org/r/20190527070032.746973796@linutronix.de
    Signed-off-by: Greg Kroah-Hartman <gregkh@linuxfoundation.org>

diff --git a/drivers/misc/cxl/cxllib.c b/drivers/misc/cxl/cxllib.c
index 5a3f91255258..258c43a95ac3 100644
--- a/drivers/misc/cxl/cxllib.c
+++ b/drivers/misc/cxl/cxllib.c
@@ -1,10 +1,6 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
 /*
  * Copyright 2017 IBM Corp.
- *
- * This program is free software; you can redistribute it and/or
- * modify it under the terms of the GNU General Public License
- * as published by the Free Software Foundation; either version
- * 2 of the License, or (at your option) any later version.
  */
 
 #include <linux/hugetlb.h>

commit 2bf1071a8d50928a4ae366bb3108833166c2b70c
Author: Nicholas Piggin <npiggin@gmail.com>
Date:   Thu Jul 5 18:47:00 2018 +1000

    powerpc/64s: Remove POWER9 DD1 support
    
    POWER9 DD1 was never a product. It is no longer supported by upstream
    firmware, and it is not effectively supported in Linux due to lack of
    testing.
    
    Signed-off-by: Nicholas Piggin <npiggin@gmail.com>
    Reviewed-by: Michael Ellerman <mpe@ellerman.id.au>
    [mpe: Remove arch_make_huge_pte() entirely]
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/drivers/misc/cxl/cxllib.c b/drivers/misc/cxl/cxllib.c
index 0bc7c31cf739..5a3f91255258 100644
--- a/drivers/misc/cxl/cxllib.c
+++ b/drivers/misc/cxl/cxllib.c
@@ -102,10 +102,6 @@ int cxllib_get_xsl_config(struct pci_dev *dev, struct cxllib_xsl_config *cfg)
 	rc = cxl_get_xsl9_dsnctl(dev, capp_unit_id, &cfg->dsnctl);
 	if (rc)
 		return rc;
-	if (cpu_has_feature(CPU_FTR_POWER9_DD1)) {
-		/* workaround for DD1 - nbwind = capiind */
-		cfg->dsnctl |= ((u64)0x02 << (63-47));
-	}
 
 	cfg->version  = CXL_XSL_CONFIG_CURRENT_VERSION;
 	cfg->log_bar_size = CXL_CAPI_WINDOW_LOG_SIZE;

commit ad7b4e8022b9864c075fe71e1328b1d25cad82f6
Author: Frederic Barrat <fbarrat@linux.vnet.ibm.com>
Date:   Tue Apr 3 15:54:02 2018 +0200

    cxl: Fix possible deadlock when processing page faults from cxllib
    
    cxllib_handle_fault() is called by an external driver when it needs to
    have the host resolve page faults for a buffer. The buffer can cover
    several pages and VMAs. The function iterates over all the pages used
    by the buffer, based on the page size of the VMA.
    
    To ensure some stability while processing the faults, the thread T1
    grabs the mm->mmap_sem semaphore with read access (R1). However, when
    processing a page fault for a single page, one of the underlying
    functions, copro_handle_mm_fault(), also grabs the same semaphore with
    read access (R2). So the thread T1 takes the semaphore twice.
    
    If another thread T2 tries to access the semaphore in write mode W1
    (say, because it wants to allocate memory and calls 'brk'), then that
    thread T2 will have to wait because there's a reader (R1). If the
    thread T1 is processing a new page at that time, it won't get an
    automatic grant at R2, because there's now a writer thread
    waiting (T2). And we have a deadlock.
    
    The timeline is:
    1. thread T1 owns the semaphore with read access R1
    2. thread T2 requests write access W1 and waits
    3. thread T1 requests read access R2 and waits
    
    The fix is for the thread T1 to release the semaphore R1 once it got
    the information it needs from the current VMA. The address space/VMAs
    could evolve while T1 iterates over the full buffer, but in the
    unlikely case where T1 misses a page, the external driver will raise a
    new page fault when retrying the memory access.
    
    Fixes: 3ced8d730063 ("cxl: Export library to support IBM XSL")
    Cc: stable@vger.kernel.org # 4.13+
    Signed-off-by: Frederic Barrat <fbarrat@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/drivers/misc/cxl/cxllib.c b/drivers/misc/cxl/cxllib.c
index bea1eb004b49..0bc7c31cf739 100644
--- a/drivers/misc/cxl/cxllib.c
+++ b/drivers/misc/cxl/cxllib.c
@@ -208,49 +208,74 @@ int cxllib_get_PE_attributes(struct task_struct *task,
 }
 EXPORT_SYMBOL_GPL(cxllib_get_PE_attributes);
 
-int cxllib_handle_fault(struct mm_struct *mm, u64 addr, u64 size, u64 flags)
+static int get_vma_info(struct mm_struct *mm, u64 addr,
+			u64 *vma_start, u64 *vma_end,
+			unsigned long *page_size)
 {
-	int rc;
-	u64 dar;
 	struct vm_area_struct *vma = NULL;
-	unsigned long page_size;
-
-	if (mm == NULL)
-		return -EFAULT;
+	int rc = 0;
 
 	down_read(&mm->mmap_sem);
 
 	vma = find_vma(mm, addr);
 	if (!vma) {
-		pr_err("Can't find vma for addr %016llx\n", addr);
 		rc = -EFAULT;
 		goto out;
 	}
-	/* get the size of the pages allocated */
-	page_size = vma_kernel_pagesize(vma);
-
-	for (dar = (addr & ~(page_size - 1)); dar < (addr + size); dar += page_size) {
-		if (dar < vma->vm_start || dar >= vma->vm_end) {
-			vma = find_vma(mm, addr);
-			if (!vma) {
-				pr_err("Can't find vma for addr %016llx\n", addr);
-				rc = -EFAULT;
-				goto out;
-			}
-			/* get the size of the pages allocated */
-			page_size = vma_kernel_pagesize(vma);
+	*page_size = vma_kernel_pagesize(vma);
+	*vma_start = vma->vm_start;
+	*vma_end = vma->vm_end;
+out:
+	up_read(&mm->mmap_sem);
+	return rc;
+}
+
+int cxllib_handle_fault(struct mm_struct *mm, u64 addr, u64 size, u64 flags)
+{
+	int rc;
+	u64 dar, vma_start, vma_end;
+	unsigned long page_size;
+
+	if (mm == NULL)
+		return -EFAULT;
+
+	/*
+	 * The buffer we have to process can extend over several pages
+	 * and may also cover several VMAs.
+	 * We iterate over all the pages. The page size could vary
+	 * between VMAs.
+	 */
+	rc = get_vma_info(mm, addr, &vma_start, &vma_end, &page_size);
+	if (rc)
+		return rc;
+
+	for (dar = (addr & ~(page_size - 1)); dar < (addr + size);
+	     dar += page_size) {
+		if (dar < vma_start || dar >= vma_end) {
+			/*
+			 * We don't hold the mm->mmap_sem semaphore
+			 * while iterating, since the semaphore is
+			 * required by one of the lower-level page
+			 * fault processing functions and it could
+			 * create a deadlock.
+			 *
+			 * It means the VMAs can be altered between 2
+			 * loop iterations and we could theoretically
+			 * miss a page (however unlikely). But that's
+			 * not really a problem, as the driver will
+			 * retry access, get another page fault on the
+			 * missing page and call us again.
+			 */
+			rc = get_vma_info(mm, dar, &vma_start, &vma_end,
+					&page_size);
+			if (rc)
+				return rc;
 		}
 
 		rc = cxl_handle_mm_fault(mm, flags, dar);
-		if (rc) {
-			pr_err("cxl_handle_mm_fault failed %d", rc);
-			rc = -EFAULT;
-			goto out;
-		}
+		if (rc)
+			return -EFAULT;
 	}
-	rc = 0;
-out:
-	up_read(&mm->mmap_sem);
-	return rc;
+	return 0;
 }
 EXPORT_SYMBOL_GPL(cxllib_handle_fault);

commit 9dbcbfa1fe0c3b556e889ea213a73eb80d74307b
Author: Philippe Bergheaud <felix@linux.vnet.ibm.com>
Date:   Fri Mar 2 10:56:12 2018 +0100

    cxl: read PHB indications from the device tree
    
    Configure the P9 XSL_DSNCTL register with PHB indications found
    in the device tree, or else use legacy hard-coded values.
    
    Signed-off-by: Philippe Bergheaud <felix@linux.vnet.ibm.com>
    Reviewed-by: Frederic Barrat <fbarrat@linux.vnet.ibm.com>
    Reviewed-by: Christophe Lombard <clombard@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/drivers/misc/cxl/cxllib.c b/drivers/misc/cxl/cxllib.c
index 30ccba436b3b..bea1eb004b49 100644
--- a/drivers/misc/cxl/cxllib.c
+++ b/drivers/misc/cxl/cxllib.c
@@ -99,7 +99,7 @@ int cxllib_get_xsl_config(struct pci_dev *dev, struct cxllib_xsl_config *cfg)
 	if (rc)
 		return rc;
 
-	rc = cxl_get_xsl9_dsnctl(capp_unit_id, &cfg->dsnctl);
+	rc = cxl_get_xsl9_dsnctl(dev, capp_unit_id, &cfg->dsnctl);
 	if (rc)
 		return rc;
 	if (cpu_has_feature(CPU_FTR_POWER9_DD1)) {

commit b1db551324f72fa14ad82ca31237a7ed418104df
Author: Christophe Lombard <clombard@linux.vnet.ibm.com>
Date:   Thu Jan 11 09:55:25 2018 +0100

    cxl: Add support for ASB_Notify on POWER9
    
    The POWER9 core supports a new feature: ASB_Notify which requires the
    support of the Special Purpose Register: TIDR.
    
    The ASB_Notify command, generated by the AFU, will attempt to
    wake-up the host thread identified by the particular LPID:PID:TID.
    
    This patch assign a unique TIDR (thread id) for the current thread which
    will be used in the process element entry.
    
    Signed-off-by: Christophe Lombard <clombard@linux.vnet.ibm.com>
    Reviewed-by: Philippe Bergheaud <felix@linux.vnet.ibm.com>
    Acked-by: Frederic Barrat <fbarrat@linux.vnet.ibm.com>
    Reviewed-by: Vaibhav Jain <vaibhav@linux.vnet.ibm.com>
    Acked-by: Andrew Donnellan <andrew.donnellan@au1.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/drivers/misc/cxl/cxllib.c b/drivers/misc/cxl/cxllib.c
index dc9bc1807fdf..30ccba436b3b 100644
--- a/drivers/misc/cxl/cxllib.c
+++ b/drivers/misc/cxl/cxllib.c
@@ -199,10 +199,11 @@ int cxllib_get_PE_attributes(struct task_struct *task,
 		 */
 		attr->pid = mm->context.id;
 		mmput(mm);
+		attr->tid = task->thread.tidr;
 	} else {
 		attr->pid = 0;
+		attr->tid = 0;
 	}
-	attr->tid = 0;
 	return 0;
 }
 EXPORT_SYMBOL_GPL(cxllib_get_PE_attributes);

commit 4fc0870d7e462fe3b86e0f938ae75ce884728c7d
Author: Christophe Lombard <clombard@linux.vnet.ibm.com>
Date:   Tue Sep 26 10:15:21 2017 +0200

    cxl: Fix memory page not handled
    
    The in-kernel 'library' API can be called by drivers to help
    interaction with an IBM XSL on a POWER9 system.
    
    The cxllib_handle_fault() API is used to handle memory fault. All memory
    pages of the specified buffer have to be handled but under certain
    conditions,the last page may not be touched, and the address the
    adapter is trying to access is never sent to the kernel for resolution.
    
    This patch reworks start address of the loop with an address aligned on
    the page size. In this context, the last page is not missed.
    
    Signed-off-by: Christophe Lombard <clombard@linux.vnet.ibm.com>
    Acked-by: Frederic Barrat <fbarrat@linux.vnet.ibm.com>
    Acked-by: Andrew Donnellan <andrew.donnellan@au1.ibm.com>
    
    Fixes: 3ced8d730063 ("cxl: Export library to support IBM XSL");
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/drivers/misc/cxl/cxllib.c b/drivers/misc/cxl/cxllib.c
index 5dba23ca2e5f..dc9bc1807fdf 100644
--- a/drivers/misc/cxl/cxllib.c
+++ b/drivers/misc/cxl/cxllib.c
@@ -219,8 +219,17 @@ int cxllib_handle_fault(struct mm_struct *mm, u64 addr, u64 size, u64 flags)
 
 	down_read(&mm->mmap_sem);
 
-	for (dar = addr; dar < addr + size; dar += page_size) {
-		if (!vma || dar < vma->vm_start || dar > vma->vm_end) {
+	vma = find_vma(mm, addr);
+	if (!vma) {
+		pr_err("Can't find vma for addr %016llx\n", addr);
+		rc = -EFAULT;
+		goto out;
+	}
+	/* get the size of the pages allocated */
+	page_size = vma_kernel_pagesize(vma);
+
+	for (dar = (addr & ~(page_size - 1)); dar < (addr + size); dar += page_size) {
+		if (dar < vma->vm_start || dar >= vma->vm_end) {
 			vma = find_vma(mm, addr);
 			if (!vma) {
 				pr_err("Can't find vma for addr %016llx\n", addr);

commit 3ced8d73006321bd2a0412fa0ff4b065a02e7514
Author: Christophe Lombard <clombard@linux.vnet.ibm.com>
Date:   Thu Jun 22 15:07:27 2017 +0200

    cxl: Export library to support IBM XSL
    
    This patch exports a in-kernel 'library' API which can be called by
    other drivers to help interacting with an IBM XSL on a POWER9 system.
    
    The XSL (Translation Service Layer) is a stripped down version of the
    PSL (Power Service Layer) used in some cards such as the Mellanox CX5.
    Like the PSL, it implements the CAIA architecture, but has a number
    of differences, mostly in it's implementation dependent registers.
    
    The XSL also uses a special DMA cxl mode, which uses a slightly
    different init sequence for the CAPP and PHB.
    
    Signed-off-by: Andrew Donnellan <andrew.donnellan@au1.ibm.com>
    Signed-off-by: Christophe Lombard <clombard@linux.vnet.ibm.com>
    Acked-by: Frederic Barrat <fbarrat@linux.vnet.ibm.com>
    Signed-off-by: Michael Ellerman <mpe@ellerman.id.au>

diff --git a/drivers/misc/cxl/cxllib.c b/drivers/misc/cxl/cxllib.c
new file mode 100644
index 000000000000..5dba23ca2e5f
--- /dev/null
+++ b/drivers/misc/cxl/cxllib.c
@@ -0,0 +1,246 @@
+/*
+ * Copyright 2017 IBM Corp.
+ *
+ * This program is free software; you can redistribute it and/or
+ * modify it under the terms of the GNU General Public License
+ * as published by the Free Software Foundation; either version
+ * 2 of the License, or (at your option) any later version.
+ */
+
+#include <linux/hugetlb.h>
+#include <linux/sched/mm.h>
+#include <asm/pnv-pci.h>
+#include <misc/cxllib.h>
+
+#include "cxl.h"
+
+#define CXL_INVALID_DRA                 ~0ull
+#define CXL_DUMMY_READ_SIZE             128
+#define CXL_DUMMY_READ_ALIGN            8
+#define CXL_CAPI_WINDOW_START           0x2000000000000ull
+#define CXL_CAPI_WINDOW_LOG_SIZE        48
+#define CXL_XSL_CONFIG_CURRENT_VERSION  CXL_XSL_CONFIG_VERSION1
+
+
+bool cxllib_slot_is_supported(struct pci_dev *dev, unsigned long flags)
+{
+	int rc;
+	u32 phb_index;
+	u64 chip_id, capp_unit_id;
+
+	/* No flags currently supported */
+	if (flags)
+		return false;
+
+	if (!cpu_has_feature(CPU_FTR_HVMODE))
+		return false;
+
+	if (!cxl_is_power9())
+		return false;
+
+	if (cxl_slot_is_switched(dev))
+		return false;
+
+	/* on p9, some pci slots are not connected to a CAPP unit */
+	rc = cxl_calc_capp_routing(dev, &chip_id, &phb_index, &capp_unit_id);
+	if (rc)
+		return false;
+
+	return true;
+}
+EXPORT_SYMBOL_GPL(cxllib_slot_is_supported);
+
+static DEFINE_MUTEX(dra_mutex);
+static u64 dummy_read_addr = CXL_INVALID_DRA;
+
+static int allocate_dummy_read_buf(void)
+{
+	u64 buf, vaddr;
+	size_t buf_size;
+
+	/*
+	 * Dummy read buffer is 128-byte long, aligned on a
+	 * 256-byte boundary and we need the physical address.
+	 */
+	buf_size = CXL_DUMMY_READ_SIZE + (1ull << CXL_DUMMY_READ_ALIGN);
+	buf = (u64) kzalloc(buf_size, GFP_KERNEL);
+	if (!buf)
+		return -ENOMEM;
+
+	vaddr = (buf + (1ull << CXL_DUMMY_READ_ALIGN) - 1) &
+					(~0ull << CXL_DUMMY_READ_ALIGN);
+
+	WARN((vaddr + CXL_DUMMY_READ_SIZE) > (buf + buf_size),
+		"Dummy read buffer alignment issue");
+	dummy_read_addr = virt_to_phys((void *) vaddr);
+	return 0;
+}
+
+int cxllib_get_xsl_config(struct pci_dev *dev, struct cxllib_xsl_config *cfg)
+{
+	int rc;
+	u32 phb_index;
+	u64 chip_id, capp_unit_id;
+
+	if (!cpu_has_feature(CPU_FTR_HVMODE))
+		return -EINVAL;
+
+	mutex_lock(&dra_mutex);
+	if (dummy_read_addr == CXL_INVALID_DRA) {
+		rc = allocate_dummy_read_buf();
+		if (rc) {
+			mutex_unlock(&dra_mutex);
+			return rc;
+		}
+	}
+	mutex_unlock(&dra_mutex);
+
+	rc = cxl_calc_capp_routing(dev, &chip_id, &phb_index, &capp_unit_id);
+	if (rc)
+		return rc;
+
+	rc = cxl_get_xsl9_dsnctl(capp_unit_id, &cfg->dsnctl);
+	if (rc)
+		return rc;
+	if (cpu_has_feature(CPU_FTR_POWER9_DD1)) {
+		/* workaround for DD1 - nbwind = capiind */
+		cfg->dsnctl |= ((u64)0x02 << (63-47));
+	}
+
+	cfg->version  = CXL_XSL_CONFIG_CURRENT_VERSION;
+	cfg->log_bar_size = CXL_CAPI_WINDOW_LOG_SIZE;
+	cfg->bar_addr = CXL_CAPI_WINDOW_START;
+	cfg->dra = dummy_read_addr;
+	return 0;
+}
+EXPORT_SYMBOL_GPL(cxllib_get_xsl_config);
+
+int cxllib_switch_phb_mode(struct pci_dev *dev, enum cxllib_mode mode,
+			unsigned long flags)
+{
+	int rc = 0;
+
+	if (!cpu_has_feature(CPU_FTR_HVMODE))
+		return -EINVAL;
+
+	switch (mode) {
+	case CXL_MODE_PCI:
+		/*
+		 * We currently don't support going back to PCI mode
+		 * However, we'll turn the invalidations off, so that
+		 * the firmware doesn't have to ack them and can do
+		 * things like reset, etc.. with no worries.
+		 * So always return EPERM (can't go back to PCI) or
+		 * EBUSY if we couldn't even turn off snooping
+		 */
+		rc = pnv_phb_to_cxl_mode(dev, OPAL_PHB_CAPI_MODE_SNOOP_OFF);
+		if (rc)
+			rc = -EBUSY;
+		else
+			rc = -EPERM;
+		break;
+	case CXL_MODE_CXL:
+		/* DMA only supported on TVT1 for the time being */
+		if (flags != CXL_MODE_DMA_TVT1)
+			return -EINVAL;
+		rc = pnv_phb_to_cxl_mode(dev, OPAL_PHB_CAPI_MODE_DMA_TVT1);
+		if (rc)
+			return rc;
+		rc = pnv_phb_to_cxl_mode(dev, OPAL_PHB_CAPI_MODE_SNOOP_ON);
+		break;
+	default:
+		rc = -EINVAL;
+	}
+	return rc;
+}
+EXPORT_SYMBOL_GPL(cxllib_switch_phb_mode);
+
+/*
+ * When switching the PHB to capi mode, the TVT#1 entry for
+ * the Partitionable Endpoint is set in bypass mode, like
+ * in PCI mode.
+ * Configure the device dma to use TVT#1, which is done
+ * by calling dma_set_mask() with a mask large enough.
+ */
+int cxllib_set_device_dma(struct pci_dev *dev, unsigned long flags)
+{
+	int rc;
+
+	if (flags)
+		return -EINVAL;
+
+	rc = dma_set_mask(&dev->dev, DMA_BIT_MASK(64));
+	return rc;
+}
+EXPORT_SYMBOL_GPL(cxllib_set_device_dma);
+
+int cxllib_get_PE_attributes(struct task_struct *task,
+			     unsigned long translation_mode,
+			     struct cxllib_pe_attributes *attr)
+{
+	struct mm_struct *mm = NULL;
+
+	if (translation_mode != CXL_TRANSLATED_MODE &&
+		translation_mode != CXL_REAL_MODE)
+		return -EINVAL;
+
+	attr->sr = cxl_calculate_sr(false,
+				task == NULL,
+				translation_mode == CXL_REAL_MODE,
+				true);
+	attr->lpid = mfspr(SPRN_LPID);
+	if (task) {
+		mm = get_task_mm(task);
+		if (mm == NULL)
+			return -EINVAL;
+		/*
+		 * Caller is keeping a reference on mm_users for as long
+		 * as XSL uses the memory context
+		 */
+		attr->pid = mm->context.id;
+		mmput(mm);
+	} else {
+		attr->pid = 0;
+	}
+	attr->tid = 0;
+	return 0;
+}
+EXPORT_SYMBOL_GPL(cxllib_get_PE_attributes);
+
+int cxllib_handle_fault(struct mm_struct *mm, u64 addr, u64 size, u64 flags)
+{
+	int rc;
+	u64 dar;
+	struct vm_area_struct *vma = NULL;
+	unsigned long page_size;
+
+	if (mm == NULL)
+		return -EFAULT;
+
+	down_read(&mm->mmap_sem);
+
+	for (dar = addr; dar < addr + size; dar += page_size) {
+		if (!vma || dar < vma->vm_start || dar > vma->vm_end) {
+			vma = find_vma(mm, addr);
+			if (!vma) {
+				pr_err("Can't find vma for addr %016llx\n", addr);
+				rc = -EFAULT;
+				goto out;
+			}
+			/* get the size of the pages allocated */
+			page_size = vma_kernel_pagesize(vma);
+		}
+
+		rc = cxl_handle_mm_fault(mm, flags, dar);
+		if (rc) {
+			pr_err("cxl_handle_mm_fault failed %d", rc);
+			rc = -EFAULT;
+			goto out;
+		}
+	}
+	rc = 0;
+out:
+	up_read(&mm->mmap_sem);
+	return rc;
+}
+EXPORT_SYMBOL_GPL(cxllib_handle_fault);
